[
  {
    "objectID": "quant-fin.html#about-the-course",
    "href": "quant-fin.html#about-the-course",
    "title": "Practical Applications in Quantitative Finance",
    "section": "About the course",
    "text": "About the course\nThis is a hands-on, practical course on Quantitative Finance with applications using R and Python, two of the most widely used open-source software for data analysis. This course aims to attract undergraduate students that are aiming to translate theoretical concepts learned on the core finance courses at FGV-EAESP to practical applications that can guide decision making.\nThe course will be structured in topics that are of interest to Finance practitioners, aiming to include, but not limited to: collecting and organizing financial data, equity valuation, sensitivity analysis and simulation, portfolio optimization, and backtesting. As a final evaluation, students are expected to deliver a capstone data application project showcasing at least one of the topics covered in the course. All applications will be hosted online to enable students to use the capstone project as a showcase of their acquired skills. The course will also host guest presentations from leading industry practitioners and software developers focused on financial applications using open-source languages, where students can interact, ask questions, and get to know more about the possibilities of applying programming, data science, and data analysis skills in the financial industry.\n\n\n\n\n\n\n1. Getting started\n\n\n\nTo make things easier, ensure to install these packages in your computer and load it at the beginning of every session - I’ll make sure to update this list whenever needed throughout the sessions:\n\n\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\",\"highcharter\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\nlapply(packages, library, character.only = TRUE)\n\n\n\n\n\n\n\n2. Using Quarto\n\n\n\nIn this course, you’ll be assigned with three data cases, where you’ll need to manipulate code and write your insights altogether. I want to encourage you to give Quarto a try.\nTo install Quarto, follow this link and choose your Operating System. RStudio will automatically locate it and make it as an option whenever creating a new file with Ctrl+N. Why you should give Quarto a try:\n\nIt has multi-language support (Python, R, Julia, JavaScript), parses equations and mathematical notations via pandoc, and integrates seamlessly with GitHub\nAdvanced document formatting and output options: you can choose pdf, html, docx, or even a reveal.js presentation (like the ones from this course!)\nIt is easy, intuitive, and lets you focus on the most important aspect of your work\n\n\n\n\n\n\n\n\n\n3. Replications\n\n\n\nAlong with the slides, each lecture will also contain a replication file, in .qmd format, containing a thorough discussion for all examples that have been showcased. This file, that will also be posted on eClass®, can be downloaded and replicated on your side.\nTo do that, download the file, open it up in RStudio, and render the Quarto document using the Render button (shortcut: Ctrl+Shift+K).\nYou can find the permanent links to the replication files (.qmd) below:\n\nBridging Finance with Programming - access here\nCollecting, Organizing, and Manipulating Financial Data - access here\nManipulating Time Series Data - access here\nData Visualization - access here"
  },
  {
    "objectID": "quant-fin-replications/L2/L2-Replication.html",
    "href": "quant-fin-replications/L2/L2-Replication.html",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nIn this lecture, we will be working with daily stock price data from the Magnificent Seven (AAPL, GOOG, MSFT, NVDA, TSLA, AMZN, and META). I have already downloaded the data for you using the tidyquant package, which allows us to pull stock price data from multiple securities in a convenient format. You can hit the Download button to get a grasp on how the data looks like or download it directly on eClass® - file name: M7.csv. Before you start, make sure to follow the instructions from our previous replication to set up your working directory correctly."
  },
  {
    "objectID": "quant-fin-replications/L2/L2-Replication.html#about-this-document",
    "href": "quant-fin-replications/L2/L2-Replication.html#about-this-document",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nIn this lecture, we will be working with daily stock price data from the Magnificent Seven (AAPL, GOOG, MSFT, NVDA, TSLA, AMZN, and META). I have already downloaded the data for you using the tidyquant package, which allows us to pull stock price data from multiple securities in a convenient format. You can hit the Download button to get a grasp on how the data looks like or download it directly on eClass® - file name: M7.csv. Before you start, make sure to follow the instructions from our previous replication to set up your working directory correctly."
  },
  {
    "objectID": "quant-fin-replications/L2/L2-Replication.html#loading-packages",
    "href": "quant-fin-replications/L2/L2-Replication.html#loading-packages",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "Loading packages",
    "text": "Loading packages\nAs we get started, we will be loading all packages referred in our official website.\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\", \"glue\",\"scales\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nNote that you could easily get around this by installing and loading all necessary packages using a more simple syntax:\n\n#Install if not already available - I have commented these lines so that R does not attempt to install it everytime\n  #install.packages('tidyverse')\n  #install.packages('tidyquant')\n  #install.packages('glue')\n  #install.packages('scales')\n  #install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)"
  },
  {
    "objectID": "quant-fin-replications/L2/L2-Replication.html#using-dplyr-the-data-manipulation-package-in-the-tidyverse",
    "href": "quant-fin-replications/L2/L2-Replication.html#using-dplyr-the-data-manipulation-package-in-the-tidyverse",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "Using dplyr, the data manipulation package in the tidyverse",
    "text": "Using dplyr, the data manipulation package in the tidyverse\nThe dplyr package is one of the core packages in the tidyverse and is designed for efficient and readable data manipulation. It provides a set of functions (also called “verbs”) that make working with data frames (or tibbles) intuitive and expressive. Key Features:\n\nFilter rows: filter()\nSelect columns: select()\nMutate (create new columns): mutate()\nSummarize data: summarize()\nGroup operations: group_by()\nJoin tables: left_join(), right_join(), inner_join(), full_join()\n\nTo get started with our exercises, we will refer to the M7.csv file that has been provided. After setting the current directory of the file, we load the data using the read.csv() function:\n\n#Apply function to the data\nM7=read.csv('M7.csv')\n\n#Show the first 10 observations\nhead(M7)\n\n  symbol       date    open    high     low   close    volume adjusted\n1   AAPL 2020-01-02 74.0600 75.1500 73.7975 75.0875 135480400 72.79604\n2   AAPL 2020-01-03 74.2875 75.1450 74.1250 74.3575 146322800 72.08828\n3   AAPL 2020-01-06 73.4475 74.9900 73.1875 74.9500 118387200 72.66272\n4   AAPL 2020-01-07 74.9600 75.2250 74.3700 74.5975 108872000 72.32098\n5   AAPL 2020-01-08 74.2900 76.1100 74.2900 75.7975 132079200 73.48434\n6   AAPL 2020-01-09 76.8100 77.6075 76.5500 77.4075 170108400 75.04523"
  },
  {
    "objectID": "quant-fin-replications/L2/L2-Replication.html#the-mutate-function",
    "href": "quant-fin-replications/L2/L2-Replication.html#the-mutate-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "1. The mutate() function",
    "text": "1. The mutate() function\nThe mutate() function adds new variables that are functions of existing variables:\n\nmutate(.data, #The object you are performing the calculations \n       new_variable_1 = var1 * 2, #Can use basic operations...\n       new_variable_2 = median(var2), #Or predefined functions)\n       variable_3 = as.character(var3) #And can be used to modify existing variables)\n       ) \n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nIt sequentially creates the columns you asked for and place them to the right of your data.frame (or tibble)\nYou can use any function, predefined or custom, and apply it to mutate()\nIt can also modify any columns you want (if the name is the same as an existing column)\n\nExercise: use columns high and low and create a new column, mid, defined as the average between daily high and low prices.\n\n#Apply function to the data\nM7=mutate(M7, mid= (high+low)/2)\n\n#Show the first 10 observations\nhead(M7)\n\n  symbol       date    open    high     low   close    volume adjusted      mid\n1   AAPL 2020-01-02 74.0600 75.1500 73.7975 75.0875 135480400 72.79604 74.47375\n2   AAPL 2020-01-03 74.2875 75.1450 74.1250 74.3575 146322800 72.08828 74.63500\n3   AAPL 2020-01-06 73.4475 74.9900 73.1875 74.9500 118387200 72.66272 74.08875\n4   AAPL 2020-01-07 74.9600 75.2250 74.3700 74.5975 108872000 72.32098 74.79750\n5   AAPL 2020-01-08 74.2900 76.1100 74.2900 75.7975 132079200 73.48434 75.20000\n6   AAPL 2020-01-09 76.8100 77.6075 76.5500 77.4075 170108400 75.04523 77.07875"
  },
  {
    "objectID": "quant-fin-replications/L2/L2-Replication.html#the-select-function",
    "href": "quant-fin-replications/L2/L2-Replication.html#the-select-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "2. The select() function",
    "text": "2. The select() function\nThe select() function select (and optionally rename) variables in a data frame, using a concise mini-language that makes it easy to refer to variables based on their name (e.g. a:f selects all columns from a on the left to f on the right) or type (e.g. where(is.numeric) selects all numeric columns):\n\nselect(.data, #The object which you are performing the operations \n       variable_3, #Can reorder columns\n       variable_1, \n       variable_2:variable_4, #Matches position patterns \n       where(is.numeric) #Can select all columns that match a given pattern\n       ) \n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd select only the columns you’ve asked for\nYou can also use select(.data,-variable) to remove a variable\nIt keeps the structure of the data.frame intact - no rows are affected\n\nThe select() function also comes with a handy companion of selectors, which are functions that help you cherry pick columns in a concise way, rather than hardcoding them altogether:\n\n: for selecting a range of consecutive variables.\nstarts_with() starts with a string\nends_with() ends with a string\ncontains() contains a string\nmatches()matches a regular expression.\nwhere()a function to all variables and selects those for which the function returns TRUE\n\nExercise: Select only the symbol, date, volume, and adjusted, in that order.\n\n#Apply function to the data\nM7=select(M7,symbol,date,volume,adjusted)\n\n#Show the first 10 observations\nhead(M7)\n\n  symbol       date    volume adjusted\n1   AAPL 2020-01-02 135480400 72.79604\n2   AAPL 2020-01-03 146322800 72.08828\n3   AAPL 2020-01-06 118387200 72.66272\n4   AAPL 2020-01-07 108872000 72.32098\n5   AAPL 2020-01-08 132079200 73.48434\n6   AAPL 2020-01-09 170108400 75.04523"
  },
  {
    "objectID": "quant-fin-replications/L2/L2-Replication.html#the-filter-function",
    "href": "quant-fin-replications/L2/L2-Replication.html#the-filter-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "3. The filter() function",
    "text": "3. The filter() function\nThe filter() function is used to subset a data frame, retaining all rows that satisfy your conditions. To be retained, the row must produce a value of TRUE for all conditions:\n\nfilter(.data, #The object which you are performing the operations\n       variable_1 &gt;10, #Simple arithmetic operators\n       variable_2 %in% c('AAPL','MSFT','FORD'), #Pattern search\n       !(variable_3 %in% c('Boston','Mass','Silicon Valley')), #Negate pattern search\n       variable_4 &gt;=10 & variable_3&lt;= 4 | is.na(variable_4) #IF and OR conditions\n       ) \n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd filter the rows based on the conditions outlined\nYou can use any function, predefined or custom, and apply it to filter()\nIt returns a subset of the whole object, keeping the columns and the data structure intact\n\nExercise: filter for observations that occurred in 2025, only. You can use the year() function with the date variable to retrieve the year.\n\n#Apply function to the data\nM7=filter(M7,year(date)==2025)\n\n#Show the first 10 observations\nhead(M7)\n\n  symbol       date   volume adjusted\n1   AAPL 2025-01-02 55740700   243.85\n2   AAPL 2025-01-03 40244100   243.36\n3   AAPL 2025-01-06 45045600   245.00\n4   AAPL 2025-01-07 40856000   242.21\n5   AAPL 2025-01-08 37628900   242.70\n6   AAPL 2025-01-10 61710900   236.85"
  },
  {
    "objectID": "quant-fin-replications/L2/L2-Replication.html#the-arrange-function",
    "href": "quant-fin-replications/L2/L2-Replication.html#the-arrange-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "3. The arrange() function",
    "text": "3. The arrange() function\nThe arrange() function reorders the rows of a data frame by the values of selected columns:\n\n#Some Options, always in the following format: the object you are rearranging + the reordering scheme\narrange(.data, variable1) #Ascending by variable_1\narrange(.data, variable1, variable_2) #Ascending by variable_1 and then variable_2\narrange(.data, variable2, variable_1) #Ascending by variable_2 and then variable 1\narrange(.data, variable1, desc(variable_2)) #Ascending by variable_1, and then descending by variable_2\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd reorders the rows of your data.frame (or tibble)\nThis can be useful for visualization, but also for applying position-dependent functions, like lag(), lead(), head(), and tail()\n\nExercise: arrange the dataset by descending date (newest to oldest) and symbol..\n\n#Apply function to the data\nM7=arrange(M7,desc(date),symbol)\n\n#Show the first 10 observations\nhead(M7)\n\n  symbol       date    volume adjusted\n1   AAPL 2025-01-29  45486100   239.36\n2   AMZN 2025-01-29  26091700   237.07\n3   GOOG 2025-01-29  12287800   197.18\n4   META 2025-01-29  21377800   676.49\n5   MSFT 2025-01-29  23581400   442.33\n6   NVDA 2025-01-29 467120600   123.70"
  },
  {
    "objectID": "quant-fin-replications/L2/L2-Replication.html#the-summarize-function",
    "href": "quant-fin-replications/L2/L2-Replication.html#the-summarize-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "4. The summarize() function",
    "text": "4. The summarize() function\nThe summarise() - or summarize() - function creates a new data frame. It returns one row for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarising all observations in the input. It will contain one column for each grouping variable and one column for each of the summary statistics that you have specified.\n\nsummarize(.data, #The object which you are performing the operations \n       new_variable_1 = mean(var1,na.rm=TRUE), #Average of var1, removing NA values\n       new_variable_2 = median(var2,na.rm=TRUE), #Median of var1, removing, NA values\n       new_variable_3 = n_distinct(var2) #Number of unique values of var2\n       ) \n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd reshapes the data.frame (or tibble) by the aggregation functions\nAs the name suggests, it is used to summarize a table\n\nExercise: summarize the dataset by creating an average column, defined as the average adjusted prices. You can use the mean() function to get the average. Use the option na.rm=TRUE inside the mean function to make sure that NA values are disregarded.\n\n#Apply function to the data\nSummary=summarize(M7,average=mean(adjusted,na.rm=TRUE))\n\n#Show the first observations\nhead(Summary)\n\n   average\n1 322.1803"
  },
  {
    "objectID": "quant-fin-replications/L2/L2-Replication.html#slice-and-dice-through-group_by",
    "href": "quant-fin-replications/L2/L2-Replication.html#slice-and-dice-through-group_by",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "5. Slice and dice through group_by",
    "text": "5. Slice and dice through group_by\nThe group_by() function takes an existing table and converts it into a grouped table where operations are performed “by group”. Using ungroup() removes grouping.\n\nData=group_by(Data,v1,v2,v3)\nData=summarize(avg=mean(x,na.rm=TRUE))\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd creates the avg variable taking the average of x within each tuple defined by the grouping variables (in this case, v1,v2, and v3 )\nIt returns a grouped dataframe, with the results of avg displayed for each unique combination of v1,v2, and v3\n\nThe group_by() function in R is part of the dplyr package and is used to create grouped data frames. It is commonly used in combination with summarize(), mutate(), and other dplyr functions to perform operations within groups.\n\nImportant\nAfter grouping, it’s often necessary to ungroup the data to prevent unintended behavior in subsequent operations:\n\nData=group_by(Data,v1,v2,v3)\nData=summarize(avg=mean(x,na.rm=TRUE))\nData=ungroup(Data)\n\n\nLet’s try the latest summarize() call again, but now grouping the data by symbol first:\n\n#Apply function to the data\nM7=group_by(M7,symbol)\nSummary=summarize(M7,average=mean(adjusted,na.rm=TRUE))\n\n#Show the first 10 observations\nhead(Summary,10)\n\n# A tibble: 7 × 2\n  symbol average\n  &lt;chr&gt;    &lt;dbl&gt;\n1 AAPL      234.\n2 AMZN      227.\n3 GOOG      196.\n4 META      625.\n5 MSFT      430.\n6 NVDA      137.\n7 TSLA      405."
  },
  {
    "objectID": "quant-fin-replications/L2/L2-Replication.html#pipe-your-way-through-the-code",
    "href": "quant-fin-replications/L2/L2-Replication.html#pipe-your-way-through-the-code",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "6. Pipe your way through the code %>%",
    "text": "6. Pipe your way through the code %&gt;%\nThe dplyr verbs, in isolation, are a great tool for data analysts, but what really makes them to shine is what glues them together. The pipe operator (%&gt;% or |&gt;) is a key feature of the magrittr package (included in the tidyverse) and is widely used in R, especially together with dplyr, for improving code readability and structuring data transformation workflows. Key benefits include:\n✅ Improved Readability – The sequence of transformations is clear.\n✅ No Need for Temporary Variables – Each step directly passes its result to the next function.\n✅ Avoids Nesting – No deeply nested function calls.\nThe pipe operator allows you to pass the result of one function as the first argument to the next function, making code more readable and eliminating the need for nested function calls. To show its functionality in action, in the code chunk below, both parts of the code produce the exact same result, but the latter, using the pipe operator, is much simpler to read:\n\n#Instead of \nData = read.csv('Data.csv') #Start with the data\nData = mutate(Data, new_var_1=var_1*10)#Mutate\nData = select(Data, var_1,var_2,new_var_1,where(is.numeric))#Select\nData = filter(Data, new_var_1&gt;5)#Filter\nData = arrange(Data, new_var_1,desc(var2))#Arrange\nData = summarize(Data, new_var=mean(new_var_1,na.rm=TRUE))#Summarize\n\n#Do\nData = read.csv('Data.csv')%&gt;% #Start with the data\n        mutate(new_var_1=var_1*10)%&gt;% #Mutate\n        select(var_1,var_2,new_var_1,where(is.numeric))%&gt;% #Select\n        filter(new_var_1&gt;5)%&gt;% #Filter\n        arrange(new_var_1,desc(var2))%&gt;% #Arrange\n        summarize(new_var=mean(new_var_1,na.rm=TRUE))#Summarize"
  },
  {
    "objectID": "quant-fin-replications/L2/L2-Replication.html#hands-on-exercise",
    "href": "quant-fin-replications/L2/L2-Replication.html#hands-on-exercise",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\nOn January \\(25^{th}\\), chinese startup DeepSeek disrupted the tech stock market as investors reassessed the likely future investment in Artificial Intelligence hardware. As part of your work as a buy-side analyst, you were asked to analyze how the Magnificent 7 performed after the DeepSeek. To this point, follow the instructions and answer to the following question: which stock suffered the most during January 2025?\n\nTo answer this question, you will be using all dplyr verbs you’ve practiced so far\nFurthermore, you will be also using some common base R and ther dplyr functions, like lag(), prod(), as.Date() and drop_na()\n\nThe expected result is a data.frame object that shows, for each symbol, the monthly return on January, 2025, ordered from lowest-to-highest.\n\n\n\n\n\n\nInstructions\n\n\n\nThe data, stored in M7.csv, can be loaded using read.csv('M7.csv'). You can download it using the link shown in Slide 4.\n\nSelect only the symbol, date, and adjusted columns, and arrange the dataset from oldest to newest\nMutate your date variable, making sure to read it as a Date object using as.Date()\nCreate a Year variable and filter only on observations happening in 2025. You can use the year() function to retrieve the year of a given Date column.\nGroup data by symbol\nCreate, for each different symbol, a Return variable that is defined as \\(P_{t+1}/P_{t}\\), where \\(t\\) refers to a date. You can use the lag() function for this\nYou will see that lag produces an NA whenever you try to lag the first observation. To make sure your data does not contain any NA, call drop_na()\nCreate, for each different symbol, a Cum_Return variable that is defined as the cumulative return. Compounded returns over time can be written as \\(\\small \\prod(1+R_t)=(1+R_1)\\times(1+R_2)\\times...\\times(1+R_t)\\). For this, you can use the prod() function.\nPick the latest observation from each symbol and arrange the table from lowest-to-highest return. The function slice_tail(n=x) retrieves the bottom x observations, whereas slice_head(n=y) retrieves the top y."
  },
  {
    "objectID": "quant-fin-replications/L2/L2-Replication.html#solution-walkthrough",
    "href": "quant-fin-replications/L2/L2-Replication.html#solution-walkthrough",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "Solution walkthrough",
    "text": "Solution walkthrough\n\n#Read the Data\nM7%&gt;%\n#Select only the columns of interest\nselect(symbol,date,adjusted)%&gt;%\n#Make sure date is read as a Date object\nmutate(date=as.Date(date))%&gt;%\n#Filter for observations happening in 2025\nfilter(year(date)==2025)%&gt;%\n#Arrange from chronological order\narrange(date)%&gt;%\n#Group by Symbol to perform the calculations\ngroup_by(symbol)%&gt;%\n#Create the return\nmutate(Return = adjusted/lag(adjusted,default = NA))%&gt;%\n#Remove NAs before doing the cumulative product\ndrop_na()%&gt;%\nmutate(Cum_Return = cumprod(Return)-1)%&gt;%\n#Select the latest observation from each symbol\nslice_tail(n=1)%&gt;%\n#Select symbol, date, and cumulative return\nselect(symbol,date,Cum_Return)%&gt;%\n#Arrange from lowest-to-highest\narrange(Cum_Return)\n\n# A tibble: 7 × 3\n# Groups:   symbol [7]\n  symbol date       Cum_Return\n  &lt;chr&gt;  &lt;date&gt;          &lt;dbl&gt;\n1 NVDA   2025-01-29    -0.106 \n2 AAPL   2025-01-29    -0.0184\n3 TSLA   2025-01-29     0.0259\n4 GOOG   2025-01-29     0.0344\n5 MSFT   2025-01-29     0.0567\n6 AMZN   2025-01-29     0.0765\n7 META   2025-01-29     0.129 \n\n\nThis code processes stock price data from M7 using the dplyr package. It calculates the cumulative return for each stock (symbol) in the year 2025, then selects the latest available observation per stock and sorts them from lowest to highest cumulative return.\n\nRead the Data. M7 is assumed to be a data frame or tibble containing stock data ready in your session. You can use read.csv() and store it in an R object. The pipe operator %&gt;%) is used to chain functions together.\nSelect Relevant Columns. Keeps only the relevant columns for the analysis:\n\n\nsymbol → The stock ticker\ndate → The trading date\nadjusted → The adjusted closing price (used for return calculations)\n\nMaking sure the select function is applied as one of the first adjustments can facilitate data wrangling as it shrinks the dataset for the upcoming operations.\n\nEnsure date is a Date object in your session. The code converts the date column to a Date object to enable time-based filtering and calculations, like year().\nFilter Data for 2025. The code uses year(date) == 2025 (from the lubridate package, loaded together with the tidyverse) to keep only data from 2025.\nSort Data in Chronological Order. The code ensures that stock prices are arranged earliest to latest for correct return calculations.\nGroups the dataset by stock (symbol). Using group_by() ensures that return calculations are performed for each stock separately\nCalculate Daily Returns. After the dataset is grouped, we use the mutate() function to create our return metric:\n\n\\[\nReturn=\\dfrac{P_{t}}{P_{t-1}}\n\\]\n\nUses lag(adjusted) to get the previous day’s adjusted price.\nThe first row in each group will have NA (because there’s no previous price)\n\nBecause of that, we also need a call to drop_na() to make sure that whenever we are multiplying these indices, we are not including NA values.\n\n\n\n\n\n\nImportant\n\n\n\nThe function cumprod(), which calculates the cumulative product of a series, multiplies values sequentially. However, if there are missing values (NA) in the sequence, cumprod() propagates NA to all subsequent values. This can corrupt the entire computation.\nFor example, ommitting the drop_na() step in the solution code would produce NA all over Cum_Return:\n\n#Read the Data\nM7%&gt;%\n#Select only the columns of interest\nselect(symbol,date,adjusted)%&gt;%\n#Make sure date is read as a Date object\nmutate(date=as.Date(date))%&gt;%\n#Filter for observations happening in 2025\nfilter(year(date)==2025)%&gt;%\n#Arrange from chronological order\narrange(date)%&gt;%\n#Group by Symbol to perform the calculations\ngroup_by(symbol)%&gt;%\n#Create the return\nmutate(Return = adjusted/lag(adjusted,default = NA))%&gt;%\n#Remove NAs before doing the cumulative product\nmutate(Cum_Return = cumprod(Return)-1)\n\n# A tibble: 126 × 5\n# Groups:   symbol [7]\n   symbol date       adjusted Return Cum_Return\n   &lt;chr&gt;  &lt;date&gt;        &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n 1 AAPL   2025-01-02     244. NA             NA\n 2 AMZN   2025-01-02     220. NA             NA\n 3 GOOG   2025-01-02     191. NA             NA\n 4 META   2025-01-02     599. NA             NA\n 5 MSFT   2025-01-02     419. NA             NA\n 6 NVDA   2025-01-02     138. NA             NA\n 7 TSLA   2025-01-02     379. NA             NA\n 8 AAPL   2025-01-03     243.  0.998         NA\n 9 AMZN   2025-01-03     224.  1.02          NA\n10 GOOG   2025-01-03     193.  1.01          NA\n# ℹ 116 more rows\n\n\n\n\n\nCalculate Cumulative Returns. With the series of daily returns in place cumulative return over time can be retrieved by compounding each individual return over time:\n\n\\[\n\\text{Cumulative Return}_{t=1\\rightarrow T}= (1+R_1)\\times(1+R_2)\\times(1+R_3)\\times...\\times(1+R_t)\\equiv\\prod_{t=1}^{T}(1+R_t)\n\\] To perform such calculations, the code uses cumprod(Return), which multiplies returns over time. In the end, we also need to subtract 1 to express it as a percentage return.\n\nSelect the Latest Observation Per Stock. The slice_tail() function keeps only the last row (i.e., the most recent date) for each stock. Note that this behavior is only possible because our data has been grouped by symbol in the subsequent steps. In an ungrouped case, slice_tail() would retrieve the latest observation considering the data as a whole - in this case, META cumulative returns.\nKeep Only Key Columns and Rearrange. After we’re done creating the relevant variables, we can use the select() function to keep only the columns that are of interest: symbol,date, and Cum_Return, and use the arrange() function to sort observations by ascending order of cumulative returns (i.e, lowest-to-highest)."
  },
  {
    "objectID": "quant-fin-replications/L2/L2-Replication.html#try-doing-some-edits-on-your-own",
    "href": "quant-fin-replications/L2/L2-Replication.html#try-doing-some-edits-on-your-own",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "Try doing some edits on your own!",
    "text": "Try doing some edits on your own!\nTry thinking about changes you could do to either improve code readability of the analysis. A couple of edits that can be made include, but are not limited, to:\n\nAdding more time periods to the analysis\nIncreasing the set of assets to include more tech firms other than the magnificent seven\nCalculate volatility metrics using var() or stdev() functions\n\nPlay around with these concepts to get familiar with all the data manipulation tools that come with dplyr!"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#outline",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#outline",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\nTidy Finance (Scheuch, Voigt, and Weiss 2023)\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\n\n\n\n\n\n\n\n\nCoding Replications\n\n\nFor coding replications, whenever applicable, please follow this page or hover on the specific slides with containing coding chunks.\n\nEnsure that you have your  session properly set-up according to the instructions outlined in the course webpage\nIn the webpage, you can also find a detailed discussion of the examples covered in this lecture"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#disclaimer",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#disclaimer",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\n\n\n\n\nDisclaimer\n\n\nThe information presented in this lecture is for educational and informational purposes only and should not be construed as investment advice. Nothing discussed constitutes a recommendation to buy, sell, or hold any financial instrument or security. Investment decisions should be made based on individual research and consultation with a qualified financial professional. The presenter assumes no responsibility for any financial decisions made based on this content.\nAll code used in this lecture is publicly available and is also shared on my GitHub page. Participants are encouraged to review, modify, and use the code for their own learning and research purposes. However, no guarantees are made regarding the accuracy, completeness, or suitability of the code for any specific application.\nFor any questions or concerns, please feel free to reach out via email at lucas.macoris@fgv.br"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#background",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#background",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Background",
    "text": "Background\n\nThe Capital Asset Pricing Model (CAPM) is a very practical, robust and straightforward implementation for modeling expected returns\nIt gets managers to think about risk in the correct way: instead of thinking about total risk, the CAPM shows us that we only the market risk (non-diversifiable) should be the concern\n\n\n\nThere are three simplifying assumptions around investor behavior that the CAPM establishes:\n\n\n\n\n\n\n\nThe Capital Asset Pricing Model (CAPM) Assumptions\n\n\n\n\nInvestors can buy and sell all securities at competitive market prices without incurring taxes or transactions costs can borrow and lend at the risk-free interest rate\nInvestors hold only efficient portfolios of traded securities\nInvestors have homogeneous expectations regarding the volatilities, correlations, and expected returns of securities\n\n\n\n\n\n\n\nQuestion: why these assumptions are important?"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#pricing-the-risk-premium-under-the-capm",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#pricing-the-risk-premium-under-the-capm",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Pricing the Risk Premium under the CAPM",
    "text": "Pricing the Risk Premium under the CAPM\n\nRecall that the expected return of any given asset \\(i\\) is given by:\n\n\\[\n\\small E[R_i]  =  R_f + \\beta_i^P  \\times (E[R_p] - R_f)\n\\]\n\nHow can we find \\(\\beta_i^P\\), the sensitivity of asset \\(i\\) returns to the efficient portfolio, \\(P\\)?\n\nTo identify the efficient portfolio (Markowitz 1952), we need to know the expected returns, volatilities, and correlations between all available investments!\nHowever, if the CAPM assumptions are valid, we can now identify the efficient portfolio: it is equal to the market portfolio!\n\nWhat does that mean for us in terms of determining expected equity returns? Until now, we were agnostic on what \\(P\\) was. Under the CAPM, we can change the subscript \\(P\\) to \\(M\\):\n\n\\[\n\\small E[R_i] =  R_f + \\beta_i^M  \\times (E[R_m] - R_f)\n\\]"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#capm-implication-1-the-capital-market-line-cml",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#capm-implication-1-the-capital-market-line-cml",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "CAPM Implication 1: the Capital Market Line (CML)",
    "text": "CAPM Implication 1: the Capital Market Line (CML)"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#the-cml-shows-no-clear-relationship-between-risk-and-return",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#the-cml-shows-no-clear-relationship-between-risk-and-return",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "The CML shows no clear relationship between risk and return…",
    "text": "The CML shows no clear relationship between risk and return…"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#capm-implication-2-the-security-market-line-sml-makes-the-relationship-clear-when-focusing-only-betam_i",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#capm-implication-2-the-security-market-line-sml-makes-the-relationship-clear-when-focusing-only-betam_i",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "CAPM Implication 2: the Security Market Line (SML) makes the relationship clear when focusing only (\\(\\beta^M_i\\))!",
    "text": "CAPM Implication 2: the Security Market Line (SML) makes the relationship clear when focusing only (\\(\\beta^M_i\\))!"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#hands-on-exercise",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#hands-on-exercise",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\nYou work as a buy-side analyst at Pierpoint Capital, focusing on the chemicals industry. You job is replicate the CAPM for a handful of securities from the Chemical (basic) industry and provide insights for the fund manager:\n\n\nWhich stocks, according to the CAPM, are undervalued and why?\nWhich stocks, according to the CAPM, are overvalued and why?\nIf the fund were to implement your strategy, what are the risks associated with?\n\n\n\n\n\n\n\nSpecific Instructions\n\n\n\nThe securities to be included in the analysis are: Dow (ticker: DOW), LyondellBasell (LYB), Perimeter (PRM), Flotek (FTK), Rayonier (RYAM), Albemarle (ALB), Celanese (CE),The Chemours (CC), Ginkgo Bioworks (DNA), and American Vanguard (AVD).\nCAPM estimation should be done at a weekly level using data from 2024"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#estimating-the-equity-cost-of-capital-in-practice",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#estimating-the-equity-cost-of-capital-in-practice",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Estimating the Equity Cost of Capital in practice",
    "text": "Estimating the Equity Cost of Capital in practice\n\nThe dynamics behind the pricing of securities under the CAPM are:\n\n\\[R_i = R_f + \\beta \\times (E[R_m] - R_f)\\]\n\nHowever, no one really told you from where the numbers came from\nRecall that, under the CAPM, we need to have estimates related to the market portfolio:\n\nIt is is equal to the risk-free interest rate, \\(R_f\\)…\nThe expected return on the market portfolio, \\(E[R_m]\\)…\nAnd a stock’s s sensitivity to the market portfolio, denoted by \\(\\beta\\)"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-the-risk-free-rate",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-the-risk-free-rate",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Cost of Equity Components: the risk-free rate",
    "text": "Cost of Equity Components: the risk-free rate\n\nThe first ingredient of CAPM is risk-free rate, which is the interest rate that investors can earn while having zero to limited volatility\nSuggestions on how to pick the Risk-Free (\\(R_f\\)) rate to be used:\n\nThe yield on U.S. Treasury securities\nSurveys suggest most practitioners use 10- to 30-year treasuries\nHighest quality assets\n\nOften, we use a short-term risk-free rate to evaluate a short-term investment, and a long-term rate when evaluating a long-term investment\n\n\n\n\n\n\n\nCountry-specific risk-free rates\n\n\nWhenever modeling assets outside of the U.S, we can either use the yields for local treasuries (i.e, relatively safer assets) or use U.S treasuries by adjusting the calculations for country-specific risk premium - see, for example, Brazilian’s EMBI."
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-the-market-risk-premium",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-the-market-risk-premium",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Cost of Equity Components: the market risk premium",
    "text": "Cost of Equity Components: the market risk premium\n\nAnother component of the Cost of Equity is the difference between \\(E[R_m]\\) and \\(R_f\\) (the market risk premium)\nWays to estimate the market risk premium:\n\nEstimate the risk premium (\\(E[R_m] − R_f\\)) using the historical average excess return of the market over the risk-free interest rate\nNotice that, even with long periods, we often have large standard errors\nImplicitly, you are assuming that the past is a good proxy for the future\n\n\n\n\n\n\n\n\nWatch-out!\n\n\nIndexes like the S&P500 and Ibovespa are not considered the market portfolio, but rather, they are proxies for the market portfolios - in other words, they are reasonable approximations of the market portfolio for a given set universe of securities"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-1-collecting-data",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-1-collecting-data",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 1: Collecting Data",
    "text": "Step 1: Collecting Data\n\nAs a suggestion, we will be collecting data on U.S. Treasury yields and Market Risk Premium using Kenneth French’s website, which hosts a data library with updated on U.S. returns from a wide varieaty of risk factors and asset classes\n\nTreasury yields (\\(R_F\\)) are defined as the daily returns on the 1-month Treasury Bill\nMarket Returns (\\(R_M\\)) are defined as the value-weighted returns on a bundle of U.S. stocks 1\n\nI have already worked on the data for you, and you can download it using the Download button - details on the code used to manipulate the data and put it into tidy format are presented in the next slide\nTo load the data in your session, call:\n\n\n#Read the data, assign it, and make sure it reads the Data column as a Date object\nFF_Data=read.csv('MRP_and_RF.csv')%&gt;%mutate(Date=as.Date(Date))\n\n\n\n Download Raw data\n\n\nClick here for details around the stock selection criteria."
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-1-r_m-r_f-and-the-market-risk-premium",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-1-r_m-r_f-and-the-market-risk-premium",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 1: \\(R_M\\), \\(R_F\\), and the Market Risk Premium",
    "text": "Step 1: \\(R_M\\), \\(R_F\\), and the Market Risk Premium\n\nCodeOutput\n\n\n\n# Use Fama-French Data to retrieve Rf and MRP\nFF_url &lt;- \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_daily_CSV.zip\"\ntemp_file &lt;- tempfile()\ndownload.file(FF_url, temp_file)\n\n#Download and manipulate the data\nunzip(temp_file)%&gt;%\nread.csv(skip=3)%&gt;%\n#Select only the data, Excess Returns, and Risk-Free Columns\nselect(1,2,7)%&gt;%\n#Change the names of the variables\nsetNames(c('Date','MRP','Rf'))%&gt;%\n#Make sure the date columns is read as a Date object\nmutate(Date=as.Date(strptime(Date,format='%Y%m%d')))%&gt;%\n#Filter for 2024\nfilter(year(Date)==2024)%&gt;%\n#Manipulate data to aggregate\nmutate(across(where(is.numeric),\\(x) (1+x/100)))%&gt;%\n#Pivot to get only one column\npivot_longer(names_to = \"Key\",values_to = \"Value\",-Date)%&gt;%\n#Group by the newly created Key column\ngroup_by(Key)%&gt;%\n#Apply nest() for functional programming and perform aggregation\nnest()%&gt;%\nmutate(data = map(data,as.xts))%&gt;%\nmutate(data = map(data,apply.weekly,\\(x) prod(x)-1))%&gt;%\nmutate(data = map(data,as.data.frame))%&gt;%\nmutate(data = map(data,~rownames_to_column(.,'Date')))%&gt;%\nunnest(data)%&gt;%\n#Pivot back to wide format\npivot_wider(names_from = Key,values_from = Value)%&gt;%\n#Wite csv\nwrite.csv('MRP_and_RF.csv',row.names = FALSE)\n\n\n\n\n\n         Date           MRP           Rf\n1  2024-01-05 -1.880351e-02 0.0008802904\n2  2024-01-12  1.630595e-02 0.0011004841\n3  2024-01-19  9.912377e-03 0.0008802904\n4  2024-01-26  1.043348e-02 0.0011004841\n5  2024-02-02  1.185523e-02 0.0010804666\n6  2024-02-09  1.484388e-02 0.0010504411\n7  2024-02-16 -2.926148e-03 0.0010504411\n8  2024-02-23  1.254919e-02 0.0008402646\n9  2024-03-01  1.060239e-02 0.0010504411\n10 2024-03-08 -3.840512e-03 0.0010504411\n11 2024-03-15 -3.377924e-03 0.0010504411\n12 2024-03-22  2.306942e-02 0.0010504411\n13 2024-03-28  4.564621e-03 0.0008402646\n14 2024-04-05 -1.150557e-02 0.0010504411\n15 2024-04-12 -1.675444e-02 0.0010504411\n16 2024-04-19 -3.211752e-02 0.0010504411\n17 2024-04-26  2.707724e-02 0.0010504411\n18 2024-05-03  5.745092e-03 0.0010204162\n19 2024-05-10  1.656236e-02 0.0010004001\n20 2024-05-17  1.543225e-02 0.0010004001\n21 2024-05-24 -1.968430e-03 0.0010004001\n22 2024-05-31 -6.544949e-03 0.0008002400\n23 2024-06-07  9.163612e-03 0.0011004841\n24 2024-06-14  1.293156e-02 0.0011004841\n25 2024-06-21  5.376481e-03 0.0008802904\n26 2024-06-28  8.368114e-05 0.0011004841\n27 2024-07-05  1.761172e-02 0.0008402646\n28 2024-07-12  1.057404e-02 0.0010504411\n29 2024-07-19 -1.923631e-02 0.0010504411\n30 2024-07-26 -6.090287e-03 0.0010504411\n31 2024-08-02 -2.779540e-02 0.0010704580\n32 2024-08-09 -1.139424e-03 0.0011004841\n33 2024-08-16  3.955583e-02 0.0011004841\n34 2024-08-23  1.501644e-02 0.0011004841\n35 2024-08-30  9.239343e-04 0.0011004841\n36 2024-09-06 -4.546808e-02 0.0008002400\n37 2024-09-13  4.114533e-02 0.0010004001\n38 2024-09-20  1.494732e-02 0.0010004001\n39 2024-09-27  4.994023e-03 0.0010004001\n40 2024-10-04  1.992034e-03 0.0008803095\n41 2024-10-11  1.142536e-02 0.0008502890\n42 2024-10-18  8.262229e-03 0.0008502890\n43 2024-10-25 -1.139132e-02 0.0008502890\n44 2024-11-01 -1.142076e-02 0.0008803095\n45 2024-11-08  5.133601e-02 0.0010004001\n46 2024-11-15 -2.267490e-02 0.0010004001\n47 2024-11-22  2.167504e-02 0.0010004001\n48 2024-11-29  9.707892e-03 0.0008002400\n49 2024-12-06  1.021493e-02 0.0008502890\n50 2024-12-13 -9.249021e-03 0.0008502890\n51 2024-12-20 -2.273584e-02 0.0008502890\n52 2024-12-27  5.566751e-03 0.0006801734\n53 2024-12-31 -1.544986e-02 0.0003400289"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-2-collecting-stock-price-information",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-2-collecting-stock-price-information",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 2: collecting stock price information",
    "text": "Step 2: collecting stock price information\n\nNow that we have the right-hand side components of your CAPM equation, it is time to collect information on stock prices for the selected stocks. By now, you can pretty much apply the rationale you’ve done in previous lectures:\n\nCreate a vector assets containing the tickers that you wish to request information from\nCreate a start and end objects containing the analysis period\nUse tq_get() and pipe assets onto the function along with from=start and to=end arguments\nManipulate the data to calculate weekly returns, assigning it to an object called Stock_Data\n\nFinally, you can use left_join() to merge Stock_Data with FF_Data:\n\n\nFinal_Data=Stock_Data%&gt;%left_join(FF_Data)"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-2-collecting-stock-price-information-1",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-2-collecting-stock-price-information-1",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 2: collecting stock price information",
    "text": "Step 2: collecting stock price information\n\nCodeOutput\n\n\n\n#Create the start and end dates\nstart=as.Date('2024-01-01')\nend=as.Date('2024-12-31')\n\n#Create the list of assets\nassets=c('DOW','LYB','PRM','FTK','RYAM',\n         'ALB','CE','CC','DNA','AVD')\n\n#Collect data, select necessary columns, and calculate weekly returns\nStock_Data=assets%&gt;%\n  tq_get(from=start,to=end)%&gt;%\n  select(symbol,date,adjusted)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = weeklyReturn,\n               col_rename = 'weekly_return')\n\n#Left join when column names are different\nFull_Data=Stock_Data%&gt;%left_join(FF_Data,by=c('date'='Date'))\n\n\n\n\n\n# A tibble: 530 × 5\n# Groups:   symbol [10]\n   symbol date       weekly_return      MRP       Rf\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 DOW    2024-01-05      -0.00922 -0.0188  0.000880\n 2 DOW    2024-01-12      -0.0265   0.0163  0.00110 \n 3 DOW    2024-01-19      -0.0105   0.00991 0.000880\n 4 DOW    2024-01-26       0.0237   0.0104  0.00110 \n 5 DOW    2024-02-02      -0.0118   0.0119  0.00108 \n 6 DOW    2024-02-09       0.0107   0.0148  0.00105 \n 7 DOW    2024-02-16       0.0276  -0.00293 0.00105 \n 8 DOW    2024-02-23       0.0164   0.0125  0.000840\n 9 DOW    2024-03-01       0.00146  0.0106  0.00105 \n10 DOW    2024-03-08       0.0151  -0.00384 0.00105 \n# ℹ 520 more rows\n\n\n\\(\\rightarrow\\) You now have the weekly returns for all selected stocks, the weekly risk-free returns, and the weekly market returns!"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-beta-estimation",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-beta-estimation",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Cost of Equity Components: \\(\\beta\\) estimation",
    "text": "Cost of Equity Components: \\(\\beta\\) estimation\n\nAll that’s left is to estimate the stocks’s sensitivity to the returns of the market portfolio, \\(\\beta\\)\nFrom what we know on the theory on portfolio returns, a new asset \\(i\\) should be enhance the performance of a portfolio if:\n\n\\[\n\\small \\underbrace{\\frac{E[R_i] - R_f}{\\sigma_{i} \\times Corr(R_i,R_m)}}_{\\text{Sharpe Ratio of } i} &gt; \\underbrace{\\frac{E[R_m] - R_f}{\\sigma_{m}}}_{\\text{Sharpe Ratio of Market}}\n\\]\n\nWith that, we saw that the expected return from an asset \\(i\\) should be:\n\n\\[\n\\small R_i - R_f = \\underbrace{\\frac{\\sigma_{i} \\times Corr(R_i,R_m)}{\\sigma_{m}}}_{\\beta^M_i}  \\times (E[R_m] - R_f)\n\\]"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-beta-estimation-continued",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-beta-estimation-continued",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Cost of Equity Components: \\(\\beta\\) estimation, continued",
    "text": "Cost of Equity Components: \\(\\beta\\) estimation, continued\n\nBecause \\(\\small Corr(R_i,R_m)=\\frac{Cov(R_i,R_m)}{\\sigma_i\\sigma_m}\\), we have that:\n\n\\[\n\\small (R_i - R_f)=\\frac{\\sigma_{i} \\times Cov(R_i,R_m)}{\\sigma_i \\sigma_m\\sigma_{m}}  \\times (E[R_p] - R_f)\\rightarrow  (R_i - R_f)=  \\underbrace{\\frac{Cov(R_i,R_m)}{\\sigma^2_m}}_{\\text{OLS formula for slope}}\\times (E[R_p] - R_f)\n\\]\n\nWe can then estimate \\(\\beta\\) using an Ordinary Least Squares regression:\n\n\\[\n\\small \\underbrace{(R_i - R_f)}_{\\text{Excess Return}} = \\underbrace{\\alpha_i}_{\\text{Uncorrelated Return}} + \\underbrace{\\beta_i}_{\\text{Stock's Market Sensitivity}} \\times \\underbrace{(R_m - R_f)}_{\\text{Risk Premium}} + \\epsilon_i\n\\]\n\n\\(\\epsilon_i\\) is the error term (or the residual). It represents the deviations from the best-fitting line and is, by definition, zero on average (or else we could improve the fit), and represent firm-specific risk that is diversifiable and that averages out in a large portfolio"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-3-estimating-alpha-and-beta",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-3-estimating-alpha-and-beta",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 3: estimating \\(\\alpha\\) and \\(\\beta\\)",
    "text": "Step 3: estimating \\(\\alpha\\) and \\(\\beta\\)\n\nWe know need estimate the following equation for each stock in our analysis:\n\n\\[\n\\small Excess_t = \\alpha + \\beta \\times (R_m - R_f) + \\epsilon_t\n\\]\n\nThe naivest way to do it is to repeat the process \\(10\\) times, filtering each stock at a time, and running an OLS model with the lm() function:\n\nStart with the Full_Data object and pipe onto mutate to creat the excess_return variable\nselect only the symbol, the excess_return, and the MRP columns\nUse filter to work with a single ticker, say, symbol=='DOW'\nCall the lm() function to run an OLS regression of excess returns on market returns\n\nThe next slide shows the result of estimating the CAPM model for DOW"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-3-estimating-alpha-and-beta-dow-only",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-3-estimating-alpha-and-beta-dow-only",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 3: estimating \\(\\alpha\\) and \\(\\beta\\) (DOW only)",
    "text": "Step 3: estimating \\(\\alpha\\) and \\(\\beta\\) (DOW only)\n\nCodeOutput\n\n\n\n#Manipulate data\nDOW=Full_Data%&gt;%\n  mutate(excess_return=weekly_return-Rf)%&gt;%\n  select(symbol,excess_return,MRP)\n\n#Run the OLS regression\nOLS=lm(excess_return~MRP,data=DOW)\n\n#Inspect the results using summary()\nsummary(OLS)\n\n\n\n\n\n\nCall:\nlm(formula = excess_return ~ MRP, data = DOW)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.41436 -0.03266 -0.00274  0.02747  0.38600 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.006931   0.003482  -1.991   0.0471 *  \nMRP          1.118565   0.190283   5.878 7.43e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07769 on 518 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.06254,   Adjusted R-squared:  0.06073 \nF-statistic: 34.56 on 1 and 518 DF,  p-value: 7.426e-09"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#understanding-the-beta-term-inside-the-ols-estimation",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#understanding-the-beta-term-inside-the-ols-estimation",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Understanding the \\(\\beta\\) term inside the OLS estimation",
    "text": "Understanding the \\(\\beta\\) term inside the OLS estimation\n\\[\n\\small \\underbrace{(R_i - R_f)}_{\\text{Excess Return}} = \\underbrace{\\alpha}_{\\text{Uncorrelated Return}} + \\underbrace{\\beta}_{\\text{Stock's Market Sensitivity}} \\times \\underbrace{(R_m - R_f)}_{\\text{Risk Premium}} + \\epsilon\n\\]\n\n\\(\\beta\\) is the sensitivity to market risk. It measures the historical variation of the security relative to the market\nAccording to the CAPM, all assets should line on the Security Market Line (SML)\n\n\nIf \\(\\beta&gt;1\\), it means that a 1% variation in market returns implies a variation that is greater than 1% in stock returns (either up or down!)\nIf \\(\\beta&lt;1\\) it means that a 1% variation in market returns implies a variation that is less than 1% in stock returns (either up or down!)"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#assessing-required-returns",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#assessing-required-returns",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Assessing Required Returns",
    "text": "Assessing Required Returns\n\nSuppose you need to price the long-run required returns for investing in an opportunity that has the same equity risk and as DOW. It is now a straightforward application of the CAPM:\n\n\\[\n\\small \\text{Required Return} = R_f +  \\beta \\times (R_m - R_f)\n\\]\n\nSay, for example that you have the following information:\n\nThe historical long-run risk-free rate return, \\(\\small R_f\\), is \\(\\small 4.50\\%\\)\nThe historical long-run market return, \\(\\small R_m\\), is \\(\\small 9.94\\%\\)\nThe \\(\\beta\\) you’ve just found is \\(\\small 1.12\\)\n\nThen, the long-run required return is simply:\n\n\\[\n\\small \\text{Required Return} = 4.5\\%+ 1.12\\times(9.94\\%-4.5\\%)=10.59\\%\n\\]"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#understanding-the-alpha-term-inside-the-ols-estimation",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#understanding-the-alpha-term-inside-the-ols-estimation",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Understanding the \\(\\alpha\\) term inside the OLS estimation",
    "text": "Understanding the \\(\\alpha\\) term inside the OLS estimation\n\\[\n\\small \\underbrace{(R_i - R_f)}_{\\text{Excess Return}} = \\underbrace{\\alpha}_{\\text{Uncorrelated Return}} + \\underbrace{\\beta}_{\\text{Stock's Market Sensitivity}} \\times \\underbrace{(R_m - R_f)}_{\\text{Risk Premium}} + \\epsilon\n\\]\n\n\\(\\alpha_i\\) is the constant term. It measures the historical performance of the security relative to the expected return predicted by the security market line\nIt is the distance that the stock’s average return is above or below the SML. Thus, we can say \\(\\alpha_i\\) is a risk-adjusted measure of the stock’s historical performance.\nAccording to the CAPM, \\(\\alpha_i\\) should not be significantly different from zero\n\n\nIf \\(\\alpha&gt;0\\) consistently, it would mean that a security delivers a constant positive return and, by definition, independent from the market returns\nIf that is the case, investors would buy the security up to a point where price adjusts so that \\(\\alpha\\) goes to zero (recall Assumption #1)!"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#interpreting-alpha",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#interpreting-alpha",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Interpreting \\(\\alpha\\)",
    "text": "Interpreting \\(\\alpha\\)\n\\[\\small \\alpha_i = \\underbrace{E[R_i]}_{\\text{Observed by the analyst}} - \\underbrace{R_i}_{\\text{Implied by the CAPM}}\\]\n\nA positive alpha means that the stock is above the SML\n\nIn words, the expected return is higher than its required return. Before prices adjust, investors will anticipate that the price will rise and will likely put in buy orders at the current prices\n\nA negative alpha means that the stock is below the SML\n\nThe expected return is lower than its required return. Before prices adjust, investors will anticipate that the price will fall and will likely put in sell orders at the current prices\n\n\n\\(\\rightarrow\\) In either case, we’ll be able to improve portfolio results. However, as we do so, prices will change and their alphas will shrink towards zero!"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-4-putting-all-together",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-4-putting-all-together",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 4: putting all together",
    "text": "Step 4: putting all together\n\nOur final step is to use the CAPM to assess which stocks are overvalued, and which ones are undervalued\nYou could proceed by doing the same procedure as before, but now focusing on the \\(\\alpha\\) term that has been estimated for you\n\nTo do that for all 10 stocks, you could do a for loop, store the results, and analyze\nIn general, for loops are inefficient: they run sequentially, have slower performance, and are difficult to read\n\nAn alternative is to use the tidyverse excellent capabilities for functional programming using the map function from the purrr package, which breaks the problem into sub-pieces and estimate the models in parallel\nFor detailed information on functional programming, see purrr documentation here"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-4-putting-all-together-continued",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#step-4-putting-all-together-continued",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 4: putting all together continued",
    "text": "Step 4: putting all together continued\n\nFor LoopFunctional Programming\n\n\n\n#Start an empty data.frame\nStored_Data=data.frame()\n\nfor (i in assets){\n  \n  #Manipulate data\n  Filtered_Data=Full_Data%&gt;%\n    #Filter for the specific ticker\n    filter(symbol== i)%&gt;%\n    mutate(excess_return=weekly_return-Rf)%&gt;%\n    select(symbol,excess_return,MRP)\n  \n  #Run the OLS regression\n  OLS=lm(excess_return~MRP,data=Filtered_Data)\n\n  #Get the coefficients using the coefficients() function and add it to a temp data\n  \n  Temp_Data = data.frame(ticker=i,\n                         alpha=coefficients(OLS)[1],\n                         beta=coefficients(OLS)[2])\n  \n  #Bind it to the dataframe\n  Stored_Data=Stored_Data%&gt;%rbind(Temp_Data)\n  \n  }\n\n\n\n\nCAPM_Estimation=Full_Data%&gt;%\n  mutate(excess_return=weekly_return-Rf)%&gt;%\n  select(symbol,excess_return,MRP)%&gt;%\n  group_by(symbol)%&gt;%\n  nest()%&gt;%\n  mutate(CAPM = map(data,~ lm(excess_return~MRP,data=.)))%&gt;%\n  mutate(coefficients = map(CAPM,tidy))"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#charting-the-result",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#charting-the-result",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Charting the result",
    "text": "Charting the result\n\nCodeOutput\n\n\n\nFull_Data%&gt;%\n  mutate(excess_return=weekly_return-Rf)%&gt;%\n  select(symbol,excess_return,MRP)%&gt;%\n  group_by(symbol)%&gt;%\n  nest()%&gt;%\n  mutate(CAPM = map(data,~ lm(excess_return~MRP,data=.)))%&gt;%\n  mutate(coefficients = map(CAPM,broom::tidy))%&gt;%\n  select(coefficients)%&gt;%\n  unnest()%&gt;%\n  filter(term=='(Intercept)')%&gt;%\n  select(symbol,estimate)%&gt;%\n  setNames(c('Ticker','Alpha'))%&gt;%\n  ggplot(aes(x=Ticker,y=Alpha))+\n  geom_point()+\n  geom_segment(aes(yend = 0), linetype = \"dashed\", color = \"gray50\")+\n  geom_hline(yintercept=0,linetype='dashed')+\n  #Annotations\n  labs(title='Using CAPM to analyze over/undervalued investment opportunities',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Ticker',\n       y = 'Adjusted Prices')+\n  #Scales\n  scale_y_continuous(labels = percent)+\n  #Custom 'The Economist' theme\n  theme_tq()+\n  #Adding further customizations\n  theme(legend.position='none',\n        axis.title.y = element_text(vjust=+4,face='bold'),\n        axis.title.x = element_text(vjust=-3,face='bold'),\n        axis.text = element_text(size=8))"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#capm-shortcomings",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#capm-shortcomings",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "CAPM shortcomings",
    "text": "CAPM shortcomings\n\nIn our estimation, none of the \\(\\small \\alpha\\) results were statistically significant, meaning that we cannot reject the hypothesis that \\(\\alpha\\) is different from zero, which is in line with the CAPM\nHowever, when trying to use the CAPM to predict future performance, results are not consitent with the CAPM predictions. What could be going on?\nIt is important to recall some of the model’s shortcomings:\n\nMarket Returns are really context-dependent, and the use of NYSE/DOW/Ibovespa etc is problem-specific\nThe sensitivity to market returns, \\(\\small \\beta\\), might not be stable over time\nSystematic Risk might not be the only factor that matters!\nAssumptions of the CAPM might not be that realistic"
  },
  {
    "objectID": "quant-fin/Lecture 5 - CAPM in Practice/index.html#references",
    "href": "quant-fin/Lecture 5 - CAPM in Practice/index.html#references",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nMarkowitz, Harry. 1952. “Portfolio Selection.” The Journal of Finance 7 (1): 77–91. http://www.jstor.org/stable/2975974.\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#outline",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#outline",
    "title": "Manipulating time series Data",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\nTidy Finance (Scheuch, Voigt, and Weiss 2023)\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\n\n\n\n\n\n\n\n\nCoding Replications\n\n\nFor coding replications, whenever applicable, please follow this page or hover on the specific slides with containing coding chunks.\n\nEnsure that you have your  session properly set-up according to the instructions outlined in the course webpage\nAlong with the slides, this lecture will also contain a replication file, in .qmd format, containing a thorough discussion for all examples that have been showcased. This file, that will be posted on eClass®, can be downloaded and replicated on your side. To do that, download the file, open it up in RStudio, and render the Quarto document using the Render button (shortcut: Ctrl+Shift+K).\nAt the end of this lecture, you will be prompted with a hands-on exercise to test your skills using the tools you’ve learned as you made your way through the slides. A suggested solution will be provided in the replication file."
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#organizing-financial-data",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#organizing-financial-data",
    "title": "Manipulating time series Data",
    "section": "Organizing Financial Data",
    "text": "Organizing Financial Data\n\nIn the previous lecture, you worked your way through the exercises by using the amazing dplyr functionalities on data.frames\nIn some cases, you had to do some workarounds with drop_na(), slice_tail() and lag() simply because you were manipulating time series data\nIn this lecture, you will be introduced to a particular type of class in R: xts\n\n\n\n\n\n\n\n\nDefinition\n\n\nxts is an R package that provides an extension of the zoo class, a class with methods for totally ordered indexed observations - in special, time series\n\nWith xts, you get a lot of flexibility in handling time series observations that are of interest of financial analysts, such as:\n\nSubsetting data by years/months/days\nCalculating rolling functions (e.g, yearly averages)\nAggregating data at different intervals (e.g, convert daily to weekly prices)\n\n\n\n\n\n\n\nQuestion: but wait, why are we departing from dplyr?"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#bridging-the-tidyverse-with-time-series",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#bridging-the-tidyverse-with-time-series",
    "title": "Manipulating time series Data",
    "section": "Bridging the tidyverse with time series",
    "text": "Bridging the tidyverse with time series\n\nUnfortunately, there is an issue: the tidyverse is not fully designed to work with time series classes, such as xts and zoo\nAs a consequence, you won’t be able to use a lot of interesting functionalities that would perfectly apply for time series\n\nBut don’t you worry, I got you covered: the tidyquant package1 integrates the best resources for collecting and analyzing financial data\nIt integrates several financial packages, like zoo, xts, quantmod, TTR, and PerformanceAnalytics, with the tidy data infrastructure of the tidyverse, allowing for seamless interaction between each\n\nYou can now perform complete financial analyses using the same functionalities you’ve learned so far!\n\nClick here for a thorough documentation on the tidyquant package."
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#the-tidyquant-package-a-short-video",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#the-tidyquant-package-a-short-video",
    "title": "Manipulating time series Data",
    "section": "The tidyquant package, a short-video",
    "text": "The tidyquant package, a short-video"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#the-tq_mutate-function",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#the-tq_mutate-function",
    "title": "Manipulating time series Data",
    "section": "The tq_mutate() function",
    "text": "The tq_mutate() function\n\n\n\n\n\n\nDefinition\n\n\nThe tq_mutate() function adds adds new variables to an existing tibble:\n\ntq_mutate(.data, #The object you are performing the calculations \n       selected_variables, #The columns to send to the mutation function\n       mutate_fun, #The mutation function from either the xts, quantmod, or TTR package.\n       col_rename #A string or character vector containing names that can be used to quickly rename columns\n       ) \n\n\n\n\n\nThe main advantage is the results are returned as a tibble and the function can be used with the tidyverse\nIt is used when you expected additional columns to be added to the resulting data frame\nYou can use several time series related functions from other R packages - call tq_mutate_fun_options() to see the list of available options\nAll in all, it is similar in spirit to mutate()"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#the-tq_transmute-function",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#the-tq_transmute-function",
    "title": "Manipulating time series Data",
    "section": "The tq_transmute() function",
    "text": "The tq_transmute() function\n\n\n\n\n\n\nDefinition\n\n\nThe tq_transmute() returns only newly created columns and is typically used when periodicity changes.\n\ntq_transmute(.data, #The object you are performing the calculations \n       selected_variables, #The columns to send to the mutation function\n       mutate_fun, #The mutation function from either the xts, quantmod, or TTR package.\n       col_rename #A string or character vector containing names that can be used to quickly rename columns\n       )\n\n\n\n\n\ntq_transmute() works exactly like tq_mutate() except it only returns the newly created columns\nThis is helpful when changing periodicity where the new columns would not have the same number of rows as the original tibble\nAll in all, it is similar in spirit to summarize()"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-i",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-i",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise I",
    "text": "Working with time series objects, Exercise I\n\nAn immediate useful example of using a time series specific functionality with a tidyverse logic relates to filtering:\n\nSometimes, we may be interested in getting only a subset of the data (for example, only GOOG information)\nFurthermore, we may be interested in subsetting only a specific time frame for our analysis\n\nIt is relatively straightforward to do it with tidyquant:\n\nUse filter() to select only rows where symbol=='GOOG'\nIn the same call, filter for date&gt;= min_date and date&lt;=max_date"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-i-1",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-i-1",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise I",
    "text": "Working with time series objects, Exercise I\n\nCodeOutput\n\n\n\nlibrary(tidyquant)\nlibrary(tidyverse)\n\n#Set up the list of assets\nassets=c('AMZN','GOOG','META','GME')\n\n#Filter out\nassets%&gt;%\n  tq_get()%&gt;%\n  filter(symbol=='GOOG',\n         date&gt;='2020-01-01',\n         date&lt;='2024-12-31')\n\n\n\n\n\n# A tibble: 1,258 × 8\n   symbol date        open  high   low close   volume adjusted\n   &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 GOOG   2020-01-02  67.1  68.4  67.1  68.4 28132000     68.1\n 2 GOOG   2020-01-03  67.4  68.6  67.3  68.0 23728000     67.8\n 3 GOOG   2020-01-06  67.5  69.8  67.5  69.7 34646000     69.5\n 4 GOOG   2020-01-07  69.9  70.1  69.5  69.7 30054000     69.4\n 5 GOOG   2020-01-08  69.6  70.6  69.5  70.2 30560000     70.0\n 6 GOOG   2020-01-09  71.0  71.4  70.5  71.0 30018000     70.7\n 7 GOOG   2020-01-10  71.4  71.7  70.9  71.5 36414000     71.2\n 8 GOOG   2020-01-13  71.8  72.0  71.3  72.0 33046000     71.7\n 9 GOOG   2020-01-14  72.0  72.1  71.4  71.5 31178000     71.3\n10 GOOG   2020-01-15  71.5  72.1  71.5  72.0 25654000     71.7\n# ℹ 1,248 more rows"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-ii",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-ii",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise II",
    "text": "Working with time series objects, Exercise II\n\nAnother example of using a time series specific functionality is working with leads and lags:\n\nSometimes, we need to shift our variables by a specific interval, like getting the previous day’s price\nSay, for example, that you want to understand how S&P returns levels relate to NFLX returns one-week ahead\n\nIt is relatively straightforward to do it with tidyquant:\n\nDownload S&P 500 and NFLX data using the tq_get() function\nUse tq_transmute() to compute the weekly returns for each security based on daily data\nUse tq_mutate() to generate a lagged series of S&P 500 returns"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-ii-1",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-ii-1",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise II",
    "text": "Working with time series objects, Exercise II\n\nCodeOutput\n\n\n\n#Assuming you have already loaded the tidyquant and the tidyverse packages\n\n#Netflix Data\nNFLX=tq_get('NFLX')%&gt;%\n  #Select only the necessary columns\n  select(date,symbol,adjusted)%&gt;%\n  #Apply the weeklyReturn function and call the new column 'NFLX'\n  tq_transmute(mutate_fun = weeklyReturn,\n               col_rename = 'NFLX')\n\n#S&P Data\nSP500=tq_get('^GSPC')%&gt;%\n  #Select only the necessary columns\n  select(date,symbol,adjusted)%&gt;%\n  #Apply the weeklyReturn function and call the new column 'SP500'\n  tq_transmute(mutate_fun = weeklyReturn,\n               col_rename = 'SP500')%&gt;%\n  #Apply the lag function for n=1 week and call the new column 'SP500'\n  tq_transmute(mutate_fun = lag.xts,\n            n=1,\n            col_rename = 'SP500')%&gt;%\n  #Drop all rows with NA information (row 1, in this case)\n  drop_na()\n\n#Merge Data \ninner_join(NFLX,SP500)\n\n\n\n\n\n# A tibble: 528 × 3\n   date           NFLX    SP500\n   &lt;date&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 2015-01-09 -0.0563   0      \n 2 2015-01-16  0.0244  -0.00651\n 3 2015-01-23  0.297   -0.0124 \n 4 2015-01-30  0.00992  0.0160 \n 5 2015-02-06  0.00579 -0.0277 \n 6 2015-02-13  0.0489   0.0303 \n 7 2015-02-20  0.0260   0.0202 \n 8 2015-02-27 -0.00688  0.00635\n 9 2015-03-06 -0.0438  -0.00275\n10 2015-03-13 -0.0346  -0.0158 \n# ℹ 518 more rows"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iii",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iii",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise III",
    "text": "Working with time series objects, Exercise III\n\nFinance practitioners are often asked to perform analysis on a rolling basis:\n\nWe may want to calculate a given signal on day \\(t\\) based on past \\(x\\) periods of information\nSay, for example, that you want to calculate a simple and exponential moving average of adjusted prices from 5 days back for a given stock\n\nIt is relatively straightforward to do it with tidyquant:\n\nDownload stock data using the tq_get() function\nUse tq_mutate() twice along with the SMA() and EMA() functions setting n=5"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iii-1",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iii-1",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise III",
    "text": "Working with time series objects, Exercise III\n\nCodeOutput\n\n\n\n#Assuming you have already loaded the tidyquant and the tidyverse packages\n\n#Set up the list of assets\nassets=c('AMZN')\n\nassets%&gt;%\n  tq_get()%&gt;%\n  select(date,symbol,adjusted)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_mutate(adjusted, mutate_fun = SMA, n = 5)%&gt;%\n  tq_mutate(adjusted, mutate_fun = EMA, n = 5)\n\n\n\n\n\n# A tibble: 2,546 × 5\n# Groups:   symbol [1]\n   symbol date       adjusted   SMA   EMA\n   &lt;chr&gt;  &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 AMZN   2015-01-02     15.4  NA    NA  \n 2 AMZN   2015-01-05     15.1  NA    NA  \n 3 AMZN   2015-01-06     14.8  NA    NA  \n 4 AMZN   2015-01-07     14.9  NA    NA  \n 5 AMZN   2015-01-08     15.0  15.0  15.0\n 6 AMZN   2015-01-09     14.8  14.9  15.0\n 7 AMZN   2015-01-12     14.6  14.8  14.8\n 8 AMZN   2015-01-13     14.7  14.8  14.8\n 9 AMZN   2015-01-14     14.7  14.8  14.8\n10 AMZN   2015-01-15     14.3  14.6  14.6\n# ℹ 2,536 more rows"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iv",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iv",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise IV",
    "text": "Working with time series objects, Exercise IV\n\nLastly, financial analysts often cover a collection of securities on a rolling basis\nFor example, a buy-side analyst will monitor stocks from a given industry so as to understand which ones are overvalued, and which ones are undervalued\nSay, for example, that you want to focus on a subset of 4 stocks, and you need to compare the cumulative return up to the latest closing price\nIt is easy to integrate the tidyquant functions along with the group_by() function you’ve learned when working with dplyr:\n\nGet the information using tq_get()\nGroup the data by symbol\nApply the tq_mutate and tq_transmute functions to pass time series functions to the data - in this case, the dailyReturn() and the Return.cumulative() function"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iv-1",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iv-1",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise IV",
    "text": "Working with time series objects, Exercise IV\n\nCodeOutput\n\n\n\n#Assuming you have already loaded the tidyquant and the tidyverse packages\n\n#Set up the list of assets\nassets=c('AMZN','GOOG','META','GME')\n\nassets%&gt;%\n  tq_get()%&gt;%\n  select(date,symbol,adjusted)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_mutate(adjusted, mutate_fun = dailyReturn,col_rename = 'daily_return')%&gt;%\n  tq_transmute(daily_return,mutate_fun = Return.cumulative)%&gt;%\n  mutate(across(where(is.numeric),percent,big.mark='.'))%&gt;%\n  setNames(c('Ticker','Cumulative Return up-to-date'))\n\n\n\n\n\n# A tibble: 4 × 2\n# Groups:   Ticker [4]\n  Ticker `Cumulative Return up-to-date`\n  &lt;chr&gt;  &lt;chr&gt;                         \n1 AMZN   1 382%                        \n2 GOOG   617%                          \n3 META   843%                          \n4 GME    329%"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#wrapping-up-on-tidyquant",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#wrapping-up-on-tidyquant",
    "title": "Manipulating time series Data",
    "section": "Wrapping-up on tidyquant",
    "text": "Wrapping-up on tidyquant\n\nThere is so much you can use from tidyquant in your journey as a quantitative financial analyst:\n\nI strongly recommend looking at all the predefined functions supported by tidyquant - click here for a detailed discussion around all supported functions\nYou can also customize your own functions that work with time series (for example, your secret trading indicator) and pass it over through tq_mutate() or tq_transmute()\n\nIn the package’s official website, you can find a variety of examples to nurture your creativity around what you can do using this package\nAll in all, that’s the motto: time series analysis made easy with tidyquant and the tidyverse"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#does-getting-ripped-increase-returns",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#does-getting-ripped-increase-returns",
    "title": "Manipulating time series Data",
    "section": "Does getting ripped increase returns?",
    "text": "Does getting ripped increase returns?\n\n\n\n\n\n\n\\(\\rightarrow\\) See Deadlift: The ETF World’s Latest Headscratcher"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#hands-on-exercise",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#hands-on-exercise",
    "title": "Manipulating time series Data",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\nYour manager (who did not lift any weights past the last 5 years) wanted to replicate the returns of the Deadlift ETF from 2020 to 2024. You job is to create a simple table of yearly returns comparing the Deadlift ETF vis-a-vis the S&P 500 Index\nFollow the instructions and answer to the following questions:\n\nWhen looking at the yearly results from both the Deadlift ETF and S&P 500, which one did perform better?\nWhat are the potential explanations for the result you have found?\n\nTo answer to these questions, you will be using the a combination of dplyr and tidyquant functions you have learned so far\nThe expected result is a data.frame object that shows both the Deadlift ETF as well as the S&P 500 returns (columns) on a yearly basis (rows)\n\n\\(\\rightarrow\\) Suggested solution will be provided in the replication file for this lecture."
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#hands-on-exercise-continued",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#hands-on-exercise-continued",
    "title": "Manipulating time series Data",
    "section": "Hands-On Exercise, continued",
    "text": "Hands-On Exercise, continued\n\n\n\n\n\n\nExercise\n\n\nBefore you start, make sure to have the tidyverse and the tidyquant packages loaded in your session. Following the instructions from the previous lectures, you can either make a direct call to each package, library(tidyverse) and library(tidyquant), or copy-paste the script from the course’s official website.\n\nUse tq_get() to load information from the S&P Index and the Deadlift ETF constituents in two separate objects. You can use the code ^GSPC to retrieve information for the index, and you can pass a vector c('ticker1','ticker2',...,'ticker_n') to get information on the Deadlift ETF constituents\nFilter for observations starting between 2020 (beginning of) and 2024 (end of) using the from and to arguments of the tq_get() function\nGroup the Deadlift ETF data by symbol using the group_by() function\nFor both data sets, create a yearly_ret variable that calculates the yearly return of a given security. You can use the tq_transmute() function, passing the yearlyReturn() function along the chain\nFor the Deadlift data set, regroup the data by date and calculate the Deadlift returns using a mutate() function (Hint: it is an equally weighted portfolio)\nMerge both datasets using inner_join()"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#solution",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#solution",
    "title": "Manipulating time series Data",
    "section": "Solution",
    "text": "Solution\n\nCodeOutput\n\n\n\n# Set up the list of assets\ndeadlift=c('META','AMZN','GS','UBER','MSFT','AAPL','BLK','NVDA')\n\n#Set up the starting date\nstart='2020-01-01'\nend='2024-12-31'\n\n#Step 1: Read the Deadlift data using tidyquant\nDeadlift_Performance=deadlift%&gt;%\n  tq_get(from=start,to=end)%&gt;%\n  #Select only the columns of interest\n  select(symbol,date,adjusted)%&gt;%\n  #Group by symbol and date\n  group_by(symbol)%&gt;%\n  #Use tq_transmute to aggregate and calculate weekly returns\n  tq_transmute(selected=adjusted,\n               mutate_fun=yearlyReturn,\n               col_rename = 'Deadlift')%&gt;%\n  #Group by date\n  group_by(date)%&gt;%\n  #Summarize average return (since it is an equally-weighted portfolio)\n  summarize(Deadlift=mean(Deadlift,na.rm=TRUE))\n\n#Step 2: Read the S&P 500 data using tidyquant\nSP500_Performance=tq_get('^GSPC',from=start,to=end)%&gt;%\n  #Select only the columns of interest\n  select(symbol,date,adjusted)%&gt;%\n  #Group by symbol and date\n  group_by(symbol)%&gt;%\n  #Use tq_transmute to aggregate and calculate weekly returns\n  tq_transmute(selected=adjusted,\n               mutate_fun=yearlyReturn,\n               col_rename = 'SP500')%&gt;%\n  ungroup()%&gt;%\n  select(-symbol)\n    \n#Merge\nSP500_Performance%&gt;%\n  inner_join(Deadlift_Performance)%&gt;%\n  mutate(across(where(is.numeric),percent))%&gt;%\n  mutate(date=year(date))%&gt;%\n  setNames(c('Year','S&P 500','DeadLift ETF'))\n\n\n\n\n\n# A tibble: 5 × 3\n   Year `S&P 500` `DeadLift ETF`\n  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;         \n1  2020 15.29%    57.9%         \n2  2021 26.89%    37.2%         \n3  2022 -19.44%   -36.0%        \n4  2023 24.23%    100.5%        \n5  2024 23.84%    52.1%"
  },
  {
    "objectID": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#references",
    "href": "quant-fin/Lecture 3 - Manipulating Time Series/index.html#references",
    "title": "Manipulating time series Data",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#outline",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#outline",
    "title": "Bridging Finance with Programming",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\nTidy Finance (Scheuch, Voigt, and Weiss 2023)\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\n\n\n\n\n\n\n\n\nCoding Replications\n\n\nFor coding replications, whenever applicable, please follow this page or hover on the specific slides with containing coding chunks.\n\nEnsure that you have your  session properly set-up according to the instructions outlined in the course webpage\nAlong with the slides, this lecture will also contain a replication file, in .qmd format, containing a thorough discussion for all examples that have been showcased. This file, that will be posted on eClass®, can be downloaded and replicated on your side. To do that, download the file, open it up in RStudio, and render the Quarto document using the Render button (shortcut: Ctrl+Shift+K).\nAt the end of this lecture, you will be prompted with a hands-on exercise to test your skills using the tools you’ve learned as you made your way through the slides. A suggested solution will be provided in the replication file."
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tools-of-the-trade-part-i-the-data",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tools-of-the-trade-part-i-the-data",
    "title": "Bridging Finance with Programming",
    "section": "The Tools of the Trade, part I: the data",
    "text": "The Tools of the Trade, part I: the data\n\nFor most of the topics within the study of finance, there is a well-grounded, established use of statistical, economic, and mathematical concepts that set the stage for data analysis:\n\nMacroeconomic analysts use time-series models to predict future interest rates\nFinancial analysts study the potential effects in stock prices of issuing equity\nHedge Fund Managers use models to predict inflation and adjust their positions\n\n\n\n\nBack in the pre-internet era, the use of technology to support those activities was limited to a smaller set of players (e.g, hedge funds, banks, investment trusts). Nowadays, financial information is accessible to the broader public almost in real time:\n\nYahoo! Finance provides data on stocks, ETFs, and market indices\nEDGAR provides information on all periodic fillings provided by US-listed companies\nA wide range of social media platforms, such as X (formerly Twitter) and Reddit, have been recently use as a way to spread and collect financial information"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tools-of-the-trade-part-ii-the-technology",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tools-of-the-trade-part-ii-the-technology",
    "title": "Bridging Finance with Programming",
    "section": "The Tools of the Trade, part II: the technology",
    "text": "The Tools of the Trade, part II: the technology\n\nNot only the availability of financial data, but also the necessary technology to process it, were among the bottlenecks for the adoption of such methods in financial practice\nNowadays, the widespread adoption of open-source technologies, such as  and , helped bridging the gap towards a more inclusive environment for those methods\nDespite such advances, one quickly learns that the actual implementation of models to solve problems in the area of financial economics is typically rather opaque:\n\nThere is lack of public, centralized code readily available for use\nAnalysts employ a substantial amount of wasteful efforts trying to replicate results\n\n\n\n\nIt is often said that more than 80 percent of data analysis is spent on preparing data rather than analyzing it. How do you solve for that?"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#why-tidy",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#why-tidy",
    "title": "Bridging Finance with Programming",
    "section": "Why Tidy?",
    "text": "Why Tidy?\n\nIt is often said that more than 80 percent of data analysis is spent on preparing data rather than analyzing it\nAs you start working with data, you quickly realize that you indeed spend a lot of time reading, cleaning, and transforming your data just\n\n\n\n\n\n\n\nA note on Tidy Data\n\n\n“Tidy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).”\n\n\n\n\nIn its essence, tidy data mainly follows three principles:\n\nEvery column is a variable\nEvery row is an observation\nEvery cell is a single value"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#why-tidy-continued",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#why-tidy-continued",
    "title": "Bridging Finance with Programming",
    "section": "Why Tidy? Continued",
    "text": "Why Tidy? Continued\n\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we’ll try to follow:\n\nReuse existing data structures\nCompose simple functions with chaining methods\nEmbrace functional programming\nDesign for humans, improved readability\n\nLuckily, the  community has already took a stab at creating tools and organizing a unified approach towards tidy analysis\nAmongst a diverse set of option for tidy data manipulation, the tidyverse contains packages that follow a unified approach"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#introducing-the-tidyverse",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#introducing-the-tidyverse",
    "title": "Bridging Finance with Programming",
    "section": "Introducing: the tidyverse",
    "text": "Introducing: the tidyverse\n\n\n\n\nThe tidyverse is an opinionated collection of  packages designed for data science\nAll packages share an underlying design philosophy, grammar, and data structures\nIt is supported by Posit, the maintainer of RStudio and R’s largest open-source contributor1\nYou can install the complete tidyverse using:\n\n\ninstall.packages(\"tidyverse\")\n\n\nTo load tidyverse in your session, simply run:\n\n\nlibrary(tidyverse)\n\n\n\n\n\n\n\n\n\nPosit recently hired Wes McKinney, the creator of pandas, highlighting its efforts to harmonize innovations among Python users."
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-dplyr",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-dplyr",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: dplyr",
    "text": "The tidyverse packages: dplyr\n\n\ndplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges:\n\n\n\n\n\n\nmutate() adds new variables that are functions of existing variables\nselect() picks variables based on their names\nfilter() picks cases based on their values\nsummarise() reduces multiple values down to a single summary\narrange() changes the ordering of the rows\n\n\nKey Highlights\n\nThese all combine with group_by(), allowing users to perform operations groupwise\nLazy evaluation methods, as well as the pipe operator, %&gt;%, increases code readability and reproducibility"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#using-dplyr",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#using-dplyr",
    "title": "Bridging Finance with Programming",
    "section": "Using dplyr",
    "text": "Using dplyr"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-ggplot2",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-ggplot2",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: ggplot2",
    "text": "The tidyverse packages: ggplot2\n\n\nThe core tidyverse includes the packages that you’re likely to use in everyday data analyses. As of its 1.3.0 version, the following packages are included in the core tidyverse:\n\n\n\n\n\n\nggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics\nYou provide the data, tell how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details\n\n\nKey Highlights\n\nIt is, by and large, the richest and most widely used plotting ecosystem in the  language\nggplot2 has a rich ecosystem of extensions - ranging from annotations and interactive visualizations to specialized genomics - click here a community maintained list"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#using-ggplot2",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#using-ggplot2",
    "title": "Bridging Finance with Programming",
    "section": "Using ggplot2",
    "text": "Using ggplot2"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-tidyr",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-tidyr",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: tidyr",
    "text": "The tidyverse packages: tidyr\n\n\nThe goal of tidyr is to help you create tidy data. Tidy data is data where:\n\n\n\n\n\n\nEach variable is a column; each column is a variable\nEach observation is a row; each row is an observation\nEach value is a cell; each cell is a single value\n\n\nKey Highlights\n\nTidy data describes a standard way of storing data that is used wherever possible throughout the tidyverse\nIt makes it easier to put reshape data in a way that it can be used as an input to other tidyverse packages"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#using-tidyr",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#using-tidyr",
    "title": "Bridging Finance with Programming",
    "section": "Using tidyr",
    "text": "Using tidyr\n\n\n\n\n\n\n\n\n\n\n Download raw data"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#accessing-and-managing-financial-data-1",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#accessing-and-managing-financial-data-1",
    "title": "Bridging Finance with Programming",
    "section": "Accessing and Managing Financial Data",
    "text": "Accessing and Managing Financial Data\n\nEverybody who has experience working with data is also familiar with storing and reading data in formats like .csv, .xls, .xlsx or other delimited value storage\nHowever, if your goal is to replicate a common task at a predefined time interval, like charting weekly stock prices for a selected bundle of stocks every end-of-week, it might be overwhelming to manually perform these tasks every week\nIn what follows, we’ll dive in the various sources of financial data - both global as well as specific to the Brazilian financial markets that can be directly fed into your R session:\n\nWe will cover the most widely used free data resources for Finance, like Yahoo! Finance\nWe will also discuss linkages to private information sources, such as Bloomberg\nFinally, we will take a look at some output data examples from some data providers"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-basics-stock-level-information",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-basics-stock-level-information",
    "title": "Bridging Finance with Programming",
    "section": "The basics: stock-level information",
    "text": "The basics: stock-level information\n\nSo… you have been prompted with the task of collecting daily stock price information for a subset of the U.S Big Techs. How should you do it?\nIn a nutshell, Yahoo! Finance is your go-to guy:\n\nIt provides financial news, data and commentary including stock quotes, press releases, financial reports, and original content\nIt has an extensive list of open-source solutions that enables users to retrieve financial information using several coding languages\n\nHighlights: free, quick and easy to setup, with an impressive range of data containing stock prices, dividends, and splits. There is an extensive list of R packages can be used to retrieve Yahoo! Finance information - including, but not limited to, tidyquant, quantmod and yfR\nDrawbacks: its API is no longer a fully official API: as a consequence, solutions tipically used to retrieve information may not work in the future if Yahoo Finance change its structure. Importantly, data is not in real-time - often, it comes with a 15-minute delay (see here)"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#using-yahoo-finance-continued",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#using-yahoo-finance-continued",
    "title": "Bridging Finance with Programming",
    "section": "Using Yahoo! Finance, continued",
    "text": "Using Yahoo! Finance, continued\n\n\nBelow, you can find an example of how to use tq_get(), from the tidyquant package, to download both single and multiple stock price information\nData is stored in a convenient way that allows users to manipulate data seamlessly - hit Download Data and see how the output would look like in Excel format\n\n\n#Load tidyquant\nlibrary(tidyquant)\n\n#Using tidyquant to download single stock prices\ntq_get('AAPL',from='2020-01-01',to='2024-12-31')\n\n#Using tidyquant to download multiple stock prices\ntq_get(c('AAPL','GOOGL','NVDA'),from='2020-01-01',to='2024-12-31')\n\n\n\n Download Data\n\n\n\n\n\n\n\n\nImportant\n\n\nYahoo! Finance provides Open, High, Low, Close, and Adjusted Close trading prices for each asset that is being tracked, where Adjusted Close is defined by the closing price adjusted for dividends and stock splits. If you are using R, Python, or any API to pull this data, ensure to use the information adjusted by dividends and splits."
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#macroeconomic-data-providers",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#macroeconomic-data-providers",
    "title": "Bridging Finance with Programming",
    "section": "Macroeconomic data providers",
    "text": "Macroeconomic data providers\nApart from price-level information, there are plenty of available resources to efficiently download the most commonly used macroeconomic variables directly within an R session:\n\nThe Federal Reserve Bank of St. Louis has as extense set of U.S and international time series from more than 100 sources via its API, FRED, for free\n\n\\(\\rightarrow\\) Related packages: tidyquant, FredR, quantmod, and quandl\n\nThe World Bank’s International Debt Statistics (IDS) provides creditor-debtor relationships between countries, regions, and institutions\n\n\\(\\rightarrow\\) Related packages: wbids\n\nThe European Central Bank’s Statistical Data Warehouse provides data on Euro area monetary policy, financial stability, and other relevant topics\n\n\\(\\rightarrow\\) Related packages: ecb"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#macroeconomic-data-providers-examples",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#macroeconomic-data-providers-examples",
    "title": "Bridging Finance with Programming",
    "section": "Macroeconomic data providers, examples",
    "text": "Macroeconomic data providers, examples\n\nFREDIBSECB\n\n\n\n#Load the tidyquant library\nlibrary(tidyquant)\n\n#Go to FRED's website, search for a time series, and copy-paste its code\nseries='CUSR0000SETB01'\n\n#Use the tq_get() function to retrieve the information\ntq_get(series,get='economic.data')\n\n\n\n Download Data\n\n\n\\(\\rightarrow\\) For full details and implementation of the R package tidyquant, click here\n\n\n\n#Load the wbids package\nlibrary(wbids)\n\n#Get information for Brasil, Russia, \nids_get(\n  geographies = c(\"BRA\", \"ARG\"),\n  series = c(\"DT.MAT.DPPG\"), #Average maturity on new external debt commitments (years)\n  counterparts = c(\"302\"), #United States\n  start_year = 2000,\n  end_year = 2023\n)\n\n\n\n Download Data\n\n\n\\(\\rightarrow\\) For full details and implementation of the R package wbids, click here\n\n\n\n#Load the ecb package\nlibrary(ecb)\n\n#Get information of headline and core inflation for Eurozone countries\nkey &lt;- \"ICP.M.DE+FR+ES+IT+NL+U2.N.000000+XEF000.4.ANR\"\n\n#Get the latest 12 observations\nfilter &lt;- list(lastNObservations = 12, detail = \"full\")\n\n#Retrieve the data\nhicp &lt;- get_data(key, filter)\n\n#Parse time component to proper format\nhicp$obstime &lt;- convert_dates(hicp$obstime)\n\n\n\n Download Data\n\n\n\\(\\rightarrow\\) For full details and implementation of the R package ecb, click here"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#financial-data-providers",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#financial-data-providers",
    "title": "Bridging Finance with Programming",
    "section": "Financial data providers",
    "text": "Financial data providers\n\nFor some widely known paid data providers, there are interfaces that enable analysts to retrieve information directly within an R session through the provider’s official API1\n\n\nBloomberg: the Rblpapi provides access to data and calculations from Bloomberg\nRefinitiv Eikon: the DatastreamDSWS2R provides a set of functions and a class to connect, extract and upload information from the LSEG Datastream database\nQuandl: publishes free/paid data, scraped from many different sources from the web. The Quandl package can be used to retrieve data\nSimfin: fundamental financial data freely available to private investors, researchers, and students. The simfinapi package can be used to retrieve data\nFMP: accurate financial data (balance-sheet, income statements, etc), with historical information dating back 30 years in history. The fmpapi package can be used to retrieve data\n\nFor paid data providers, you must provide your API key in order to successfully download data."
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#other-data-providers-and-r-packages",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#other-data-providers-and-r-packages",
    "title": "Bridging Finance with Programming",
    "section": "Other data providers (and R packages)",
    "text": "Other data providers (and R packages)\n\nBanco Central do Brasil (BACEN): interface to the Brazilian Central Bank web services - see package rbcb\nTesouro Direto (Brazilian Government Bonds): prices and yields of bonds issued by the Brazilian government - see package GetTDData\nCoinMarketCap: provides cryptocurrency information and historical prices - see package crypto2\nAlpha Vantage: free and paid subscriptions for financial data (including intraday) - see package alphavantager\n\n\n\n\n\n\n\nWrapping up on data providers\n\n\nWhile some data providers provide their official API for developers, other solutions rely on scraping historical data from the web. As such, some solutions can deprecated after some time if, for example, access is blocked. It is always important to check whether an R package is not deprecated by looking into the Comprehensive R Archive Network (CRAN) or its GitHub repository."
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-purrr",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-purrr",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: purrr",
    "text": "The tidyverse packages: purrr\n\n\nThe goal of purrr is to enhances R’s functional programming toolkit by providing a complete and consistent set of tools for working with functions and vectors\n\n\n\n\n\n\nFunctional programming allows you to replace many for loops with code that is both more succinct and easier to read\nYou provide a function and a list of elements to map to, and purrr takes care of the nitty-gritty details\n\n\nKey Highlights\n\nIt seamlessly integrates with all tidyverse packages and functions, allowing users to apply functional programming in the most straightforward way possible\nSimplifies the code pipeline to solve fairly realistic problems - e.g, estimating the CAPM for 100+ industries where we have a different number of observations per industry"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-readr",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-readr",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: readr",
    "text": "The tidyverse packages: readr\n\n\nThe goal of readr is to provide a fast and friendly way to read rectangular data from delimited files, such as comma-separated values (.csv) and tab-separated values (.tsv)\n\n\n\n\n\n\nIt is designed to parse many types of data found in the wild, while providing an informative problem report when parsing leads to unexpected results\nHandles column-type guessing, allowing users to specify how it should parse information, providing informative problem reports when parsing leads to unexpected results\n\n\nKey Highlights\n\nIs generally much faster than base R functions (up to 10x-100x), depending on the dataset\nAll functions work exactly the same way regardless of the current locale (e.g., thousands and decimal separators)"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-tibble",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-tibble",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: tibble",
    "text": "The tidyverse packages: tibble\n\n\nThe tibble package provides a modern reimagining of a data.frame, keeping what time has proven to be effective, and throwing out what is not\n\n\n\n\n\n\nTibbles are a modern take on data frames. They keep the features that have stood the test of time, and drop the features that used to be convenient but are now frustrating\nIt is a nice way to create data frames. It encapsulates best practices for data frames and handles various data formats in an easier way\n\n\nKey Highlights\n\nTibbles also have an enhanced print() method which makes them easier to use with large datasets containing complex objects.\nIt can store various data formats in a data-frame-like format (e.g, store a whole list as a column)"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-stringr",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-stringr",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: stringr",
    "text": "The tidyverse packages: stringr\n\n\nThe stringr package provides a cohesive set of functions designed to make working with strings (e.g, qualitative data, such as stock tickers, names, etc) as easy as possible:\n\n\n\n\n\n\nstr_detect() tells you if there’s any match to the pattern\nstr_locate() gives the position of the match\nstr_count() counts the number of pattern\nstr_subset() extracts the matching components\nstr_extract() extracts the text of the match\nstr_match() extracts parts of the match defined by parentheses\nstr_replace() replaces the matches with new text\nstr_split() splits up a string into multiple pieces"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-forcats",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-forcats",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: forcats",
    "text": "The tidyverse packages: forcats\n\n\nThe goal of the forcats package is to provide a suite of tools that solve common problems with factors, variables that have a fixed and known set of possible values (e.g, a vector that contains all possible days in a week)\n\n\n\n\n\n\nfct_reorder() reorders a factor by another variable\nfct_infreq() reorders a factor by the frequency of values\nfct_relevel() changes the order of a factor by hand\nfct_lump() collapses the least/most frequent values of a factor into a consolidated group\n\n\nKey Highlights\n\nWorking with factors makes it easier to display, visualize, and communicate data\nExplicitly defining a variable as a factor handles several issues regarding inserting new data"
  },
  {
    "objectID": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#references",
    "href": "quant-fin/Lecture 1 - Bridging Finance with Programming/index.html#references",
    "title": "Bridging Finance with Programming",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lecture Notes and Handouts",
    "section": "",
    "text": "Within this section, you’ll find an organized repository containing my lecture notes from FGV-EAESP, technical handouts, and illustrative code samples. These resources extensively leverage Quarto, an open-source scientific and technical publishing system that supports R, Python, Julia, and Observable. Quarto offers a seamless and scalable solution for sharing technical content, encompassing a blend of plain text and executable code, being applicable to static document formats like .pdf files and dynamic, interactive presentations facilitated by Reveal JS.\nFor comprehensive guidance on harnessing Quarto for presentations and technical content in a standardized and comprehensive manner, please refer to the official reference material.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinancial Management\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinancial Strategy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Applications in Quantitative Finance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValuation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#outline",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#outline",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\n(Berk and DeMarzo 2023)\n(Brealey, Myers, and Allen 2020)\n\nStudy review and practice: I strongly recommend using Prof. Henrique Castro (FGV-EAESP) materials. Below you can find the links to the corresponding exercises related to this lecture:\n\nMultiple Choice Exercises - click here\n\n\n\n\\(\\rightarrow\\) For coding replications, whenever applicable, please follow this page or hover on the specific slides with coding chunks."
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#default-and-bankruptcy-in-perfect-markets",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#default-and-bankruptcy-in-perfect-markets",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Default and Bankruptcy in Perfect Markets",
    "text": "Default and Bankruptcy in Perfect Markets\n\nAn important consequence of leverage is the risk of bankruptcy:\n\nAlthough equityholders hope to receive dividends, the firm is not legally obligated to pay\nOn the other hand, debtholders have an enforceable claim over the firms cash flow and may require the payment regardless of the state of the economy\n\nWhen a firm has difficulty meeting its debt obligations, we say that it is in Financial Distress:\n\nThere can be delays in payments from principal and interest amounts\nWhen a firm fails to make the required interest or principal payments on its debt or violates conditions established on its debt contracts, we say that a firm is in default\n\n\n\n\\(\\rightarrow\\) After the firm defaults, debtholders are given certain rights to the assets of the firm and may even take legal ownership of the firm’s assets through bankruptcy"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#default-and-bankruptcy-in-perfect-markets-1",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#default-and-bankruptcy-in-perfect-markets-1",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Default and Bankruptcy in Perfect Markets",
    "text": "Default and Bankruptcy in Perfect Markets\n\nWhy is bankruptcy and default so important? In order to see that, consider that Armin, a firm that is considering a new project. Although the new product represents a significant advance over the firms’ competitors’ products, the product’s success is uncertain:\n\nIf it succeeds, Armin will be worth \\(\\small \\$150\\) million at the end of the year\nIf it fails, Armin will be worth only \\(\\small \\$80\\) million\n\n\n\n\nAs in previous lectures, Armin may employ one of two alternative capital structures to integrate this project:\n\nIt can use all-equity financing\nIt can use debt that matures at the end of the year with a total of \\(\\small \\$100\\) million due\n\n\n\n\n\\(\\rightarrow\\) The next slides will show how the funding structure affect the value of Armin in both states of the economy"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#default-and-bankruptcy-in-perfect-markets-2",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#default-and-bankruptcy-in-perfect-markets-2",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Default and Bankruptcy in Perfect Markets",
    "text": "Default and Bankruptcy in Perfect Markets\n\n\n\n\n\n\n\n\n\n\n\nAll-Equity\nAll-Equity\nLeverage\nLeverage\n\n\n\n\n\nSuccess\nFailure\nSuccess\nFailure\n\n\nDebt Value\n-\n-\n$100\n$80\n\n\nEquity Value\n$150\n$80\n$50\n$0\n\n\nAll Investors\n$150\n$80\n$150\n$80\n\n\n\n\n\n\n\n\n\n\n\n\nBoth debt and equity holders are worse off if the product fails rather than succeeds:\n\nWithout leverage, equity holders lose \\(\\small (\\$150-\\$80)=\\$70\\)\nWith leverage, equity holders lose \\(\\small \\$50\\) million, and debt holders lose \\(\\small \\$20\\) million, but the total loss is the same, \\(\\small \\$70\\) million\n\nImportantly, since the firms is worth \\(\\small \\$80\\) and debt is \\(\\small \\$100\\), the firm is in default if the project fails with leverage!"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#default-and-bankruptcy-in-perfect-markets-3",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#default-and-bankruptcy-in-perfect-markets-3",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Default and Bankruptcy in Perfect Markets",
    "text": "Default and Bankruptcy in Perfect Markets\n\nNote that the decline in value is not caused by bankruptcy, as such decline is the same whether or not the firm has leverage. As a consequence, the value of the firm is essentially the same. Assuming that both states are equally likely and a \\(\\small5\\%\\) discount rate:\n\n\n\\[\n\\small V^U= \\dfrac{\\dfrac{1}{2}\\times 150 + \\dfrac{1}{2}\\times80}{(1+5\\%)}=109.52\\\\\n\\]\n\\[\n\\small V^L= \\dfrac{\\overbrace{\\dfrac{1}{2}\\times 100 + \\dfrac{1}{2}\\times80}^{\\text{Debtholders}}+ \\overbrace{\\dfrac{1}{2}\\times 50 + \\dfrac{1}{2}\\times0}^{\\text{Equityholders}}}{(1+5\\%)}=\\dfrac{115}{(1+5\\%)}=109.52\n\\]\n\n\n\\(\\rightarrow\\) With perfect capital markets, Modigliani and Miller (MM) Proposition I applies: the total value to all investors does not depend on the firm’s capital structure"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#bankruptcy-and-financial-distress-costs",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#bankruptcy-and-financial-distress-costs",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Bankruptcy and Financial Distress Costs",
    "text": "Bankruptcy and Financial Distress Costs\n\nWith perfect capital markets, we saw that the risk of bankruptcy is not a disadvantage of debt:\n\nThe value of the levered firm, \\(\\small V^L\\), is exactly the same as of the unlevered one, \\(\\small V^U\\)\nRather, bankruptcy shifts the ownership of the firm from equityholders to debtholders without changing the total value available to all investors\n\nIn reality, however, a bankruptcy process is a complex, time-consuming, and costly process:\n\nCostly outside experts are often hired by the firm to assist with the bankruptcy.\nCreditors also incur similar costs during the bankruptcy process: they may wait several years to receive payment, and may hire their own experts for legal and professional advice\n\nWe refer to these costs as the direct costs of bankruptcy, as they reduce the value of the assets that the firm’s investors will ultimately receive: it is estimated that the average direct costs of bankruptcy are approximately \\(\\small 3\\%-4\\%\\) of the pre-bankruptcy market value of total assets"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#bankruptcy-and-financial-distress-costs-1",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#bankruptcy-and-financial-distress-costs-1",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Bankruptcy and Financial Distress Costs",
    "text": "Bankruptcy and Financial Distress Costs\n\nOn top of the direct costs associated due bankruptcy and financial distress, a firm can also suffer from the indirect costs of bankruptcy\n\nLoss of Customers\nLoss of Suppliers\nLoss of Employees\nLoss of Receivables\nFire Sale of Assets\nDelayed Liquidation\nCosts to Creditors\n\nDue to its nature, although indirect costs are difficult to measure accurately, they are often much larger than the direct costs of bankruptcy\nThe estimated potential loss due to financial distress is around \\(\\small 10\\%-20\\%\\) of firm value"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#accounting-for-default-and-bankruptcy-costs",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#accounting-for-default-and-bankruptcy-costs",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Accounting for Default and Bankruptcy Costs",
    "text": "Accounting for Default and Bankruptcy Costs\n\nLet’s go back to our previous example on Armin. How do bankruptcy costs fit into that situation?\n\nWith all-equity financing, the firms’ assets will be worth \\(\\small\\$150\\) million if its new product succeeds, and \\(\\small\\$80\\) million if the new product fails\nHowever, we have assumed that there weren’t any financial distress costs!\n\nSay that, after some calculations, Armin has estimated financial distress costs of \\(\\small \\$20\\) million\n\nWith debt outstanding worth \\(\\small\\$100\\) million, Armin will be forced into bankruptcy if the new product fails!\nIn this case, some of the value of the firms’ assets will be lost to bankruptcy and financial distress costs\n\n\n\n\\(\\rightarrow\\) As a result, debtholders will receive less than 80 million!"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#accounting-for-default-and-bankruptcy-costs-continued",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#accounting-for-default-and-bankruptcy-costs-continued",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Accounting for Default and Bankruptcy Costs, continued",
    "text": "Accounting for Default and Bankruptcy Costs, continued\n\nIn this case, debtholders receive only \\(\\small \\$80-\\$20=\\$60\\) million after accounting for the costs of financial distress\nThese costs will lower the total value of the firm with leverage, \\(\\small V^L\\) and Modigliani and Miller’s Proposition I will no longer hold: the total value to all investors is less with leverage than it is without leverage when the new product fails!\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll-Equity\nAll-Equity\nWith Leverage\nWith Leverage\n\n\n\n\n\nSuccess\nFail\nSuccess\nFail\n\n\nDebt Value\n-\n-\n$100\n$60\n\n\nEquity Value\n$150\n$80\n$50\n$0\n\n\nAll Investors\n$150\n$80\n$150\n$60"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#firm-value-under-bankruptcy-costs",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#firm-value-under-bankruptcy-costs",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Firm value under Bankruptcy Costs",
    "text": "Firm value under Bankruptcy Costs\n\nWe can now recalculate the firm’s value under the assumption that there are direct and/or indirect costs of bankruptcy:\n\n\n\\[\n\\small\nV^U = \\dfrac{\\dfrac{1}{2} \\times 150 + \\dfrac{1}{2} \\times 80}{(1+5\\%)}= \\dfrac{115}{(1+5\\%)}= 109.52\\\\\n\\]\n\\[\n\\small V^L= \\dfrac{\\overbrace{\\dfrac{1}{2} \\times 100+\\dfrac{1}{2}\\times60}^{\\text{Debtholders}}+\\overbrace{\\dfrac{1}{2} \\times 50+\\dfrac{1}{2}\\times 0 }^{\\text{Equityholders}}}{(1+5\\%)}= \\dfrac{105}{(1+5\\%)}=100\n\\]\n\nThe difference between \\(\\small V^U\\) and \\(\\small V^L\\), \\(\\small\\$109.52 − \\$100 = \\$9.52\\) million is the loss in value due to financial distress. These costs will lower the total value of the firm with leverage, \\(\\small V^L\\), and Modigliani and Miller’s Proposition I will no longer hold!"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#who-pays-for-financial-distress-costs",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#who-pays-for-financial-distress-costs",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Who pays for Financial Distress Costs?",
    "text": "Who pays for Financial Distress Costs?\n\nWe saw that the value of the levered firm, \\(\\small V^L\\), was lower due to the financial distress costs. An important question regarding financial distress costs is who ultimately bears it:\n\nFor Armin, if the new product fails, equityholders lose their investment in the firm and will not care about bankruptcy costs ex-ante\nHowever, debtholders recognize that if the new product fails and the firm defaults, they will not be able to get the full value of the assets\n\nAs a result, debtholders will require to pay less for the debt:\n\nIf the debt holders initially pay less for the debt, less money is available for the firm to pay dividends, repurchase shares, and make investments.\nUltimately, this difference comes out of the equity holders’ pockets!\n\n\n\n\\(\\rightarrow\\) When securities are fairly priced, the original shareholders of a firm pay the present value of the costs associated with bankruptcy and financial distress!"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#who-pays-for-financial-distress-costs-continued",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#who-pays-for-financial-distress-costs-continued",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Who pays for Financial Distress Costs? Continued",
    "text": "Who pays for Financial Distress Costs? Continued\n\nIf debtholders pay less for the debt initially, by how much precisely is this difference? To see that, that at the beginning of the year, Armin Industries has \\(\\small 10\\) million shares outstanding and no debt. Armin then announces plans to issue one-year debt with a face value of \\(\\small \\$100\\) million and to use the proceeds to repurchase shares\n\nAs shown before, the value of the unlevered firm, \\(\\small V^U\\), is \\(\\small \\$109.52\\) million\nWith \\(\\small 10\\) million shares outstanding, this corresponds to a share price of \\(\\small \\$10.952\\) per share before any recapitalization\nIn anticipation of the recap, the total value of the firm drops to \\(\\small \\$100\\) million, and the price per share is now only \\(\\small \\$10\\) on the announcement\n\nDue to financial distress costs, the present value of debt is:\n\n\n\\[\nD=\\small \\dfrac{\\small 1/2\\times100+1/2\\times 60}{(1+5\\%)}=76.19 \\text{ million}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#who-pays-for-financial-distress-costs-continued-1",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#who-pays-for-financial-distress-costs-continued-1",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Who pays for Financial Distress Costs? Continued",
    "text": "Who pays for Financial Distress Costs? Continued\n\nFrom the data in the previous slide, Armin will be able to issue \\(\\small\\$76.19\\), instead of \\(\\small \\$80\\), and this translates to buying \\(\\small 7,619\\) shares, leaving \\(\\small 2,381\\) outstanding shares\nAs the value of the levered equity is \\(\\small \\$23.81\\) million, the share price is now only \\(\\$10\\) per share. All in all, all shareholders1 are worse off by:\n\n\n\\[\n\\small \\underbrace{(10.952-10.00)}_{\\text{Unlevered - Levered Equity}}\\times \\underbrace{10,000,000}_{\\text{Shares outstanding}}=9.52 \\text{ million}\n\\]\n\nThis difference in value is exactly the decrease in firm value due to financial distress costs!\n\n\n\n\\(\\rightarrow\\) Although debt holders bear these costs in the end, shareholders pay the present value of the costs of financial distress up front, and ultimately born the costs of financial distress!\n\nBoth shareholders that have sold as well as the ones that did not lose this value."
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#reconciling-financial-distress-with-capital-structure",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#reconciling-financial-distress-with-capital-structure",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Reconciling Financial Distress with Capital Structure",
    "text": "Reconciling Financial Distress with Capital Structure\n\nLet’s take a step back and recall what we have defined in earlier classes:\n\nUnder perfect capital markets, the value of the levered firm, \\(\\small V^L\\), was identical to the value of an all-equity firm, \\(\\small V^U\\)\nWhen we assume that there are tax imperfections, such as the fact that interest expenses are tax-deductible, we saw that the value levered firm could increase by the same amount as of the present value of the tax-shiedls:\n\n\n\n\\[\n\\small  V^L= V^U+PV(ITS)\n\\]\n\nIf that is true, then why firms do not always have high leverage? As it turns out, if we assume the existence of financial distress costs, increased leverage may get to a point where the benefits of debt are fully offset by the costs of financial distress!"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-trade-off-theory",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-trade-off-theory",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "The Trade-off Theory",
    "text": "The Trade-off Theory\n\n\n\n\n\n\nDefinition\n\n\nThe Trade-off Theory states that the firm picks its capital structure by trading off the benefits of the tax shield from debt against the costs of financial distress and agency costs.\nAccording to the theory, the total value of a levered firm, \\(\\small V^L\\) equals the value of the firm without leverage plus the present value of the tax savings from debt, less the present value of financial distress costs:\n\\[\nV^L = V^U + PV(ITS) - PV(\\text{Financial Distress Costs})\n\\]\n\n\n\n\nThere are basically three key factors determine the present value of financial distress costs:\n\nThe probability of financial distress\nThe magnitude of the costs after a firm is in distress\nThe appropriate discount rate for the distress costs"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#factors-that-determine-the-value-of-distress-costs",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#factors-that-determine-the-value-of-distress-costs",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Factors that determine the value of distress costs",
    "text": "Factors that determine the value of distress costs\n\nThere are basically three key factors determine the present value of financial distress costs:\n\nThe probability of financial distress\n\nThe probability of financial distress increases with the amount of a firm’s liabilities (relative to its assets)\nIt also increases with the volatility of a firm’s cash flows and asset values\n\nThe magnitude of the costs after a firm is in distress\n\nFinancial distress costs will vary by industry, and depend on factors such as asset redeployability\n\nThe appropriate discount rate for the distress costs\n\nDepends on the firm’s market risk\nThe present value of distress costs will be higher for high beta firms"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#implications-of-the-the-trade-off-theory",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#implications-of-the-the-trade-off-theory",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Implications of the the Trade-off Theory",
    "text": "Implications of the the Trade-off Theory\n\nAn implication of the Trade-Off Theory is that each firm will have an optimal leverage level:\n\nFor low levels of debt, the risk of default remains low, and the main effect of an increase in leverage is an increase in the interest tax shield\nAs the level of debt increases, the probability of default increases. Then, the costs of financial distress increase, reducing the value of the levered firm\n\n\n\n\nThe Trade-off Theory states that firms should increase their leverage until it reaches the level for which the firm value is maximized - in order words, the capital structure choice in terms of % of debt and equity is chosen so as solve:\n\n\n\n\\[\n\\small \\max V^L_{\\{\\%E,\\%D\\}} =  V^U + PV(ITS) - PV(\\text{Financial Distress Costs})\n\\]\n\nAt the optimal, the tax savings that result from increasing leverage are perfectly offset by the increased probability of incurring the costs of financial distress"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-trade-off-theory-and-optimal-leverage",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-trade-off-theory-and-optimal-leverage",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "The Trade-off Theory and Optimal Leverage",
    "text": "The Trade-off Theory and Optimal Leverage"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-trade-off-theory-practice",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-trade-off-theory-practice",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "The Trade-off Theory, practice",
    "text": "The Trade-off Theory, practice\nHolland is considering adding leverage to its capital structure. Holland’s managers believe they can add as much as \\(\\small \\$50\\) million in debt and exploit the benefits of the tax shield. They estimate that the corporate tax rate is \\(\\tau_c = 39\\%\\). However, they also recognize that higher debt increases the risk of financial distress. Based on simulations of the firm’s future cash flows, the CFO has made the following estimates (in millions of dollars):\n\n\n\n\n\n\n\n\n\n\n\n\n\nDebt-to-Value\n0%\n10%\n20%\n30%\n40%\n50%\n\n\n\n\nPV(Int. Tax. Shield)\n-\n$3.9\n$7.8\n$11.7\n$15.6\n$19.5\n\n\nPV(Fin. Dis. Costs)\n-\n$0\n$0\n$3.38\n$19.23\n$23.47\n\n\n\nQuestion: what of debt should the firm choose?"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-trade-off-theory-practice-1",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-trade-off-theory-practice-1",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "The Trade-off Theory, practice",
    "text": "The Trade-off Theory, practice\nHolland is considering adding leverage to its capital structure. Holland’s managers believe they can add as much as \\(\\small \\$50\\) million in debt and exploit the benefits of the tax shield. They estimate that the corporate tax rate is \\(\\tau_c = 39\\%\\). However, they also recognize that higher debt increases the risk of financial distress. Based on simulations of the firm’s future cash flows, the CFO has made the following estimates (in millions of dollars):\n\n\n\n\n\n\n\n\n\n\n\n\n\nDebt-to-Value\n0%\n10%\n20%\n30%\n40%\n50%\n\n\n\n\nPV(Int. Tax. Shield)\n-\n$3.9\n$7.8\n$11.7\n$15.6\n$19.5\n\n\nPV(Fin. Dis. Costs)\n-\n$0\n$0\n$3.38\n$19.23\n$23.47\n\n\nNet Benefit\n-\n$3.9\n$7.8\n$8.32\n$-3.63\n$-3.97\n\n\n\n\n\n\\(\\rightarrow\\) Solution: the level of debt that leads to the highest net benefit is 30 million. Holland will gain \\(\\small\\$11.7\\) million due to tax shields and lose \\(\\small \\$3.38\\) million due to the present value of financial distress costs, for a net gain of \\(\\small \\$8.32\\) million."
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-agency-costs-of-leverage",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-agency-costs-of-leverage",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "The Agency Costs of Leverage",
    "text": "The Agency Costs of Leverage\n\nApart from any direct costs and benefits from increased leverage, debtholders may also be concerned around which managerial actions will be taken after debt is in place - and whether it will ultimately lead to increasing indirect costs to them\nThe potential costs due to misalignment between different stakholders of the firm are called Agency Costs\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nAgency Costs are costs that arise when there are conflicts of interest between the firm’s stakeholders:\n\nManagement will generally make decisions that increase the value of the firm’s equity\nHowever, when a firm has leverage, managers may make decisions that benefit shareholders but harm the firm’s creditors and lower the total value of the firm"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-agency-costs-of-leverage-continued",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-agency-costs-of-leverage-continued",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "The Agency Costs of Leverage, continued",
    "text": "The Agency Costs of Leverage, continued\n\nAlthough not directly measurable, Agency Costs can vary significantly and have a substantial impact on the value of the levered firm\nUnder Modigliani and Miller’s hypotheses for Perfect Capital Markets, there are no Agency Costs - this is embedded in the assumption that there are no transaction costs and/or information asymmetry\nExamples of managerial actions that are supposedly costly for debtholders are:\n\nRisk-taking & Asset Substitution\nDebt overhang & Under-investment\nCashing out\nLeverage Ratchet effect"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#risk-taking-asset-substitution",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#risk-taking-asset-substitution",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Risk-taking & Asset Substitution",
    "text": "Risk-taking & Asset Substitution\n\n\n\n\n\n\nDefinition\n\n\nWhen a firm faces financial distress, shareholders can gain at the expense of debt holders by taking a negative-NPV project if it is sufficiently risky - also called risk-shifting problem\n\n\n\n\nConsider that Baxter is facing a situation of Financial Distress:\n\nIt has a loan of \\(\\small \\$1\\) million due at the end of the year\nWithout a change in its strategy, the market value of its assets will be only \\(\\small \\$900,000\\) at that time, and the firm will default on its debt\n\nBaxter’s manager is considering a new strategy that requires no upfront investment, but it has only a \\(\\small50\\%\\) chance of success:\n\nIf the new strategy succeeds, it will increase the value of the firm’s asset to \\(\\small\\$1.3\\) million\nIf the new strategy fails, the value of the firm’s assets will fall to \\(\\small\\$300,000\\)"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#risk-taking",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#risk-taking",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Risk-taking",
    "text": "Risk-taking\nQuestion: should Baxter execute this strategy?\n\n\nTo see that, note that the expected value of the firm’s assets under the new strategy is \\(\\small \\$800,000\\), a decline of \\(\\small\\$100,000\\) from the old strategy:\n\n\n\n\\[\n\\small V^L_{\\text{New}}=\\small 50\\% \\times 1.3 + 50\\% \\times 0.3 = 0.8 \\text{ million}\n\\]\n\nWhy managers would even consider this strategy? It is important to note that, if Baxter does nothing, it will ultimately default and equity holders will get nothing with certainty:\n\nEquity holders have nothing to lose if Baxter tries the risky strategy\nIf the strategy succeeds, equity holders will receive \\(\\small\\$300,000\\) after paying off the debt\nGiven a \\(\\small50\\%\\) chance of success, the equityholders’ expected payoff is positive:\n\n\n\n\n\\[\n\\small E^L_{\\text{New}}=50\\% \\times 0 + 50\\% \\times 300,000 = 150,000\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#risk-taking-continued",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#risk-taking-continued",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Risk-taking, continued",
    "text": "Risk-taking, continued\n\n\n\n\n\n\n\n\n\n\n\n\nOld Strategy\nNew - Success\nNew - Failure\nNew - Exp.\n\n\n\n\n\\(V^L\\)\n$900\n$1,300\n$300\n$800\n\n\nDebt\n$900\n$1,000\n$300\n$650\n\n\nEquity\n$0\n$300\n$0\n$150\n\n\n\n\nNote that, in such a way, equityholders are gambling with the debt holders’ money:\n\nIf project succeeds, debtholders receive \\(\\small \\$1\\) million\nIf fails, deb holders receive \\(\\small\\$300,000\\)\n\nDebtholders’ expected payoff is \\(\\small\\$650,000\\), a loss of \\(\\small\\$250,000\\) compared to the old strategy:\n\n\n\n\\[\n\\small 50\\% \\times 1,000,000 + 50\\% \\times 300,000 = 650,000 \\text{ million}\n\\]\n\n\n\\(\\rightarrow\\) This loss corresponds to the 100,000 expected decline in firm value due to the risky strategy and the equityholder’s 150,000 gain"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#asset-substitution",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#asset-substitution",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Asset Substitution",
    "text": "Asset Substitution\n\nAs we saw before, if managers are acting on behalf of shareholders, they may choose to gamble with debtholder’s money by choosing projects that with asymmetrical payoffs (in expectation, benefits shareholders at the expense of debtholders)\nA way to materialize this is to choose projects that are risky enough so that shareholders are better-off if the project is succesfull:\n\n\nShareholders have an incentive to invest in negative-NPV projects that are risky, even though a negative-NPV project destroys value for the firm overall, but can bail-out shareholders in case of success\nAnticipating this bad behavior, security holders will pay less for the firm ex-ante"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#debt-overhang-and-under-investment",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#debt-overhang-and-under-investment",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Debt Overhang and Under-Investment",
    "text": "Debt Overhang and Under-Investment\n\nThe previous example has shown how potential a misalignment between shareholders and debtholders may result in a move towards more risky projects\nWhat if there are actually projects less risky projects that would otherwise increase the firm’s NPV?\n\n\n\n\n\n\n\n\nDefinition\n\n\nDebt Overhang (or simply Under-Investment) is a situation in which equity holders choose not to invest in a positive NPV project because the firm is in financial distress and the value of undertaking the investment opportunity will accrue to bondholders rather than themselves\n\n\n\n\\(\\rightarrow\\) As a result, when a firm faces financial distress, it may choose not to finance new, positive-NPV projects!"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#debt-overhang-and-under-investment-continued",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#debt-overhang-and-under-investment-continued",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Debt Overhang and Under-Investment, continued",
    "text": "Debt Overhang and Under-Investment, continued\n\nAssume that Baxter does not pursue the risky strategy, but instead the firm is considering an investment opportunity that requires an initial investment of \\(\\small\\$100,000\\) and will generate a risk-free return of \\(\\small50\\%\\)\nIf the current risk-free rate is \\(\\small5\\%\\), this investment clearly has a positive NPV\n\nWhat if Baxter does not have the cash on hand to make the investment?\nCould Baxter raise \\(\\small\\$100,000\\) in new equity to make the investment?\n\n\n\nQuestion: would managers, acting on behalf of the shareholders, be willing to raise funding and invest in this project?"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#debt-overhang-and-under-investment-continued-1",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#debt-overhang-and-under-investment-continued-1",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Debt Overhang and Under-Investment, continued",
    "text": "Debt Overhang and Under-Investment, continued\n\n\n\n\nWithout project\nWith new project\n\n\n\n\nExisting assets\n$900,000\n$900,000\n\n\nNew project\n-\n$150,000\n\n\nTotal firm value\n$900,000\n$1,050,000\n\n\nDebt\n$900,000\n$1,000,000\n\n\nEquity\n$0\n$50,000\n\n\n\n\n\nIf equityholders contribute \\(\\small 100,000\\) to fund the project, they get back only \\(\\small\\$50,000\\)\nThe other \\(\\small\\$100,000\\) from the project accrues to debtholders, whose payoff increases from \\(\\small\\$900,000\\) to \\(\\small\\$1,000,000\\). As a consequence, the payoff to equityholders is negative: \\(\\small NPV = -100,000 + 50,000=-\\$50,000\\)\n\n\n\n\\(\\rightarrow\\) As debtholders receive most of the benefit, thus this project is a negative-NPV investment opportunity for equityholders, even though it offers a positive NPV for the firm!"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#cashing-out",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#cashing-out",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Cashing Out",
    "text": "Cashing Out\n\nIf it is likely the company will default, the firm may sell assets below market value and use the funds to pay an immediate cash dividend to the shareholders\n\n\n\n\n\n\n\n\nDefinition\n\n\nCashing out is a situation where shareholders are more likely to liquidate assets and return back to shareholders, even if this is a negative-NPV transaction. When in financial distress, shareholders have an incentive to withdraw money from the firm, if possible.\n\n\n\n\nUltimately, if the firm defaults, as debtholders seize the assets, they may have even more difficulties in liquidating them as they do not understand the business:\n\nFor example, Tiffany is worth more in the hands of LVMH rather than a bank due to the potential synergy benefits\nThis helps explaining why, in some cases, a takeover from another firm within the same industry may preserve more value than an asset liquidation by banks!"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-leverage-ratchet-effect",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-leverage-ratchet-effect",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "The Leverage Ratchet Effect",
    "text": "The Leverage Ratchet Effect\n\nAs shown before, when an unlevered firm issues new debt, equityholders will bear any anticipated agency/bankruptcy costs that might affect the firm\nA different story arises when our starting point is a levered firm: once a firm has debt already in place, some of the agency or bankruptcy costs that result from taking on additional leverage will fall on existing debtholders:\n\nShareholders may have an incentive to increase leverage even if it decreases the value of the firm, and\nShareholders will not have an incentive to decrease leverage by buying back debt, even if it will increase the value of the firm\n\nWhile it will induce firms to borrow less initially in order to avoid these costs, over time it may lead to excessive leverage as shareholders prefer to increase, but not decrease, the firm’s debt\nFor instance, by reducing debt, equity holders lose their incentive to take on a risky negative NPV investment"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#default-and-bankruptcy",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#default-and-bankruptcy",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Default and Bankruptcy",
    "text": "Default and Bankruptcy\n\nHow to prevent bankruptcy events from the perspective of a debtholder? One way of doing so is establishing debt covenants:\n\nDebt covenants are conditions (both positive or negative) that lenders (creditors, debt holders, investors) put on lending agreements to limit the actions of the borrower (debtor)\nIn other words, debt covenants are agreements between a company and its lenders that the company will operate within certain rules set by the lenders\n\nWhen a debt covenant is violated, depending on the severity, the lender can do several things:\n\nDemand penalty payment\nIncrease the predetermined interest rate\nIncrease the amount of collateral\nDemand full immediate repayment of the loan\nTerminate the debt agreement"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#positive-and-negative-covenants",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#positive-and-negative-covenants",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Positive and Negative Covenants",
    "text": "Positive and Negative Covenants\n\nPositive Debt Covenants are covenants that state what the borrower must do:\n\nAchieve a certain threshold in certain financial ratios\nEnsure facilities and factories are in good working condition\nPerform regular maintenance of capital assets\nProvide yearly audited financial statements\nEnsure accounting practices are in accordance accounting principles\n\nNegative Debt Covenants are covenants that state what the borrower cannot do:\n\nPay cash dividends over a certain amount or predetermined threshold\nSell certain assets\nLimitations on borrowing more debt\nIssue debt more senior than the current debt\nEnter into certain types of agreements or leases"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#scenes-from-succession",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#scenes-from-succession",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Scenes from Succession",
    "text": "Scenes from Succession\n\n\\(\\rightarrow\\) See “Succession” on IMBd - access here"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-agency-benefits-of-leverage",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-agency-benefits-of-leverage",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "The Agency Benefits of Leverage",
    "text": "The Agency Benefits of Leverage\n\nSo far, we assumed that managers act in the interests of the firm’s equityholders, and we considered the potential conflicts of interest between debtholders and equityholders when a firm has leverage\nNotwithstanding, managers also have their own personal interests, which may differ from those of both equity and debtholders - also known as management entrenchment:\n\nFacing little threat of being fired and replaced, managers are free to run the firm in their own best interests\nAs a result, managers may make decisions that benefit themselves at investors’ expense\n\nHow can a firm insulate from this issue? In what follows, we will see that leverage can provide incentives for managers to run the firm more efficiently and effectively\nTogether with other benefits we will describe, in addition to the tax benefits of leverage, give the firm an incentive to use debt rather than equity!"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#management-entrenchment",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#management-entrenchment",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Management Entrenchment",
    "text": "Management Entrenchment\n\n\n\n\n\n\nDefinition\n\n\nManagement Entrenchment is a situation arising as the result of the separation of ownership and control in which managers may make decisions that benefit themselves at investors’ expenses\n\n\n\n\nEntrenchment may allow managers to run the firm in their own best interests:\n\nNepotism and personal tastes\nSpending on perks, like private jets, and sponsoring their “pet-projects”\n\n\n\nHow can leverage prevent this issue?\n\nLeverage can reduce the degree of managerial entrenchment because managers are more likely to be fired when a firm faces financial distress\nIn addition, when the firm is highly levered, creditors themselves will closely monitor the actions of managers, providing an additional layer of management oversight"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#concentration-of-ownership",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#concentration-of-ownership",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Concentration of Ownership",
    "text": "Concentration of Ownership\n\nAnother advantage of using leverage is that it allows the original owners of the firm to maintain their equity stake in the company (i.e, avoids the dilution of their participation)\nWith more “skin in the game”, they are likely having more interest in doing what is best for the firm overall\nTo see that, assume that you are the owner of a firm and plans to expand. You can either borrow the funds needed for expansion or raise the money by selling shares in the firm:\n\nIf issuing equity, you will need to sell \\(\\small40\\%\\) of the firm to raise the necessary funds\nIf issuing debt, you retain full ownership of the firm\n\nAs we will see, having higher ownership will provide additional incentives to run the firm more efficiently"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#concentration-of-ownership-continued",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#concentration-of-ownership-continued",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Concentration of Ownership, continued",
    "text": "Concentration of Ownership, continued\nWithout leverage\n\n\nIf you sell new shares, you will only retain \\(\\small60\\%\\) ownership and thereby receive \\(\\small60\\%\\) of the increase in firm value: if the value largely depends on your personal effort, you’d have more incentives if you kept \\(\\small100\\%\\) of the firm\nHaving \\(\\small60\\%\\), you would be more tempted to overspend in perks: by selling equity, you bear only \\(\\small60\\%\\) of the perks costs, while the other \\(\\small40\\%\\) will be paid for by the new equityholders\n\n\n\nHow can leverage prevent this issue?\n\n\n\nWith leverage, you retain full ownership and will bear the full cost of any perks, like country club memberships or private jets\n\n\nIs likely that you work harder as you will receive the totality of the increase in firm value\nSince you bear the totality of the perks’ costs, you would less tempted to overspend"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#reduction-of-wasteful-investment",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#reduction-of-wasteful-investment",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Reduction of Wasteful Investment",
    "text": "Reduction of Wasteful Investment\n\nAnother concern for is that managers may make large, unprofitable investments:\n\n\nFor example, managers may engage in empire building by taking over companies - this may increase his/her individual utility but not the value of the firm!\nThey often prefer to run larger firms rather than smaller ones, so they will take on investments that increase the size, but not necessarily the profitability, of the firm\n\n\nAs a consequence, managers may expand unprofitable divisions, pay too much for acquisitions, make unnecessary capital expenditures, or hire unnecessary employees\nHaving leverage limits these actions as debtholders may prevent empire building through covenants - note that this can also benefit equityholders"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#managerial-overconfidence",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#managerial-overconfidence",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Managerial Overconfidence",
    "text": "Managerial Overconfidence\n\nManagers may over-invest because they are overconfident: they tend to be bullish on the firm’s prospects and may believe that new opportunities are better than they actually are\nDebt forces managers to be more realistic with the true prospects of the firm because they have the commitment to pay back debtholders"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#free-cash-flow-hypothesis",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#free-cash-flow-hypothesis",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Free Cash Flow Hypothesis",
    "text": "Free Cash Flow Hypothesis\n\nThe occurrence of wasteful investments and room for managerial overconfidence is fueled by the fact that a firm has resources (cash) to invest\nHowever, when cash is tight, managers may be motivated to run the firm more efficiently:\n\nManagers cannot over-invest or put money on risky bets as there’s not enough cash\nAs a consequence, they may totally avoid these negative-NPV transactions\n\n\n\n\n\n\n\n\n\nDefinition\n\n\nThe Free Cash Flow Hypothesis is the view that wasteful spending is more likely to occur when firms have high levels of cash flow in excess of what is needed after making all positive-NPV investments and payments to debt holders\n\n\n\n\nIn such a way, leverage may increase firm value because it commits the firm to making future interest payments, thereby reducing excess cash flows and wasteful investment by managers"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#netting-out-agency-costs-and-benefits",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#netting-out-agency-costs-and-benefits",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Netting out Agency Costs and Benefits",
    "text": "Netting out Agency Costs and Benefits\n\nWe saw that, when adding leverage, there are both expected positive and negative effects:\n\n\n\nOn the one hand, leverage encourage managers to act in ways that reduce value:\n\nIt appears that the equity holders benefit at the expense of the debt holders\nHowever, ultimately, it is the shareholders of the firm who bear these agency costs.\n\n\n\n\n\nOn the other hand, leverage also has benefits that might increase value:\n\nHaving leverage limits the extent to which managers may engage in negative-NPV transactions\nIt also disciplines managers to run the firm as efficiently as possible\n\n\n\n\nQuestion: how would you estimate net effect of leverage on the firm value?"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#agency-costs-and-the-trade-off-theory",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#agency-costs-and-the-trade-off-theory",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Agency Costs and the Trade-off Theory",
    "text": "Agency Costs and the Trade-off Theory\n\nEstimating the net effect of Agency Costs and Benefits is an empirical matter, and despite that fact that we do not know with certainty what the value is, it is rational to assume that firms would pursue an optimal debt level by taking into account both agency costs and benefits:\n\n\n\\[\n\\small V^L = V^U + PV(ITS) - PV(\\text{Financial Distress Costs})- PV(\\text{Agency Costs}) + PV(\\text{Agency Benefits})\n\\]\n\n\nR&D-Intensive Firms\n\nFirms with high R&D costs and future growth opportunities typically maintain low debt levels\nThese firms tend to have low current free cash flows and risky business strategies\n\n\n\nLow-Growth, Mature Firms\n\nMature, low-growth firms with stable cash flows and tangible assets often carry more debt\nThese firms tend to have high free cash flows with few good investment opportunities"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#asymmetric-information",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#asymmetric-information",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Asymmetric Information",
    "text": "Asymmetric Information\n\nAnother assumption of Perfect Capital Markets is the fact that there is no information asymmetry - i.e, all market agents act upon the same set of information\n\n\n\n\n\n\n\n\nDefinition\n\n\nAsymmetric Information is a situation in which parties have different information. For example, when managers have superior information to investors regarding the firm’s future cash flows.\n\n\n\n\nIf we depart from this assumption, can asymmetric information be related to leverage? The Signaling Theory of Debt implies a relationship between the degree of information asymmetry and the level of debt:\n\nLeverage is used as a way to signal information to investors\nA firm can use leverage as a way to convince investors that it does have information that the firm will grow, even if it cannot provide verifiable details about the sources of growth"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#adverse-selection-a-general-example",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#adverse-selection-a-general-example",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Adverse Selection, a general example",
    "text": "Adverse Selection, a general example\n\nWhenever we allow for information asymmetry, we no longer guarantee that all agents operate on the same level of information: for example, there might be more informed traders (such as insider traders) that have a different estimate for a firm’s value\nWhat happens if one party anticipates that the other party has different information?\n\n\n\nAdverse Selection is the idea that when the buyers and sellers have different information, the average quality of assets in the market will differ from the average quality overall\n\n\nA manifestation of adverse selection in any market is known as the “Lemons Principle”: when a seller has private information about the value of a good, buyers will discount the price they are willing to pay due to adverse selection\n\nIf the seller has private information about the quality of the car, then her desire to sell reveals the car is probably of low quality\nBuyers are therefore reluctant to buy except at heavily discounted prices"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#adverse-selection-in-capital-markets",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#adverse-selection-in-capital-markets",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Adverse Selection in Capital Markets",
    "text": "Adverse Selection in Capital Markets\n\nThe Lemons Principle is widely used to depict a situation of adverse selection. There are implications for both high and low quality cars:\n\nOwners of high-quality cars are reluctant to sell because they know buyers will think they are selling a “lemon” and would only do it at a low price\nConsequently, the only cars that remain the market are the ones where both quality and prices are low\n\nNotwithstanding, this same principle can be applied to the market for equity:\n\nFirms that are willing to issue new equity have private information about the quality of the firm’s future projects\nHowever, due to adverse selection, buyers are reluctant to believe management’s assessment of the new projects and are only willing to buy the new equity at heavily discounted prices - if any, why would the firm sell equity at a given price?"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#adverse-selection-in-capital-markets-1",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#adverse-selection-in-capital-markets-1",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Adverse Selection in Capital Markets",
    "text": "Adverse Selection in Capital Markets\n\nIn other words:\n\nIf the firm is issuing equity at market prices, it must be that managers think the stock is expensive\nIf the firm is buying back shares at market prices, it must be that managers think the stock is cheap\n\nAs a consequence, due to the fact that managers and potential investors have different levels of information regarding the firm’s prospects, issuing/repurchasing equity can be seen as a signal from managers about their assessment\nThis helps explaining why startups tend to rely more on internal funding before going through an external funding process:\n\nIt is very difficult to assess the expected cash-flows from these firms\nAs a consequence, they would be heavily discounted when issuing securities to external investors that do not know the company’s prospects"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#asymmetric-information-practice",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#asymmetric-information-practice",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Asymmetric Information, practice",
    "text": "Asymmetric Information, practice\nYou are an analyst who follows Great Windows’ stock. Although the current stock price is \\(\\small \\$37.50\\), you believe the stock is worth either \\(\\small \\$25\\) or \\(\\small \\$50\\), depending on the success of a new product launch. If Great Windows’ CEO announces that she plans to buy \\(\\small 10,000\\) additional shares in the company, how will the share price change?\n\n\\(\\rightarrow\\) Solution\n\n\n\nIf the CEO knows the new product launch is a failure, she would sell the stock at \\(\\small \\$37.50\\) given that the she knows true value is \\(\\small \\$25\\)\nIf the CEO announces that she is buying \\(\\small 10,000\\) additional shares, she must know that the new product launch is a success and the true value is \\(\\small \\$50\\) per share\nThus, the stock price should rise to \\(\\small \\$50\\)."
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-pecking-order-of-financing-choices",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#the-pecking-order-of-financing-choices",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "The Pecking Order of Financing Choices",
    "text": "The Pecking Order of Financing Choices\n\nWhat does information asymmetry tells us when it comes to all the financing options we’ve studied so far? It is important to note that these options have different degrees of information asymmetry\nFor example, managers who perceive the firm’s equity is underpriced will have a preference to fund investment using retained earnings, or debt, rather than equity.\nTherefore, the Pecking-Order Theory of financing choices states a rank-ordering in terms of the preferable funding options for a firm:\n\nFirst and foremost, firms prefer to finance investments with internally-generated funds, as these suffer less from information asymmetry\nIf external finance is required, firms issue the safest security first: they start with debt, then possibly hybrid securities, such as convertible bonds, then equity as a the last resort"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#asymmetric-information-practice-1",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#asymmetric-information-practice-1",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Asymmetric Information, practice",
    "text": "Asymmetric Information, practice\nNesbat Industries needs to raise \\(\\small \\$25\\) million for a new investment project. If the firm issues one year debt, it may have to pay an interest rate of \\(\\small 10\\%\\), although Nesbat’s managers believe that \\(\\small 8\\%\\) would be a fair rate given the level of risk. However, if the firm issues equity, they believe the equity may be underpriced by \\(\\small 7.5\\%\\). What is the cost to current shareholders of financing the project out of retained earnings, debt, and equity?\n\n\\(\\rightarrow\\) Solution\n\n\n\nIf the firm spends \\(\\small \\$25\\) million out of retained earnings, rather than paying that money out to shareholders as a dividend, the cost to shareholders is \\(\\small \\$25\\) million\nUsing debt costs the firm pays \\(\\small \\$25 \\times (1+10\\%) = \\$27.5\\) million in one year, which has a present value based on management’s view of the firm’s risk of \\(\\small \\$27.5/(1+8\\%)= \\$25.46\\) million\nFinally, if equity is underpriced by \\(\\small 7.5\\%\\), then to raise \\(\\small \\$25\\) million, the firm will need to issue \\(\\small 25/(1-7.5\\%) = \\small \\$27.03\\) million of new equity. Thus, the cost to existing shareholders will be \\(\\small \\$27.03\\) million"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#practice",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#practice",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "Practice",
    "text": "Practice\n\n\n\n\n\n\nImportant\n\n\n\nPractice using the following links:\n\nMultiple-choice Questions"
  },
  {
    "objectID": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#references",
    "href": "fin-strat/Lecture 9 - Financial Distress, Managerial Incentives, and Information/index.html#references",
    "title": "Financial Distress, Managerial Incentives, and Information",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nBrealey, R. A., S. C. Myers, and F. Allen. 2020. Principles of Corporate Finance. The Irwin/McGraw-Hill Series in Finance, Insurance, and Real Estate. McGraw-Hill Education. https://books.google.com.br/books?id=nsrHuwEACAAJ."
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#outline",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#outline",
    "title": "Capital Structure in Perfect Markets",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\n(Berk and DeMarzo 2023)\n(Brealey, Myers, and Allen 2020)\n\nStudy review and practice: I strongly recommend using Prof. Henrique Castro (FGV-EAESP) materials. Below you can find the links to the corresponding exercises related to this lecture:\n\nMultiple Choice Exercises - click here\n\n\n\n\\(\\rightarrow\\) For coding replications, whenever applicable, please follow this page or hover on the specific slides with coding chunks."
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#equity-versus-debt-financing",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#equity-versus-debt-financing",
    "title": "Capital Structure in Perfect Markets",
    "section": "Equity Versus Debt Financing",
    "text": "Equity Versus Debt Financing\n\nWhen corporations raise funds from outside investors, they must choose which type of security to issue. The most common choices are financing through equity alone and financing through a combination of debt and equity\nAs a result, the Capital Structure decision of a firm is the relative proportion of debt, equity, and other securities that a firm has outstanding constitute its capital\nFinancial Managers are often faced with the challenge of choosing the proportions of debt and equity of the firm\n\nWhen it comes to Capital Structue, a central question is: do capital structure decisions affect firm value?\nPut another way, is there an optimal Capital Structure that maximizes the value of a firm?\n\n\n\n\\(\\rightarrow\\) As we’ll see throughout this lecture, under some conditions, the value of a firm is not affected by its capital structure decisions!"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#equity-versus-debt-financing-1",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#equity-versus-debt-financing-1",
    "title": "Capital Structure in Perfect Markets",
    "section": "Equity Versus Debt Financing",
    "text": "Equity Versus Debt Financing\n\nLet’s start our discussion by analyzing the value of a firm. Imagine an economy with two potential states: a weak and a strong economy. The firm will receive cash flows as follows:\n\n\n\n\n\n\n\n\n\n\nDate 0 (investment)\nDate 1 (strong economy)\nDate 1 (Weak economy)\n\n\n\n\n-$800\n+$1,400\n+$900\n\n\n\n\nAssume that the cost of capital for this project is \\(15\\%\\), and that each scenario has a \\(50\\%\\) probability of occurrence. The value of the firm is then:\n\n\n\n\\[\n\\small V = \\frac{\\frac{1}{2} \\times 1400 + \\frac{1}{2} \\times 900 }{(1+15\\%)} = \\dfrac{1,150}{(1+15\\%)}=1000\n\\]\n\nTherefore, the value of the firm is \\(\\small \\$1,000\\). If you take the initial investment into consideration, the NPV for the investors is \\(\\small PV(CF) = -800 + 1,000=200\\). Which Capital Structure would you choose to finance this project?"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#case-1-all-equity-financing",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#case-1-all-equity-financing",
    "title": "Capital Structure in Perfect Markets",
    "section": "Case 1: all-Equity financing",
    "text": "Case 1: all-Equity financing\n\nSay that you decide to fund this project using \\(\\small100\\%\\) equity in your Capital Structure\nThe firm value today is \\(\\small\\$1,000\\). If the firm wants to finance the project 100% through equity, it could raise this amount selling equity to outside investors. In this case, the firm is called unlevered equity\nFrom the perspective of an equity investor, we have:\n\n\n\n\n\n\n\n\n\n\nDate 0 (investment)\nDate 1 (Strong economy)\nDate 1 (Weak economy)\n\n\n\n\n-$1,000\n+$1,400\n+$900\n\n\nReturn\n+40%\n-10%\n\n\n\n\nThe expected return on the unlevered equity is \\(\\small 15\\%\\): \\(\\small \\frac{1}{2} \\times 40\\% + \\frac{1}{2} \\times -10\\% = 15\\%\\)\n\n\n\n\\(\\rightarrow\\) Because the risk of unlevered equity equals the risk of the project, shareholders are earning an equivalent return for the risk they are taking"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#equity-versus-debt-financing-2",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#equity-versus-debt-financing-2",
    "title": "Capital Structure in Perfect Markets",
    "section": "Equity Versus Debt Financing",
    "text": "Equity Versus Debt Financing\n\nWhat happens to the value of a firm when we move towards a mixed Capital Structure that contains both Equity and Debt? In this situation, the firm is called levered equity\nTo see that in action, assume that the firm issues \\(\\small \\$500\\) of debt and \\(\\small\\$500\\) of equity, with the cost of debt being \\(\\small 5\\%\\). Will the value of the firm and the project’s NPV change?\nThe short answers is: no, the firm value will not change!\n\nAs we’ll see in the upcoming slides, although the cost of debt is lower than the cost of equity (\\(\\small5\\%\\) versus \\(\\small15\\%\\)), the fact that we introduced debt in the firm’s capital structure makes equity riskier\nAs a result, when we take all sources of funds into consideration, the unlevered cost of capital - i.e, the cost of capital of the firm’s assets or the underlying business - remains the same!"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#case-2-equity-and-debt-financing",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#case-2-equity-and-debt-financing",
    "title": "Capital Structure in Perfect Markets",
    "section": "Case 2: Equity and Debt Financing",
    "text": "Case 2: Equity and Debt Financing\n\nLet’s get back to our example where we added \\(\\small 50\\%\\) of debt in our Capital Structure. Because the firm needs to pay debt holders no matter the state of the economy, the Debt value (from the perspective of a debt investor) in Date 1 is:\n\n\n\\[\n\\small D_1= D_0\\times(1+5\\%)=500\\times 1.05=\\$525\n\\]\n\nWe know the value of the firm (all Assets) in each state of the economy. The difference between the firm value and the debt value is the equity value: \\(\\small V - D = E\\):\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nDate 0 (investment)\nDate 1 (strong economy)\nDate 1 (Weak economy)\n\n\n\n\nDebt (D)\n500\n525\n525\n\n\nEquity (E)\n?\n875\n375\n\n\nFirm (V)\n1,000\n1,400\n900"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#case-2-equity-and-debt-financing-continued",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#case-2-equity-and-debt-financing-continued",
    "title": "Capital Structure in Perfect Markets",
    "section": "Case 2: Equity and Debt Financing (continued)",
    "text": "Case 2: Equity and Debt Financing (continued)\n\nYou may have noticed something strage: if the debt cost is only \\(\\small5\\%\\), why the firm value still is \\(\\small\\$1.000\\)? Shouldn’t the value of equity increase? The answer is that the cost of equity increases for levered equity:\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\n\\(T_0\\)\n\\(T_1\\) (strong)\n\\(T_1\\) (Weak)\n\\(R_{\\text{Strong}}\\)\n\\(R_{\\text{Weak}}\\)\n\n\n\n\nDebt\n500\n525\n525\n5%\n5%\n\n\nEquity\n500\n875\n375\n75%\n-25%\n\n\nValue\n1,000\n1,400\n900\n40%\n-10%\n\n\n\n\nUnlevered Equity has returns of \\(\\small40\\%\\) or \\(\\small-10\\%\\) \\(\\rightarrow\\) on average, \\(\\small15\\%\\), as before\nDebt has return of \\(\\small5\\%\\), regardless of the state of the economy\nLevered Equity has returns of \\(\\small75\\%\\) (\\(\\small\\frac{875}{500}-1\\)) or \\(\\small-25\\%\\) (\\(\\small\\frac{375}{500}-1\\)). On average, \\(\\small25\\%\\)."
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#case-2-equity-and-debt-financing-continued-1",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#case-2-equity-and-debt-financing-continued-1",
    "title": "Capital Structure in Perfect Markets",
    "section": "Case 2: Equity and Debt Financing (continued)",
    "text": "Case 2: Equity and Debt Financing (continued)\n\nThe key takeaway is: Levered equity is riskier, so the cost of capital is higher:\n\n\n\n\n\nSource\nReturn sensitivity\nRisk premium\n\n\n\n\nDebt\n5% - 5% = 0%\n5% - 5% = 0%\n\n\nUnlevered Equity\n40% - (-10%) = 50%\n15% - 5% = 10%\n\n\nLevered Equity\n75% - (-25%) = 100%\n25% - 5% = 20%\n\n\n\n\nBecause the debt’s return bears no systematic risk, its risk premium is zero - it pays \\(\\small5\\%\\) regardless of the state of the economy!\nIn this particular case, the levered equity has twice the systematic risk of the unlevered equity and, as a result, has twice the risk premium!\n\n\nNow, if you were to calculate the cost of capital of the firm, assuming \\(\\small 50\\%\\) equity and \\(\\small 50\\%\\) debt, the required return is \\(\\small \\frac{1}{2}\\times25\\%+\\frac{1}{2}\\times 5\\%=15\\%\\rightarrow\\) the firm value remains at \\(\\small \\$1,000\\)!"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#equity-versus-debt-financing-a-summary",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#equity-versus-debt-financing-a-summary",
    "title": "Capital Structure in Perfect Markets",
    "section": "Equity versus Debt Financing, a summary",
    "text": "Equity versus Debt Financing, a summary\n\nIn the case of perfect capital markets, if the firm is \\(\\small100\\%\\) equity financed, the equity holders will require a \\(\\small15\\%\\) expected return\nWhen financingwith \\(\\small50\\%\\)/\\(\\small50\\%\\) equity and debt, debtholders will receive a return of \\(\\small5\\%\\), while the levered equity holders will require an expected return of \\(\\small25\\%\\) (because of increased risk)\nLeverage increases the risk of equity even when there is no risk that the firm will default\nThus, while debt may be cheaper, its use raises the cost of capital for equity\n\n\n\\(\\rightarrow\\) Considering both sources of capital together, the firm’s average cost of capital with leverage is the same as for the unlevered firm!\n\nModigliani and Miller argued that with perfect capital markets, the total value of a firm should not depend on its capital structure\nThey reasoned that the firm’s total cash flows still equal the cash flows of the project and, therefore, have the same present value!"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#equity-versus-debt-financing-exercise",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#equity-versus-debt-financing-exercise",
    "title": "Capital Structure in Perfect Markets",
    "section": "Equity versus Debt Financing, exercise",
    "text": "Equity versus Debt Financing, exercise\n\nUsing the same values as before, suppose the firm borrows \\(\\small\\$700\\) when financing the project. According to Modigliani and Miller, what should the value of the equity be? What is the expected return?\nBecause the value of the firm’s total cash flows is still \\(\\small\\$1,000\\), if the firm borrows \\(\\small\\$700\\), its equity will be worth \\(\\small\\$300\\). The firm will owe \\(\\small \\$700 \\times 1.05 = \\$735\\) in one year to debtholders:\n\nIf the economy is strong, equity holders will receive \\(\\small 1,400 − 735 = 665\\), a return of \\(\\small \\frac{665}{300}-1 = \\small 121.67\\%\\).\nIf the economy is weak, equity holders will receive \\(\\small 900 − 735 = 165\\), a return of \\(\\small \\frac{165}{300}-1 = \\small -45\\%\\).\n\nThe expected return is then:\n\n\n\\[\n\\small \\frac{1}{2} \\times 121.67\\% + \\frac{1}{2} \\times -45\\% = 38.33\\%\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#equity-versus-debt-financing-exercise-1",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#equity-versus-debt-financing-exercise-1",
    "title": "Capital Structure in Perfect Markets",
    "section": "Equity versus Debt Financing, exercise",
    "text": "Equity versus Debt Financing, exercise\n\nNote that the equity has a return sensitivity of \\(\\small 121.67\\% − (−45.0\\%) = 166.67\\%\\), and its risk premium is \\(\\small 38.33\\% − 5\\% = 33.33\\%\\):\n\n\n\n\n\n\n\n\n\n\n\n\nSource\n\\(R_\\text{Strong}\\)\n\\(R_\\text{Weak}\\)\nSyst. risk\nRisk premium\n\n\n\n\nDebt\n5%\n5%\n5% - 5% = 0%\n5% - 5% = 0%\n\n\nEquity\n122%\n-45%\n122% - (-45%) = 167%\n38% - 5% = 33%\n\n\nFirm\n40%\n-10%\n75% - (-25%) = 100%\n25% - 5% = 20%\n\n\n\n\n\n\nAgain, debt increases equity risk, and the unlevered value (the value of the firm as whole) remains the same because the unlevered cost of capital is still 15%:\n\n\n\n\\[\n\\small \\underbrace{30\\%\\times5\\%}_{\\text{Debt}}+\\underbrace{70\\%\\times38.33\\%}_{\\text{Equity}}=15\\%\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#modigliani-and-miller-why-it-makes-sense",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#modigliani-and-miller-why-it-makes-sense",
    "title": "Capital Structure in Perfect Markets",
    "section": "Modigliani and Miller: why it makes sense?",
    "text": "Modigliani and Miller: why it makes sense?\n\nBack in our previous slides, we achieved the conclusion that the value of the firm remained at \\(\\small \\$1,000\\) regardless of the Capital Structure\nThat was just a direct application of the Law of One Price: the choice of firm’s leverage merely changes the allocation of value between debt and equity - i.e, you’re just slicing the pizza in different ways, but the size of the pizza remains the same!\nModigliani and Miller showed that this result holds more generally under a set of conditions referred to as perfect capital markets:\n\nInvestors and firms can trade the same set of securities at competitive market prices equal to the present value of their future cash flows\nThere are no taxes, transaction costs, or issuance costs associated with security trading\nA firm’s financing decisions do not change the cash flows generated by its investments, nor do they reveal new information about them"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#modigliani-and-miller---proposition-i",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#modigliani-and-miller---proposition-i",
    "title": "Capital Structure in Perfect Markets",
    "section": "Modigliani and Miller - Proposition I",
    "text": "Modigliani and Miller - Proposition I\n\n\n\n\n\n\nProposition I - Modigliani and Miller\n\n\nIn a perfect capital market, the total value of a firm’s securities is equal to the market value of the total cash flows generated by its assets and is not affected by its choice of capital structure.\n\n\n\n\nWhy is this important? It establishes that, under certain conditions, the irrelevance of the choice of leverage applies to more general cases\nAs we’ll see in the upcoming slides, this idea applies to a case where a given investor might desire a different capital structure choice than the one chosen by the firms\nRegardless of the case (either a more or less leverage preference), the value of the firm for this investor is the same!\nIn other words, because different choices of capital structure offer no benefit to investors, they do not affect the value of the firm"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-i---levering-up",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-i---levering-up",
    "title": "Capital Structure in Perfect Markets",
    "section": "Proposition I - Levering up",
    "text": "Proposition I - Levering up\n\nLet’s say the firm selects a given capital structure, but the investor likes an alternative capital structure (either more or less leveraged)\nModigliani and Miller demonstrated that if investors prefer an alternative capital structure to the one the firm has chosen, they can borrow or lend on their own personal account and achieve the same result in terms of firm value!\nTo illustrate that, assume the firm is an all−equity firm (zero leverage)…\n\n\nAn investor who would prefer to have a levered equity firm. In perfect capital markets, he can do so by levering his own personal portfolio\nHe chooses to borrow \\(\\small\\$500\\) and add leverage to his or her own portfolio (\\(\\small\\$500\\) personal + \\(\\small\\$500\\) debt)\nThe investor then borrows \\(\\small\\$500\\) and buys the firm’s stock"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-i---levering-up-1",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-i---levering-up-1",
    "title": "Capital Structure in Perfect Markets",
    "section": "Proposition I - Levering Up",
    "text": "Proposition I - Levering Up\n\n\n\n\n\n\n\n\n\n\nSource\n\\(T_0\\)\n\\(T_1\\) (strong)\n\\(T_0\\) (weak)\n\n\n\n\nUnlevered Equity (Firm)\n1,000\n1,400\n900\n\n\nMargin loan (Investor borrowing)\n-500\n-525\n-525\n\n\nLevered equity (Investor’s return)\n500\n875\n375\n\n\n\n\nIf the cash flows of the unlevered equity serve as collateral for the margin loan (at the risk−free rate of 5%), then by using homemade leverage, the investor has replicated the payoffs to the levered equity!\nAs long as investors can borrow or lend at the same interest rate as the firm, homemade leverage is a perfect substitute for the use of leverage by the firm\nWhat if investors actually want a less leveraged firm? Is it possible for them to unlever on their own?"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-i---unlevering",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-i---unlevering",
    "title": "Capital Structure in Perfect Markets",
    "section": "Proposition I - Unlevering",
    "text": "Proposition I - Unlevering\n\nNow, assume the firm uses debt, but investors prefer to hold unlevered equity (\\(\\small100\\%\\) equity):\n\nThe investor can again replicate the payoffs of Unlevered Equity firm by buying both the debt and the equity of the firm!\nCombining the cash flows of the two securities produces cash flows identical to Unlevered Equity, for a total cost of \\(\\small\\$1,000\\):\n\n\n\n\n\n\n\n\n\n\n\n\nSource\n\\(T_0\\)\n\\(T_1\\) (strong)\n\\(T_0\\) (weak)\n\n\n\n\nDebt (Investor’s lending)\n500\n525\n525\n\n\nLevered equity (Firm)\n500\n875\n375\n\n\nUnlevered equity (Investor’s return)\n1,000\n1,400\n900"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#does-proposition-i-hold-in-reality",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#does-proposition-i-hold-in-reality",
    "title": "Capital Structure in Perfect Markets",
    "section": "Does Proposition I hold in reality?",
    "text": "Does Proposition I hold in reality?\n\nModigliani and Miller showed that a firm’s financing choice does not affect its value. But how can we reconcile this conclusion with the fact that the cost of capital differs for different securities?\n\nWhen the project is financed solely through equity, the equity holders require a \\(15\\%\\) expected return\nAs an alternative, the firm could borrow at the risk-free rate of \\(\\small5\\%\\)\n\nAll in all, isn’t debt a cheaper and better source of capital than equity? Although debt does have a lower cost of capital than equity, we cannot consider this cost in isolation:\n\nWhile debt itself may be cheap, it increases the risk and therefore the cost of capital of the firm’s equity\nIn the end, the savings a lower debt cost of capital are exactly offset by a higher equity cost of capital, and there are no net savings for the firm!"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#modigliani-and-miller---proposition-ii",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#modigliani-and-miller---proposition-ii",
    "title": "Capital Structure in Perfect Markets",
    "section": "Modigliani and Miller - Proposition II",
    "text": "Modigliani and Miller - Proposition II\n\n\nThe insights from the first proposition can be used to derive an explicit relationship between leverage and the equity cost of capital, which is Modigliani Miller’s Proposition II:\n\n\n\n\n\n\n\nProposition II - Modigliani and Miller\n\n\nThe cost of capital of levered equity increases with the firm’s market value debt equity ratio:\n\\[\n\\begin{align}\n&r_E=r_U+\\dfrac{D}{E}(r_U-r_D), \\text{where:} \\\\\n&\\\\\n&\\text{ -   E = Market value of equity in a levered firm}\\\\\n&\\text{ -   D = Market value of debt in a levered firm}\\\\\n&\\text{ -   U = Market value of equity in an unlevered firm}\\\\\n&\\text{ -   A = Market value of the firm's assets}\\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-ii---modigliani-and-miller-continued",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-ii---modigliani-and-miller-continued",
    "title": "Capital Structure in Perfect Markets",
    "section": "Proposition II - Modigliani and Miller (continued)",
    "text": "Proposition II - Modigliani and Miller (continued)\n\nProposition I stated that: \\(\\small E+D = U = A\\). In words, the total market value of the firm’s securities is equal to the market value of its assets, whether the firm is unlevered or levered\nFurthermore, remember that the return of a portfolio is the weighted average of the returns\nSo, we can write that the return on unlevered equity (\\(r_U\\)) is related to the returns of levered equity (\\(r_E\\)) and debt (\\(r_D\\)):\n\n\n\\[\n\\small r_U = \\frac{E}{E+D} \\times r_E + \\frac{D}{E+D} \\times r_D\n\\]\n\nRearranging terms and solving for \\(r_E\\)1:\n\n\n\n\\[\n\\small r_E = r_U + \\frac{D}{E} \\times (r_U - r_D)\n\\]\n\nSee details in the Appendix"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-ii---modigliani-and-miller-continued-1",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-ii---modigliani-and-miller-continued-1",
    "title": "Capital Structure in Perfect Markets",
    "section": "Proposition II - Modigliani and Miller (continued)",
    "text": "Proposition II - Modigliani and Miller (continued)\n\nThe levered equity return equals the unlevered return, plus a premium due to leverage. The amount of the premium depends on the amount of leverage, measured by the market value debt−to-equity ratio!\n\n\n\\[\n\\small r_E = \\underbrace{r_U}_{\\text{Risk without leverage}} + \\underbrace{\\frac{D}{E} \\times (r_U - r_D)}_{\\text{Add. risk due to leverage}}\n\\]\n\nUsing the previous example’s numbers (\\(\\small r_U=15\\%\\), \\(\\small r_D=5\\%\\) and a debt-to-equity ratio of 1 \\(\\frac{500}{500}=1\\), we have:\n\n\n\n\\[\n\\small r_E = r_U + \\frac{D}{E} \\times (r_U - r_D)\\rightarrow 15\\%+1\\times(15\\%-5\\%)=25\\%\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-ii---modigliani-and-miller-continued-2",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-ii---modigliani-and-miller-continued-2",
    "title": "Capital Structure in Perfect Markets",
    "section": "Proposition II - Modigliani and Miller (continued)",
    "text": "Proposition II - Modigliani and Miller (continued)\n\nSuppose the entrepreneur borrows \\(\\small\\$700\\) when financing the project. Recall that the expected return on unlevered equity is \\(\\small15\\%\\) and the risk−free rate is \\(\\small5\\%\\). According to Proposition II, what will be the firm’s equity cost of capital?\n\n\n\\[\n\\small r_E = r_U + \\frac{D}{E} \\times (r_U - r_D) = 15\\% + \\frac{700}{300} \\times (15\\%-5\\%) = 38.33\\%\n\\]\n\nIf a firm is financed with both equity and debt, then the risk of its underlying assets will match the risk of a portfolio of its equity and debt (the unlevered cost of capital or the pre-tax WACC):\n\n\n\n\\[\n\\small r_{WACC} = \\frac{E}{E+D} \\times r_E + \\frac{D}{E+D} \\times r_D \\rightarrow  30\\% \\times 38.33\\% + 70\\% \\times 5\\% = 15\\%\n\\]\n\\(\\rightarrow\\) With perfect capital markets, a firm’s WACC is independent of its capital structure and is equal to its equity cost of capital if it is unlevered"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-ii---modigliani-and-miller-example",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-ii---modigliani-and-miller-example",
    "title": "Capital Structure in Perfect Markets",
    "section": "Proposition II - Modigliani and Miller (example)",
    "text": "Proposition II - Modigliani and Miller (example)\n\nHoneywell (ticker: HON) has a market debt−equity ratio of \\(\\small0.5\\). Assume its current debt cost of capital is \\(\\small6.5\\%\\), and its equity cost of capital is \\(\\small 14\\%\\). If HON issues equity and uses the proceeds to repay its debt and reduce its debt−equity ratio to \\(0.4\\), it will lower its debt cost of capital to \\(\\small 5.75\\%\\).\nWith perfect capital markets, what effect will this transaction have on HON’s equity cost of capital and WACC?\n\n\n\\[\n\\small r_{WACC} = \\frac{E}{E+D} \\times r_E + \\frac{D}{E+D} \\times r_D \\rightarrow \\frac{2}{2+1} \\times 14\\% + \\frac{1}{2+1} \\times 6.5\\% = 11.5\\%\n\\]\n\nThe new Cost of Equity (\\(\\small r_E\\)) will decrease due to lower leverage and lower cost of debt:\n\n\n\n\\[\n\\small r_E = r_U + \\frac{D}{E} (r_U - r_D) \\rightarrow 11.5\\% +0.4 \\times (11.5\\% - 5.75\\%) = 13.8\\%\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-ii---modigliani-and-miller-example-1",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#proposition-ii---modigliani-and-miller-example-1",
    "title": "Capital Structure in Perfect Markets",
    "section": "Proposition II - Modigliani and Miller (example)",
    "text": "Proposition II - Modigliani and Miller (example)\n\nThe WACC, on the other hand, remains unchanged:\n\n\n\\[\n\\small r_{WACC} = \\frac{1}{1+0.4} \\times 13.8\\% + \\frac{0.4}{1+0.4} \\times 5.75\\% = 11.5\\%\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#levered-and-unlevered-betas",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#levered-and-unlevered-betas",
    "title": "Capital Structure in Perfect Markets",
    "section": "Levered and Unlevered Betas",
    "text": "Levered and Unlevered Betas\n\nRemember that\n\n\n\\[\n\\small \\beta_U = \\frac{E}{D+E} \\times \\beta_E + \\frac{D}{D+E} \\times \\beta_D\n\\]\n\nWhen a firm changes its capital structure without changing its investments, its unlevered beta will remain unaltered. However, its equity beta will change to reflect the effect of the capital structure change on its risk:\n\n\n\n\\[\n\\small \\beta_E = \\beta_U + \\frac{D}{E} (\\beta_U - \\beta_D)\n\\]\n\n\n\\(\\rightarrow\\) Therefore, the unlevered beta is a measure of the risk of a firm as if it did not have leverage, which is equivalent to the beta of the firm’s assets!"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#levered-and-unlevered-betas-example",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#levered-and-unlevered-betas-example",
    "title": "Capital Structure in Perfect Markets",
    "section": "Levered and unlevered Betas (example)",
    "text": "Levered and unlevered Betas (example)\nIn August 2018, Reenor had a market capitalization of 140 billion. It had debt of 25.4 billion as well as cash and short−term investments of 60.4 billion. Its equity beta was 1.09 and its debt beta was approximately zero. What was Reenor’s enterprise value at time? Given a risk−free rate of 2% and a market risk premium of 5%, estimate the unlevered cost of capital of Reenor’s business.\n\n\nNet Debt is \\(\\small 25.4 − 60.4=-35\\) billion. As a result, the Enterprise Value is \\(\\small 140−35=105\\) billion\n\n\n\n\\[\n\\small \\beta_U = \\frac{E}{E+D} \\times \\beta_E + \\frac{D}{E+D} \\times \\beta_D = \\frac{140}{105} \\times 1.09 + \\frac{-35}{105} \\times 0 = 1.45\n\\]\n\n\n\\[\n\\small r_U = 2\\% + 1.45 \\times 5\\% = 9.25\\%\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#capital-structure-fallacies",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#capital-structure-fallacies",
    "title": "Capital Structure in Perfect Markets",
    "section": "Capital Structure Fallacies",
    "text": "Capital Structure Fallacies\nWe will discuss now two fallacies concerning capital structure:\n\nLeverage increases earnings per share (EPS), thus increase firm value\nIssuing new equity will dilute existing shareholders, so debt should be issued\n\n\n\\(\\rightarrow\\) As we’ll see, using Modigliani and Miller’s proposition under perfect capital markets, we can conclude that both claims are incorrect - in other words, these actions do not change the value of a firm"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#capital-structure-fallacies---eps",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#capital-structure-fallacies---eps",
    "title": "Capital Structure in Perfect Markets",
    "section": "Capital Structure Fallacies - EPS",
    "text": "Capital Structure Fallacies - EPS\n\nAssume that LVI’s EBIT is not expected to grow in the future and that all earnings are paid as dividends. Is the increase in expected EPS lead to an increase in the share price?\nWithout leverage, expected earnings per share and therefore dividends are \\(\\small\\$1\\) each year, and the share price is \\(\\small\\$7.50\\)\nLet \\(r_U\\) be LVI’s cost of capital without leverage. The value LVI as a perpetuity is simply:\n\n\n\\[\n\\small P = 7.50 = \\frac{Div}{r_U} = \\frac{EPS}{r_U} = \\frac{1}{r_U}\n\\]\n\n\n\\(\\rightarrow\\) Therefore, current stock price implies that \\(\\small r_U = \\frac{1}{7.50} = 13.33\\%\\)"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#capital-structure-fallacies---eps-continued",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#capital-structure-fallacies---eps-continued",
    "title": "Capital Structure in Perfect Markets",
    "section": "Capital Structure Fallacies - EPS (continued)",
    "text": "Capital Structure Fallacies - EPS (continued)\n\nThe market value of LVI without leverage is \\(\\small \\$7.50 \\times 10\\) million shares = \\(\\small \\$75\\) million. Assume that LVI uses debt to repurchase \\(\\small 15\\) million worth of the firm’s equity. Then, the remaining equity will be worth \\(\\small 75\\) − \\(\\small \\$15\\) = \\(\\small \\$60\\) million. After the transaction, \\(\\frac{D}{E} = \\frac{1}{4}\\), thus, we can write:\n\n\n\\[\n\\small r_E= r_U +\\frac{D}{E} \\times (r_U - r_D) = 13.33\\% + 0.25 \\times (13.33\\% - 8\\%) = 14.66\n\\]\n\nAlso, note that the EPS will actually increase to \\(\\small 1.1\\): because total earnings (earnings times number of shares) are \\(\\small \\$10,000,000\\) and now the firm has only \\(\\small 8,000,000\\) (since \\(\\small \\$15,000,000/\\$7.5=2,000,000\\)) has been bought back with debt, we have:\n\n\n\n\\[\n\\small\nEPS=\\dfrac{Earnings}{Share}=\\dfrac{\\overbrace{10,000,000}^{\\text{EBIT}}-\\overbrace{8\\%\\times15,000,000}^{\\text{Interest}}}{8,000,000}=\\dfrac{8,800,000}{8,000,000}=1.1\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#capital-structure-fallacies---eps-continued-1",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#capital-structure-fallacies---eps-continued-1",
    "title": "Capital Structure in Perfect Markets",
    "section": "Capital Structure Fallacies - EPS (continued)",
    "text": "Capital Structure Fallacies - EPS (continued)\n\nGiven that expected EPS is now \\(\\small \\$1.10\\) per share, the new value of the shares equals:\n\n\n\\[\n\\small P=\\frac{1.10}{14.66\\%} = 7.50\n\\]\n\\(\\rightarrow\\) Thus, even though EPS is higher, due to the additional risk, shareholders will demand a higher return. These effects cancel out, so the price per share is unchanged\n\nAs Earnings-per-Share (EPS), Price-Earnings (P/E) and even ROE ratios are affected by leverage, we cannot reliably compare these measures across firms with different capital structures!\nTherefore, most analysts prefer to use performance measures and valuation multiples that are based on the firm’s earnings before interest has been deducted\nFor example, the ratio of enterprise value to EBIT (or EBITDA) is more useful when analyzing firms with very different capital structures than is comparing their P/E ratios"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#capital-structure-fallacies---equity-issuances-and-dilution",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#capital-structure-fallacies---equity-issuances-and-dilution",
    "title": "Capital Structure in Perfect Markets",
    "section": "Capital Structure Fallacies - Equity Issuances and Dilution",
    "text": "Capital Structure Fallacies - Equity Issuances and Dilution\n\nIt is sometimes (incorrectly) argued that issuing equity will dilute existing shareholders’ ownership value, so debt financing should be used instead\nTo see that, suppose Jet Sky Airlines (JSA) currently has no debt and \\(\\small 500\\) million shares of stock outstanding, which is currently trading at a price of \\(\\small \\$16\\). Last month, the firm announced that it would expand and the expansion will require the purchase of \\(\\small \\$1\\) billion of new planes, which will be financed by issuing new equity:\n\nThe current (prior to the issue) value of the equity and the assets of the firm is \\(\\small \\$8\\) billion\nTherefore, \\(\\small 500\\) million shares \\(\\times\\) \\(\\small \\$16\\) per share leads to a firm value of \\(\\small \\$8\\) billion"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#capital-structure-fallacies---equity-issuances-and-dilution-continued",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#capital-structure-fallacies---equity-issuances-and-dilution-continued",
    "title": "Capital Structure in Perfect Markets",
    "section": "Capital Structure Fallacies - Equity Issuances and Dilution (continued)",
    "text": "Capital Structure Fallacies - Equity Issuances and Dilution (continued)\n\nSuppose now JSA sells \\(\\small 62.5\\) million new shares at the current price of \\(\\small \\$16\\) per share to raise the additional \\(\\small \\$1\\) billion needed to purchase the planes:\n\n\n\n\n\nAssets\nBefore\nAfter\n\n\n\n\nCash\n-\n$1,000\n\n\nExisting Assets\n$8,000\n$8,000\n\n\nTotal Value\n$8,000\n$9,000\n\n\n# Shares (out)\n500\n562.5\n\n\nValue per share\n$16\n$16\n\n\n\n\n\n\\(\\rightarrow\\) As a consequence, share prices don’t change. Any gain or loss associated with the transaction will result from the NPV of the investments the firm makes with the funds raised"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#why-should-i-bother-about-modigliani-and-miller",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#why-should-i-bother-about-modigliani-and-miller",
    "title": "Capital Structure in Perfect Markets",
    "section": "Why should I bother about Modigliani and Miller?",
    "text": "Why should I bother about Modigliani and Miller?\n\nModigliani and Miller is truly about the conservation of the value principle in perfect financial markets: with perfect capital markets, financial transactions neither add nor destroy value, but instead represent just a repackaging of risk and therefore return\nAs a result, what really matters for the firm value is to find good investment opportunities that increase the future cash-flows!\nPractitioners often question why Modigliani and Miller’s results are important if, after all, capital markets are not perfect in the real world\n\nWhile it is true that capital markets are not perfect, all scientific theories begin with a set of idealized assumptions from which conclusions can be drawn\nWhen we apply the theory, we must then evaluate how closely the assumptions hold, and consider the consequences of any important deviations"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#why-should-i-bother-about-modigliani-and-miller-1",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#why-should-i-bother-about-modigliani-and-miller-1",
    "title": "Capital Structure in Perfect Markets",
    "section": "Why should I bother about Modigliani and Miller?",
    "text": "Why should I bother about Modigliani and Miller?\n\nYou may well think…but we are not in a perfect capital market, so why should I bother? That goes hand in hand with the Modigliani and Miller findings:\n\nIf capital markets were perfect (as in the Modigliani and Miller world), then the value of a firm should not be affected by any financial transaction…\n… this implies that if there is a financial transaction that appears to be a good, it must be that it is exploiting some type of market imperfection!\n\nKnowing how these imperfections affect the firm value is important for business and policy considerations\n\nFor example, what happens if you remove the tax-shield from debt interest payments? What if the government decides to tax dividends differently?\nThese (and other) actions may create/remove market imperfections that will tweak the real-practice results away/closer to the Modigliani-Miller findings!"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#practice",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#practice",
    "title": "Capital Structure in Perfect Markets",
    "section": "Practice",
    "text": "Practice\n\n\n\n\n\n\n\nImportant\n\n\nPractice using the following links:\n\nMultiple-choice Questions"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#levered-and-unlevered-beta",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#levered-and-unlevered-beta",
    "title": "Capital Structure in Perfect Markets",
    "section": "Levered and Unlevered \\(\\beta\\)",
    "text": "Levered and Unlevered \\(\\beta\\)\n\nRecall that the unlevered beta, \\(\\beta_U\\), is simply:\n\n\n\\[\n\\small \\beta_U = \\frac{E}{E+D}\\times \\beta_E + \\frac{D}{E+D} \\times \\beta_D\n\\]\n\nPutting it into \\(\\beta_E\\) terms, we have:\n\n\n\n\\[\n\\small \\begin{align}\n&\\beta_U = \\frac{(E\\times \\beta_E +D\\times \\beta_D)}{E+D}\\\\\n&\\rightarrow E\\times \\beta_E =(E+D)\\times\\beta_U - D\\times \\beta_D\\\\\n&\\rightarrow\\beta_E =\\dfrac{(E+D)\\times\\beta_U - D\\times \\beta_D}{E}=\\beta_U+\\dfrac{D}{E}\\times \\beta_U-\\dfrac{D}{E}\\times \\beta_D\\\\\n&\\rightarrow \\beta_E= \\beta_U+\\dfrac{D}{E}\\times(\\beta_U-\\beta_D)\n\\end{align}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#levered-and-unlevered-cost-of-capital",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#levered-and-unlevered-cost-of-capital",
    "title": "Capital Structure in Perfect Markets",
    "section": "Levered and Unlevered Cost of Capital",
    "text": "Levered and Unlevered Cost of Capital\n\nNote that, as we can lever (or unlever) \\(\\beta\\), we can do the same with the required returns from equity, \\(r_E\\):\n\n\n\\[\n\\small r_U = \\frac{E}{E+D}\\times r_E + \\frac{D}{E+D} \\times r_D\n\\]\n\nPutting it into \\(r_E\\) terms and using the same rationale, we have:\n\n\n\n\\[\nr_E= r_U+\\dfrac{D}{E}\\times(r_U-r_D)\n\\]\n\nIn words, if leverage increases (through higher debt-to-equity ratios), the equity sensitivity should increase as well, and the required returns for equity should increase!"
  },
  {
    "objectID": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#references",
    "href": "fin-strat/Lecture 7 - Capital Structure in Perfect Capital Markets/index.html#references",
    "title": "Capital Structure in Perfect Markets",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nBrealey, R. A., S. C. Myers, and F. Allen. 2020. Principles of Corporate Finance. The Irwin/McGraw-Hill Series in Finance, Insurance, and Real Estate. McGraw-Hill Education. https://books.google.com.br/books?id=nsrHuwEACAAJ."
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#outline",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#outline",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\n(Berk and DeMarzo 2023)\n(Brealey, Myers, and Allen 2020)\n\nStudy review and practice: I strongly recommend using Prof. Henrique Castro (FGV-EAESP) materials. Below you can find the links to the corresponding exercises related to this lecture:\n\nMultiple Choice Exercises - click here\nNumeric Exercises - click here\n\n\n\n\\(\\rightarrow\\) For coding replications, whenever applicable, please follow this page or hover on the specific slides with coding chunks"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#competition-and-capital-markets-1",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#competition-and-capital-markets-1",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Competition and Capital Markets",
    "text": "Competition and Capital Markets\n\nOur previous lecture showed us the backbone of the Capital Asset Pricing Model, also known as CAPM\n\nThe CAPM allows us to price the required returns for any given security given its relationship with the market portfolio\nHowever, to reach such result, we have made some assumptions regarding the dynamics of investors and the market\n\nWhat if these assumptions are not met? How does that impact the insights derived from the CAPM?\nIn this chapter, we will discuss several inefficiencies and biases that appear in the stock market"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#capital-market-dynamics",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#capital-market-dynamics",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Capital Market Dynamics",
    "text": "Capital Market Dynamics\n\nThe CAPM tells us that there is only one efficient portfolio, and this portfolio is the market portfolio\nHowever, how can we identify the market portfolio? It is important to note the following aspects regarding this point:\n\nThe market is always getting new information. Therefore, security prices are always adjusting…\n…as a consequence, this makes the efficient portfolio move from time to time…\n…and as such, investors will rebalance their portfolios continuously to meet the new efficient portfolio\n\nAll in all, the CAPM is an equilibrium model. This means that all investors will converge to the same portfolio until new information arrives, but there is no such th"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#beating-the-market",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#beating-the-market",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Beating the market",
    "text": "Beating the market\n\nSay that you are evaluating the performance of several stocks relative to their expected returns. To improve the performance of their portfolios, investors will compare the expected return of a security \\(i\\) with its required return from the security market line:\n\n\n\\[R_i = R_f + \\beta_S \\times (E[R_m - R_f])\\]\n\nNow, after calculating expected returns for \\(i\\), you go ahead and compare it with the actual, realized return that \\(i\\) gave during a given period:\n\n\n\n\\[\\alpha_i = \\underbrace{E[R_i]}_{\\text{Observed by the analyst}} - \\underbrace{R_i}_{\\text{Implied by the CAPM}}\\]\n\\(\\rightarrow\\) \\(\\alpha\\) is the difference between a stock’s expected return and its required return according to the Security Market Line"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#interpreting-alpha",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#interpreting-alpha",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Interpreting \\(\\alpha\\)",
    "text": "Interpreting \\(\\alpha\\)\n\\[\\small \\alpha_i = \\underbrace{E[R_i]}_{\\text{Observed by the analyst}} - \\underbrace{R_i}_{\\text{Implied by the CAPM}}\\]\n\nA positive alpha means that the stock is above the SML\n\nIn words, the expected return is higher than its required return. Before prices adjust, investors will anticipate that the price will rise and will likely put in buy orders at the current prices\n\nA negative alpha means that the stock is below the SML\n\nThe expected return is lower than its required return. Before prices adjust, investors will anticipate that the price will fall and will likely put in sell orders at the current prices\n\n\n\n\\(\\rightarrow\\) In either case, we’ll be able to improve portfolio results. However, as we do so, prices will change and their alphas will shrink toward zero!"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#interpreting-alpha-graphical-intuition",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#interpreting-alpha-graphical-intuition",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Interpreting \\(\\alpha\\), graphical intuition",
    "text": "Interpreting \\(\\alpha\\), graphical intuition"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#beating-the-market-and-market-competition",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#beating-the-market-and-market-competition",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Beating the Market and Market competition",
    "text": "Beating the Market and Market competition\n\nLet’s analyze what happens in the previous figure:\n\nStocks above the SML are cheap, so the prices should rise (positive alpha)\nStocks below the SML are expensive, so the prices should drop (negative alpha)\n\nBecause of that, we say that the CAPM is also a competitive market in equilibrium:\n\nInvestors trying to “beat the market” are always looking for stocks with positive alpha (Walmart and Nike) to buy\nOnce they buy them, prices rise, making stocks once again on the Security Market Line\n\nThere is a competition in the market and that competition brings efficiency to the CAPM! Note that such competition may be so intense that prices move before any investor can actually trade at the old prices, so no investor can profit from the news"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#information-rational-expectactions-theory",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#information-rational-expectactions-theory",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Information & Rational Expectactions, theory",
    "text": "Information & Rational Expectactions, theory\n\nIn the CAPM world, investors should hold the market portfolio combined with \\(R_f\\). Why? Recall that we have assumed that investors had homogeneous expectations (Assumption #3). However, for the CAPM to hold, we only need a Rational Expectations Hypothesis:\n\nAll investors correctly interpret and use their own information, as well as information that can be inferred from market prices or the trades of others\nRegardless of how much information an investor has access to, he/she can guarantee a zero \\(\\alpha\\) by holding the market portfolio\n\nTherefore, the market portfolio can be inefficient only if a significant number of investors:\n\nMisinterpret information and believe they are earning a positive alpha when they are actually earning a negative alpha, or\nCare about aspects of their portfolios other than expected return and volatility, and so are willing to hold inefficient portfolios (in terms of risk and return) of securities"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#information-rational-expectations-practice",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#information-rational-expectations-practice",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Information & Rational Expectations, practice",
    "text": "Information & Rational Expectations, practice\n\nIn the real world, what usually happens is that informed investors (i.e, those that have more attention, such as security analysts, professional traders, fund managers etc) get the information and trade faster than naive investors\nThis unbalance of information makes the market not fully efficient sometimes (especially when new information arrives)\n\nIf all investors had the same information, when new information arrives, the prices wouuld adjust right away, often without trade\nAs some investors have (or process) the information faster than others, the market portfolio may not reflect the the efficient one"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#the-behavior-of-individual-investors-1",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#the-behavior-of-individual-investors-1",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "The Behavior of Individual Investors",
    "text": "The Behavior of Individual Investors\n\nIf there is something that may put the market portfolio away from the efficient portfolio, it has to do with how investors make decisions around their portfolios\nIn this subsection, we discuss several biases that individual investors have when building their personal portfolio\nSome of these individual biases included, but are not limited, to:\n\n\nFamiliarity bias\nRelative Wealth Concerns\nOverconfidence\nSensation Seeking"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#individual-bias-1-underdiversification-bias",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#individual-bias-1-underdiversification-bias",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Individual Bias #1: Underdiversification bias",
    "text": "Individual Bias #1: Underdiversification bias\n\nBy appropriately diversifying their portfolios, investors can reduce risk without reducing their expected return. In that sense, diversification is a “free lunch” that all investors should take advantage of\nOne bias that appears in many countries is the underdiversification bias. In other words, there is much evidence that individual investors fail to diversify their portfolios adequately\nSome potential explanations for the underdiversification bias are:\n\nFamiliarity Bias: investors favor investments in companies with which they are familiar\nRelative Wealth Concerns: investors care more about the performance of their portfolios relative to their peers"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#individual-bias-2-excessive-trading-and-overconfidence",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#individual-bias-2-excessive-trading-and-overconfidence",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Individual Bias #2: Excessive Trading and Overconfidence",
    "text": "Individual Bias #2: Excessive Trading and Overconfidence\n\nAccording to the CAPM, investors should hold risk-free assets in combination with the market portfolio of all risky securities\nBecause the market portfolio is a value-weighted portfolio, it is also a passive portfolio in the sense that an investor does not need to trade frequently to maintain it\nEmpirical evidence shows that individual investors often trade beyond what is predicted by the CAPM. Some reasons may include:\n\nOverconfidence Bias: like sports fans, investors believe they can pick winners and losers when, in fact, they cannot; this leads them to trade too much! Furthermore, men tend to be more overconfident than women, with more trading and lower returns\nSensation Seeking: an individual’s desire for novel and intense risk-taking experiences\n\nBy the simple fact that naive investors trade too often, they should get lower returns due to trading costs (brokerage costs, fees, etc)"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#biases-and-the-capm",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#biases-and-the-capm",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Biases and the CAPM",
    "text": "Biases and the CAPM\n\nIf investors do not hold the market portfolio, does it mean that CAPM is not a good model?\n\nIf individuals depart from the CAPM in random ways, then these departures will tend to cancel out\nThese naive investors might only be trading erratically, thus not really affecting the market\n\nThis should not put us far from what is predicted by the CAPM: if investors are just departing from the the market porfolio in random ways, it should still be the efficient portfolio…\n…if that is true, under which circumstances the CAPM implication that the market portfolio is efficient fail?\n\n\nIf investors depart from the market portfolio in a consistent and predictable manner, imparting systematic uncertainty into prices\nFor investors’ trades to be correlated in this way, they must share a common motivation, which we’ll refer to as a “systematic bias”"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#systematic-trading-biases",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#systematic-trading-biases",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Systematic Trading Biases",
    "text": "Systematic Trading Biases\n\nFor the behavior of individual investors to impact market prices, and thus create a profitable opportunity for more sophisticated investors, there must be predictable, systematic patterns in the types of errors individual investors make\nMuch of these biases are studied within a relatively new field of behavioral economics (and behavioral finance)\nAs we’ll see in the upcoming slides, there are some predictable, systematic patterns in the types of errors individual investors make and that would create a profit opportunity for sophisticated investors that include, but are not limited, to:\n\n\nDisposition Effect\nInvestor attention, Mood, and Experience\nHerd Behavior"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#systematic-bias-1-disposition-effect",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#systematic-bias-1-disposition-effect",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Systematic Bias #1: Disposition Effect",
    "text": "Systematic Bias #1: Disposition Effect\n\nAn investor holds on to stocks that have lost their value and sell stocks that have risen in value since the time of purchase\n\nSuggests a reluctance to “admit a mistake” by taking the loss, which is more common in non-sophisticated investors\nThis behavioral tendency to sell winners and hang on to losers is costly from a tax perspective: as capital gains are taxed only when the asset is sold, it is optimal for tax purposes to postpone taxable gains by continuing to hold profitable investments\n\nHanging on to losers and selling winners might make sense if investors forecast that the losing stocks would ultimately “bounce back” and outperform the winners going forward.\nWhile investors may in fact have this belief, it does not appear to be justified in practice, as “losing stocks” continue to underperform the “winner stocks” that were sold\n\n\n\\(\\rightarrow\\) See Thinking, Fast and Slow, by Daniel Kahneman"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#systematic-bias-2-attention-grabbing-stories-and-mood",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#systematic-bias-2-attention-grabbing-stories-and-mood",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Systematic Bias #2: Attention-grabbing stories and mood",
    "text": "Systematic Bias #2: Attention-grabbing stories and mood\n\nIndividual investors generally are not full-time traders. As a result, they have limited time and attention to spend on their investment decisions, and so may be influenced by attention grabbing news stories or other events\nStudies show that individuals are more likely to buy stocks that have recently been in the news, engaged in advertising, experienced exceptionally high trading volume, or have had extreme returns\nA recent study from Brazil shows that living in a small city close to a firm’s local store more than doubles the likelihood of an individual picking its stock to day-trade - see here\nFurthermore, there also seems to be the case that investors mood is a relevant driver:\n\nSunshine generally has a positive effect on mood, and studies have found that stock returns tend to be higher when it is a sunny day at the location of the stock exchange\nPeople who grew up and lived during a time of high stock returns are more likely to invest in stocks than are people who experienced times when stocks performed poorly"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#systematic-bias-3-herd-behavior",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#systematic-bias-3-herd-behavior",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Systematic Bias #3: Herd Behavior",
    "text": "Systematic Bias #3: Herd Behavior\n\nThus far, we have considered common factors that might lead to correlated trading behavior by investors, such as psychological biases that affected all investors. But what if investors are actually trying to mimic each others behavior?\nWe call herd behavior a situation when investors make similar trading errors because they are actively trying to follow each others behavior\n\nNon-sophisticated investors might believe others have superior information that they can take advantage of by copying their trades\nDue to relative wealth concerns, individuals choose to herd in order to avoid the risk of underperforming their peers\n\n\n\n\n\n\n\n\n\n(Coordinated) Herd Behavior - The Reddit - Wall St. Bets coordination\n\n\nIn late January 2021, Reddit traders took on the short-sellers by forcing them to liquidate their short positions using GameStop stocks. This coordinated behavior had significant repercussions for various investment funds, such as Melvin Capital - see here."
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#gamestop-ticker-gme-during-the-wall-street-bets-frenzy",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#gamestop-ticker-gme-during-the-wall-street-bets-frenzy",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "GameStop (ticker: GME) during the Wall Street Bets frenzy",
    "text": "GameStop (ticker: GME) during the Wall Street Bets frenzy"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#implications-of-systematic-behavioral-biases",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#implications-of-systematic-behavioral-biases",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Implications of Systematic Behavioral Biases",
    "text": "Implications of Systematic Behavioral Biases\n\nIf non-sophisticated individual investors are engaging in strategies that earn negative alphas, it may be possible for more sophisticated investors to take advantage of this behavior and earn positive alphas at their expense\nWhat is surprising, however, is that these mistakes persist even though they may be economically costly and there is a relatively easy way to avoid them: buying and holding the market portfolio!\nRegardless of why individual investors choose not to protect themselves by holding the market portfolio, the fact that they don’t has an important implication for the CAPM: when individual investors make mistakes, sophisticated investors may earn a positive return at the expense of the non-sophisticated ones!"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#the-efficiency-of-the-market-portfolio-1",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#the-efficiency-of-the-market-portfolio-1",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "The Efficiency of the Market Portfolio",
    "text": "The Efficiency of the Market Portfolio\n\nWhen individual investors make mistakes, can sophisticated investors easily profit at their expense? In other words, are these biases systematic and pervasive enough so that sophisticated investors can profit from them?\nIn order for sophisticated investors to profit from investor mistakes, two conditions must hold:\n\nThe mistakes must be sufficiently pervasive and persistent to affect stock prices - they need to push prices so that the non-zero \\(\\alpha\\) opportunities become apparent\nThere must be limited competition to exploit these non-zero alpha opportunities - if competition is competition is too intense, these opportunities will be quickly eliminated before any trader can take advantage of them in a significant way\n\nIn what follows, we’ll see some potential evidence that individual or professional investors can outperform the market without taking on additional risk"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#trading-on-news-takeover-announcements",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#trading-on-news-takeover-announcements",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Trading on News: takeover announcements",
    "text": "Trading on News: takeover announcements\n\nA natural place to look for profitable trading opportunities is in reaction to big news announcements or analysts’ recommendations: if enough other investors are not paying attention, perhaps one can profit from these public sources of information\nFor example, investors can try to profit from takeover offers (Mergers and Acquisitions):\n\nTypically, the offer has for a significant premium to the target’s current stock price\nWhile the target’s stock price typically jumps on the announcement, it often does not jump completely to the offer price\nAs a result, a reasonable trading strategy would be to buy at the announcement and sell after during the effective takeover\n\nHowever, there is uncertainty regarding i) if the deal will actually occur; and ii) conditional on occurring, if it will be at the previous offer price\nPredict whether the firm would ultimately be acquired, we could earn profits trading on that information"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#trading-on-news-takeover-announcements-illustration",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#trading-on-news-takeover-announcements-illustration",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Trading on News: takeover announcements, illustration",
    "text": "Trading on News: takeover announcements, illustration"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#trading-on-news-stock-recommendations",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#trading-on-news-stock-recommendations",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Trading on News: Stock Recommendations",
    "text": "Trading on News: Stock Recommendations\n\nYou may have heard about Jim Cramer, from Mad Money, or other “stockpicker” social influencers covering financial markets. Do investors profit from following recommendations from these influencers? Evidence shows that:\n\nIn the case where there is news about the stock that is being recommended, it appears that the stock price correctly reflects this information the next day, and stays flat (relative to the market) subsequently\nOn the other hand, for the stocks that have been recommended, but without relevant news, there appears to be a significant jump in the stock price the next day, but the stock price then tends to fall relative to the market over the next several weeks\n\nThese “losing stocks” tended to be smaller, suggesting that individual investors who buy these stocks based on Cramer’s recommendation pushed the price too high\nSo why don’t we bet against Jim Cramer? As a matter of fact, someone did - see here"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#mad-money-jim-cramer-effect",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#mad-money-jim-cramer-effect",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Mad Money Jim Cramer effect",
    "text": "Mad Money Jim Cramer effect"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#mad-money-jim-cramer-effect-1",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#mad-money-jim-cramer-effect-1",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Mad Money Jim Cramer effect",
    "text": "Mad Money Jim Cramer effect"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#performance-of-fund-managers",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#performance-of-fund-managers",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Performance of fund managers",
    "text": "Performance of fund managers\n\nAfter these disappointing evidence, you decide not to try actively beating the market - the best you can do is to hire someone that is a more informed investor than you. Will this strategy pay out? Empirical evidence shows that:\n\nThe average mutual fund manager can generate value (before computing trading costs and fees, i.e., “gross alpha”)\nThe median mutual fund manager, on the other hand, destroys value\nOnly a small portion of managers are skilled enough to add value, according to this reference, in terms of net alpha\n\nBecause individual investors pay fees to fund managers, the net alpha is negative - you should be better-off by putting your money on a passively-managed fund!\nThat is, on average, fund managers (“active” strategies) do not provide value after fees, comparing to index funds (“passive strategies”)"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#performance-of-fund-managers-continued",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#performance-of-fund-managers-continued",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Performance of fund managers, continued",
    "text": "Performance of fund managers, continued\n\nIf fund managers are high-skilled investors, why they have a hard time adding value?\nOne reason why it might be difficult to add value is because there is a trap of liquidity:\n\nIf a manager is perceived as skilled, the deposits will grow, making harder to find above-average investment opportunities - that is why you see a lot of closed-end funds\nPerformance would converge to the mean, at best\n\nAt the end of the day, the market is competitive and people profit following the theoretical predictions\n\nSkilled managers are recompensated for their skills. They capture the economic rents associated with their skills\nInvestors are not recompensated for the skills of the managers they select - in the end, they derive little benefit, because this superior performance is captured by the manager in the form of fees"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#the-efficiency-of-the-market-portfolio-between-winners-and-losers",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#the-efficiency-of-the-market-portfolio-between-winners-and-losers",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "The Efficiency of the Market Portfolio: between winners and losers",
    "text": "The Efficiency of the Market Portfolio: between winners and losers\n\nWe saw a series of potential biases that appear in real financial markets and how they can impact investor behavior\nTo what it concerns us, we should ask yourselves: is this changing the CAPM prediction? As it stands, the evidence seems to support the CAPM prediction that investors should still hold the market portfolio!\nAll in all, beating the market should requires special skills or lower trading costs, which uninformed, individual investors don’t have"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#style-based-techniques-efficiency-1",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#style-based-techniques-efficiency-1",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Style-Based Techniques & efficiency",
    "text": "Style-Based Techniques & efficiency\n\nIn the previous section, we discussed potential biases that individual investors might have\nAll in all, they all point to the fact that sophistication plays a role:\n\nMore sophisticated investors should be less prone to individual and systematic biases (in theory, at least)\nAs a consequence, uninformed, low-skilled investors would be better-off if they simply hold the market portfolio\n\nIn particular, many fund managers distinguish their trading strategies based on the types of stocks they tend to hold; specifically, small versus large stocks, and value versus growth stocks\nIn what follows, we will consider these alternative investment styles, and see whether some strategies have generated higher returns historically than the CAPM predicts"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#size-effect",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#size-effect",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Size Effect",
    "text": "Size Effect\n\nIdea: small market capitalization stocks have historically earned higher average returns than the market portfolio, even after accounting for their higher betas. A way to replicate this thesis is to split stocks each year into 10 portfolios by ranking them based on their market capitalizations:\n\nThe first portfolio had the 10% smallest stocks in terms of market capitalization\nThe second portfolio had the 20% smallest stocks; and so on, until…\nThe tenth portfolio had the 10% biggest stocks in terms of market capitalization\n\n\n\nCalculating the monthly excess returns and the beta of each decile portfolio, we see that:\n\nPortfolios with higher betas yield higher future returns (as expected)\nMost portfolios were above the security market (\\(\\small \\alpha&gt;0\\)). The smallest deciles exhibit the most extreme effect - See (Berk and DeMarzo 2023) for a detailed explanation and an illustrative example (“Risk and the Market Value of Equity”)"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#book-to-market-ratio",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#book-to-market-ratio",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Book-to-Market Ratio",
    "text": "Book-to-Market Ratio\n\nAs with Size, a similar rationale could be applied to stocks that have higher levels of market-value of Equity vis-a-vis their historical values (book value of Equity)\nIdea: small market capitalization stocks have historically earned higher average returns than the market portfolio, even after accounting for their higher betas\n\nHigh book-to-market stocks have historically earned higher average returns than low book-to-market stocks\nStocks with high book-to-market ratios are value stocks, and those with low book-to-market ratios are growth stocks\n\nAs before, value stocks tend to present positive \\(\\alpha\\)"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#momentum",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#momentum",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Momentum",
    "text": "Momentum\n\nDo past returns explain future performance? Ideally, that shouldn’t be the case, but…\nIdea: rank stocks each month by their realized returns over the prior 6–12 months. They found that the best-performing stocks had positive alphas over the next 3–12 months:\n\nThis evidence goes against the CAPM: When the market portfolio is efficient, past returns should not predict alphas\nAs an investor, you could buy stocks that have had past high returns and (short) sell stocks that have had past low returns\n\n\n\n\\(\\rightarrow\\) Click here for an application that simulates a momentum-based strategy for U.S. stocks\n\nAll in all, these three factors (Size, Book-to-Market, and Momentun) are widely famous as the three Fama-French factors"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#what-if-alpha-is-consistently-different-from-zero",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#what-if-alpha-is-consistently-different-from-zero",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "What if \\(\\alpha\\) is consistently different from zero?",
    "text": "What if \\(\\alpha\\) is consistently different from zero?\n\nLet’s go back to our \\(\\alpha\\) definition for a given stock \\(i\\):\n\n\n\\[\n\\alpha_i = E[R_i] - R_i\n\\]\n\nAs we discussed, if you assume that CAPM is the correct model to explain expected returns, competition in financial markets should make \\(\\alpha \\rightarrow 0\\) in equilibrium:\n\nStocks above the SML are cheap, so the prices should rise (positive alpha).\nStocks below the SML are expensive, so the prices should drop (negative alpha).\n\nHowever, over the years since the discovery of the CAPM, it has become increasingly clear that forming portfolios based on market capitalization, book-to-market ratios, and past returns, investors can construct trading strategies that have a positive alpha\nWhy? There can be two reasons why positive-alpha strategies exist in a persistent way"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#why-alpha-is-consistently-different-from-zero",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#why-alpha-is-consistently-different-from-zero",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Why \\(\\alpha\\) is consistently different from zero?",
    "text": "Why \\(\\alpha\\) is consistently different from zero?\n\nReason #1: Investors are systematically ignoring positive-NPV investment opportunities:\n\nThe CAPM correctly computes required risk premiums, but investors are ignoring opportunities to earn extra returns without bearing any extra risk\nThat could happen either because they are unaware of them or because the costs to implement the strategies are larger than the NPV of undertaking them\n\n\n\n\\(\\rightarrow\\) This explanation goes straight to the hypotheses outlined by the CAPM!\n\nAccording to the CAPM, the only way a positive-NPV opportunity can persist in a market is if some barrier to entry restricts competition. Nowadays, this hypothesis seems unlikely:\n\nInformation required to form the portfolios is readily available;\nTrading costs are decreasing"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#why-alpha-is-consistently-different-from-zero-continued",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#why-alpha-is-consistently-different-from-zero-continued",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Why \\(\\alpha\\) is consistently different from zero? Continued",
    "text": "Why \\(\\alpha\\) is consistently different from zero? Continued\nReason #2: The positive-alpha trading strategies contain risk that investors are unwilling to bear but the CAPM does not capture:\n\nA stock’s beta with the market portfolio does not adequately measure a stock’s systematic risk\nBecause of that, the CAPM does not correctly compute the risk premium as it leaves out important risk factors that investors care about!\n\n\nIn other words way, the profits (positive alphas) from the trading strategy are really returns for bearing risk that investors are averse to but the CAPM does not capture\nAs a consequence, the market portfolio is not efficient. The next slide discuss some reasons why the market portfolio might not be the efficient one"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#potential-explanations",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#potential-explanations",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Potential explanations",
    "text": "Potential explanations\n\nSome reasons of why positive-alpha strategies can persist can be inherently tied to the assumptions tied out to the CAPM definition:\n\nProxy error: we might be not using a good proxy for the market portfolio\nBehavioral biases: we have made assumptions on investor behavior, but it might be the case that non-sophisticated investors find hard do approximate their portfolio to the market portfolio\nAlternative Risk Preferences and Non-Tradable Wealth:: we assumed that investor would always seek for the best risk \\(\\times\\) return combination. However, investors may stick with inefficient portfolios because they care about risk characteristics other than the volatility of their traded portfolio. For instance, they prefer to not be exposed to the sector they work in or to specific industries (i.e., ESG-based decisions)"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#multifactor-models-of-risk-1",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#multifactor-models-of-risk-1",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Multifactor Models of Risk",
    "text": "Multifactor Models of Risk\n\nWe previously defined that the required return for any given security \\(i\\) should follow:\n\n\n\\[\nE[R_i] = R_f + \\beta_i^P \\times (E[R_P - R_f])\n\\]\n\nAt first, we were agnostic on what \\(P\\) should stand for\nWhen we introduced the CAPM, we claimed that \\(\\small P=M\\) - i.e, the efficient portfolio is the market portfolio\nHowever, we saw several real-world frictions that might yield us the uncomfortable outcome that market portfolio is not efficient!\n\n\n\n\\(\\rightarrow\\) When the market portfolio is not efficient, we have to find a method to identify an efficient portfolio before we can use the above equation!"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#multifactor-models-of-risk-2",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#multifactor-models-of-risk-2",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Multifactor Models of Risk",
    "text": "Multifactor Models of Risk\n\nWhen we introduced the CAPM, we implicitly assumed that there was a single portfolio (or “factor”) that represented the efficient portfolio: the market (a “single factor” portfolio)\nHowever, it is not actually necessary to identify the efficient portfolio itself, as long as you identify a collection of portfolios from which the efficient portfolio can be constructed\nA Multi-Factor Model is a pricing model that uses more than one portfolio (“factors”) to approximate the efficient portfolio:\n\n\n\\[\n\\small E[R_i] = R_f + \\beta_i^{\\text{F1}} \\times \\underbrace{(E[R_{\\text{F1}} - R_f])}_{\\text{Excess return for Factor 1}}+ \\beta_i^{\\text{F2}} \\times \\underbrace{(E[R_{\\text{F2}} - R_f])}_{\\text{Excess return for Factor 2}}+...+\\beta_i^{\\text{Fn}} \\times \\underbrace{(E[R_{\\text{Fn}} - R_f])}_{\\text{Excess return for Factor n}}\n\\]\n\nEach \\(\\beta_i^{n}\\) here is called a factor beta: like the CAPM, it is the expected % change in the excess return of a security for a 1% change in the excess return of that factor portfolio, holding everything else constant"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#multifactor-models-of-risk-continued",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#multifactor-models-of-risk-continued",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Multifactor Models of Risk, continued",
    "text": "Multifactor Models of Risk, continued\n\nThe previous equation showed that that we can write the risk premium of any marketable security as the sum of the risk premium of each factor multiplied by the sensitivity of the stock with that factor:\n\nSingle-factor: We use an presumably efficient portfolio, it will alone capture all systematic risk (for example, the CAPM)\nMultifactor: If we use multiple portfolios as factors, then together these factors will capture all systematic risk - this is also known as the Arbitrage Pricing Theory (APT)\n\nMultifactor models allow investors to break the risk premium down into different risk factors:\n\nAs they might not be equally averse to the different factors, multifactor models allows investors to tailor their risk exposure\nThis idea of tailoring risk exposures based on common risk factors has become increasingly known amongst practitioners as a smart beta strategy - click here for an extensive list of factor ETFs from Fidelity"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#which-factors-portfolios-to-use",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#which-factors-portfolios-to-use",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Which factors (portfolios) to use?",
    "text": "Which factors (portfolios) to use?\n\nIf investors can tailor their risk exposure to specific risk factors, then the next question is: which risk factors an investor should be exposed to? Some examples:\n\nMarket Strategy: the most straightforward example is to expose to the market itself, like the CAPM did. Even if the market portfolio is not efficient, it still captures many components of systematic risk\nMarket Capitalization Strategy: a trading strategy that each year buys portfolio S (small stocks) and finances this position by short selling portfolio B (big stocks) has produced positive risk-adjusted returns historically. This is called a small-minus-big (SMB) portfolio\nBook-to-Market Strategy: a trading strategy that each year buys a portfolio of growth stocks and finances it by selling value stocks. This is called a high-minus-low (HML)) portfolio\nPast Returns Strategy: a portfolio that goes long the top past-return stocks (1 year) and short the bottom ones. The resulting self-financing portfolio is known as the prior one-year momentum (PR1YR) portfolio"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#example-fama-french-carhart-ffc",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#example-fama-french-carhart-ffc",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Example: Fama-French-Carhart (FFC):",
    "text": "Example: Fama-French-Carhart (FFC):\n\nA direct application of the previous slide is the Fama-French-Carhart (FFC) portfolio, which aggregates all the risk factors discussed before:\n\n\n\\[\\small E[R_i] = R_f + \\beta_s^m \\times \\underbrace{(E[R_m]− R_f)}_{\\text{Market}}  + \\beta_s^{SMB} \\times \\underbrace{E[R_{SMB}]}_{\\text{Size}} + \\beta_s^{HML} \\times \\underbrace{E[R_{HML}]}_{\\text{Market Cap.}} + \\beta_s^{Mon} \\times \\underbrace{E[R_{Mom}]}_{\\text{Past Returns}} \\]\n\nNote that we can price the required returns for a given security \\(i\\) according to its exposure (the \\(\\beta\\)’s) to each of the factor portfolios\nBefore, we claimed using the CAPM that only the first factor should drive required returns (i.e., the market)\nNow, our measure for the efficient portfolio is to say that investors also care about other risk factors, and because of that, the exposure of a given security needs to take that into account when estimating the required returns!"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#fama-french-carhart-portfolio-returns",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#fama-french-carhart-portfolio-returns",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Fama-French-Carhart portfolio returns",
    "text": "Fama-French-Carhart portfolio returns\n\n\n\n\n\n\nThese are the returns for investing in each portfolio\nIf the investment that you are trying to estimate the required returns is exposed to these portfolios, we then use these estimates with the estimates \\(\\beta\\)’s to get to the expected return"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#multifactor-models-of-risk-example",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#multifactor-models-of-risk-example",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Multifactor Models of Risk, example",
    "text": "Multifactor Models of Risk, example\n\nYou are considering making an investment in a project in the fast food industry. You determine that the project has the same level of non-diversifiable risk as investing in McDonald’s stock. Determine the cost of capital by using the FFC factor specification assuming a monthly risk-free rate of 0.20% and the factor returns from the previous slide.\n\n\n\n\n\nFactor\nBeta\n\n\n\n\nMarket\n0.72\n\n\nSMB\n-0.6\n\n\nHML\n0.14\n\n\nPR1YR\n0.09"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#multifactor-models-of-risk-example-1",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#multifactor-models-of-risk-example-1",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Multifactor Models of Risk, example",
    "text": "Multifactor Models of Risk, example\n\nUsing the FFC specification:\n\n\n\\[\n\\small \\underbrace{0.2}_{\\text{Risk-free}}\\%+\\underbrace{(0.68\\%-0.2\\%)}_{\\text{Market Excess Return}}\\times{0.72}+\\underbrace{(0.2\\%)}_{SMB}\\times{-0.6}+\\underbrace{(0.35\\%)}_{HML}\\times{0.14}+\\underbrace{(0.64\\%)}_{PR1YR}\\times{0.09}=0.68\\%\n\\]\n\nIn annual terms (no compoundind), this is approximately \\(\\small 0.68\\% \\times 12=8.16\\%\\)\nAs a comparison, a standard CAPM regression over the same time period leads to an estimated market beta of \\(\\small 0.58\\) for McDonald’s —the market \\(\\beta\\) differs from the estimate of 0.72 above because we are using only a single factor in the CAPM regression\nUsing the CAPM would have given us an estimated annual required return of \\(\\small (0.2\\%+0.58\\times0.68\\%)\\times12=7.1\\%\\)"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#methods-used-in-practice",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#methods-used-in-practice",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Methods Used in Practice",
    "text": "Methods Used in Practice\n\nGiven the evidence against and in favor of the CAPM and market efficiency, is the CAPM used in real-world applications?\nAccording to Financial Managers:\n\nA survey of CFOs found that 73.5% of the firms used the CAPM\n40% used historical average returns\n16% used the dividend discount model\nLarger firms were more likely to use the CAPM than were smaller firms\n\nAccording to investors: Investors\n\nIn a recent study of the different risk models examined, investor behaviorwas found to be most consistent with the CAPM.\nThe idea of this study is that by observing which investments investors rush into, it is possible to infer the risk model they are using"
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#practice",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#practice",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "Practice",
    "text": "Practice\n\n\n\n\n\n\nImportant\n\n\n\nPractice using the following links:\n\nMultiple-choice Questions\nNumeric Questions\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Material: an interview with Eugene Fama\n\n\nEugene Fama is widely known for his “Efficient Markets Hypothesis”. But what does it mean in practice? Do we really believe in efficient markets? Click here for an interview with Eugene Fama - access via Financial Times.\n\\(\\rightarrow\\) All FGV-EAESP students are entitled to a Financial Times subscription at no cost."
  },
  {
    "objectID": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#references",
    "href": "fin-strat/Lecture 5 - Investor Behavior and Capital Market Efficiency/index.html#references",
    "title": "Investor Behavior and Capital Market Efficiency",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nBrealey, R. A., S. C. Myers, and F. Allen. 2020. Principles of Corporate Finance. The Irwin/McGraw-Hill Series in Finance, Insurance, and Real Estate. McGraw-Hill Education. https://books.google.com.br/books?id=nsrHuwEACAAJ."
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#outline",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#outline",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\n(Berk and DeMarzo 2023)\n(Brealey, Myers, and Allen 2020)\n\nStudy review and practice: I strongly recommend using Prof. Henrique Castro (FGV-EAESP) materials. Below you can find the links to the corresponding exercises related to this lecture:\n\nMultiple Choice Exercises - click here\nNumeric Exercises - click here\n\n\n\n\\(\\rightarrow\\) For coding replications, whenever applicable, please follow this page or hover on the specific slides with coding chunks"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-expected-return-of-a-two-stock-portfolio",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-expected-return-of-a-two-stock-portfolio",
    "title": "Optimal Portfolio and the CAPM",
    "section": "The Expected Return of a two-stock portfolio",
    "text": "The Expected Return of a two-stock portfolio\n\nPreviously, we looked at returns from individual assets, but investors are always looking for ways to invest in multiple assets at the same time. What if you hold a portfolio of \\(n\\) assets?\nLet’s keep it simple for now. Suppose you have a two-stock portfolio of assets that hipotetically consists of:\n\nAmazon (AMZN) 40% of the portfolio, 10% return and standard deviation (volatility) of 25%\nFerrari (RACE): 60% of the portfolio, 15% return and stardard deviation (volatility) of 30%\n\nIf your portfolio is 40% Amazon and 60% Ferrari, then the return of your portfolio, \\(R_p\\) is:\n\n\n\\[\\small R_p= \\sum_{i=1}^{2}\\big(x_i\\times R_i\\big)=(0.4 \\times 10\\%) +(0.6 \\times 15\\%) = 13\\%\\]\n\n\n\\(\\rightarrow\\) In other words, the return of a portfolio is the weighted average of the individual asset returns!"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-expected-return-of-a-two-stock-portfolio-continued",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-expected-return-of-a-two-stock-portfolio-continued",
    "title": "Optimal Portfolio and the CAPM",
    "section": "The Expected Return of a two-stock portfolio, continued",
    "text": "The Expected Return of a two-stock portfolio, continued\n\n\n\n\n\n\nImportant\n\n\nThe weights are selected by the investors and may change over time if the prices change (unless the investor actively rebalances it to match the same proportion by buying/selling). Without trading, the weights increase for those stocks whose returns exceed the portfolio’s return.\n\n\n\n\nTo see that, assume that your initial holdings are $100,000:\n\nIf Amazon is 40% of the portfolio with a 10% return \\(\\small \\rightarrow 40,000\\times (1+10\\%)=44,000\\)\nIf Ferrari is 60% of the portfolio with a 15% return \\(\\small \\rightarrow 60,000\\times (1+15\\%)=69,000\\)\n\nYour holdings are now worth \\(\\small 44,000 + 69,000= 113,000\\), which yields the 13% return, but now the weights from each stock are different:\n\nAmazon holdings are \\(\\small \\frac{44}{113}\\approx 38.9\\%\\) of the portfolio\nFerrari holdings are \\(\\small \\frac{69}{113}\\approx 61.1\\%\\) of the portfolio"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-two-stock-portfolio",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-two-stock-portfolio",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Volatility of a two-stock portfolio",
    "text": "Volatility of a two-stock portfolio\n\nAs we now have a portfolio of two different assets, what should be the volatility? Recall that:\n\nAmazon (AMZN)’s standard deviation (volatility) was hipothetically 25%\nFerrari (RACE)’s stardard deviation (volatility) was hipothetically 30%\n\nAs we’ll see in the next slides, to compute the standard deviation of a portfolio, we cannot rely on the weighted average anymore!\nIn order to see that, let’s look at the historical prices from both stocks and compare:\n\nThe price trend from Amazon\nThe price trend from Ferrari\nThe dollar holdings for a portfolio of $100 that holds 40% Amazon and 60% Ferrari\n\nIn what follows, we’ll see how these dynamics shed light on the covariance of the portfolio"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-two-stock-portfolio-historical-trends",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-two-stock-portfolio-historical-trends",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Volatility of a two-stock portfolio, historical trends",
    "text": "Volatility of a two-stock portfolio, historical trends"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-two-stock-portfolio-summary-statistics",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-two-stock-portfolio-summary-statistics",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Volatility of a two-stock portfolio, summary statistics",
    "text": "Volatility of a two-stock portfolio, summary statistics\n\nIf we now look at the annualized statistics, we see that..\n\n\n\n\n\n\n\n\n\nThe weighted average between the individual volatilies is around 25.9%\nBut our portfolio volatility is lower than the individual volatilities. Why? Diversification!\n\n\nIntuitively, the returns from both stocks do not move in lockstep: whenever Ferrari prices goes down, Amazon prices move in a different fashion\nIn fact, the correlation between the returns of these stocks is around 0.39. In other words, these assets do not move in the same direction all the time!"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#an-interlude-variance-covariance-and-correlation",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#an-interlude-variance-covariance-and-correlation",
    "title": "Optimal Portfolio and the CAPM",
    "section": "An interlude: Variance, Covariance, and Correlation",
    "text": "An interlude: Variance, Covariance, and Correlation\n\nYou can find the proofs for both definitions in the Appendix\n\n\n\n\n\n\n\n\nDefinition\n\n\n\n\nThe Variance (\\(\\sigma^2\\)) is the squared deviations of the returns from their means: \\(\\small E[X-E(X)]^2\\)\nCovariance (\\(Cov\\)) is the expected product of the deviations of two returns from their means: \\(\\small [X-E(X)][Y-Y(X)]\\)\n\n\n\n\n\n\n\n\n\n\n\nTheorem I: the variance of the sum of two random variables equals the sum of the variances of those random variables, plus two times their covariance:\n\n\n\\[\n  \\sigma^2(A+B) = \\sigma^2_A + \\sigma^2_B + 2\\times Cov(A,B)\n  \\]\n\n\n\n\n\n\n\n\n\nTheorem II: The variance scales upon multiplication with a constant:\n\n\n\\[\n  \\sigma^2(\\beta \\times A ) = \\beta^2\\times \\sigma^2_A\n  \\]"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-two-stock-portfolio-continued",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-two-stock-portfolio-continued",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Volatility of a two-stock portfolio, continued",
    "text": "Volatility of a two-stock portfolio, continued\n\nLet’s now apply this to understand our portfolio’s volatility using daily data. Define:\n\n\\(\\sigma^2_1\\) = variance of Asset 1 (Amazon).\n\\(\\sigma^2_2\\) = variance of Asset 2 (Ferrari)\n\\(\\sigma_{1,2}\\) = covariance between both assets\n\\(w_1\\) and \\(w_2\\) are the weights for both assets\n\nUsing the definitions highlighted before, that the variance of our portfolio returns, \\(R_p\\), is:\n\n\n\\[\n\\small \\sigma^2(R_p)=\\sigma^2(w_1\\times R_1+w_2\\times R_2)=w_1^2\\times \\sigma^2_1 + w_2^2 \\times \\sigma^2_2 + 2\\times w_1\\times w_2\\times \\sigma_{1,2}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#variance-in-terms-of-correlation",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#variance-in-terms-of-correlation",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Variance in terms of correlation",
    "text": "Variance in terms of correlation\n\nWe can further simplify this using the fact that the Covariance is the product of the assets’ standard deviations and their correlation:\n\n\n\\[\n\\small \\sigma^2(R_p)=w_1^2\\times \\sigma^2_1 + w_2^2 \\times \\sigma^2_1 + 2\\times w_1\\times w_2\\times \\underbrace{Cov(R_1,R_2)}_{\\sigma_1 \\times \\sigma_2 \\times Corr_{12}}\\\\\n\\small = \\underbrace{w_1^2\\times \\sigma^2_1}_{1} + \\underbrace{w_2^2 \\times \\sigma^2_1}_{2} + \\underbrace{2\\times w_1\\times w_2\\times \\sigma_1 \\times \\sigma_2 \\times Corr_{12}}_{3}\n\\]\n\n\n\nThe first term relates to the first asset (in our case, Amazon) variance and its proportion in the portfolio\nThe second term relates to the second asset (in our case, Ferrari) variance and its proportion in the portfolio\nThe third term relates to the relationship between both assets"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#a-note-on-sample-analogues",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#a-note-on-sample-analogues",
    "title": "Optimal Portfolio and the CAPM",
    "section": "A note on sample analogues",
    "text": "A note on sample analogues\n\nRecall that we cannot rely on expectations to future outcomes and/or probabilities to calculate the expected returns of the portfolio and its volatility\nBecause of that, as we did before, we replace the expectation estimator, \\(E(\\cdot)\\), with our sample analogue, which is the sample average, and it is always backward-looking:\n\nFix a period for calculation (for example, the latest 30 days)\nCalculate the risk and return using sample averages\n\nTo facilitate the notation, we’ll always refer to it using a bar: \\(\\overline{R}\\). For example the covariance between the returns from stocks \\(i\\) and \\(j\\) is:\n\n\n\\[Cov(R_i,R_j) = \\frac{\\sum_{1}^{T}(R_i-\\overline{R_i} ) \\times (R_j-\\overline{R_j})}{T-1}\\]"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#getting-back-to-our-portfolio",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#getting-back-to-our-portfolio",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Getting back to our portfolio",
    "text": "Getting back to our portfolio\n\n\n\n\n\n\n\nUsing our formula for the variance of the portfolio:\n\n\n\\[\n\\small \\sigma^2_p= w_1^2\\times \\sigma^2_1 + w_2^2 \\times \\sigma^2_1 + 2\\times w_1\\times w_2\\times \\sigma_1 \\times \\sigma_2 \\times Corr_{12}\\\\\n\\small \\underbrace{\\small 0.4^2\\times 0.3290^2+0.6^2\\times 0.2124^2}_{I} + \\underbrace{2\\times 0.4\\times0.6\\times 0.3290\\times 0.2124\\times 0.3886}_{II} = 0.04659402\n\\]\n\nTherefore, \\(\\sigma_{p}\\) is simply \\(\\small \\sqrt{0.04659402}=0.2159\\) or 21.59%. You could also calculate the daily portfolio variance and annualize it at the end multiplying it by \\(\\sqrt{n}\\)"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#variance-of-a-2-stock-portfolio-continued",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#variance-of-a-2-stock-portfolio-continued",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Variance of a 2-stock portfolio, continued",
    "text": "Variance of a 2-stock portfolio, continued\n\nLooking at our formula, we have:\n\n\\[\n\\small \\sigma^2_p= w_1^2\\times \\sigma^2_1 + w_2^2 \\times \\sigma^2_1 + 2\\times w_1\\times w_2\\times \\sigma_1 \\times \\sigma_2 \\times Corr_{12}\\\\\n\\small \\underbrace{\\small 0.4^2\\times 0.3290^2+0.6^2\\times 0.2124^2}_{I} + \\underbrace{2\\times 0.4\\times0.6\\times 0.3290\\times 0.2124\\times 0.3886}_{II} = 0.04659402\n\\]\n\nThe first term (I) captures the individual variances contribution\nThe second term (II) captures the relationship between assets (note that 0.3312 is the correlation between both individual returns)"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#wrapping-up---2-stock-portfolio-volatility",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#wrapping-up---2-stock-portfolio-volatility",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Wrapping up - 2-stock portfolio volatility",
    "text": "Wrapping up - 2-stock portfolio volatility\n\nThese assets have roughly similar historical return and volatility, but they ‘move’ very differently:\n\n\nFor example, when Amazon performed well, Ferrari did not move in lockstep\nAs a matter of fact, their correlation is about 0.39, meaning that their prices do not trend in the same direction most of the time!\n\n\nAs you now see, the return of the portfolio is equal to the weighted average of the individual returns…\n…However, the volatility of is much lower than the volatility of the two individual stocks, as the assets are offseting each other and making the return “smoother”!\nHow much risk is eliminated when creating a portfolio? It depends on the degree to which the stocks face common risks and their prices move together (in mathematical terms, this will be captured by the correlation parameter, \\(\\small Corr_{12}\\)!"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-large-portfolio",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-large-portfolio",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Volatility of a large portfolio",
    "text": "Volatility of a large portfolio\n\nWhat if we added a third asset to our portfolio? Let’s say that you’re really into discretionary goods and decide to place a bet on Victoria’s Secret (ticker: VSCO), which had 30% of return in the previous last\nYou decided to keep the weight on Amazon and split Ferrari and Victoria’s Secret evenly, with 30% each\nHow does that play a role in your portfolio? As before, your portfolio Return is simply a weighted average of the individual returns:\n\n\n\\[\nR_{p}=\\sum_{i=1}^{3}w_i\\times R_i = \\underbrace{(0.4 \\times 10\\%)}_{\\text{Amazon}} +\\underbrace{(0.3 \\times 15\\%)}_{\\text{Ferrari}}+ \\underbrace{(0.3 \\times 25\\%)}_{\\text{VSCO}}\\approx 16\\%\n\\]\n\nLet’s see how this strategy performed from 2023 up to now!"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#ouch-youve-seen-to-be-worse-off-now",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#ouch-youve-seen-to-be-worse-off-now",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Ouch! You’ve seen to be worse off now!",
    "text": "Ouch! You’ve seen to be worse off now!"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#your-vsco-bet-did-not-play-well-unfortunately",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#your-vsco-bet-did-not-play-well-unfortunately",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Your VSCO bet did not play well, unfortunately",
    "text": "Your VSCO bet did not play well, unfortunately\n\n\n\n\n\n\n\nAdding VSCO to the portfolio severely hit the overall return, \\(\\small R_p\\)\nVSCO was also very volatile: its standard deviation was twice as high as Amazon. However, the portfolio volatility is substantially lower\nOne way to understand this is to look at the correlation matrix between the assets"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#vsco-is-not-strongly-correlated-with-the-other-assets",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#vsco-is-not-strongly-correlated-with-the-other-assets",
    "title": "Optimal Portfolio and the CAPM",
    "section": "VSCO is not strongly correlated with the other assets",
    "text": "VSCO is not strongly correlated with the other assets\n\n\n\n\n\n\n\nLooking at the correlation matrix, we can see that:\n\n\\(\\small Corr_{\\text{AMZN,VSCO}}=0.16\\)\n\\(\\small Corr_{\\text{RACE,VSCO}}=0.14\\)\n\nAs we can see, because the correlation coefficient is very small, the contribution of VSCO to the overall portfolio volatility is limited"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-variance-of-a-3-stock-portfolio",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-variance-of-a-3-stock-portfolio",
    "title": "Optimal Portfolio and the CAPM",
    "section": "The variance of a 3-stock portfolio",
    "text": "The variance of a 3-stock portfolio\n\nHow can we extend your variance calculation for three assets?\nThe general formulation of the variance formula shows us that:\n\n\n\\[\n\\small  Var(X)=Var(A+B+C) = E[((A+B+C)-E(A+B+C))]^2 =\\\\\n\\small E[(\\underbrace{[A-E(A)]}_{\\text{First Term}}+[\\underbrace{B-E(B)}_{\\text{Second Term}}]+[\\underbrace{C-E(C)}_{\\text{Third Term}}])^2] \\\\\n\\]\n\nThis is a quadratic form, and we can decompose it as:\n\n\n\n\\[\n\\small (A+B+C)^2= A^2+B^2+C^2 + 2AB + 2AC +2BC\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-variance-of-a-3-stock-portfolio-continued",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-variance-of-a-3-stock-portfolio-continued",
    "title": "Optimal Portfolio and the CAPM",
    "section": "The variance of a 3-stock portfolio, continued",
    "text": "The variance of a 3-stock portfolio, continued\n\nIn portfolio terms:\n\n\n\\[\n\\small \\sigma^2_p= \\underbrace{w_1^2\\times \\sigma^2_1}_{I} + \\underbrace{w_2^2 \\times \\sigma^2_1}_{II}\n\\small + \\underbrace{w_3^2 \\times \\sigma^3_1}_{III} \\\\\n\\small + \\underbrace{2\\times w_1\\times w_2\\times \\sigma_1\\times\\sigma_2\\times Corr_{12}}_{IV}\n\\small + \\underbrace{2\\times w_1\\times w_3\\times \\sigma_1\\times\\sigma_3\\times Corr_{13}}_{V}\n\\small + \\underbrace{2\\times w_2\\times w_3\\times \\sigma_2\\times\\sigma_3\\times Corr_{23}}_{VI}\n\\]\n\nI, II, and III capture the individual stock variance contribution\nIV captures the relationship between Amazon and Ferrari (as before)\nV captures the relationship between Amazon and VSCO (new!)\nVI captures the relationship between Ferrari and VSCO (new!)"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-variance-of-a-3-stock-portfolio-continued-1",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-variance-of-a-3-stock-portfolio-continued-1",
    "title": "Optimal Portfolio and the CAPM",
    "section": "The variance of a 3-stock portfolio, continued",
    "text": "The variance of a 3-stock portfolio, continued\n\nPlugging in the numbers from our exercise:\n\n\n\\[\n\\small \\underbrace{\\small (0.4^2\\times 0.3290^2)}_{I} + \\underbrace{(0.3^2\\times 0.2124^2)}_{II} + \\underbrace{(0.3^2\\times 0.5861^2)}_{III} \\\\\n\\small + \\underbrace{(2\\times0.4\\times0.3\\times0.3290\\times0.2124\\times0.39)}_{IV} + \\underbrace{(2\\times0.4\\times0.3\\times0.3290\\times 0.5861\\times0.16)}_{V} \\\\\n+\\small \\underbrace{(2\\times0.4\\times0.3\\times0.2124\\times 0.5861\\times0.14)}_{VI}=26.38\\%\n\\]\n\nAlthough VSCO had incurred in significant losses, its returns were minimally correlated to the the stocks"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-variance-of-a-large-portfolio",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-variance-of-a-large-portfolio",
    "title": "Optimal Portfolio and the CAPM",
    "section": "The variance of a large portfolio",
    "text": "The variance of a large portfolio\n\nWhat if you had a significantly high number \\(N\\) of assets in a portfolio? You can generalize the previous formulas using the definition below:\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe variance of a portfolio \\(P\\) with \\(N\\) stocks with weights \\(w_{1,2,...,N}\\) is:\n\\[\n\\sigma^2_p=\\sum_{i=1}^{N}w_i^2\\sigma_i^2 + 2\\times\\sum_{i=1}^{N}\\sum_{j\\neq i} w_i w_j \\sigma_{ij}\n\\]\n\nWhere:\n\n\\(\\sum_{i=1}^{N}w_i\\sigma_i^2\\) is the individual stock volatility contribution terms; and\n\\(2\\times\\sum_{i=1}^{N}\\sum_{j\\neq i} w_i w_j \\sigma_{ij}\\) represents all the covariance terms from every pairwise combination of stocks in the portfolio"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-large-portfolio---matrix-form",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-large-portfolio---matrix-form",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Volatility of a large portfolio - matrix form",
    "text": "Volatility of a large portfolio - matrix form\n\nFrom our previous slide, we saw that the variance of a portfolio was defined as: \\[\n\\sigma^2_p=\\sum_{i=1}^{N}w_i^2\\sigma_i^2 + 2\\times\\sum_{i=1}^{N}\\sum_{j\\neq i} w_i w_j \\sigma_{ij}\n\\]\nDefine \\(\\mathbf{w}\\) as the vector of stock’s weights, \\([w_1,w_2,w_3,...,w_n]\\) and \\(\\Sigma\\) as the covariance matrix of returns. Then, you can rewrite \\(\\sigma^2_p\\) as:\n\n\n\\[\n\\sigma^2_p = \\mathbf{w}'\\Sigma\\mathbf{w}\n\\]\n\nIn practice, writing the variance of a portfolio in matrix terms simplifies a lot of the calculations if you want to perform calculations using Excel with \\(n&gt;2\\)"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-large-portfolio---a-graphical-representation",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-large-portfolio---a-graphical-representation",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Volatility of a large portfolio - a graphical representation",
    "text": "Volatility of a large portfolio - a graphical representation\n\n\n\n\n\nDiagonal cells contain variance terms, and the off-diagonal cells contain the covariance terms\nAs \\(N\\) assets increases, the number of terms outside the main diagonal increases more than the main diagonal.\nTherefore, the variance of a well-diversified portfolio is mostly determined by the covariances, and not the individual stock variance terms!"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#limits-to-diversification",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#limits-to-diversification",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Limits to diversification",
    "text": "Limits to diversification\n\nHow much of the variance of a portfolio we can eliminate through diversification?\nIn practical terms:\n\nSo, if you increase the size of your portfolio, the risk decreases (until to a certain amount)\nUsually, about half of the initial variance can be eliminated through diversification!\n\nWhich type of risk is eliminated? Only the idiosyncratic risk! For example, firm-specific characteristics\nWhich type of risk remains? Only the systematic risk! For example, economic conditions\n\n\n\\(\\rightarrow\\) For details regarding the mathematics behind risk minimization through diversification, see example for an equally-weighted portfolio in the Appendix"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#limits-to-diversification-graphical-interpretation",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#limits-to-diversification-graphical-interpretation",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Limits to diversification, graphical interpretation",
    "text": "Limits to diversification, graphical interpretation"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#choosing-portfolios",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#choosing-portfolios",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Choosing Portfolios",
    "text": "Choosing Portfolios\n\nNow that we understand how to calculate the expected return and volatility of a portfolio, we can return to the main goal of the chapter: determine how an investor can create an efficient portfolio\nLet’s start of with the simplest case: create a portfolio with two stocks, Amazon and Ferrari. Previously, we’ve shown that, for the analysis period, we had the following results in terms of risk and return:\n\n\n\n\n\n\n\n\n\nLet’s create \\(5\\) different portfolios using \\(\\pm20\\%\\) of allocation weights in each asset"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#choosing-an-efficient-portfolio-continued",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#choosing-an-efficient-portfolio-continued",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Choosing an Efficient Portfolio, continued",
    "text": "Choosing an Efficient Portfolio, continued\n\n\n\n\n\n\n\nWe can plot this in a figure to show all possible risk \\(\\times\\) return combinations"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#choosing-an-efficient-portfolio-6-portfolios",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#choosing-an-efficient-portfolio-6-portfolios",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Choosing an Efficient Portfolio, 6 portfolios",
    "text": "Choosing an Efficient Portfolio, 6 portfolios"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#choosing-an-efficient-portfolio-11-portfolios",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#choosing-an-efficient-portfolio-11-portfolios",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Choosing an Efficient Portfolio, 11 portfolios",
    "text": "Choosing an Efficient Portfolio, 11 portfolios"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#choosing-an-efficient-portfolio-100-portfolios",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#choosing-an-efficient-portfolio-100-portfolios",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Choosing an Efficient Portfolio, >100 portfolios",
    "text": "Choosing an Efficient Portfolio, &gt;100 portfolios"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#choosing-an-efficient-portfolio-1",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#choosing-an-efficient-portfolio-1",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Choosing an Efficient Portfolio",
    "text": "Choosing an Efficient Portfolio\n\nAs a financial manager, one crucial job you have is to find portfolios that are not sub-optimal\n\nFor a given level of volatility, they deliver the highest possible return\nAlternatively, for a given level of return, they deliver the lowest possible volatility\n\nAn easy what to look at this is to identify the minimum variance portfolio (MVP)\n\nThis portfolio is, among all combinations, the one with the lowest volatility\nFrom there, if a given portfolio is riskier than the MVP, it needs to deliver higher returns!\nOn the other hand, if a portfolio is riskier than the MVP and deliver the same/lower returns, it can be considered inefficient\n\n\n\n\\(\\rightarrow\\) In other words: investors should look only for efficient portfolios and will choose based on his specific preferences for risk!"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-efficient-frontier",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-efficient-frontier",
    "title": "Optimal Portfolio and the CAPM",
    "section": "The Efficient Frontier",
    "text": "The Efficient Frontier\n\nIn our example, we used only two assets. What happens when we increase the number of potential assets?\nLet’s replicate the same rationale by now investing our money in three possible stocks: Amazon,Ferrari, and VSCO"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#which-of-these-portfolios-are-efficient",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#which-of-these-portfolios-are-efficient",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Which of these portfolios are efficient?",
    "text": "Which of these portfolios are efficient?"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#which-of-these-portfolios-are-efficient-1",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#which-of-these-portfolios-are-efficient-1",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Which of these portfolios are efficient?",
    "text": "Which of these portfolios are efficient?"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#choosing-an-efficient-portfolio-2",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#choosing-an-efficient-portfolio-2",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Choosing an Efficient Portfolio",
    "text": "Choosing an Efficient Portfolio\n\nWhat happens when you continuously increase the number of assets?\n\nIf you add stocks, you improve the frontier - i.e, you are able to create portfolios that span better options in terms of risk and return\nIf you continue adding assets, you will have what is called Efficient Frontier\n\nThe Efficient Frontier is the set of portfolios where:\n\nFor a given level of volatility, you have the highest possible return among all portfolios with the same volalitity level\nFor a given level of return, you have the lowest possible volatility among all portfolios with the same return level\n\nBased on this, is there a single portfolio in which all investors should hold? No! In practice, investors will choose among portfolios based on their specific preferences for risk and return"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-efficient-frontier-1",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-efficient-frontier-1",
    "title": "Optimal Portfolio and the CAPM",
    "section": "The Efficient Frontier",
    "text": "The Efficient Frontier"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#risk-free-saving-and-borrowing-1",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#risk-free-saving-and-borrowing-1",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Risk-Free Saving and Borrowing",
    "text": "Risk-Free Saving and Borrowing\n\nThus far, we have considered the risk and return possibilities that result from combining risky investments into portfolios\nBy including all risky investments in the construction of the efficient frontier, we achieve the maximum diversification possible with risky assets\nIn practice, however, we have the existence of risk-free assets, such as Treasury Bills, Treasury Bonds, Tesouro Direto etc\nUp to this point, we assumed that all the available assets had a minimum level of risk - for example, stocks\nIn what follows, we will investigate what happens when we add the possibility of investing in a risk-free asset\nAs you’ll see, when you add the risk-free asset, the implication is that there should only be only one efficient portfolio!"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#risk-free-saving-and-borrowing-2",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#risk-free-saving-and-borrowing-2",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Risk-Free Saving and Borrowing",
    "text": "Risk-Free Saving and Borrowing\n\nAssume that you have a risk-free asset, such as a Treasury Bill, where the return is \\(R_f\\) and, by definition, has no risk\nYou decided to invest a portion \\(x\\) of your holdings in the risky portfolio, and the remaining \\(1-x\\) in the risk-free asset\nThe expected return from your new portfolio, which we’ll call \\(R_{C}\\), is:\n\n\n\\[E[R_{C}] =  x \\times E[R_p] + (1-x) \\times R_f \\]\n\nRearranging terms, we have that:\n\n\n\n\\[\nE[R_{C}] =  x \\times E[R_p] + R_f - x \\times R_f\\rightarrow R_f + x \\times ( E[R_p] - R_f )\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#risk-free-saving-and-borrowing-continued",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#risk-free-saving-and-borrowing-continued",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Risk-Free Saving and Borrowing, continued",
    "text": "Risk-Free Saving and Borrowing, continued\n\nTherefore, the expected return of a portfolio that contains risky assets and a risk-free asset is defined as:\n\n\n\\[\nE[R_{C}] =  R_f + x \\times ( E[R_p] - R_f )\n\\]\n\nThis equation shows that the expected return of your portfolio is the sum of:\n\nThe risk-free rate; and\nA fraction of the portfolio’s risk premium, \\(E[R_p] - R_f\\), based on the fraction \\(x\\) that we invest in it\nA risk-premium is nothing more than the reward for bearing additional risk\n\nIn other words, if you increase \\(x\\), your expected gains should increase because you’re more exposed to risk!"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#risk-free-saving-and-borrowing-continued-1",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#risk-free-saving-and-borrowing-continued-1",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Risk-Free Saving and Borrowing, continued",
    "text": "Risk-Free Saving and Borrowing, continued\n\nWhat happens to the volatility of a portfolio when you add the possibility of a risk-free asset?\nRemember that the risk free rate is assumed to have no risk, and therefore its variance is zero. As a consequence, the standard deviation of your portfolio that contains risky and risk-free assets is:\n\n\n\\[\\small \\sigma_{R_{C}} = \\sqrt{(1-x)^2 \\times \\sigma^2_{R_f} + x^2 \\times \\sigma^2_{R_p}  + 2 \\times(1-x) \\times x \\times Cov(R_f, R_p)}\\]\n\nBecause \\(\\small \\sigma^2_{R_f}=0\\), this simplifies to:\n\n\n\n\\[\\sigma_{R_{C}} = \\sqrt{x^2 \\times \\sigma^2_{R_p}}\\rightarrow x \\times \\sigma_{R_p}\\]\n\nThat is, the volatility is only a fraction of the volatility of the portfolio, based on the amount we invest in it"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#risk-free-combinations-in-practice",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#risk-free-combinations-in-practice",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Risk-free combinations in practice",
    "text": "Risk-free combinations in practice\n\nIn order to make this point clear, let’s go back to the efficient frontier built on top of the \\(&gt;100\\) portfolios created using AMZN and RACE\nTo facilitate the comparison, we’ll convert risk and return to annual terms\nIf that is true, then the minimum-variance portfolio has:\n\nAnnual Return \\(\\approx 3.82\\%\\)\nAnnual Volatility \\(\\approx 3.24\\%\\)\n\nSuppose you have an risk-free investment opportunity, \\(R_f\\), that generates a risk-free return of 2%\nIf that is true, we can think about all linear combinations of \\([R_f,R_p]\\) in which the returns are a weighted average of \\(R_f\\) and \\(R_p\\) and the volatility is only a fraction of \\(R_p\\)’s volatility"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#risk-free-combinations-graphical-interpretation",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#risk-free-combinations-graphical-interpretation",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Risk-free combinations, graphical interpretation",
    "text": "Risk-free combinations, graphical interpretation"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#risk-free-saving-and-borrowing-3",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#risk-free-saving-and-borrowing-3",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Risk-Free Saving and Borrowing",
    "text": "Risk-Free Saving and Borrowing\n\nRemember that you can have short positions (“buying stocks on margin”) by:\n\nBorrowing at \\(R_f\\) to invest in a portfolio of risky assets\nInvesting all your funds + the money you borrow in the portolio \\(P\\)\n\nIf that is true, then you will have a negative weight in one asset (a short position) and a positive and &gt;1 weight in the other asset\nIf, as we’d imagine, \\(R_p&gt;R_f\\) (i.e, the return of the minimum-variance portfolio is higher than \\(R_f\\), we could proceed by:\n\nBorrowing money at \\(R_f\\)\nUsing this money to buy the minimum-variance portfolio, yielding a return of \\(R_p\\)\n\nIn practice, because your return is a weighted average of \\(R_f\\) and \\(R_p\\) and \\(R_p&gt;R_f\\), the linear combination line “extends” to the right as you are placing a higher weight on the asset with higher return"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#buying-stocks-on-margin",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#buying-stocks-on-margin",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Buying stocks on margin",
    "text": "Buying stocks on margin"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#selecting-the-portfolio",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#selecting-the-portfolio",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Selecting the portfolio",
    "text": "Selecting the portfolio\n\nAs before, is there a single portfolio in which all investors should hold? No! In practice, investors will choose among portfolios based on their specific preferences for risk and return\nBut you may have noticed something strange…the linear combination of \\(R_f\\) and \\(R_p\\) yields a sub-optimal portolio:\n\nThere are many portfolios that yield higher returns than the minimum-variance portfolio (and are also riskier)\nHowever, if we want to have a lower risk, we can simply combine them with the risk-free asset, \\(R_f\\), and the volatility of the portfolio will be only a fraction of \\(\\sigma_P\\)\n\nIn other words, we could be better off if we had done a linear combination using a portfolio that yields a better risk \\(\\times\\) return relationship!\nHow can we find such a portfolio? We need to think about the point where \\(\\small \\partial R_c/\\partial\\sigma_p\\) is the highest - in graphical terms, the portfolio with the highest slope!"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#buying-stocks-on-margin-1",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#buying-stocks-on-margin-1",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Buying stocks on margin",
    "text": "Buying stocks on margin"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#sharpe-ratio-and-the-tangent-portfolio",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#sharpe-ratio-and-the-tangent-portfolio",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Sharpe-ratio and the tangent portfolio",
    "text": "Sharpe-ratio and the tangent portfolio\n\nThe ideal portfolio that we want to combine with the risk-free asset is called the tangent portfolio:\n\nIt is tangent to the efficient frontier\nBecause of that, it has the highest risk \\(\\times\\) return combination\n\nTo identify the tangent portfolio, we compute the Sharpe Ratio:\n\n\n\\[\\text{Sharpe Ratio} = \\frac{E[R_p]-R_f}{\\sigma_{R_p}}\\]\n\nIn the Sharpe Ratio measures the ratio of reward-to-volatility provided by a portfolio:\n\n\nIt indicates the amount of excess returns a portfolio provides a “risk-adjusted” format\nHigher Sharpe Ratio \\(\\rightarrow\\) better risk \\(\\times\\) return relationship"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-use-of-the-sharpe-ratio",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-use-of-the-sharpe-ratio",
    "title": "Optimal Portfolio and the CAPM",
    "section": "The use of the Sharpe Ratio",
    "text": "The use of the Sharpe Ratio\n\nWhat is the optimal portfolio to combine with the risk-free asset? The one with the highest Sharpe Ratio! This portfolio is the one where the line with the risk-free investment is tangent to the efficient frontier of risky investments. There are two important facts about it:\n\n\nThe tangent portfolio is efficient.\nOnce we include the risk-free investment, all efficient portfolios are combinations of the risk-free investment and the tangent portfolio.\n\n\nTherefore, all investors should have the tangent portfolio. All investors should combine the tangent portfolio with the risk free asset to adjust the level of risk.\nIf you ignore the risk free asset, you have several efficient portfolios (efficient frontier). But once you combine with the risk free rate, there is only one!"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-efficient-portfolio-and-required-returns-1",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#the-efficient-portfolio-and-required-returns-1",
    "title": "Optimal Portfolio and the CAPM",
    "section": "The Efficient Portfolio and Required Returns",
    "text": "The Efficient Portfolio and Required Returns\n\nSuppose you hold a portfolio \\(P\\). How do you decide whether to include a new asset? In sum, you should include a new asset \\(i\\) in a portfolio if it increases the Sharpe Ratio of the resulting portfolio! Note that the new asset has the following properties:\n\nThe excess return that this asset \\(i\\) brings is \\(E[R_i] - R_f\\)\nOn the other hand, the risk that this asset \\(i\\) brings to your portfolio is \\(\\sigma_i \\times corr(R_i,R_p)\\) - see the Appendix for a detailed explanation\n\nTherefore, is the gain in excess return from investing in \\(i\\) sufficient to make up for the increase in risk? Think about this as your average grade in the university:\n\nIf you take a test and obtains a score that is higher than your average grade \\(\\rightarrow\\) your average grade will go up\nIf, on the other hand, your score is lower than your average \\(\\rightarrow\\) your average will go down\nFinally, if your score is exactly your average grade, your average grade remains unchanged"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#efficient-portfolio-and-required-returns-continued",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#efficient-portfolio-and-required-returns-continued",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Efficient Portfolio and Required Returns, continued",
    "text": "Efficient Portfolio and Required Returns, continued\n\nGoing back to our portfolio: if you compute the Sharpe Ratio of \\(i\\) and \\(P\\), including \\(i\\) is beneficial if and only if:\n\n\n\\[\\underbrace{\\frac{E[R_i] - R_f}{\\sigma_{R_i} \\times corr(R_i,R_p)}}_{\\text{Sharpe Ratio of } i} &gt; \\underbrace{\\frac{E[R_p] - R_f}{\\sigma_{R_p}}}_{\\text{Sharpe Ratio of } P}\\]\n\nMoving the denominator to the right-hand side, we have:\n\n\n\n\\[E[R_i] - R_f &gt;\\sigma_{R_i} \\times corr(R_i,R_p) \\times \\frac{E[R_p] - R_f}{\\sigma_{R_p}}\\]"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#efficient-portfolios-and-required-returns-continued",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#efficient-portfolios-and-required-returns-continued",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Efficient Portfolios and Required Returns, continued",
    "text": "Efficient Portfolios and Required Returns, continued\n\\[E[R_i] - R_f &gt; \\underbrace{\\frac{\\sigma_{R_i} \\times corr(R_i,R_p)}{\\sigma_{R_p}}}_{\\beta^P_i}  \\times (E[R_p] - R_f)\\]\n\nUsing a \\(\\beta\\) notation and moving \\(R_f\\) to the right-hand-side:\n\n\n\\[\nE[R_i]  &gt;  R_f + \\beta_i^P  \\times (E[R_p] - R_f)\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#efficient-portfolio-and-required-returns-continued-1",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#efficient-portfolio-and-required-returns-continued-1",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Efficient Portfolio and Required Returns, continued",
    "text": "Efficient Portfolio and Required Returns, continued\n\nTo conclude, increasing the amount invested in \\(i\\) will increase the Sharpe Ratio of portfolio \\(P\\) if its expected return \\(E[R_i]\\) exceeds its required return given portfolio \\(P\\), defined as:\n\n\n\\[\nE[R_i]  &gt;  R_f + \\beta_i^P  \\times (E[R_p] - R_f)\n\\]\n\nThe required return is the expected return that is necessary to compensate for the risk investment \\(i\\) will contribute to the portfolio.\n\n\nIt is is equal to the risk-free interest rate…\n…plus the risk premium of the current portfolio, P…\n… scaled by \\(i\\)’s sensitivity to \\(P\\), denoted by \\(\\beta_i^P\\)\n\n\nIf \\(i\\) expected return exceeds this required return, then adding more of it will improve the performance of the portfolio!"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#efficient-port.-and-required-returns-conclusion",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#efficient-port.-and-required-returns-conclusion",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Efficient Port. and Required Returns, conclusion",
    "text": "Efficient Port. and Required Returns, conclusion\n\nWe saw what an efficient portfolio is and how to find it in terms of the Sharpe Ratio\nBased on this notion, this equation establishes the relation between an investment’s risk and its expected return:\n\n\n\\[R_i =  R_f + \\beta_i^P  \\times (E[R_p] - R_f)\\]\n\nIt states that we can determine the appropriate risk premium for an investment based on its \\(\\beta\\) with the efficient portfolio\nIn such a way, it enables us to “price” the required returns for investing in any asset based on the amount of required returns that are needed to improve the performance of an efficient portfolio"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#practice-2",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#practice-2",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Practice",
    "text": "Practice\n\n\n\n\n\n\n\nImportant\n\n\nPractice using the following links:\n\nMultiple-choice Questions\nNumeric Questions"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#proof---variance-of-a-sum-of-two-random-variables",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#proof---variance-of-a-sum-of-two-random-variables",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Proof - Variance of a sum of two random variables",
    "text": "Proof - Variance of a sum of two random variables\n\n\n\n\n\n\nTheorem I: the variance of the sum of two random variables equals the sum of the variances of those random variables, plus two times their covariance:\n\n\n\\[\n\\small\n\\sigma^2(A+B) = \\sigma^2_A + \\sigma^2_B + 2\\times Cov(A,B)\n\\] Proof: variance is defined as:\n\\[\n\\small Var(X)=E[(X-E(X))]^2\n\\] Therefore, if \\(X=A+B\\), with \\(A\\) and \\(B\\) being two random variables:\n\\[\n\\small  Var(X)=Var(A+B) = E[((A+B)-E(A+B))]^2 = E[(\\underbrace{[A-E(A)]}_{\\text{First Term}}+[\\underbrace{B-E(B)}_{\\text{Second Term}}])^2] \\\\\n\\] Which is now in the form \\(\\small (A+B)^2=A^2+B^2+2AB\\). Using the fact that \\(E(\\cdot)\\) is a linear operator, we can apply it to each of the terms:\n\\[\n\\small\n= \\underbrace{E([A-E(A)]^2}_{\\sigma^2_A}+\\underbrace{E[B-E(B)]^2}_{\\sigma^2_B}+2\\times\\underbrace{E[A-E(A)][B-E(B)]}_{Cov(A,B)} = \\sigma^2_A + \\sigma^2_B + 2\\times Cov(A,B)\\\\\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#proof---variance-multiplied-by-a-scalar",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#proof---variance-multiplied-by-a-scalar",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Proof - Variance multiplied by a scalar",
    "text": "Proof - Variance multiplied by a scalar\n\n\n\n\n\n\nTheorem II: The variance scales upon multiplication with a constant:\n\n\n\\[\n  \\sigma^2(\\beta \\times A ) = \\beta^2\\times \\sigma^2_A\n  \\] Proof: define the variance in terms of \\(\\beta\\) and \\(A\\):\n\\[\n\\sigma^2(\\beta\\times A)=E[\\beta\\times A-E(\\beta\\times A)]^2 \\\\\n\\] Since \\(\\beta\\) is a constant, \\(E[\\beta]=\\beta\\) and we can write:\n\\[\nE[\\beta\\times A - \\beta\\times E(A)]^2= E[\\beta\\times (A-E[A])]^2=\\beta^2\\times \\underbrace{E(A-E[A])]^2}_{\\sigma^2_A}\n\\] Therefore, whenever there is a scale constant multiplying a random variable, it scales the variance: \\(\\beta^2 \\times\\sigma^2_A\\)"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#limits-to-diversification-1",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#limits-to-diversification-1",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Limits to diversification",
    "text": "Limits to diversification\n\nHow much of the variance of a portfolio we can eliminate through diversification? To see that, assume that we hold a portfolio with equal weights \\(\\frac{1}{N}\\) to all assets\n\n\nIn the diagonal, we have \\(N\\) boxes with \\(\\dfrac{1}{N^2}\\times \\overline{Var}\\), where \\(\\overline{Var}\\) is the average variance\nOff the diagonal, we have \\(N^2-N\\) boxes with \\(\\dfrac{1}{N^2}\\times \\overline{Cov}\\), where \\(\\overline{Cov}\\) is the average covariance\n\n\nTherefere, you can rearrange terms to get:\n\n\n\\[\n\\small Var(R_p) = \\bigg(N\\times\\dfrac{1}{N^2}\\times \\overline{Var}\\bigg)+ \\bigg[(N^2-N)\\dfrac{1}{N^2}\\times \\overline{Cov}\\bigg]\\\\\n\\small = \\bigg(\\frac{1}{N} \\times \\overline{Var}\\bigg) + \\bigg[\\bigg(1-\\frac{1}{N}\\bigg) \\times \\overline{Cov}\\bigg]\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#limits-to-diversification-continued",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#limits-to-diversification-continued",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Limits to diversification, continued",
    "text": "Limits to diversification, continued\n\nWhat happens if \\(N\\rightarrow\\infty\\)? If you look at our equation, you’ll see that only the covariance terms remain:\n\n\n\\[\n\\small \\underset{N\\rightarrow\\infty}{\\small Var(R_p)} = \\bigg(\\frac{1}{\\infty} \\times \\overline{Var}\\bigg) + \\bigg[\\bigg(1-\\frac{1}{\\infty}\\bigg) \\times \\overline{Cov}\\bigg]\\rightarrow \\overline{Cov}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-large-portfolio-with-arbitrary-weights",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-large-portfolio-with-arbitrary-weights",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Volatility of a large portfolio with arbitrary weights",
    "text": "Volatility of a large portfolio with arbitrary weights\n\nWhat if we don’t have an equally-weighted portfolio? As \\(N\\) increases, you can rewrite the variance of the portfolio1 as:\n\n\n\\[\\small \\sigma^2_{R_p} = \\sum_i w_i \\times Cov(R_i,R_p)\\]\n\nThe variance of a portfolio approaches the weighted average covariance of each stock with the portfolio!\nBecause \\(\\small Cov(R_i,R_P) = \\sigma_i \\times \\sigma_p \\times Corr_{i,p}\\) you can also write it as:\n\n\n\n\\[\\sigma^2_{R_p} = \\sum_i x_i \\times \\sigma_{i} \\times \\sigma_{p} \\times Corr_{i,p}\\]\n\nSee Berk and DeMarzo (2023), pg. 409 for a detailed explanation."
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-large-portfolio-with-arbitrary-weights-1",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#volatility-of-a-large-portfolio-with-arbitrary-weights-1",
    "title": "Optimal Portfolio and the CAPM",
    "section": "Volatility of a large portfolio with arbitrary weights",
    "text": "Volatility of a large portfolio with arbitrary weights\n\nIf we divide both sides of this equation by \\(\\sigma_{p}\\), we find:\n\n\n\\[\\sigma_{p} = \\sum_i x_i \\times \\sigma_{i}  \\times Corr(R_i,R_p)\\]\n\nWhy this equation is important?\n\nIt shows the amount of risk that each security brings to portfolio\nEach asset \\(i\\) contributes to the portfolio’s volatility according to its standard deviation (\\(\\sigma_i\\)) scaled by its correlation with the portfolio"
  },
  {
    "objectID": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#references",
    "href": "fin-strat/Lecture 3 - Optimal Portfolio and CAPM/index.html#references",
    "title": "Optimal Portfolio and the CAPM",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nBrealey, R. A., S. C. Myers, and F. Allen. 2020. Principles of Corporate Finance. The Irwin/McGraw-Hill Series in Finance, Insurance, and Real Estate. McGraw-Hill Education. https://books.google.com.br/books?id=nsrHuwEACAAJ."
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#outline",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#outline",
    "title": "Payout Policy",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\n(Berk and DeMarzo 2023)\n(Brealey, Myers, and Allen 2020)\n\nStudy review and practice: I strongly recommend using Prof. Henrique Castro (FGV-EAESP) materials. Below you can find the links to the corresponding exercises related to this lecture:\n\nMultiple Choice Exercises - click here\n\n\n\n\\(\\rightarrow\\) For coding replications, whenever applicable, please follow this page or hover on the specific slides with coding chunks."
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#distributions-to-shareholders",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#distributions-to-shareholders",
    "title": "Payout Policy",
    "section": "Distributions to Shareholders",
    "text": "Distributions to Shareholders\n\n\n\n\n\n\nDefinition\n\n\nPayout Policy is the the way a firm chooses between alternative ways to distribute free cash flow to its equityholders]"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#dividends-and-share-repurchases",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#dividends-and-share-repurchases",
    "title": "Payout Policy",
    "section": "Dividends and Share Repurchases",
    "text": "Dividends and Share Repurchases\n\nThere are basically two ways by which a firm can return free cash flow to its shareholders:\n\nBy paying out as a dividend, where the firm literally provides an influx of cash to its existing shareholders\nBy repurchasing existing shares, a firm can return cash to its shareholders\n\nImportantly, in Brazil, there are two important aspects that need to be taken into consideration:\n\nApart from dividends, we also have interest on equity (“Juros sobre o Capital Próprio - JCP”)\nFurthermore, in most countries, investors pay taxes over dividends and over capital gains. In Brazil, dividends are not taxed\n\n\n\n\\(\\rightarrow\\) In this lecture, we will study each of these options, understand their main aspects, usages, and discuss how they relate to a firm’s capital structure decision"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#dividends",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#dividends",
    "title": "Payout Policy",
    "section": "Dividends",
    "text": "Dividends\n\nThe most straightforward way to return cash to the shareholders of a firm is through the payment of dividends\n\n\n\n\n\n\n\n\nDefinition\n\n\nDividends are the compensation (in cash) that firms provide to its shareholders. Apart from capital gains (i.e, the appreciation of the share price over time), shareholders can also benefit from receiving a cash influx related to the excess cash from the firm’s operations\n\n\n\n\nBut how do firms decide how much to pay in dividends?\n\nA public company’s board of directors determines the amount of the firm’s dividend\nThe board sets the amount per share that will be paid and decides when the payment will occur\n\nNote that dividends need not be limited to the net income generated by a firm - in fact, some firms issue additional equity or debt to pay out as dividends"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#dividends---a-practical-timeline",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#dividends---a-practical-timeline",
    "title": "Payout Policy",
    "section": "Dividends - a practical timeline",
    "text": "Dividends - a practical timeline\n\n\n\n\n\n\nDeclaration Date: the date on which the board of directors authorizes the payment\nEx-Dividend Date: a date, two days prior to a dividend’s record date, on or after which anyone buying the stock will not be eligible for the dividend\nRecord Date: when a firm pays a dividend, only shareholders on record on this date receive\nPayable Date (Distribution Date): a date, generally within a month after the record date, on which a firm mails dividend checks to its registered stockholders"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#types-of-divends",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#types-of-divends",
    "title": "Payout Policy",
    "section": "Types of divends",
    "text": "Types of divends\n\nApart from the important marks within the timeline of a dividend distribution, firm’s can distribute dividends in a variety of ways:\n\n\nRegular Dividends: often paid every quarter\nSpecial Dividend: a one-time dividend payment a firm makes, which is usually much larger than a regular dividend\nStock Split (Stock Dividend): when a company issues a dividend in shares of stock rather than cash to its shareholders\nLiquidation Dividend: when the dividend is paid while liquidating the firm’s business. Contrary to the others, it is taxed as capital gain"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#share-repurchases",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#share-repurchases",
    "title": "Payout Policy",
    "section": "Share Repurchases",
    "text": "Share Repurchases\n\n\n\n\n\n\nDefinition\n\n\nShare Repurchases are an alternative way to pay cash to investors is through a share repurchase or buyback. In short, the firm uses cash to buy shares of its own outstanding stock.\n\n\n\n\nLike dividends, there are several ways in which a firm may operationalize share repurchases. The most common terms are:\n\nOpen Market Repurchases: when a firm repurchases shares in the open market - these represent about \\(\\small95\\%\\) of all repurchase transactions\nTender Offer: a public announcement to all existing security holders to buy back a specified amount of outstanding securities at a given price (typically set at a \\(\\small10\\%\\) to \\(\\small20\\%\\) premium to the current market price) over a prespecified period of time. If shareholders do not tender enough shares, the firm may cancel the offer, and no buyback occurs\nTargeted Repurchase: when a firm purchases shares directly from a specific shareholder"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#interest-on-equity",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#interest-on-equity",
    "title": "Payout Policy",
    "section": "Interest on Equity",
    "text": "Interest on Equity\n\nIn Brazil, there is also a possibility of paying interest on equity: this used to be an important instrument when inflation rates were very high in Brazil. The goal was to create an incentive to hold equity when inflation was high:\n\nInterest on equity is tax deductible at the corporate level but subject to a withholding tax at the shareholder level\nFrom an accounting perspective, interest on equity is treated similarly to interest (to creditors)\n\nThe tax rate is different for different types of investors. Usually, the interest on equity tax rate is lower than the corporate tax rate\nFirms have a limit to pay as interest on equity. This limit increases every year by TJLP, an annual interest rate established by Brazilian Central Bank - access here"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#dividends-versus-share-repurchases",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#dividends-versus-share-repurchases",
    "title": "Payout Policy",
    "section": "Dividends versus Share Repurchases",
    "text": "Dividends versus Share Repurchases\n\nWhen it comes to payout policy, an important question relates to the method in which cash will be returned to shareholders - pay out as a dividend or through share repurchases?\nTo see that, assume that the board of Genron Corporation is meeting to decide how to pay out \\(\\small \\$20\\) million in excess cash to shareholders. The firms has no debt, and its equity cost of capital equals its unlevered cost of capital of \\(\\small 12\\%\\)\nThere are \\(\\small 10\\) million shares outstanding, and the estimated free cash flows for the upcoming years are \\(\\small \\$48\\) million per year\nWhich option (if any) should be the preferred one?\n\nPaying out as dividends\nShare repurchases\n\nAs a starting point, assume that there are no market imperfections - i.e, Perfect Capital Markets"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#option-1-dividends",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#option-1-dividends",
    "title": "Payout Policy",
    "section": "Option 1: Dividends",
    "text": "Option 1: Dividends\n\nWith \\(\\small 10\\) million shares, Genron will be able to pay a dividend of \\(\\small \\$20/10=\\$2\\) per share\nThe firm expects to generate future free cash flows of \\(\\small\\$48\\) million per year. Therefore, it anticipates paying a dividend of \\(\\small \\$4.80\\) per share each year thereafter\nAs a result, the value to equityholders is:\n\n\n\\[\n\\small \\underbrace{2}_{\\text{Payment today}} + \\underbrace{\\sum_{t=1}^{\\infty}\\dfrac{4.80}{(1+12\\%)}}_{\\text{Perpetuity}} \\rightarrow 2 + \\frac{4.80}{12\\%} = 2 + 40 = 42\n\\]\n\nAfter the \\(\\small \\$2\\) dividend payment, the value of each share is simply \\(\\small \\$42-\\$2=\\$40\\) per share. Notice that after the dividend payment, the share price drops by the dividend per share value\n\n\n\n\\(\\rightarrow\\) In a perfect capital market, when a dividend is paid, the share price drops by the amount of the dividend when the stock begins to trade ex-dividend"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#option-2-share-repurchase",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#option-2-share-repurchase",
    "title": "Payout Policy",
    "section": "Option 2: Share Repurchase",
    "text": "Option 2: Share Repurchase\n\nInstead of paying a dividend this year, Genron can use the \\(\\small 20\\) million to repurchase its shares on the open market. In this case, the company can buy \\(\\small \\$20/\\$42 = 0.476\\) million shares\nTherefore, the new number of outstanding shares are: \\(\\small 10 - 0.476 = 9.524\\) million. Note that this has not changed the free cash flow available to shareholders, but now, there are less shareholders to be paid for! After the repurchase, the future dividend value is:\n\n\n\\[\n\\small \\dfrac{\\text{Free Cash Flow}}{\\text{# of Shares}}=\\dfrac{48,000,000}{9,524,000} = 5.04 \\text{ per share}\n\\]\n\nTherefore, the stock price is \\(\\small \\frac{\\$5.04}{12\\%}=\\$42\\) per share\n\n\n\n\\(\\rightarrow\\) In perfect capital markets, an open market share repurchase has no effect on the stock price, and the stock price is the same as the cum-dividend1 price if a dividend were paid instead.\n\nWhen a stock trades before the ex-dividend date, entitling anyone who buys the stock to the dividend ($42 in this example)."
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#comparing-the-options",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#comparing-the-options",
    "title": "Payout Policy",
    "section": "Comparing the options",
    "text": "Comparing the options\n\nAn important implication of Perfect Capital Markets is that investors should be indifferent between the firm distributing funds via dividends or share repurchases:\n\nWhen receiving via dividends, they receive a cash payment of \\(\\small \\$2\\) and keep the value of their shares trading at \\(\\small \\$40\\)\nVia share repurchases, regardless of tendering their shares, share prices are the same (\\(\\small \\$ 42\\))\n\nFrom a purely value perspective, both alternatives yield the same value. As in previous lectures, a similar argument can be made when assuming perfect capital markets: if investors have preferences over a given option, they can make a homemade dividend1\n\nIf the firm repurchases shares and the investor wants cash, the investor can raise cash by selling his/her shares\nIf the firm pays a dividend and the investor prefers stock, they can use the dividend to purchase additional fractional shares\n\n\nSee the Appendix for a detailed example on homemade dividends."
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#dividends-versus-share-repurchases-alternative-policy",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#dividends-versus-share-repurchases-alternative-policy",
    "title": "Payout Policy",
    "section": "Dividends versus Share Repurchases, alternative policy",
    "text": "Dividends versus Share Repurchases, alternative policy\n\nWhat if, as opposed to the first option, Genror decides to pay a higher dividend? To see that, suppose that the firm wants to pay dividends larger than \\(\\small\\$2\\) per share right now, but it only has \\(\\small 20\\) million in cash today\nThus, Genron needs an additional \\(\\small\\$28\\) million to pay the larger dividend now. To do this, the firm decides to raise the cash by selling new shares. Given a current share price of \\(\\small\\$42\\), the firm could raise \\(\\small \\$28\\) million by issuing \\(\\small \\$28/\\$42= \\$0.67\\) million new shares\nThe new dividend per share is: \\(\\small \\$48/10.67=\\$4.5\\). The cum-dividend share price is:\n\n\n\\[\n\\small \\underbrace{4.5}_{\\text{Actual Dividend}} + \\underbrace{\\frac{4.5}{12\\%}}_{\\text{Perpetuity}}= 4.5 + 37.5 = 42\n\\]\n\\(\\rightarrow\\) As issuing new equity does not change the free cash flows, stock price does not change as a result of the share repurchase!"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#modigliani-and-miller-and-the-dividend-policy-irrelevance",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#modigliani-and-miller-and-the-dividend-policy-irrelevance",
    "title": "Payout Policy",
    "section": "Modigliani and Miller and the Dividend Policy Irrelevance",
    "text": "Modigliani and Miller and the Dividend Policy Irrelevance\n\nNote that there is a tradeoff between current and future dividends:\n\nIf Genron pays a higher current dividend, future dividends will be lower\nIf Genron pays a lower current dividend, future dividends will be higher\n\nRegardless of the payout policy, shareholders can create a homemade dividend on their own, matching his/her specific payout preferences, by buying or selling shares themselves\nTherefore, the only factor that determines a firm’s payout policy is the free cash flow.\n\n\n\n\n\n\n\n\nDefinition\n\n\nThe Dividend Policy Irrelevance Condition: in perfect capital markets, holding fixed the investment policy, the firm’s choice of dividend policy is irrelevant and does not affect the initial share price. While dividends do determine share prices, a firm’s choice of dividend policy does not."
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#payout-policy-in-practice",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#payout-policy-in-practice",
    "title": "Payout Policy",
    "section": "Payout Policy in Practice",
    "text": "Payout Policy in Practice\n\nBased on our discussion, when we assume that capital markets are perfect, the payout policy is only determined by the firm’s future cash flows, and the specific type of payment and timing are irrelevant to determine the firm’s value\nIn reality, capital markets are not perfect:\n\nSome types of payout may have higher costs than others, such as taxes\nThere could be limits and/or regulations around a given payout type\nFinally, different payout policies may be subject to a different degree of information asymmetry\n\nAs a result, if the firm’s payout policy is relevant to determine the firm’s value, it is only due to these imperfections!\n\n\n\\(\\rightarrow\\) In what follows, we will look at the most important market imperfections that would make the firm’s payout policy relevant to the firm’s value"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#the-tax-disadvantage-of-dividends",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#the-tax-disadvantage-of-dividends",
    "title": "Payout Policy",
    "section": "The Tax Disadvantage of Dividends",
    "text": "The Tax Disadvantage of Dividends\n\nFirst and foremost, there could be differences in the taxation of Dividends and Capital Gains\n\nShareholders must pay taxes on the dividends they receive, and they must also pay capital gains taxes when they sell their shares\nDividends are typically taxed at a higher rate than capital gains1. In fact, long-term investors can defer the capital gains tax forever by not selling shares\n\n\n\n\nThe higher the tax rate on dividends, the more undesirable is for a firm to raise funds to pay a dividend\nWhen dividends are taxed at a higher rate than capital gains, if a firm raises money by issuing shares and then gives that money back to shareholders as a dividend, shareholders are hurt because they will receive less than their initial investment\n\n\nIn Brazil, dividends are not taxed."
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#the-tax-disadvantage-of-dividends-continued",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#the-tax-disadvantage-of-dividends-continued",
    "title": "Payout Policy",
    "section": "The Tax Disadvantage of Dividends, continued",
    "text": "The Tax Disadvantage of Dividends, continued\n\nTo see that, assume that a firm raises \\(\\small\\$25\\) million from shareholders and uses it to pay \\(\\small\\$25\\) million in dividends. Dividends are taxed at a \\(\\small39\\%\\) tax rate, and Capital Gains are taxed at a \\(\\small20\\%\\) tax rate. We then have the following cases:\n\nIn terms of dividend taxes, shareholders will owe \\(\\small 39\\% \\times \\$25 = 9.75\\) million in taxes\nBecause the firm value falls after dividend, the capital gain (when selling shares) will be \\(\\small \\$25\\) million less, lowering the capital gains taxes by \\(\\small 20\\% \\times \\$25 = \\$5\\) million\n\nShareholders will pay a total of \\(\\small \\$9.75 − \\$5.00 = \\$4.75\\) million in taxes\nShareholders will receive back only \\(\\small \\$25 − \\$4.75 = 20.25\\) million"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#the-dividend-puzzle",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#the-dividend-puzzle",
    "title": "Payout Policy",
    "section": "The Dividend Puzzle",
    "text": "The Dividend Puzzle\n\nBy the rationale shown before, when the tax rate on dividends is greater than the tax rate on capital gains, shareholders will pay lower taxes if a firm uses share repurchases rather than dividends\nThis tax savings will increase the value of a firm that uses share repurchases rather than dividends. As a consequence, the optimal dividend policy when the dividend tax rate exceeds the capital gain tax rate is to pay no dividends at all\n\n\n\\(\\rightarrow\\) In reality, however, firms continue to issue dividends despite their tax disadvantage - also known as the “Dividend Puzzle”. Why?"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#dividend-capture-and-tax-clienteles",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#dividend-capture-and-tax-clienteles",
    "title": "Payout Policy",
    "section": "Dividend Capture and Tax Clienteles",
    "text": "Dividend Capture and Tax Clienteles\n\nThe preference for share repurchases rather than dividends depends on the difference between the dividend tax rate and the capital gains tax rate\nThe Effective Dividend Tax Rate: consider buying a stock just before it goes ex-dividend and selling the stock just after. The equilibrium condition is:\n\n\n\\[\n\\small (P_{\\text{Cum}}-P_{\\text{Ex}})\\times\\underbrace{(1-\\tau_g)}_{\\text{Net of Capital Gain Taxes}} = D\\times \\underbrace{(1-\\tau_d)}_{\\text{Net of Dividend Taxes}}\n\\]\n\nRearranging terms, we have;\n\n\n\n\\[\n\\small P_{\\text{Cum}}-P_{\\text{Ex}} = \\frac{D\\times(1-\\tau_d)}{1-\\tau_g} \\rightarrow D \\times \\bigg[ 1 - \\frac{\\tau_d - \\tau_g}{1-\\tau_g}  \\bigg]\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#dividend-capture-and-tax-clienteles-1",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#dividend-capture-and-tax-clienteles-1",
    "title": "Payout Policy",
    "section": "Dividend Capture and Tax Clienteles",
    "text": "Dividend Capture and Tax Clienteles\n\\[\n\\small P_{\\text{Cum}}-P_{\\text{Ex}} = D \\times \\bigg[ 1 - \\frac{\\tau_d - \\tau_g}{1-\\tau_g}  \\bigg]\n\\]\n\nIf we set \\(\\tau^*_d = \\frac{\\tau_d - \\tau_g}{1-\\tau_g}\\), we have that:\n\n\n\\[\n\\small P_{\\text{Cum}}-P_{\\text{Ex}} = D \\times (1-\\tau^*_d)\n\\]\n\n\\(\\small \\tau^\\star_d\\) measures the additional tax paid by the investor per dollar of after-tax capital gains income that is received as a dividend. Assuming \\(\\small \\tau_g=15\\%\\) and \\(\\small \\tau_d=30\\%\\):\n\n\n\n\\[\n\\small \\tau^*_d = \\frac{0.30 - 0.15}{1-0.15} = 0.176\n\\]\n\\(\\rightarrow\\) In words, there is a significant tax disadvantage of dividends: each \\(\\small \\$1\\) of dividends is worth only \\(\\small\\$0.824\\) in capital gains."
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#dividend-capture-and-tax-clienteles-2",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#dividend-capture-and-tax-clienteles-2",
    "title": "Payout Policy",
    "section": "Dividend Capture and Tax Clienteles",
    "text": "Dividend Capture and Tax Clienteles\n\nIn practice, \\(\\tau_d^\\star\\), the effective dividend tax rate, differs across investors for a variety of reasons:\n\nIncome Levels\nInvestment Horizons\nTax Jurisdictions\nTypes of Investor or Type of investment Account\n\nAs a result of their different tax rates, investors will have varying preferences regarding dividends. This opens room for firms to try matching their payout policy to the preferences of investors:\n\nThe Clientele Effect is when the dividend policy of a firm reflects the tax preference of its investor clientele\nFor example, individuals in the highest tax brackets have a preference for stocks that pay no or low dividends, whereas tax-free investors and corporations have a preference for stocks with high dividends"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#dividend-and-tax-clienteles---u.s.-market",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#dividend-and-tax-clienteles---u.s.-market",
    "title": "Payout Policy",
    "section": "Dividend and Tax Clienteles - U.S. Market",
    "text": "Dividend and Tax Clienteles - U.S. Market\n\n\n\n\n\n\\(\\rightarrow\\) The increase in volume consistent with tax-exempt traders buying before the ex-date and selling it afterwards. Price hikes may have to do with signaling - see more in the upcoming slides"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#payout-versus-retention-of-cash",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#payout-versus-retention-of-cash",
    "title": "Payout Policy",
    "section": "Payout Versus Retention of Cash",
    "text": "Payout Versus Retention of Cash\n\nAs of now, we discussed why firms may not be indifferent regarding different payout options (i.e., dividends versus share repurchases)\nAnother strand of the discussion relates to the decision to payout or retain free cash flow:\n\nIn perfect capital markets, once a firm has taken all positive-NPV projects, it is indifferent between saving excess cash and paying it out\nHowever, when such market imperfections do exist, there will be an important trade-off: while retaining cash can reduce the costs of raising capital in the future, but it can also increase taxes and agency costs\n\n\n\n\\(\\rightarrow\\) As a result, the extent to which a firm will prefer one or the other will depend on the trade-off between the benefits and costs of retaining cash!"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#payout-versus-retention-of-cash-in-perfect-capital-markets",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#payout-versus-retention-of-cash-in-perfect-capital-markets",
    "title": "Payout Policy",
    "section": "Payout versus Retention of Cash in Perfect Capital Markets",
    "text": "Payout versus Retention of Cash in Perfect Capital Markets\n\nIf a firm has already taken all positive-NPV projects, any additional projects it takes on are zero or negative-NPV projects\n\nRather than wasting excess cash on negative-NPV projects, a firm can use the cash to purchase financial assets\nIn perfect capital markets, buying and selling securities is a zero-NPV transaction, so it should not affect firm value\n\nThis indifference is also known as the Payout Irrelevance Condition\n\n\n\n\n\n\n\n\nDefinition\n\n\nThe Modigliani-Miller Payout Irrelevance: with perfect capital markets, the retention versus payout decision is irrelevant. In perfect capital markets, if a firm invests excess cash flows in financial securities, the firm’s choice of payout versus retention is irrelevant and does not affect the initial share price."
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#payout-versus-retention-of-cash-practice",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#payout-versus-retention-of-cash-practice",
    "title": "Payout Policy",
    "section": "Payout Versus Retention of Cash, practice",
    "text": "Payout Versus Retention of Cash, practice\nPayne Enterprises has \\(\\small\\$20,000\\) in excess cash. Payne is considering investing the cash in one-year Treasury bill paying \\(\\small5\\%\\) interest and then using the cash to pay a dividend next year. Alternatively, the firm can pay a dividend immediately, and shareholders can invest the cash on their own. In a perfect capital market, which option will shareholders prefer?\n\n\\(\\rightarrow\\) Solution:\n\n\n\nIf Payne pays an immediate dividend, the shareholders receive \\(\\small\\$20,000\\) today\nIf Payne retains the cash, at the end of one year the company will be able to pay a dividend of \\(\\small \\$20,000 \\times (1+5\\%)=\\$21,000\\)\n\n\nFinally, note that if shareholders invest in Treasury bills themselves, they would also have \\(\\small\\$21,000\\) at the end of one year\nIn either case, investors have the same payoff!"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#corporate-taxes-and-cash-retention-practice",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#corporate-taxes-and-cash-retention-practice",
    "title": "Payout Policy",
    "section": "Corporate Taxes and Cash Retention, practice",
    "text": "Corporate Taxes and Cash Retention, practice\n\nWhat happens to the trade-off between dividends and cash retention when we assume that there are market imperfections?\n\nAn important market imperfection relates to Corporate Taxes: these takes make it costly for a firm to retain excess cash\nFrom a valuation perspective, cash is equivalent to negative leverage, so the tax advantage of leverage implies a tax disadvantage to holding cash\n\nIn this scenario, whenever market imperfections are present, firms may not be indifferent between receivind dividends and retaining cash anymore!"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#corporate-taxes-and-cash-retention-practice-1",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#corporate-taxes-and-cash-retention-practice-1",
    "title": "Payout Policy",
    "section": "Corporate Taxes and Cash Retention, practice",
    "text": "Corporate Taxes and Cash Retention, practice\n\nTo see that, assume that Payne has a marginal tax rate of \\(\\small39\\%\\). Would a tax-exempt endowment prefer that Payne use its excess cash to pay the dividend immediately or invest the cash in a Treasury bill paying \\(\\small5\\%\\) interest and then pay out a dividend?\n\n\n\\(\\rightarrow\\) Solution:\n\nIf Payne pays a dividend today, shareholders receive \\(\\small\\$20,000\\)\nIf Payne retains the cash for one year, it will earn an after-tax return on the Treasury bills of \\(\\small 5\\% \\times (1-0.39\\%)=3.05\\%\\)\n\n\nTherefore, Payne will pay a dividend of \\(\\small\\$20,000 \\times (1+3.05\\%) = 20,610\\), which is less than the \\(\\small\\$21,000\\) the endowment would have earned if they had invested in the Treasury bills themselves!"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#investor-taxes-and-cash-retention",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#investor-taxes-and-cash-retention",
    "title": "Payout Policy",
    "section": "Investor Taxes and Cash Retention",
    "text": "Investor Taxes and Cash Retention\n\nThe decision to pay out versus retain cash may also affect the taxes paid by shareholders:\n\nWhen a firm retains cash, it must pay corporate tax on the interest it earns on that cash. In addition, the investor will owe capital gains tax on the increased firm value. The interest on retained cash is taxed twice\nIf the firm paid the cash to its shareholders instead, they could invest it and be taxed only once on the interest that they earn\n\nThe cost of retaining cash thus depends on the combined effect of the corporate and capital gains taxes, compared to the tax on interest income:\n\n\n\\[\n\\small \\tau^*_{\\text{Retain}} = 1-\\frac{(1-\\tau_c)(1-\\tau_g)}{(1-\\tau_i)}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#issuance-and-distress-costs",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#issuance-and-distress-costs",
    "title": "Payout Policy",
    "section": "Issuance and Distress Costs",
    "text": "Issuance and Distress Costs\n\nBesides any tax considerations, firms retain cash balances to cover potential future cash shortfalls, despite the tax disadvantage to retaining cash:\n\nA firm might accumulate a large cash balance if there is a reasonable chance that future earnings will be insufficient to fund future positive-NPV investment opportunities\nThe cost of holding cash to cover potential shortfalls should be compared to the reduction in transaction, agency, and adverse selection costs of raising capital through new issuances\n\nHowever, there are also Agency Costs of Retaining Cash:\n\nWhen firms have excessive cash, managers may use the funds inefficiently by paying excessive executive perks, over-paying for acquisitions, etc\nPaying out excess cash through dividends or share repurchases, rather than retaining cash, can boost the stock price by reducing managers’ ability and temptation to waste resources\n\n\n\n\\(\\rightarrow\\) The optimal payout policy trades-off these benefits against agency costs!"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#payout-versus-retention-of-cash-practice-1",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#payout-versus-retention-of-cash-practice-1",
    "title": "Payout Policy",
    "section": "Payout Versus Retention of Cash, practice",
    "text": "Payout Versus Retention of Cash, practice\nAltgreen is an all-equity firm with \\(\\small\\$250\\) million shares outstanding. It has \\(\\small\\$300\\) million in cash and expects future free cash flows of \\(\\small\\$150\\) million per year. Management plans to use the cash to expand the firm’s operations, which will in turn increase future free cash flows by \\(\\small10\\%\\). If the cost of capital of Altgreen’s investments is \\(\\small7\\%\\), how would a decision to use the cash for a share repurchase rather than the expansion change the share price?\n\n\\(\\rightarrow\\) Solution:\n\n\n\nIf Altgreen uses the cash to expand, its future free cash flows will increase by \\(\\small10\\%\\) to \\(\\small\\$165\\) million per year. Using the perpetuity formula, its market value will be:\n\n\n\n\\[\n\\small \\dfrac{165}{7\\%} = 2.357 \\text{ billion, or } \\dfrac{2.357}{250} = 9.43 \\text{ per share}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#payout-versus-retention-of-cash-practice-2",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#payout-versus-retention-of-cash-practice-2",
    "title": "Payout Policy",
    "section": "Payout Versus Retention of Cash, practice",
    "text": "Payout Versus Retention of Cash, practice\n\nIf, on the other hand, Altgreen does not expand, the value of its future free cash flows will be \\(\\small 150/7\\% = \\$2.143\\) billion. Adding cash, we have \\(\\small \\$2.443/250 = \\$9.77\\) per share. When repurchasing shares, it will repurchase \\(\\small \\$300/\\$9.77=30.71\\) million shares:\n\n\n\\[\n\\small \\dfrac{2.143}{219.29}= 9.77 \\text{ per share}\n\\]\n\nAll in all, note that the firm could avoid a negative NPV project by retaining cash/share repurchases:\n\n\n\n\\[\n\\small \\underbrace{-300}_{\\text{Cash}}+\\underbrace{\\dfrac{15}{7\\%}}_{\\text{Growth Option}}=-300+214=-75\n\\]\n\n\n\\(\\rightarrow\\) The decrease of \\(\\small \\$75\\) million in value matches the \\(\\small 75/250\\approx \\$0.34\\) decrease in stock price!"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#signaling-with-payout-policy",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#signaling-with-payout-policy",
    "title": "Payout Policy",
    "section": "Signaling with Payout Policy",
    "text": "Signaling with Payout Policy\n\nIn theory, we would expect dividends to trend with earnings. In practice, however firms change dividends infrequently, and dividends are much less volatile than earnings. Why?\nAs with Debt, Dividends can also be used as a way to signal information to the market. We call Dividend Smoothing the practice of maintaining relatively constant dividends\nThe Dividend Signaling Hypothesis is the idea that dividend changes reflect managers’ views about future earnings:\n\nWhen a firm increases dividend, it sends a positive signal to investors that they expect to be able to afford the higher dividend for the foreseeable future\nWhen a firm decreases dividend, it may signal that they have given up hope that earnings will rebound in the near term and so need to save cash\n\nIf that is true, what is the price reaction to dividends? Although an increase of a firm’s dividend may signal optimism regarding its future cash flows, it might also signal a lack of investment opportunities, which might lead to a positive stock price reaction!"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#dividend-smoothing-in-practice-u.s.-markets",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#dividend-smoothing-in-practice-u.s.-markets",
    "title": "Payout Policy",
    "section": "Dividend Smoothing in practice (U.S. markets)",
    "text": "Dividend Smoothing in practice (U.S. markets)\n\n\n\n\n\n\\(\\rightarrow\\) It is possible to see that, for GM, dividend dynamics were much more stable than earnings"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#signaling-and-share-repurchases",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#signaling-and-share-repurchases",
    "title": "Payout Policy",
    "section": "Signaling and Share Repurchases",
    "text": "Signaling and Share Repurchases\n\nLike Dividends, Share Repurchases can be also be seen as a credible signal the current shares are underpriced, as if they were overpriced, a share repurchase would be costly for current shareholders\nNotwithstanding, a firm may announce a share repurchase program but not repurchase any shares, while on the other hand, when a firm announces dividends payment, it has a commitment to pay:\n\nIf investors believe that managers have better information regarding the firm’s prospects and act on behalf of current shareholders, then investors will react positively to share repurchase announcements\nNotwithstanding, announcing a share repurchase today does not necessarily represent a long-term commitment to repurchase shares in the future. Because of that, Dividends have a stronger, more credible signal than Share Repurchases"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#stock-dividends",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#stock-dividends",
    "title": "Payout Policy",
    "section": "Stock Dividends",
    "text": "Stock Dividends\n\nAs discussed before, with a stock dividend, a firm does not pay out any cash to shareholders. As a result, the total market value of the firm’s equity is unchanged\nThe only thing that is different is the number of shares outstanding. The stock price will therefore fall because the same total equity value is now divided over a larger number of shares\nTo see that, suppose that Genron firm paid a \\(\\small50\\%\\) stock dividend rather than a cash dividend\n\nA shareholder who owns \\(\\small\\$100\\) shares as \\(\\small\\$42\\) before the dividend has a portfolio worth \\(\\small \\$4,200\\)\nAfter the dividend, the shareholder owns \\(\\small150\\) shares\n\nBecause the portfolio is still worth \\(\\small4,200\\), the stock price will fall to \\(\\small \\$4,200/150=\\$28\\).\nSometimes, firms finance new shares. So, it is like shareholders have received cash and immediately bought new shares. In this case, decline in price is not as large as above"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#stock-splits",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#stock-splits",
    "title": "Payout Policy",
    "section": "Stock Splits",
    "text": "Stock Splits\n\nSplits are known as situations where a firm a smaller pool of highly valued shares share into a higher number of shares\nThe typical motivation for a stock split is to keep the share price in a range thought to be attractive to small investors:\n\nIf the share price rises “too high”, it might be difficult for small investors to invest\nKeeping the price “low” may make the stock more attractive to small investors and increase the stock’s demand and liquidity, which may in turn boost the stock price\n\nOn average, announcements of stock splits are associated with a small increase in prices\nOn the other hand, when the price of a company’s stock falls too low and the company reduces the number of outstanding shares, \\(\\small 2\\) shares become \\(\\small 1\\) of twice the previous price - also known as reverse split or insplit"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#stock-splits-in-practice---u.s.-market",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#stock-splits-in-practice---u.s.-market",
    "title": "Payout Policy",
    "section": "Stock Splits in Practice - U.S. market",
    "text": "Stock Splits in Practice - U.S. market\n\n\n\n\n\n\\(\\rightarrow\\) Most stocks in the U.S. market trade between \\(\\small \\$5-\\$20\\), which is consistent with firms keeping a reasonable range to boost liquidity and avoid excessive volatility in returns"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#spin-offs",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#spin-offs",
    "title": "Payout Policy",
    "section": "Spin-Offs",
    "text": "Spin-Offs\n\nWhen a firm sells a subsidiary by selling shares in the subsidiary alone. That is, the parent turns a subsidiary into a separate company\nFor instance, Itaú-XP spin-off:\n\nItaú could have sold XP and paid cash as dividends\nInstead, Itaú’s current shareholders received a given number of XP’s stocks per share they hold of Itaú\n\nSpin-offs mainly offer two advantages:\n\nIt avoids the transaction costs associated with the actual sale\nThe special dividend is not taxed as a cash distribution. Investors will only pay capital gains taxes when they sell the received stocks"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#practice",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#practice",
    "title": "Payout Policy",
    "section": "Practice",
    "text": "Practice\n\n\n\n\n\n\nImportant\n\n\n\nPractice using the following links:\n\nMultiple-choice Questions\n\n\n\n\n\n\nClick here to access public information regarding dividends, share repurchases, and other relevant announcements for Brazilian publicly-traded firms (Comissão de Valores Mobiliários - CVM)"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#appendix",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#appendix",
    "title": "Payout Policy",
    "section": "Appendix",
    "text": "Appendix"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#creating-a-homemade-dividend",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#creating-a-homemade-dividend",
    "title": "Payout Policy",
    "section": "Creating a homemade dividend",
    "text": "Creating a homemade dividend\nSuppose Genron does not adopt the third alternative policy, and instead pays a 2 dividend per share today. Show how an investor holding \\(\\small 2,000\\) shares could create a homemade dividend of \\(\\small \\$4.5\\times 2,000=\\$9,000\\) per year on her own.\n\n\\(\\rightarrow\\) Solution\n\nIf Genron pays a \\(\\small\\$2\\) dividend, the investor receives \\(\\small \\$4,000\\) in cash and holds the rest in stock. Therefore, To receive \\(\\small \\$9,000\\) in total today, she can raise an additional \\(\\small \\$5,000\\) by selling \\(\\small 125\\) shares at \\(\\small \\$40\\) per share just after the dividend is paid\nIn future years, Genron will pay a dividend of \\(\\small \\$4.80\\) per share. Because she will own \\(\\small 2,000 − 125 = 1,875\\) shares, the investor will receive dividends of \\(\\small 1,875 \\times 4.80 = 9,000\\) per year from then on"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#creating-a-homemade-dividend-continued",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#creating-a-homemade-dividend-continued",
    "title": "Payout Policy",
    "section": "Creating a homemade dividend, continued",
    "text": "Creating a homemade dividend, continued\nSuppose you own \\(\\small1,000\\) shares in a firm that has historically paid dividends at a rate of \\(\\small 50\\%\\) of earnings per share. Although earnings per share this year are \\(\\small\\$5\\), the firm has decided to retain all of the earnings and not pay a dividend. The current market price is \\(\\$50\\) per share. How could you create a homemade dividend based on the firm’s dividend history?\n\n\\(\\rightarrow\\) Solution\n\nBased on the firm’s history, its “normal” dividend would have been \\(\\$5 × 50\\% = \\$2.50\\) per share. To create a homemade dividend, you need to sell enough shares to generate the desired cash flow:\n\n\n\n\\[\n\\small \\underbrace{1,000}_{\\text{Number of Shares}} \\times \\underbrace{2.50}_{\\text{Dividend per Share}} = 2,500\n\\]\n\nTherefore, given a current market value of \\(\\small\\$50\\) per share, you would need to sell \\(\\small \\$2,500/\\$50=50\\) shares to create the desired homemade dividend"
  },
  {
    "objectID": "fin-strat/Lecture 10 - Payout Policy/index.html#references",
    "href": "fin-strat/Lecture 10 - Payout Policy/index.html#references",
    "title": "Payout Policy",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nBrealey, R. A., S. C. Myers, and F. Allen. 2020. Principles of Corporate Finance. The Irwin/McGraw-Hill Series in Finance, Insurance, and Real Estate. McGraw-Hill Education. https://books.google.com.br/books?id=nsrHuwEACAAJ."
  },
  {
    "objectID": "fin-strat/Introduction/index.html#welcome-to-the-course",
    "href": "fin-strat/Introduction/index.html#welcome-to-the-course",
    "title": "Introduction and Course Overview",
    "section": "Welcome to the Course",
    "text": "Welcome to the Course\n\n\nOverview and Course Organization\nGrading and Evaluations\nNavigating through the syllabus\nHow you can get the best of this course\nOverall Q&A"
  },
  {
    "objectID": "fin-strat/Introduction/index.html#overview",
    "href": "fin-strat/Introduction/index.html#overview",
    "title": "Introduction and Course Overview",
    "section": "Overview",
    "text": "Overview\n\nFinancial agents are constantly making strategic decisions trying to balance the trade-off between risk and return:\n\nHow many assets should I hold on my portfolio?\nHow much holdings from risky assets should I hold in my portfolio?\nHow can I fund profitable investment opportunities from my company?\n\nIn this course, we’ll look at two sets of strategic decisions that financial agents (e.g, CFOs, Portfolio Managers) make:\n\nThe first part will be focused at looking from a portfolio manager view: if you have the money and need to allocate it across a set of potential assets, how should you do it?\nThe second part (after the mid-term) will be more focused on corporate finance decisions: if you have a good investment opportunity, how can you fund it, and What are the implications for the value of your company?"
  },
  {
    "objectID": "fin-strat/Introduction/index.html#course-organization",
    "href": "fin-strat/Introduction/index.html#course-organization",
    "title": "Introduction and Course Overview",
    "section": "Course organization",
    "text": "Course organization\n\n\nBasic text-book\n\nWe will extensively follow Corporate Finance (Berk and DeMarzo 2023), as our text-book\nThroughout the syllabus, it will be referred as [BDM]\n\nSupplementary text-book\n\nPrinciples of Corporate Finance (Brealey, Myers, and Allen 2020)\nInvestments (Bodie, Kane, and Marcus 2021)\nFinancial Management: Theory and Practice (Brigham and Ehrhardt 2023)\nDictionary of Finance and Investment Terms(Downes and Goodman 2014)\n\nSupplementary Reading\n\nOther optional contents will be referenced throughout the the course"
  },
  {
    "objectID": "fin-strat/Introduction/index.html#grading-and-evaluations",
    "href": "fin-strat/Introduction/index.html#grading-and-evaluations",
    "title": "Introduction and Course Overview",
    "section": "Grading and Evaluations",
    "text": "Grading and Evaluations\n\nGrading will be composed of the following activities:\n\nMid-term Examination (40%)\nFinal Examination (40%)\n4x Quizzes throughout the semester (15%)\nHandout Exercises (5%)\n\n\n\n\nYou can find the details of any of these activities in the official syllabus (available on eClass®)\nIn case of any questions, feel free to reach out to lucas.macoris@fgv.br\n\n\n\n\n\n\n\n\n\nOffice-hours\n\n\nI also host office-hours (by appointment) on Thursdays, 5PM-6PM. In these sessions, I’ll be more than happy to help you with anything you need from this course. Use the Office-hour Appointments link at the bottom of this slide to schedule some time (or click here)."
  },
  {
    "objectID": "fin-strat/Introduction/index.html#getting-the-best-of-this-course",
    "href": "fin-strat/Introduction/index.html#getting-the-best-of-this-course",
    "title": "Introduction and Course Overview",
    "section": "Getting the best of this course",
    "text": "Getting the best of this course"
  },
  {
    "objectID": "fin-strat/Introduction/index.html#getting-the-best-of-this-course-1",
    "href": "fin-strat/Introduction/index.html#getting-the-best-of-this-course-1",
    "title": "Introduction and Course Overview",
    "section": "Getting the best of this course",
    "text": "Getting the best of this course\n\n How you can get the best of this course\n\nFollow FGV-EAESP code of conduct\nBe organized: pay attention to pre-readings and deliverables!\nBe proactive: ask questions and participate in discussions\nTake the lead on your learning: you are the ultimate responsible for your success!\n\n\n\n How the professor can facilitate you getting the best of this course\n\nAll mandatory content will be provided in English, classes will be held in Portuguese\nProvide a safe and open space for questions, both in-person and remote\nMotivate students, both from the academic and practitioner standpoints\nProvide opportunities to extend knowledge beyond the mandatory readings"
  },
  {
    "objectID": "fin-strat/Introduction/index.html#getting-the-best-of-this-course-continued",
    "href": "fin-strat/Introduction/index.html#getting-the-best-of-this-course-continued",
    "title": "Introduction and Course Overview",
    "section": "Getting the best of this course, continued",
    "text": "Getting the best of this course, continued\n\nStudying for examinations\n\nI highly recommend using Prof. Henrique Martins (FGV-EAESP) materiais. In his website, there are suggested exercises for each lecture of our course.\n\n\n\n\\(\\rightarrow\\) To access, click here - I’ll also refer to the specific links for each part of the content at the beginning of each lecture\n\nIn the Syllabus, you can find a list of recommended exercises from our main text-book, (Berk and DeMarzo 2023). Note that these are not mandatory but rather serve as a way for your to practice\nUse the handout as an opportunity to motivate your studies through real-world examples. These handouts will focus not only on providing you with a sense of how useful this is in a real-world application, but also can definitely help you better understand the mechanics behind all concepts we will study throughout the course."
  },
  {
    "objectID": "fin-strat/Introduction/index.html#references",
    "href": "fin-strat/Introduction/index.html#references",
    "title": "Introduction and Course Overview",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nBodie, Zvi, Alex Kane, and Alan J Marcus. 2021. Investments.\n\n\nBrealey, R. A., S. C. Myers, and F. Allen. 2020. Principles of Corporate Finance. The Irwin/McGraw-Hill Series in Finance, Insurance, and Real Estate. McGraw-Hill Education. https://books.google.com.br/books?id=nsrHuwEACAAJ.\n\n\nBrigham, Eugene F, and Michael C Ehrhardt. 2023. Financial Management : Theory & Practice. 16th ed. Mason, OH: CENGAGE Learning Custom Publishing.\n\n\nDownes, John, and Jordan Goodman. 2014. Dictionary of Finance and Investment Terms. 9th ed. Barron’s Business Dictionaries. Hauppauge, NY: Barron’s Educational Series."
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#about-this-lecture",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#about-this-lecture",
    "title": "Equity Valuation",
    "section": "About this lecture",
    "text": "About this lecture\n\nThis lecture aims to walk through a practical application of Valuation and Financial Modeling using the tools that we have developed thus far\n\nA Valuation is nothing more than an assessment of a firm (or project)’s value\nBased on the current and past information about the firm, project, and industry, we estimate its future results and evaluate how much it is worth in terms of the current period\n\nWe will closely follow (Berk and DeMarzo 2023), Chapter 19, which presents the valuation case\nYou can also use these guidelines to value other publicly held companies\nWhenever you’re in doubt regarding a specific term, refer to the previous lectures, which will be, in all possible cases, mentioned throughout these slides\nAn accompanying Microsoft Excel file available on eClass® with all the calculations"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#about-the-company",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#about-the-company",
    "title": "Equity Valuation",
    "section": "About the company",
    "text": "About the company\n\nIdeko is a privately held designer and manufacturer of specialty sports eyewear\nIn mid-2005, its owner and founder, June Wong, has decided to sell the business, after having relinquished management control about four years ago. As a partner in KKP Investments, you are investigating purchasing the company\nIf a deal can be reached, the acquisition will take place at the end of the current fiscal year. In that event, KKP plans to implement operational and financial improvements at Ideko over the next five years, after which it intends to sell the business\nYou believe a deal could be struck to purchase Ideko’s equity at the end of this fiscal year for an acquisition price of $150 million, which is almost double Ideko’s current book value of equity.\n\n\nQuestion: is this price reasonable?"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-1-analyzing-comparable-firms",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-1-analyzing-comparable-firms",
    "title": "Equity Valuation",
    "section": "Step 1: analyzing comparable firms",
    "text": "Step 1: analyzing comparable firms\n\nAs you saw in the Financial Analysis lectures (2-4), one of the first steps in assessing a firm’s value is to understand how comparable firms behave in terms of financial indicators\nA quick way to gauge the reasonableness of the proposed price for Ideko is to compare it to that of other publicly traded firms using the method of comparable firms (also known as multiples)\nMore specifically, market-valuation multiples rely on the fact that a reasonable estimate for a firm’s value should be a factor of its earnings:\n\nIf the industry-median EV/EBITDA is 3.2x, our firm is valued at…\nIf the industry-median EV/Sales is 1.2x, our firm is valued at…\nIf the P/E ratio is 15x, our firm is valued at…\n\n\n\n\\(\\rightarrow\\) For a more detailed discussion on market-valuation ratios, refer to the Financial Analysis lecture here"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-1-analyzing-comparable-firms-1",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-1-analyzing-comparable-firms-1",
    "title": "Equity Valuation",
    "section": "Step 1: analyzing comparable firms",
    "text": "Step 1: analyzing comparable firms\n\n\n\n\n\n\n\n\n\n\n\nRatio\nIdeko (proposed)\nOakley\nLuxxotica\nNike\nSporting Goods\n\n\n\n\nP/E\n21.6\n24.8\n28\n18.2\n20.3\n\n\nEV/Sales\n?\n2\n2.7\n1.5\n1.4\n\n\nEV/EBITDA\n?\n11.6\n14.4\n9.3\n11.4\n\n\nEBITDA/Sales\n?\n17%\n18.50%\n15.90%\n21.10%\n\n\n\n\nFor example, at a price of $150 million, which is your first estimate of the acquisition price, Ideko’s price-earnings (P/E) ratio is \\(\\small 150,000/6,939 \\approx 21.6\\)\nUse the numbers presented in the next slide to calculate the:\n\nEV/Sales\nEV/EBITDA\nEBITDA/Sales"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-1-analyzing-comparable-firms-2",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-1-analyzing-comparable-firms-2",
    "title": "Equity Valuation",
    "section": "Step 1: analyzing comparable firms",
    "text": "Step 1: analyzing comparable firms\n\n\n\n\n\n\n\n\n\n\n\nRatio\nIdeko (proposed)\nOakley\nLuxxotica\nNike\nIndustry Average\n\n\n\n\nP/E\n21.6\n24.8\n28\n18.2\n20.3\n\n\nEV/Sales\n2\n2\n2.7\n1.5\n1.4\n\n\nEV/EBITDA\n9.1\n11.6\n14.4\n9.3\n11.4\n\n\nEBITDA/Sales\n21.7%\n17%\n18.50%\n15.90%\n21.10%\n\n\n\n\nA quick look at this table reveals that:\n\nSome multiples vary substantially across firms\nOther multiples are more stable\nDifferences may stem from differences in operating performance and other firm-specific characteristics\n\nHow to use these numbers to price Ideko’s shares?"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-1-analyzing-comparable-firms-3",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-1-analyzing-comparable-firms-3",
    "title": "Equity Valuation",
    "section": "Step 1: analyzing comparable firms",
    "text": "Step 1: analyzing comparable firms\n\nAt the proposed price, Ideko’s P/E ratio is low relative to those of Oakley and Luxottica, although it is somewhat above the P/E ratios of Nike and the industry overall. The same can be said for Ideko’s valuation as a multiple of sales\nThus, based on these two measures, Ideko looks “cheap” relative to Oakley and Luxottica, but is priced at a premium relative to Nike and the average sporting goods firm\nThe deal stands out, however, when you compare Ideko’s enterprise value relative to EBITDA:\n\n\\(\\small 9.1\\times\\) is below that of all of the comparable firms\nHowever, \\(\\small 21.7\\%\\) EBITDA Margin is higher than all competitors\n\nOverall, using \\(\\small \\$150\\) million as the “correct” price seemed a reasonable estimate\nWhile these multiples provides some reassurance that the acquisition price is reasonable compared to other peers, they ignore important differences such as the operating efficiency and growth prospects of the firms, as well as they look only at past performance"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-1-analyzing-comparable-firms-4",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-1-analyzing-comparable-firms-4",
    "title": "Equity Valuation",
    "section": "Step 1: analyzing comparable firms",
    "text": "Step 1: analyzing comparable firms\n\nFor each multiple, we can find the highest and lowest values across all three firms and the industry portfolio. Applying each multiple to the data for Ideko yields the following results:\n\n\n\n\n\n\n\n\n\n\n\n\n\nMinimum\nMaximum\nLower Bound\nUpper Bound\n\n\n\n\nP/E\n18.2\n28\n$126.3\n$194.3\n\n\nEV/Sales\n1.4\n2.7\n$107.0\n$204.5\n\n\nEV/EBITDA\n9.1\n14.4\n$149.9\n$236.0"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-2-integrating-the-business-plan",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-2-integrating-the-business-plan",
    "title": "Equity Valuation",
    "section": "Step 2: integrating the business plan",
    "text": "Step 2: integrating the business plan\n\nWhile comparables provide a useful starting point, whether this acquisition is a successful investment for KKP depends on Ideko’s post-acquisition performance:\n\nIf the business plan for the years ahead is substantially different from past performance, then the multiple comparisons may neglect important aspects that should be included in the pricing of the security!\nThus, it is necessary to look in detail at Ideko’s operations, investments, and capital structure, and to assess its potential for improvements and future growth.\n\nAll in all, on the operational side, you are quite optimistic regarding the company’s prospects\nIn the next set of slides, you’ll see some numbers from Ideko’s estimated balance-sheet and income statement data as of 2005, as well as some operating forecasts that will be the basis for building the financial model"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#balance-sheet-and-income-statement",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#balance-sheet-and-income-statement",
    "title": "Equity Valuation",
    "section": "Balance Sheet and Income Statement",
    "text": "Balance Sheet and Income Statement\n\n\n\n\n\n\\(\\rightarrow\\) You can find the hardcoded numbers in the accompaining Microsoft Excel file"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#income-and-cost-estimates",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#income-and-cost-estimates",
    "title": "Equity Valuation",
    "section": "Income and Cost Estimates",
    "text": "Income and Cost Estimates\n\n\n\n\n\n\\(\\rightarrow\\) You can find the hardcoded numbers in the accompaining Microsoft Excel file\n\nThe market is expected to grow by \\(\\small 5\\%\\) per year as the company produces a superior product\nKKP plans to cut administrative costs and redirect resources to new product development, sales, and marketing, boosting market share from \\(\\small 10\\%\\) to \\(\\small 15\\%\\) over the next five years"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#capital-expenditures",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#capital-expenditures",
    "title": "Equity Valuation",
    "section": "Capital Expenditures",
    "text": "Capital Expenditures\n\n\n\n\n\n\\(\\rightarrow\\) You can find the hardcoded numbers in the accompaining Microsoft Excel file\n\nThe increased sales demand can be met in the short run using the existing production lines by increasing overtime and running some weekend shifts\nHowever, once the growth in volume exceeds \\(\\small 50\\%\\), Ideko will definitely need to undertake a major expansion to increase its manufacturing capacity"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#working-capital-requirements",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#working-capital-requirements",
    "title": "Equity Valuation",
    "section": "Working Capital Requirements",
    "text": "Working Capital Requirements\n\n\n\n\n\n\\(\\rightarrow\\) You can find the hardcoded numbers in the accompaining Microsoft Excel file\n\nActual Credit Policy: \\(\\small 90\\) days. While standard for the industry is \\(\\small 60\\) days, you believe that Ideko can tighten its credit policy to achieve this goal without sacrificing many sales\nWhile maintaining a certain amount of inventory is necessary to avoid production stoppages, with tighter controls of the production process, \\(\\small 30\\) days’ worth of inventory will be adequate"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#planned-debt-and-interest-payments",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#planned-debt-and-interest-payments",
    "title": "Equity Valuation",
    "section": "Planned Debt and Interest Payments",
    "text": "Planned Debt and Interest Payments\n\n\n\n\n\n\\(\\rightarrow\\) You can find the hardcoded numbers in the accompaining Microsoft Excel file\n\nYou plan to greatly increase the firm’s debt, and have obtained bank commitments for loans of \\(\\small\\$100\\) million should an agreement be reached\nThese term loans will have an interest rate of \\(\\small 6.8\\%\\), and Ideko will pay interest only during the next five years\nYou can compute interest expenses by \\(r_d \\times D_{t-1}\\)"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#sources-and-uses-of-funds",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#sources-and-uses-of-funds",
    "title": "Equity Valuation",
    "section": "Sources and Uses of Funds",
    "text": "Sources and Uses of Funds\n\n\n\n\n\n\\(\\rightarrow\\) You can find the hardcoded numbers in the accompaining Microsoft Excel file\n\nIn addition to the \\(\\small \\$150\\) million purchase price for Ideko’s equity, \\(\\small \\$4.5\\) million will be used to repay Ideko’s existing debt.\nWith \\(\\small \\$5\\) million in advisory and other fees associated with the transaction, the acquisition will require \\(\\small \\$159.5\\) million in total funds\nRequired Equity: \\(\\small 159.5-100-6.5=53,000\\)"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-2-building-the-financial-model",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-2-building-the-financial-model",
    "title": "Equity Valuation",
    "section": "Step 2: Building the Financial Model",
    "text": "Step 2: Building the Financial Model\n\nThe value of any investment opportunity arises from the future cash flows it will generate\nWith the past information in mind, you begin to forecast Ideko’s future earnings\nTo convert this to cash flows, you will also consider Ideko’s working capital and investment needs"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-2-the-pro-forma-income-statement",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-2-the-pro-forma-income-statement",
    "title": "Equity Valuation",
    "section": "Step 2: The pro-forma income statement",
    "text": "Step 2: The pro-forma income statement\n\nWe can forecast Ideko’s income statement for the five years following the acquisition based on the operational and capital structure changes proposed\nThis income statement is often referred to as a pro-forma income statement, because it is not based on actual data but rather depicts the firm’s financials under a given set of hypothetical assumptions\nThe pro-forma income statement translates our expectations regarding the operational improvements KKP can achieve at Ideko into consequences for the firm’s earnings.\nStart from the sales forecast using your inputs for market size, market-share, and unit prices:\n\n\n\\[\n\\small Sales_t=\\text{Market Size}_t\\times \\text{Market Share}_t \\text{Unit Price}_t\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-2-the-pro-forma-income-statement-1",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-2-the-pro-forma-income-statement-1",
    "title": "Equity Valuation",
    "section": "Step 2: The pro-forma income statement",
    "text": "Step 2: The pro-forma income statement\n\nYou can use a similar rationale to project COGS, and use the estimated ratios (in terms of sales) to deduct all other operating costs\nYou can plug in interest expenses and taxes based on the assumptions discussed before\nYou should be able to have your pro-forma income statement like below:"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-3-working-capital-needs",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-3-working-capital-needs",
    "title": "Equity Valuation",
    "section": "Step 3: Working Capital Needs",
    "text": "Step 3: Working Capital Needs\n\nBased on these working capital requirements, you can forecast Ideko’s net working capital (NWC) over the next five years. For example, for Accounts Receivable:\n\n\n\\[\n\\small \\text{Accounts Receivable}= \\text{Days Required}\\times \\dfrac{Annual Sales}{365}\\rightarrow 60 \\times \\dfrac{88,358}{365}\\approx 14.525\n\\]\n\nYou can then use the same rationale to project all other working capital requirements across the years, always looking at the yearly forecasts for sales and costs\nFinally, you can use the net operating working capital formula to derive the investments needed each year:\n\n\n\n\\[\n\\small \\Delta NWC_{t} = NWC_{t}-NWC_{t-1}\n\\]\nWith \\(\\small NWC_{t}\\) defined as Current Operating Assets minus Current Operating Liabilities"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-3-working-capital-needs-1",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-3-working-capital-needs-1",
    "title": "Equity Valuation",
    "section": "Step 3: Working Capital Needs",
    "text": "Step 3: Working Capital Needs\n\nUsing the estimates described before, you should be able to find:"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-4-forecasting-the-free-cash-flow",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-4-forecasting-the-free-cash-flow",
    "title": "Equity Valuation",
    "section": "Step 4: Forecasting the Free Cash Flow",
    "text": "Step 4: Forecasting the Free Cash Flow\n\nWe now have the data needed to forecast Ideko’s free cash flows over the next five years:\n\nEarnings have been estimated in the pro-forma income statement\nYou have estimated working capital requirements\nInterest expenses are a byproduct of the interest rate for the loan and the loan size each year\nDepreciation has been estimated by the tax department\nCapital Expenditures are available\n\nYou can combine these items to form the Free Cash Flow estimates:\n\n\n\\[\n\\small FCF_{t}=EBIT_t\\times(1-\\text{Tax Rate}_t)+\\text{Depreciation}_t\\pm \\Delta NWC_t \\pm CAPEX_t\n\\]\n\n\n\\(\\rightarrow\\) For a detailed discussion on Free Cash Flow, refer to this Fundamentals of Capital Budgeting lecture here."
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-4-forecasting-the-free-cash-flow-1",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-4-forecasting-the-free-cash-flow-1",
    "title": "Equity Valuation",
    "section": "Step 4: Forecasting the Free Cash Flow",
    "text": "Step 4: Forecasting the Free Cash Flow"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-4-valuing-the-investment",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-4-valuing-the-investment",
    "title": "Equity Valuation",
    "section": "Step 4: Valuing the Investment",
    "text": "Step 4: Valuing the Investment\n\nYou now have the free cash flow estimates, both at the firm level (FCF) and at the equity (FCFE) level\nAs you saw in Valuation with Leverage lecture, you will now have to evaluate these cash flow streams according to one of the methods (WACC, APV, or FTE)\nTo value KKP’s investment in Ideko, we need to assess the risk associated with Ideko and estimate an appropriate cost of capital\nBecause Ideko is a private firm, we cannot use its own past returns to evaluate its risk, but must instead rely on comparable publicly traded firms\nIn this stage, we will use data from the comparable firms identified earlier to estimate a cost of capital for Ideko"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-5-estimating-the-cost-of-capital",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-5-estimating-the-cost-of-capital",
    "title": "Equity Valuation",
    "section": "Step 5: Estimating the Cost of Capital",
    "text": "Step 5: Estimating the Cost of Capital\n\nBased on Oakley, Luxxotica, and Nike, we estimate the firms’ cost of capital using the CAPM:\n\n\n\\[\n\\small r_U=r_f + \\beta_U\\times(E[r_{mkt}-r_f])\n\\]\nwhere \\(\\small R_f\\) is the risk-free market return (in this case, 4%), \\(\\small r_{mkt}\\) is the return on the market portfolio, assumed to be 9% in this case.\n\nIf we are using the CAPM, finding \\(\\small \\beta_U\\) is the same approach as of finding \\(\\small r^U\\):\n\n\n\n\\[\n\\small \\beta_U= \\text{% of Equity} \\times \\beta_E + \\text{% of Debt} \\times \\beta_D\n\\]\n\nBased on the three comparable firms, we set \\(\\small \\beta_U=1.2\\) and then \\(\\small r_U\\) is:\n\n\n\n\\[\n\\small r_U=4\\%+1.20\\times 5\\% = 10\\%\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-6-valuing-the-investment",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-6-valuing-the-investment",
    "title": "Equity Valuation",
    "section": "Step 6: Valuing the Investment",
    "text": "Step 6: Valuing the Investment\n\nWe can apply several techniques to value the investment:\n\n\nWe can use a multiple (calculated in Step 1) and apply it to the forecasted numbers from Ideko\nBecause We can use the APV Method, knowing that because the debt is paid on a fixed schedule during the forecast period, the APV method is the easiest valuation method to apply\n\n\nIn what follows, we’ll apply each one of these techniques to assess the firm value\n\n\n\\(\\rightarrow\\) See the accompanying Excel file for the numeric calculations"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-6-valuing-the-investment-multiples",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#step-6-valuing-the-investment-multiples",
    "title": "Equity Valuation",
    "section": "Step 6: Valuing the Investment: Multiples",
    "text": "Step 6: Valuing the Investment: Multiples\n\nPractitioners generally estimate a firm’s continuation value (also called the terminal value) at the end of the forecast horizon using a valuation multiple. While forecasting cash flows explicitly is useful in capturing those specific aspects of a company, in the long run firms in the same industry typically have similar expected growth rates, profitability, and risk\nThus, applying a multiple is potentially as reliable as estimating the value based on an explicit forecast of distant cash flows\nIn most settings, the EBITDA multiple is more reliable than sales or earnings multiples because it accounts for the firm’s operating efficiency and is not affected by leverage differences between firms\n\n\n\\[\n\\small V^L=\\text{Forecasted EBITDA}\\times\\text{EBITDA Multiple}\n\\]\n\\(\\rightarrow\\) See the accompanying Excel file for the numeric calculations"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#a-reality-check",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#a-reality-check",
    "title": "Equity Valuation",
    "section": "A reality check",
    "text": "A reality check\n\nDoes our estimate make sense? Our estimate for Ideko’s initial enterprise value is \\(\\small \\$213\\) million, with an equity value of \\(\\small \\$113\\) million\nAs KKP’s initial cost to acquire Ideko’s equity is \\(\\small \\$53\\) million, based on these estimates, the deal looks attractive, with an NPV of \\(\\small 113- 53 = 60\\) million\nDoes an initial enterprise value of \\(\\small \\$213\\) million for Ideko seem reasonable compared to the values of other firms in the industry?\nHere again, multiples are helpful. Let’s compute the initial valuation multiples that would be implied by our estimated enterprise value of \\(\\small \\$213\\) million and compare them to Ideko’s closest competitors\n\n\n\\(\\rightarrow\\) See the accompanying Excel file for the numeric calculations"
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#supplementary-reading",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#supplementary-reading",
    "title": "Equity Valuation",
    "section": "Supplementary Reading",
    "text": "Supplementary Reading\n\nSee Financial Analytics Toolkit: Financial Statement Forecasting for a detailed discussion on how to forecast future cash flows\n\n\n\\(\\rightarrow\\) All contents are available on eClass®."
  },
  {
    "objectID": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#references",
    "href": "fin-mgmt/Lecture 9 - Firm Valuation/index.html#references",
    "title": "Equity Valuation",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ."
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#what-is-capital-budgeting",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#what-is-capital-budgeting",
    "title": "Fundamentals of Capital Budgeting",
    "section": "What is Capital Budgeting?",
    "text": "What is Capital Budgeting?\n\nAn important responsibility of corporate financial managers is determining which projects or investments a firm should undertake\n\n\n\n\n\n\n\n\nDefinition\n\n\nCapital Budgeting is the process of analyzing investment opportunities and deciding which ones to accept. It consists of a list of all projects and investments that a company plans to undertake in the near future.\n\n\n\n\nAs per our previous lectures on Investment Decision Rules, the Net Present Value (NPV) is the most accurate metric to evaluate investment projects, and there are a couple of other metrics that can help you get a better understanding of the investment opportunity\nYou also saw how you can use firm’s financials to draw insightful metrics about its performance\n\n\n\nQuestion: what’s next?"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#analyzing-expected-future-performance",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#analyzing-expected-future-performance",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Analyzing (expected) future performance",
    "text": "Analyzing (expected) future performance\n\nSo far, we’ve been concerned about the firm’s past performance:\n\nHow profitability was trending?\nHow does working capital was financed?\n\nNow, we’ll turn our attention to focus on (expected) future performance:\n\nHow much value can a new project bring to the firm?\nWhat are the expected future cash inflows/outflows?\n\nProblem: whenever we were analyzing project returns, its cash-flows were given\nIn what follows, you will learn how to estimate the future cash-flows that will serve as a input to our analysis!"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#introducing-the-free-cash-flow-fcf",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#introducing-the-free-cash-flow-fcf",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Introducing the Free Cash Flow (FCF)",
    "text": "Introducing the Free Cash Flow (FCF)\n\nHow to measure the incremental gains/losses due to the acceptance of a project? For this, we’ll use the Free Cash Flow measure\n\n\n\n\n\n\n\n\n\nDefinition\n\n\nThe Free Cash Flow is a measure of the project’s available cash to be repaid to its investors after all costs and investments needed to sustain the business plan were taken into account. It is calculated by:\n\nProjecting direct expected Revenue and Cost Estimates\nConsidering indirect revenues/expenses\nCalculating EBIT and Taxes\nAdjusting for non-cash effects\nConsidering future investments\nDetermining eventual adjustments, if needed\n\n\n\n\n\n\nWe’ll be doing this in using a case study that will guide us through all the steps"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#case-walkthrough",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#case-walkthrough",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Case walkthrough",
    "text": "Case walkthrough\n\nCia. Amazônia is a manufacturer of sports shoes that is analyzing the possibility of investing in a new line of sneakers, having even incurred research and market testing costs worth \\(\\small \\$125,000\\). The shoes would be manufactured in a warehouse next to the company’s factory, fully depreciated, which is vacant and could be rented for \\(\\small\\$38,000\\) per year.\nThe cost of the machine is \\(\\small \\$200,000\\), depreciated over five years using the straight-line method. Its market value, estimated at the end of five years, is \\(\\small \\$35,000\\)\nThe company needs to maintain a certain investment in working capital. As it is an industrial company, it will purchase raw materials before producing and selling the final product, which will result in an investment in inventories. The firm will maintain a cash balance as protection against unforeseen expenses. Credit sales will generate accounts receivable. In sum, working capital will represent \\(\\small10\\%\\) of sales revenue."
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#case-walkthrough-continued",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#case-walkthrough-continued",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Case walkthrough, continued",
    "text": "Case walkthrough, continued\n\nThe company projects the following sales over a \\(\\small5\\)-year horizon\n\nYear 1: \\(\\small 7,000\\)\nYear 2: \\(\\small 9,000\\)\nYear 3: $10,000\nYear 4: \\(\\small11,000\\)\nYear 5: \\(\\small9,000\\)\n\nThe unit price is \\(\\small\\$28\\), and the unit cost is \\(\\small\\$14\\). It is estimated that its operating costs will rise at an average rate of \\(\\small6\\%\\) each year\nOn the other hand, the company knows that due to market competition, it will not be able to fully pass this on to prices and projects an average increase in sales prices of \\(\\small4\\%\\) each year"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-1-revenue-and-cost-estimates",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-1-revenue-and-cost-estimates",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 1: Revenue and Cost Estimates",
    "text": "Step 1: Revenue and Cost Estimates\n\nEarnings are not actual cash flows. However, as a practical matter, to derive the forecasted cash flows of a project, financial managers often begin by forecasting earnings\nThus, we begin by determining the incremental earnings of a project—that is, the amount by which the firm’s earnings are expected to change as a result of the investment decision.\nIn our case, we begin by determining the direct earnings and cost estimates from the operation:\n\n\n\\[\n\\small \\text{Gross Profit}_{t}=\\text{Sales}_t\\times(\\text{Price per Unit}_t-\\text{Cost per Unit}_t)\n\\]\n\nThe Gross Profit is our starting point for estimating incremental earnings"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-1-revenue-and-cost-estimates-1",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-1-revenue-and-cost-estimates-1",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 1: Revenue and Cost Estimates",
    "text": "Step 1: Revenue and Cost Estimates\n\nYour Gross Profit estimation should look like the following:\n\n\n\n\n\n\n\n\nBefore we calculate tax expenses, we need to deduct all other costs that may affect taxes:\n\nFor example, depreciation and amortization are non-cash items, but they are generally tax-deductible\nFurthermore, all other incremental costs, even if they are indirect, need to be taken into account"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-2-consider-indirect-effects",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-2-consider-indirect-effects",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 2: Consider indirect effects",
    "text": "Step 2: Consider indirect effects\n\nWhen computing the incremental earnings of an investment decision, we should include all changes between:\n\nThe firm’s earnings with the project;\nThe firm’s earnings without the project\n\nThere are two important sources of indirect costs that need to be considered:\n\nOpportunity Costs: many projects use a resource that the company already owns. However, in many cases the resource could provide value for the firm in another opportunity or project.\nProject externalities: indirect effects of the project that may increase or decrease the profits of other business activities of the firm"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-2-consider-indirect-effects-1",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-2-consider-indirect-effects-1",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 2: Consider indirect effects",
    "text": "Step 2: Consider indirect effects\n\nIn our case, we saw that the firm will use existing assets that otherwise would yield $38,000 yearly. Because of that, we need to take into consideration as an opportunity cost\nWhat about the \\(\\small\\$125,000\\) R&D expenses incurred? This is an example of a sunk cost:\n\nSunk costs have been or will be paid regardless of the decision about whether or not to proceed with the project\nTherefore, they are not incremental with respect to the current decision and should not be included in its analysis\nIf our decision does not affect the cash flow, then the cash flow should not affect our decision!\n\nExamples of sunk costs may include, but are not limited to: past R&D expenses, fixed overhead costs, and unavoidable competition effects"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-3-ebit-and-taxes",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-3-ebit-and-taxes",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 3: EBIT and Taxes",
    "text": "Step 3: EBIT and Taxes\n\nBefore we calculate tax expenses, if we assume that depreciation is tax-deductible, we can then use the \\(\\small\\$40,000\\) value from the straight-line depreciation to deduct our taxable earnings:\n\n\n\\[\n\\small EBIT_{t}= [\\text{Sales}_t\\times(\\text{Price per Unit}_t-\\text{Cost per Unit}_t)-\\text{Depreciation}_t-\\text{Other Costs}_t]\n\\]\n\nAnd the Income Tax that we’ll deduct is:\n\n\n\n\\[\n\\small \\text{Income Tax}_{t}= EBIT_{t}\\times\\tau_t\n\\]\n\n\n\nWhich tax-rate to use? The correct tax rate to use is the firm’s marginal corporate tax rate, which is the tax rate it will pay on an incremental dollar of pre-tax income"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-3-ebit-and-taxes-1",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-3-ebit-and-taxes-1",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 3: EBIT and Taxes",
    "text": "Step 3: EBIT and Taxes\n\nWhat if part of the upfront investment was financed using debt? Do we need to include interest expenses in the calculation?\nBecause the Free Cash Flow is the measure of available resources to all investors of the firm (creditors + equityholders), whenever we evaluating a capital budgeting decision, we do not include interest expenses in the calculation\n\nWe wish to evaluate the project on its own, separate from the financing decision\nFurthermore, the cost of debt (along with its tax shield) can be considered in an appropriate estimate of the cost of capital for the project\nFor these reasons, we also call our incremental earnings as unlevered net income\n\n\n\n\\(\\rightarrow\\) Therefore, in our Free Cash Flow estimations, we’ll be focusing on the operating portion as if it were financed without any debt!"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-3-ebit-and-taxes-2",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-3-ebit-and-taxes-2",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 3: EBIT and Taxes",
    "text": "Step 3: EBIT and Taxes\n\nConsidering both Depreciation and the Opportunity Costs, our Unlevered Net Income is:"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-4-adjust-for-non-cash-effects",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-4-adjust-for-non-cash-effects",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 4: Adjust for non-cash effects",
    "text": "Step 4: Adjust for non-cash effects\n\nAs discussed in previous lectures, earnings are merely an accounting measure of the firm’s performance:\n\nThey do not represent real profits\nAs a consequence, the firm cannot use its earnings to buy goods, pay employees, fund new investments, or pay dividends to shareholders\n\nOn the other hand, cash does!\n\nBecause of this, to evaluate a capital budgeting decision, we must determine its consequences for the firm’s available cash\nThe incremental effect of a project on the firm’s available cash, separate from any financing decisions, is the project’s Free Cash Flow"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-4-adjust-for-non-cash-effects-1",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-4-adjust-for-non-cash-effects-1",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 4: Adjust for non-cash effects",
    "text": "Step 4: Adjust for non-cash effects\n\nThere are important differences between earnings and cash flow:\n\nEarnings include non-cash charges, such as depreciation…\nBut do not include the cost of capital investment!\n\nTo determine the Free Cash Flow, we must adjust for these differences by:\n\nAdding back Depreciation: because depreciation is not a cash flow, we do not include it in the cash flow forecast\nCapital Expenditures (CAPEX): to account for the cash that will be used to fund the equipments, we include the actual cash cost of the asset when it is purchased."
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-4-adjust-for-non-cash-effects-2",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-4-adjust-for-non-cash-effects-2",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 4: Adjust for non-cash effects",
    "text": "Step 4: Adjust for non-cash effects\n\nIn our case, we have the following adjustments:\n\n\nFor Depreciation, we need to add back \\(\\small\\$50,000\\) across Year 1-5 to account for non-cash items\nOn the other hand, to consider the actual cost of the machinery by the time that it was bought, we need to include \\(\\small\\$200,000\\) in Year 0 of the analysis\n\n\nWith these adjustments in place, we should have the following values:"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-5-consider-future-investments-in-working-capital",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-5-consider-future-investments-in-working-capital",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 5: Consider future investments in working capital",
    "text": "Step 5: Consider future investments in working capital\n\nNow that we have considered all cash effects from the investment that is needed, is there anything else that needs to be taken into consideration?\nMost projects will require the firm to continuosly invest in net working capital as time goes by:\n\nFirms may need to maintain a minimum cash balance to meet unexpected expenditures\nInventories of raw materials and finished products are needed to accommodate uncertainties and demand fluctuations\nFinally, customers may not pay for the goods they purchase immediately, and the firm may have credit with its suppliers\n\nAlthough it is difficult to consider all potential fluctuations on working capital, it is expected that a portion of it should be positively correlated with sales:\n\nAs sales go up, firms may want to keep its past terms with suppliers and customers\nAll else equal, an increase in sales should increase the amount of working capital needed"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-5-consider-future-investments-in-working-capital-1",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-5-consider-future-investments-in-working-capital-1",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 5: Consider future investments in working capital",
    "text": "Step 5: Consider future investments in working capital\n\nIn our case, we summarized this idea by taking into consideration that working capital is 10% of the Sales revenue\nTherefore, our year-over-year change in net working capital reflects the additions/deductions on the amount of net working capital for each year:\n\n\n\\[\n\\small \\Delta NWC_{t}=NWC_{t}-NWC_{t-1}\n\\]\n\nIn our case, our net working capital should look as follows:"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-5-consider-future-investments-in-working-capital-2",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-5-consider-future-investments-in-working-capital-2",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 5: Consider future investments in working capital",
    "text": "Step 5: Consider future investments in working capital\n\n\n\n\n\n\nIn the beginning of Year 0, we forecast Year 1’s sales and invest in working capital\nFor each Year 1-4, we look forward to period \\(\\small t+1\\) to determine the adequate level of working capital in \\(t\\)\nAt the end of Year 5, we know that the \\(\\small NWC=0\\), assuming that the project ends\nTherefore, \\(\\small \\Delta NWC_{t=5}\\) shows that the firm can recover its investment in working capital"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-6-calculating-the-free-cash-flow",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-6-calculating-the-free-cash-flow",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 6: Calculating the Free Cash Flow",
    "text": "Step 6: Calculating the Free Cash Flow\n\nWe can summarize what we have so far by:\n\n\n(+) Revenues\n(-) Costs\n(-) Depreciation\n(=) EBIT\n(-) Tax Expenses\n(=) Unlevered Net Income\n(+) Depreciation\n(-) CAPEX\n(-) \\(\\Delta\\) NWC\n(=) Free Cash Flow\n\nThis is the standard estimate of a Free Cash Flow, which is the amount of incremental cash that a project can actually bring to the firm!"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-6-calculating-the-free-cash-flow-1",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-6-calculating-the-free-cash-flow-1",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 6: Calculating the Free Cash Flow",
    "text": "Step 6: Calculating the Free Cash Flow\n\nWe can summarize the Free Cash Flow calculation as follows:\n\n\n\\[\n\\small FCF_{t}= \\underbrace{(\\text{Revenues}-\\text{Costs}-\\text{Depreciation})\\times(1-\\tau)}_{\\text{Unlevered Net Income}}+\\text{Depreciation}-\\text{CAPEX}-\\Delta NWC\n\\]\n\nNote that we first deduct depreciation when computing the project’s incremental earnings, and then add it back (because it is a non-cash expense) when computing Free Cash Flow\nThus, the only effect of depreciation is to reduce the firm’s taxable income!\nBecause of this, we can rewtrite the same equation as:\n\n\n\n\\[\n\\small FCF_{t}= (\\text{Revenues}-\\text{Costs})\\times(1-\\tau)-\\text{CAPEX}-\\Delta NWC+\\tau\\times\\text{Depreciation}\n\\]\n\nWhere the last term is the depreciation tax shield"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-7-adjustments-to-the-calculating-the-free-cash-flow",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-7-adjustments-to-the-calculating-the-free-cash-flow",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 7: Adjustments to the Calculating the Free Cash Flow",
    "text": "Step 7: Adjustments to the Calculating the Free Cash Flow\n\nOur standard Free Cash Flow estimates should look like the following:"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-7-adjustments-to-the-calculating-the-free-cash-flow-1",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-7-adjustments-to-the-calculating-the-free-cash-flow-1",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 7: Adjustments to the Calculating the Free Cash Flow",
    "text": "Step 7: Adjustments to the Calculating the Free Cash Flow\n\nOur final step is to account for any eventual adjustments needed. Some examples include (but are not limited) to:\n\nOther non-cash items: amortization of intangibles, for example, can be taken into consideration\nTiming of cash flows: can be estimated on a monthly or quarterly basis\nDifferent depreciation patterns: straight line depreciation may not apply to all cases\nLiquidation or Salvage value: assets that are no longer needed often have a resale value, or some salvage value if the parts are sold for scrap\nTermination Value: value for the subsequent periods whenever we have infinite-horizon projects"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-7-adjustments-to-the-calculating-the-free-cash-flow-2",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#step-7-adjustments-to-the-calculating-the-free-cash-flow-2",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Step 7: Adjustments to the Calculating the Free Cash Flow",
    "text": "Step 7: Adjustments to the Calculating the Free Cash Flow\n\nIn our case, we know that the market-value of the machinery is \\(\\small 35,000\\). Since it has been fully depreciated at Year 5, we know that the capital gain is simply \\(\\small 35,000 - 0 = 35,000\\)\nTherefore, we also need to consider that, in Year 5, as the project has ended, we can sell the machine, pay taxes on it, and recover part the liquidation value of our investment:\n\n\n\\[\n\\small \\text{Liquidation Value}= 35,000 \\times (1-\\tau)\\rightarrow 35,000\\times(1-34\\%)=23,100\n\\]\n\nAdding this value as a cash-inflow in the last year of the project:\n\n\n\n\\[\n\\small FCF_{t=5}=107,584+23,100=130,684\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#final-result",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#final-result",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Final Result",
    "text": "Final Result"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#bridging-npv-and-free-cash-flow",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#bridging-npv-and-free-cash-flow",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Bridging NPV and Free Cash Flow",
    "text": "Bridging NPV and Free Cash Flow\n\nWe can finally use our FCF estimates in the NPV formula to gauge the value of the project:\n\n\n\\[\n\\small\nNPV= \\dfrac{-219,000}{(1+15\\%)^0}+\\dfrac{46,592}{(1+15\\%)^1}+\\dfrac{69,266}{(1+15\\%)^2}+\\dfrac{80,218}{(1+15\\%)^3}+\\dfrac{101,293}{(1+15\\%)^4}+\\dfrac{130,684}{(1+15\\%)^5}=48,922.22\n\\]\n\nWhat else needs to be done? See the Appendix for a detailed discussion on some of the most common adjustments and extensions1:\n\nAdjust FCF estimates to incorporate different depreciation patterns\nIncorporate long-term value for projects that have an infinite-horizon (for example, a firm!)\nTake uncertainty of the inputs that we’re using into account through Sensitivity, Break-Even, and Scenario Analysis\n\n\n\nAll extensions discussed in the Appendix have an accompanying Excel solution that has been posted on eClass®."
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#supplementary-reading",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#supplementary-reading",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Supplementary Reading",
    "text": "Supplementary Reading\n\nSee Note on Capital Budgeting for a detailed discussion on more aspects of the capital budgeting process in practice\n\n\n\\(\\rightarrow\\) All contents are available on eClass®."
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#different-depreciation-patterns",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#different-depreciation-patterns",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Different Depreciation Patterns",
    "text": "Different Depreciation Patterns\nSuppose that instead of using foreign suppliers when buying your machinery, you have received an offer to use new, national supplier, which can provide machinery that is supposedly as efficient as the original one. Notwithstanding, because you’re financing a national capital good, you have access to an accelerated depreciation benefit:\n\nThe market-value of the machine is still worth \\(\\small \\$200,000\\), to be paid in Year 0\nYou’ll fully depreciate the machine in the first three years: \\(\\small 30\\%\\) in the first year, and \\(\\small 35\\%\\) in the second and third year\n\n\nWould you accept the offer?\n\n\n\\(\\rightarrow\\) Answer provided in Excel"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#tax-carryforwards",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#tax-carryforwards",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Tax Carryforwards",
    "text": "Tax Carryforwards\nSuppose that instead of having the previous accelerated depreciation alternative, you have received a more aggressive one: depreciate 100% of the machine in the first year. You also notice that you can deduct your taxable income with tax carryforward up to a limit of 30% of the taxable income. Calculate the new NPV of the project and discuss how it has changed.\n\n\\(\\rightarrow\\) Answer provided in Excel"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#continuation-value",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#continuation-value",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Continuation Value",
    "text": "Continuation Value\nIn our standard setting, our assumption was that the firm’s operation would cease after Year 5. Because of that, we initially included in our Free Cash Flow estimates a Termination Value that represented the sale of unused assets, after taking into account its tax effects.\n\nSuppose that the firm is able to continue its operations indefinitely after Year 5, and that the FCF is expected to stay the same as of Year 5. Calculate the new NPV of the project assuming the same cost of capital. Justify why the values have changed significantly.\nHow your answer would change if the FCF grew at a 2% rate after Year 5, indefinitely?\n\n\n\\(\\rightarrow\\) Answer provided in Excel"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Dealing with Uncertainty",
    "text": "Dealing with Uncertainty\n\nThe most difficult part of capital budgeting is deciding how to estimate the cash flows and cost of capital. Unfortunately, these estimates are often subject to significant uncertainty:\nHow we can assess the importance of this uncertainty and identify the drivers of value in the project?\nIn what follows, we’ll look at some examples outlining ways to incorporate uncertainty in our valuation model"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty-1-break-even-analysis",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty-1-break-even-analysis",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Dealing with Uncertainty #1: Break-Even Analysis",
    "text": "Dealing with Uncertainty #1: Break-Even Analysis\nShareholders at Cia Amazônia are concerned that rising costs of activity may hinder any profitable investment opportunity projected in the original project. More specifically, their main concern is that the average growth rate in Unit Costs, which was estimated to be 6%, is estimated using very unreasonable scenario, and that higher increases in unit costs might induce the project’s NPV to be negative. Estimate what is the maximum average increase in unit costs over the years that would change the decision to invest in the project.\n\\(\\rightarrow\\) Answer provided in Excel"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty-2-sensitivity-analysis",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty-2-sensitivity-analysis",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Dealing with Uncertainty #2: Sensitivity Analysis",
    "text": "Dealing with Uncertainty #2: Sensitivity Analysis\nShareholders might be reluctant to trust the NPV estimates if they are unable to understand which drivers potentially affect more the value of the project in best and worst-case situations. Consider that you now have three scenarios: base, worst, and best case scenarios. In each one, you have the following configuration:\n\nBase: the baseline exercise from Cia Amazônia. Growth rate of Unit Costs: \\(\\small4\\%\\); Growth rate of Unit Prices: \\(\\small6%\\); Cost of Capital: \\(\\small15\\%\\)\nWorst: Growth rate of Unit Costs: \\(\\small0%\\); Growth rate of Unit Prices: \\(\\small10\\%\\); Cost of Capital: \\(\\small19\\%\\)\nBest: Growth rate of Unit Costs: \\(\\small10\\%\\); Growth rate of Unit Prices: \\(\\small1\\%\\); Cost of Capital: \\(\\small10\\%\\)\n\n\nEstimate how your NPV estimates change along with your inputs, each one at a time, and identify which input is more important for the project’s NPV.\n\\(\\rightarrow\\) Answer provided in Excel"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty-3-scenario-analysis",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty-3-scenario-analysis",
    "title": "Fundamentals of Capital Budgeting",
    "section": "Dealing with Uncertainty #3: Scenario Analysis",
    "text": "Dealing with Uncertainty #3: Scenario Analysis\nWhat if you want to vary over more than one input at a time? Say that, for example, you want to think about a combination of growth rates for unit costs AND unit revenues at the same time? It is very reasonable to assume that more than one driver is going to change at a time. To do that, create a 3x3 grid of combinations considering the growth rates of Unit Costs and Unit Price and show how your NPV estimates change for each pair of growth rate estimates.\n\n\\(\\rightarrow\\) Answer provided in Excel"
  },
  {
    "objectID": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#references",
    "href": "fin-mgmt/Lecture 7 - Fundamentals of Capital Budgeting/index.html#references",
    "title": "Fundamentals of Capital Budgeting",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#financing-short-term-needs",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#financing-short-term-needs",
    "title": "Short-term Financial Planning",
    "section": "Financing Short-term needs",
    "text": "Financing Short-term needs\n\nIn our previous lecture, we saw that firms need not only long-term, but also short-term investments\nThese, in general, are referred to as net working capital:\n\nIf a firm is growing, it is likely the case that its net working capital needs are also trending upwards\nIn order to foster such growth, the firm needs to firm about how to fund it!\n\nFirms may also present seasonal sales patterns:\n\nA firm can generate surpluses in some given quarters…\nBut it might demand capital in other quarters\n\nQuestion: how does a company manage its short-term needs within the year?"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#financing-short-term-needs---arrz3",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#financing-short-term-needs---arrz3",
    "title": "Short-term Financial Planning",
    "section": "Financing Short-term needs - ARRZ3",
    "text": "Financing Short-term needs - ARRZ3"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#breaking-down-short-term-financing-needs---arrz3",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#breaking-down-short-term-financing-needs---arrz3",
    "title": "Short-term Financial Planning",
    "section": "Breaking Down Short-Term Financing Needs - ARRZ3",
    "text": "Breaking Down Short-Term Financing Needs - ARRZ3"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#breaking-down-debt-trends---arrz3",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#breaking-down-debt-trends---arrz3",
    "title": "Short-term Financial Planning",
    "section": "Breaking Down Debt Trends - ARRZ3",
    "text": "Breaking Down Debt Trends - ARRZ3"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#overview",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#overview",
    "title": "Short-term Financial Planning",
    "section": "Overview",
    "text": "Overview\n\nOverall, it seems that both Sales and Net Working Capital are trending upwards\n\nHigher demand for products \\(\\rightarrow\\) higher working capital needs\nHigher need to extend funding to foster increased activity\n\nLooking at the specifics of Arezzo’s working capital accounts, it seems it is being fueled mainly by Inventories, although Receivables have also increased significantly\nPayables have substantially increased to sustain the firm’s growth. However, overall net capital needs have increased\nHow to finance the remaining part?\nIn the next slides, we’ll study a step-by-step guide in short-term financing, following (Berk and DeMarzo 2023)"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#step-1-forecasting-short-term-financing-needs",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#step-1-forecasting-short-term-financing-needs",
    "title": "Short-term Financial Planning",
    "section": "Step 1: Forecasting Short-term Financing Needs",
    "text": "Step 1: Forecasting Short-term Financing Needs\n\nThe first step in short-term financial planning is to forecast the company’s future cash flows\n\nA company forecasts its cash flows to determine whether it will have surplus cash or a cash deficit for each period\nThe management needs to decide whether that surplus or deficit is temporary or permanent\n\nWithin short-term financing planning, we are interested in analyzing the types of cash surpluses or deficits that are temporary and, therefore, short-term in nature\nTypically, firms require short-term financing for three reasons:\n\n\nSeasonalities\nNegative Cash-Flow shocks\nPositive Cash-Flow shocks"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#seasonalities",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#seasonalities",
    "title": "Short-term Financial Planning",
    "section": "Seasonalities",
    "text": "Seasonalities\n\nWhen sales are concentrated during a few months, sources and uses of cash are also likely to be seasonal:\n\nFirms in this position may find themselves with a surplus of cash during some months that is sufficient to compensate for a shortfall during other months\nHowever, because of timing differences, such firms often have short-term financing needs\n\nThe introduction of seasonal sales creates some dramatic swings in short-term cash flows:\n\nWhile Cost of Goods Sold generally fluctuates proportionally with sales, other costs (such as administrative overhead and depreciation) do not, leading to large changes in the firm’s net income by quarter\nNet working capital changes are more pronounced\n\nSeasonal sales create large short-term cash flow deficits and surpluses: because of this, a firm may opt to invest surpluses in short-term investment options and use it during downturns"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#negative-and-positive-cash-flows",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#negative-and-positive-cash-flows",
    "title": "Short-term Financial Planning",
    "section": "Negative and Positive Cash-Flows",
    "text": "Negative and Positive Cash-Flows\nNegative Cash-Flow Shocks\n\n\nOccasionally, a company will encounter circumstances in which cash flows are temporarily negative for an unexpected reason (e.g, higher costs, legal actions, supply shortages, etc)\nSuch unexpected hits in the firm’s cash flow expectations might induce to an increase in financing needs\nExample: what happened to delivery food chains during the onset of the pandemic in Brazil?\n\n\n\nPositive Cash-Flow Shocks\n\nIncreases in firm’s expected sales can leader to increases in short-term financing needs. Going back to the example that we saw, ARZZ’s growth in sales was accompanied by a surge in working capital needs!\nA firm may have a temporary deficit before it actually reaps out the benefits of positive cash-flow shocks (e.g, Marketing investments)"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#step-2-short-term-financing-needs",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#step-2-short-term-financing-needs",
    "title": "Short-term Financial Planning",
    "section": "Step 2: Short-term Financing Needs",
    "text": "Step 2: Short-term Financing Needs\n\nAfter forecasting the need for short-term financing, it is time to decide how it will be financed:\n\n\nOn the one hand, there is an opportunity cost of holding cash in accounts that pay little or no interest - you could have been better-off by investing this money in the operation and/or in financial instruments!\nOn the other hand, firms also face high transaction costs if they need to negotiate a loan on short notice to cover a cash shortfall\n\n\nThe Matching Principle states firms can increase their value by adopting a policy that minimizes the costs associated with the aforementioned trade-off:\n\n\nLong-term needs - or permanent needs should be financed with Long-term sources of funds\nShort-term needs - or temporary needs - should be financed with Short-term debt"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#matching-principle---permanent-working-capital",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#matching-principle---permanent-working-capital",
    "title": "Short-term Financial Planning",
    "section": "Matching Principle - Permanent Working Capital",
    "text": "Matching Principle - Permanent Working Capital\n\nThe permanent portion of working capital is the amount that a firm must keep invested in its short-term assets to support its continuing operations. This investment in working capital is required so long as the firm remains in business:\n\n\nFor example, as ARZZ3 grows in sales and market-share, its working capital levels continue grow year-over-year because of the long-run trend!\nThe matching principle indicates that the firm should finance this permanent portion of working capital with long-term sources of funds\n\n\nWhy? In general, such sources have lower transaction costs than short-term sources of funds, which would have to be replaced more often\n\n\nExample: after forecasting permanent working capital needs, if funding occurs through short-term sources (say, 1 year), firms are exposed to interest rate risk – since you’ll need this money for the next years, you may have to refinance at a higher rate in the future!"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#matching-principle---temporary-working-capital",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#matching-principle---temporary-working-capital",
    "title": "Short-term Financial Planning",
    "section": "Matching Principle - Temporary Working Capital",
    "text": "Matching Principle - Temporary Working Capital\n\nAnother portion of a firm’s investment in its accounts receivable and inventory is temporary and results from seasonal fluctuations in the firm’s business or unanticipated shocks:\n\n\nThis temporary working capital need is the difference between the actual level of investment in short-term assets and the permanent working capital investment outlined before\nIt is considered to be temporary because it relates to short-term fluctuations in the need for working capital in a given period and that will likely not be needed in subsequent periods - for example, monthly seasonality due to Black Friday or Amazon Prime Day\n\n\nFollowing the Matching Principle, temporary working capital needs it should be financed with short-term sources!\nWhy? As the firm won’t need to keep a high level of working capital after some time, it is optimal from a cost perspective to shut down on any funding expenses - for example, don’t keep costly but unused credit lines active"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#financing-policies---aggresive-policy",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#financing-policies---aggresive-policy",
    "title": "Short-term Financial Planning",
    "section": "Financing Policies - Aggresive Policy",
    "text": "Financing Policies - Aggresive Policy\n\nWhat if we depart from the Matching Principle whenever financing firm’s activity? An Aggresive Policy is the case if we financed permanent working capital needs with short-term debt:\n\nWhen the short-term debt comes due, the firm will have to negotiate a new loan\nThis new loan will involve additional transaction costs, and it will carry whatever market interest rate exists at the time\n\nWhen firms can benefit from this policy? As short-term debt is less sensitive to the firm’s credit quality than long-term debt, firms can benefit from it whenever market imperfection are more acute\nFurthermore, when the yield curve is upward sloping, the interest rate on short-term debt is lower than the rate on long-term debt\nHowever, shareholders incur in funding risk, which is the risk of incurring financial distress costs if firm is not able to refinance its debt in a timely manner or at a reasonable rate"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#financing-policies---conservative-policy",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#financing-policies---conservative-policy",
    "title": "Short-term Financial Planning",
    "section": "Financing Policies - Conservative Policy",
    "text": "Financing Policies - Conservative Policy\n\nAlternatively, a firm could finance its short-term needs with long-term debt, a practice known as a Conservative Financing policy: use long-term sources of funds to finance its fixed assets, permanent working capital, and some of its seasonal needs\nWhenever implementing such policy, there will be periods where there is excess cash - i.e, those periods when the firm requires little or no investment in temporary working capital\nWhile such policy significantly reduces funding risks, it has its drawbacks:\n\nExcess cash may earn below-average interest rates\nHolding higher cash levels can also distort managers incentives - i.e, pay perks for themselves or use it non-productively"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#hands-on-exercise-analyzing-arezzos-financing-policy",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#hands-on-exercise-analyzing-arezzos-financing-policy",
    "title": "Short-term Financial Planning",
    "section": "Hands-on Exercise: analyzing Arezzo’s Financing Policy",
    "text": "Hands-on Exercise: analyzing Arezzo’s Financing Policy\n\nDownload Arezzo’s (ticker: ARZZ3) Annual Financial Statements, available on eClass®, along with the Dynamic Spreadsheet file containing the latest quarterly results\n\n\nGo straight to the supplementary notes (“Notes to the financial statements”) section and identify its loans breakdown (“Loans and Borrowings”)\nWhich funding sources are being employed and for what types of investments?\nAll in all, how would you describe the company’s short-term financing policy? Why?"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#short-term-financing-sources",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#short-term-financing-sources",
    "title": "Short-term Financial Planning",
    "section": "Short-term Financing Sources",
    "text": "Short-term Financing Sources\n\nSo far, we saw that firm’s short-term capital needs can arise due to from temporary and permanent needs, according to (Berk and DeMarzo 2023):\n\nPermanent needs relate, in general, to working capital investment that will be necessary throughout the lifetime of a firm (or a project)\nTemporary needs, on the other hand, arise due to seasonalities, positive and negative cash-flow shocks\n\nWays for financing short-term working capital needs range from a variety of sources:\n\nBank Financing\nCommercial Papers\nSecured Financing\n\nIn what follows, we’ll details the main aspects of each financing source"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#bank-financing",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#bank-financing",
    "title": "Short-term Financial Planning",
    "section": "Bank Financing",
    "text": "Bank Financing\n\nOne of the primary sources of short-term financing, especially for small businesses, is the commercial bank. Some types include:\n\n\nSingle Payment Loan: pay interest on the loan and pay back the principal in one lump sum at the end of the loan. Can have a fixed or variable interest rate structure\nCredit lines: case where a bank agrees to lend a firm any amount up to a stated maximum\n\nGenerally used for seasonal needs\nCommited versus uncommited\nRevolving Credit lines\n\nBridge Loans: used to “bridge the gap” until a firm can obtain long-term financing\n\n\nWatch-out for stipulations and fees! These increase the effective interest rate: origination fees, commitment fees, compensating balance requirements etc"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#commitment-fee---example",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#commitment-fee---example",
    "title": "Short-term Financial Planning",
    "section": "Commitment Fee - Example",
    "text": "Commitment Fee - Example\n\nVarious loan fees charged by banks affect the effective interest rate that the borrower pays\nFor example, the commitment fee associated with a committed line of credit increases the effective cost of the loan to the firm. The “fee” can really be considered an interest charge under another name!\n\n\nExample: Suppose that a firm has negotiated a committed line of credit with a stated maximum of $1 million and an interest rate of 10% ( EAR) with a bank. The commitment fee is 0.5% (EAR). At the beginning of the year, the firm borrows $800,000. It then repays this loan at the end of the year, leaving $200,000 unused for the rest of the year. The total cost of the loan is:\n\n\n(+) Interest on borrowed funds: \\(\\small \\$800,000 \\times 10\\% = \\$80,000\\)\n(+) Commitment on unused portion: \\(\\small \\$200,000 \\times 0.5\\% = \\$1,000\\)\n(=) Total Cost = \\(\\small\\$81,000\\)\n(=) Effective Interest Rate, inclusive of Fees: \\(\\small (\\$881,000/\\$800,000)-1=10.125\\%\\)"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#loan-origination-fee---example",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#loan-origination-fee---example",
    "title": "Short-term Financial Planning",
    "section": "Loan Origination Fee - Example",
    "text": "Loan Origination Fee - Example\n\nAnother common type of fee is a loan origination fee, which a bank charges to cover credit checks and legal fees:\n\nThe firm pays the fee when the loan is initiated; like a discount loan, it reduces the amount of usable proceeds that the firm receives.\nAnd like the commitment fee, it is effectively an additional interest charge.\n\n\n\nExample: assume that it is offered a $500,000 loan for 3 months at an annual percentage rate (APR) of 12%. This loan has a loan origination fee of 1% charged on the principal.\n\nThe amount of the loan origination fee is \\(\\small 1\\% \\times \\$500,000 = \\$5,000\\)\n\nThe actual amount borrowed is \\(\\small \\$500,000-\\$5,000=\\$495,000\\)\n\nInterest rate is charged on the total, and not the discounted value: \\(\\small (\\$500,000 \\times 3\\%=\\$15,000)\\)\nTherefore, the annual effective interest rate is \\(\\small \\$515,000/\\$495,000 - 1 = 4.04\\%\\)"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#compensating-balance-requirements---example",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#compensating-balance-requirements---example",
    "title": "Short-term Financial Planning",
    "section": "Compensating Balance Requirements - Example",
    "text": "Compensating Balance Requirements - Example\n\nRegardless of the loan structure, the bank may include a compensating balance requirement in the loan agreement that reduces the usable loan proceeds\n\n\nExample: assume that, in the previous example, rather than charging a loan origination fee, the bank requires that the firm keep an amount equal to 10% of the loan principal in a non-interest-bearing account with the bank as long as the loan remains outstanding\n\nIf that is the case, then the requirement amount is \\(\\small 10\\% \\times \\$500,000 = \\$50,000\\)\nThus, the firm has only \\(\\small\\$450,000\\) of the loan proceeds actually available for use, although it must pay interest on the full loan amount\nTherefore, the actual three-month interest rate paid is:\n\n\n\n\\[\n\\small \\dfrac{(500,000 + 15,000 -50,000)}{(500,000-50,000)}-1 = \\dfrac{465,000}{450,000}-1=3.33\\%\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#other-thoughts-on-bank-financing",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#other-thoughts-on-bank-financing",
    "title": "Short-term Financial Planning",
    "section": "Other thoughts on bank financing",
    "text": "Other thoughts on bank financing\n\nThe three examples outlined before are situations where banks charge extra costs from customers. Why these costs arise?\n\nLegal requirement checks\nCredit analysis\nNeed to reduce the risk of the amount to recover in case of default\n\nSome firms (in general, smaller and newer firms) may not have other options rather than a bank. But that does not mean that bank financing will always lead to higher implied costs:\n\nLong-standing client-bank relationships can convey information about the credit quality of the firm and reduce interest rates\nSome banks specialize in certain activities (e.g, Rabobank) to better manage risks and understand client’s inherent risks\n\nThere can also be subsidized operations for certain activities. See, for example, the role of BNDES in Brazil"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#commercial-papers",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#commercial-papers",
    "title": "Short-term Financial Planning",
    "section": "Commercial Papers",
    "text": "Commercial Papers\n\nCommercial paper is a short-term, unsecured debt used by large corporations\nThe interest on commercial paper is typically paid by selling it at an initial discount, in the likes of what we have with Brazilian government bonds (Tesouro Direto)\nIn Brazil, also referred to as nota promisória comercial: the goal is to target short-term financing\n\n\nExample: suppose that a firm issues three-month commercial paper with a \\(\\small\\$100,000\\) face value and receives \\(\\small\\$98,000\\). What is the annual effective rate is the firm paying for its funds?\n\nUsing our present value formula to analyze the full 3-period interest rate, we have:\n\n\n\n\\[\n\\small FV=PV\\times(1+i)^n \\rightarrow i=\\dfrac{100,000}{98,000}-1 =2.04\\%\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#commercial-papers-in-the-u.s.-at-the-onset-of-covid-19",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#commercial-papers-in-the-u.s.-at-the-onset-of-covid-19",
    "title": "Short-term Financial Planning",
    "section": "Commercial Papers in the U.S. at the onset of COVID-19",
    "text": "Commercial Papers in the U.S. at the onset of COVID-19\n\nThe Covid-19 crisis severely disrupted the functioning of short-term US dollar funding markets, in particular the commercial paper and certificate of deposit segments1\n\nInvestors become reluctant or unable to provide new credit or roll over existing ones…\nIssuers faced challenges in obtaining short-term financing at reasonable rates…\nFligh to safety: investors withdrawn money from riskier assets to move it to safer affects…\nThis movement cascaded over to fund managers in which, pressured by withdraws, had to fire-sale assets at higher discounts to make liquidity\nContagion Effect: due to these accumulated effects, other markets such as the stock market experienced unusually high volatility during the period\n\n\nSee, for example, the BIS Report on the Commercial Paper Turnmoil."
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#secured-financing",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#secured-financing",
    "title": "Short-term Financial Planning",
    "section": "Secured Financing",
    "text": "Secured Financing\n\nBusinesses can also obtain short-term financing by using secured loans, which are loans collateralized with short-term assets\nCommercial banks and Financial companies that purchase account receivables of other companies are the most common sources for secured short-term loans. Some options for secured financing include:\n\nUsing Accounts Receivable as collateral\nUsing Inventories as collateral\nProperly, Plant, and Equipment (PPE) as collateral\n\nHow does collateral help in funding? The better the collateral assets, the better the funding conditions! As funding partners assess the liquidity of the collateralized assets, they’ll assess the specific funding conditions (amount, interest rate, etc) available"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#secured-financing-in-brazil---desconto-de-duplicatas",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#secured-financing-in-brazil---desconto-de-duplicatas",
    "title": "Short-term Financial Planning",
    "section": "Secured Financing in Brazil - Desconto de Duplicatas",
    "text": "Secured Financing in Brazil - Desconto de Duplicatas\n\nThis type of operation is, in essence, a loan from the bank to the firm that is secured by accounts receivable:\n\n\nThe firm sells its products to customers, which will pay in a determined date\nThe bank then provides a loan to the firm, which will receive a discounted value immediately\nThe bank will then receive the original amount stated in the accounts receivable when the firm’s clients actually pay for the products\n\n\nAll else held constant, this operation has a lower cost than a simple loan, as accounts receivables are backing up the loan and reducing the bank’s risk\nIt is important to note that the bank does not bear the risk of not being paid - the obligation from the firm to repay the bank persists!\nOther examples of secured financing: Compor, Vendor operations"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#factorings",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#factorings",
    "title": "Short-term Financial Planning",
    "section": "Factorings",
    "text": "Factorings\n\nSimilar to a discount operation, the key difference is the risk in the event of non-payment:\n\nOn the one hand, in discount operations (desconto de duplicatas), a financial institution provides cash-in-advance to a firm using its accounts receivable as a collateral, with the firm bearing the risk of non-payment from its customers\nOn the other hand, in a factoring operation, a commercial partner acquires the credit and bears the full responsibility of its risk, providing the firm with cash-in-advance\n\nFactoring firms are not financial institutions, but rather commercial partners (“Sociedade mercantil”), which can be financed through equity or bank financing, but not publicly shares\nFactorings do not merely involve financial service, but rather a series of continuous commercial services, such as credit analysis and management, risk management, payables and receivables management, and buying the firm’s account’s receivables and bearing its risk"
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#fundos-de-investimento-em-direito-creditório---fidcs",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#fundos-de-investimento-em-direito-creditório---fidcs",
    "title": "Short-term Financial Planning",
    "section": "Fundos de Investimento em Direito Creditório - FIDCs",
    "text": "Fundos de Investimento em Direito Creditório - FIDCs\n\nA common way to organize resources to finance short-term mismatches is through the use of a FIDC1, which is similar to discount operations and factoring\nHow it works: suppose that a firm sells its products to customers with a 90-days payment, and it needs money today to finance its operations:\n\n\nA FIDC collects money from investors aiming to be exposed to credit operations\nA FIDC then buys several credit obligations, in the same way that a factoring firm does, and bears the risk of the operation\nUltimately, the shareholders of the FIDC are the ones bearing the risk in the event of non-payment by the firm’s clients\n\n\nKey benefit: FIDCs do not need to concentrate risk in only one type of credit operation and/or customer, and can be scaled more easily by through publicly issuances (emissões)\n\nFor detailed information on how FIDCs work, see a detailed description by ANBIMA."
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#institutional-stability-and-investor-protection",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#institutional-stability-and-investor-protection",
    "title": "Short-term Financial Planning",
    "section": "Institutional stability and investor protection",
    "text": "Institutional stability and investor protection\n\nWhenever property rights and contract-enforcement are put in risk, lenders adjust interest rates to cope with the expected risk\nFor example, as seizing collateral may involve a lot of bureaucracy, banks adjust the interest rates for some specific credit contracts\n\n\nThat helps to explain, for example, why countries with higher levels of institutional development, such as higher investor protection and stable laws, have lower interest rates\nIt also helps to explain why, even with decreases in the baseline Brazilian interest rate (SELIC), these changes are not fully reflected in the interest rate offered to customers and firms\n\n\n\n\n\n\n\n\nCreating (dis)incentives?\n\n\nAn interesting discussion in terms of firms’ incentives to comply with credit terms rely on Credit Renegotiation programs, such as Desenrola. When fully predicted by firms and customers, it can create distortions in terms of incentives, since firms anticipate that they’ll be able to renegotiate at lower rates, thereby creating incentives for strategic defaults."
  },
  {
    "objectID": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#references",
    "href": "fin-mgmt/Lecture 5 - Short-term Financial Planning/index.html#references",
    "title": "Short-term Financial Planning",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ."
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#outline",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#outline",
    "title": "Economic Value Added (EVA)",
    "section": "Outline",
    "text": "Outline\n\nThe contents herein are not directly discussed in any of the three main textbooks presented in the syllabus\nIn this sense, this lecture will extensively use the following material:\n\n\nUnderstanding Economic Value Added (HBS Case)\nUnderstanding Economic Value Added - supplementary .xlsx spreadsheet\n\n\n\\(\\rightarrow\\) Both contents are available on eClass®."
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#dissecting-performance",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#dissecting-performance",
    "title": "Economic Value Added (EVA)",
    "section": "Dissecting Performance",
    "text": "Dissecting Performance\nLet’s get back to the example that we’ve worked on in the Financial Analysis lecture:\n\n\nAre the managers doing a good or a bad job?"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#dissecting-performance-pt.-2",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#dissecting-performance-pt.-2",
    "title": "Economic Value Added (EVA)",
    "section": "Dissecting Performance, pt. 2",
    "text": "Dissecting Performance, pt. 2\nWe saw that we can calculate a series of performance indicators that decompose Net Income:\n\nGross Profit Margin looks at the firm’s income after paying out directly-attributable costs;\nOperating Margin shows the firm’s income after deducting all operating costs (+SG&A, Depreciation, etc);\nNOPAT deducts taxes from operational performance and insulates the analysis from the effects of debt policy;\nNet Income takes everything into consideration and provides a measure of the income generated that is attributable to the shareholders of the firm.\n\n\nWhat are we missing here?"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#accounting-vs-economic-performance",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#accounting-vs-economic-performance",
    "title": "Economic Value Added (EVA)",
    "section": "Accounting vs Economic Performance",
    "text": "Accounting vs Economic Performance\n\nLet’s make the Net Income as the starting point. If our goal is to analyze a firm’s operational performance, such metric contains a series of drawbacks:\n\n\nIt considers both Operating and non-Operating performance altogether\nIt includes both recurrent and non-recurrent results\nIt is highly influenced by non-cash items, such as depreciation and amortization\nAccounting tax expenses may be highly influenced by external factors, such as tax compensations, tax shields, subventions etc\n\n\nMore importantly, we’re lacking a key component:\n\nNet Income only considers the cost of external capital, but it does not deduct the implied cost of the shareholders capital – i.e, Cost of Equity!"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#why-this-last-drawback-is-so-important",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#why-this-last-drawback-is-so-important",
    "title": "Economic Value Added (EVA)",
    "section": "Why this last drawback is so important?",
    "text": "Why this last drawback is so important?\n\nConsider two firms that generate $ 500MM in Net Income and have the same level of Assets. These firms are comparable across all dimensions, except that these firms are from totally distinct industries:\n\nThe first company operates in a well-established business environment, a highly-predictable industry with a long track record\nThe second company operates in a new market and a very dynamic business environment, with a lot of entry/exit of competitors\n\nShould shareholders be indifferent between investing in these two firms? No! as investors will demand a different compensation for each firm!\nThe problem is that Net Income (or any derived metric) does not take this into consideration…\nTherefore, a firm can generate profits to the shareholders and, at the same time, destroy value if the compensation is not enough to offset the risk!"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#introducing-eva",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#introducing-eva",
    "title": "Economic Value Added (EVA)",
    "section": "Introducing EVA®",
    "text": "Introducing EVA®\n\nIs the firm generating value to the shareholders?\nThe first model that we’ll use to answer such questions is known as the Economic Value Added (EVA®). It has been introducted by Stern and Stewart Co., a consulting firm\n\n\n\n\n\n\n\n\nDefinition\n\n\nEVA® measures the wealth a company creates (or destroys) each year. It is a company’s after-tax profit from operations minus a charge for the cost of all capital employed to produce those profits – not just the cost of debt, but the cost of equity as well\n\n\n\n\nKey differences when comparing to Net Income:\n\n\nEVA® considers only the operating portion of the firm, disregarding any non-operating results, such as interest income\nIt considers both cost of debt (also included in Net Income) and the cost of equity (not observable, but estimated)"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#a-concise-explanation-of-how-eva-works",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#a-concise-explanation-of-how-eva-works",
    "title": "Economic Value Added (EVA)",
    "section": "A concise explanation of how EVA® works:",
    "text": "A concise explanation of how EVA® works:\n(+) Sales Revenues\n(-) Operating Costs (Direct and Indirect)\n(=) Operating Profit\n(-) Taxes on Operating Profit\n(=) Net Operating Profit After Taxes\n(-) Estimated Weighted Average Cost of Capital \\(\\times\\) Capital Invested by Debt and Equity\n(=) EVA®\n\nIf EVA®&gt;0 \\(\\rightarrow\\) firm is creating value to the shareholders\nIf EVA®&gt;0 \\(\\rightarrow\\) firm is destroying value to the shaholders\nIf EVA®=0 \\(\\rightarrow\\) shareholders are earning exactly what they should earn to compensate the risk"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#eva-and-firm-value",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#eva-and-firm-value",
    "title": "Economic Value Added (EVA)",
    "section": "EVA® and Firm Value",
    "text": "EVA® and Firm Value\n\nEVA® is not observable, but rather estimated:\n\nWe need to isolate the Operating Portion from the firms’ balance sheet to determine the Operating Capital invested\nWe also need an estimate for the Weighted Average Cost of Capital (WAcC)\n\nNotwithstanding, it expected that:\n\nThe higher the EVA® \\(\\rightarrow\\) higher expectations about the firms’ prospects \\(\\rightarrow\\) higher expected value \\(\\rightarrow\\) higher stock price\nHowever, expectations about the firms’ future results constantly change. Because of that, it is difficult to observe a direct relationship between EVA® and firm value\n\nIn the long-run, empirical studies show that, as expected, this relationship is positive"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#how-to-calculate-eva",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#how-to-calculate-eva",
    "title": "Economic Value Added (EVA)",
    "section": "How to Calculate EVA®",
    "text": "How to Calculate EVA®\n\nWe’ll need to depart from purely using financial statements at this point\n\n\nFirst, we need to determine that is the Operating Capital\n\nThis is not the same as of the firm’s Assets\nIt focuses on the portion that the firm needs to run its operations, and discards items that are not part of that (i.e, short-term investments, financial investments etc)\n\nAfter that, we need to estimate:\n\nTax-Rate - the effective tax-rate of the income statement can be misleading\nNOPAT - this can be estimated as Operating Income \\(\\times\\) (1- the estimated Tax-Rate)\nWACC - the weighted average cost of capital\n\n\n\n\\[\n\\text{EVA}= \\text{NOPAT} -\\text{Operating Capital}\\times \\text{WACC}\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#concept-check",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#concept-check",
    "title": "Economic Value Added (EVA)",
    "section": "Concept Check",
    "text": "Concept Check\nPizza Hut Ltd. has existing assets worth $500,000, of which it has operating capital invested of $150,000. The firm’s last reported Net Income was $15,000, and the Earnings Before Interest and Taxes (EBIT) was $50,000. Assuming a 40% tax-rate and a 15% cost of capital, calculate and interpret the firm’s Economic Value Added for the period.\n\n\\[\n\\small\nEVA= NOPAT- (WACC\\times \\text{Operating Capital})\\rightarrow 50,000\\times(1-40\\%)-0.15\\times 150,000\n\\]\n\nTherefore, EVA is simply:\n\n\n\n\\[\nEVA=30,000-22,500=7,500\n\\]\n\nThe firm has generated $7,500 of economic value for the period. This number, as opposed to the net income, takes into account the economic costs of all parties involved in financing the firm’s operations – i.e, creditors and shareholders."
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#a-new-way-to-look-at-the-balance-sheet",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#a-new-way-to-look-at-the-balance-sheet",
    "title": "Economic Value Added (EVA)",
    "section": "A new way to look at the balance-sheet",
    "text": "A new way to look at the balance-sheet\n\nLet’s think about a concrete case:\n\n\n\n\nIn general, we distinguish both side of the balance-sheet in terms of liquidity:\n\nCurrent Assets (Liabilities) are earned (due) at a short-period of time\nLong-Term Assets (Liabilities) are earned(due) at longer periods"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#looking-at-the-balance-sheet-in-terms-of-liquidity",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#looking-at-the-balance-sheet-in-terms-of-liquidity",
    "title": "Economic Value Added (EVA)",
    "section": "Looking at the balance sheet in terms of liquidity",
    "text": "Looking at the balance sheet in terms of liquidity\n\n\n\nIn general, we distinguish both side of the balance-sheet in terms of liquidity:\n\nCurrent Assets (Liabilities) are earned (due) at a short-period of tiume\nLong-Term Assets (Liabilities) are earned(due) at longer periods"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#looking-at-the-balance-sheet-in-terms-of-operating-vs.-financing",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#looking-at-the-balance-sheet-in-terms-of-operating-vs.-financing",
    "title": "Economic Value Added (EVA)",
    "section": "Looking at the balance sheet in terms of operating vs. financing",
    "text": "Looking at the balance sheet in terms of operating vs. financing\n\n\n\nWe can also distinguish accounts in terms of their destination:\n\nOperating Assets (Liabilities) are highlighted in darker colors\nFinancial Assets (Liabilities) are highlighted in lighter colors"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#two-ways-of-estimating-operating-capital",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#two-ways-of-estimating-operating-capital",
    "title": "Economic Value Added (EVA)",
    "section": "Two ways of estimating Operating Capital",
    "text": "Two ways of estimating Operating Capital\n\nThere are two ways to estimate the Operating Capital that will make part of our calculations:\n\n\nThe first way is to address what is the amount of Operating Assets that a firm needs to have in place in order to run its operations\n\nIt takes into account long-term operating assets, such as PPE and Machinery…\nBut also takes into account all operating assets needed for the business (e.g, Inventories, Accounts Receivable, Cash)\nFormally, we can define it by:\n\n\n\n\\[\n\\small \\text{Operating Capital}=\\text{Long-Term Operating Capital + Net Working Capital Needs}\n\\]\n\nWe’ll study Working Capital shortly, but for now, think about all the money that we need to do pay current bills, buy inventories, and produce until we receive from our clients"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#operating-capital-the-assets-side-way",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#operating-capital-the-assets-side-way",
    "title": "Economic Value Added (EVA)",
    "section": "Operating Capital, the Assets-side way",
    "text": "Operating Capital, the Assets-side way\n\nStart with the Long-Term portion of Operating Assets:\n\n\n(+) PPE: $60,000\n(+) Other Operating Assets: $6,000\n(=) Total: $66,000\n\nNow, estimate the net working capital needed to run the business:\n\n\n\n(+) Cash: $12,000\n(+) Inventories: $12,000\n(+) Account Receivable: $24,000\n(-) Accounts Payable: $15,000\n(=) Total :$33,000\n\nTotal Operating Capital: $66,000 + $33,000 = $99,000"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#operating-capital-the-liabilities-side-way",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#operating-capital-the-liabilities-side-way",
    "title": "Economic Value Added (EVA)",
    "section": "Operating Capital, the Liabilities-side way",
    "text": "Operating Capital, the Liabilities-side way\n\nAnother way to calculate the Operating Capital is to look at the sources of Capital:\n\n\n\\[\n\\small \\text{Operating Capital}=\\text{Interest-bearing Liabilities + Equity}\n\\]\n\nInterest-bearing Liabilities are, in general, all Liabilities that have interest expenses\nEquity is the totality of the book-value of Equity\n\n\n\n(+) Short-Term Loans: $19,000\n(+) Long-Term Loans: $48,000\n(-) Short-Term Investments: $8,000\n(+) Shareholder’s Equity: $40,000\n(=) Total: $99,000\n\nTotal Operating Capital: $19,000 + $48,000 - $8,000 + $40,000 = $99,000"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#step-2-calculating-nopat",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#step-2-calculating-nopat",
    "title": "Economic Value Added (EVA)",
    "section": "Step 2: Calculating NOPAT",
    "text": "Step 2: Calculating NOPAT\n\nWe can use the Income Statement to calculate the Net Operating Profit After Taxes\n\n\n\n\n\n\nMethod 1: Operating Profit \\(\\times\\) (1- Tax Rate)\n\nOperating Profit: $12,000\n\nTax-Rate: 34%\n\nNOPAT= $12,000 \\(\\times\\) (1-34%) = $7,920\n\nMethod 2: Add back Interest Expenses (net of tax)\n\nNet Income: $5,069\nInterest Expenses: $4,320\nTax Rate: 34%\nNOPAT= $5,069 + $4,320 \\(\\times\\) (1-34%) = $7,920"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#step-3-calculating-eva",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#step-3-calculating-eva",
    "title": "Economic Value Added (EVA)",
    "section": "Step 3: Calculating EVA®",
    "text": "Step 3: Calculating EVA®\n\nSuppose that you calculated a Weighted Average Cost of Capital of 9.5%\nThen, we can calculate EVA® as:\n\n\n\\[\n\\text{EVA} = \\text{NOPAT}-\\text{WACC}\\times \\text{Operating Capital}\n\\]\nTherefore, we have:\n\n\n\\[\n\\text{EVA} = 7,920-0.095\\times99,000=7,920-9,405=-1,485\n\\]\n\nA profitable firm, after considering its Cost of Equity, may be destroying value!\n\nNet Income: +$5,069\n\nEVA: -$1,485"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#return-on-invested-capital",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#return-on-invested-capital",
    "title": "Economic Value Added (EVA)",
    "section": "Return on Invested Capital",
    "text": "Return on Invested Capital\n\nAnother way to compare and contrast returns and the opportunity cost of capital is to calculate the Return on Invested Capital (ROIC):\n\n\n\\[\nROIC=\\dfrac{\\text{NOPAT}}{\\text{Operating Capital}}=\\dfrac{7,920}{99,000}=8\\%\n\\]\n\nWe see that the resulting return is lower than the cost of capital. We can recalculate the EVA:\n\n\n\n\\[\n\\small EVA= (ROIC-WACC)\\times \\text{Operating Capital}\\rightarrow(8\\%-9.5\\%)\\times 99,000= -1,485\n\\]\n\nFinally, we can also decompose ROIC in terms of Operating Margin and Operating Turnover:\n\n\n\n\\[\n\\small\nROIC=\\dfrac{NOPAT}{Sales}\\times\\dfrac{Sales}{\\text{Operating Capital}}\\rightarrow 6\\%\\times 1.22= 8\\%\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#practical-thoughts-on-eva",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#practical-thoughts-on-eva",
    "title": "Economic Value Added (EVA)",
    "section": "Practical thoughts on EVA",
    "text": "Practical thoughts on EVA\n\nAll in all, how to improve a company’s EVA? We saw that EVA can be calculated as:\n\n\n\\[\nEVA=(\\text{ROIC}-\\text{WACC})\\times \\text{Operating Capital}\n\\]\n\nMargins\n\nIncrease revenues\nSeek for efficiencies in operating costs\n\nTurnover\n\nIncrease Sales\nDecrease the level of Assets needed to generate sales\nAllocate capital towards more productive assets"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#market-value-added-mva",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#market-value-added-mva",
    "title": "Economic Value Added (EVA)",
    "section": "Market Value Added (MVA)",
    "text": "Market Value Added (MVA)\n\nIf EVA is expected to be positively correlated with increases in value, then we can think about the amount of added value that a firm has in the stock market in terms of expected EVAs:\n\n\nThe Market-Value-Added is the difference between Market Value and Book Value of Equity:\n\n\n\\[\n\\small MVA= \\text{Market Value of Equity} - \\text{Book Value of Equity}\n\\] 2. Now, in equilibirum, we should expected if the Market Value of Equity to be the Book Value of Equity plus all the expected EVAs:\n\n\n\\[\n\\small MVA^\\star \\approx \\underbrace{\\text{B.V of Equity}+\\dfrac{\\sum_{t=1}^{\\infty}\\text{EVA}_{t}}{(1+r)^t}}_{\\text{Market Value of Equity}}-\\text{B.V Equity}\\rightarrow MVA^\\star \\approx\\dfrac{\\sum_{t=1}^{\\infty}\\text{EVA}_{t}}{(1+r)^t}\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#potential-dynamics---mva",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#potential-dynamics---mva",
    "title": "Economic Value Added (EVA)",
    "section": "Potential Dynamics - MVA",
    "text": "Potential Dynamics - MVA\n\n\n\n\n\n\n\n\n\n\nCase 1: \\(\\dfrac{\\sum_{t=1}^{\\infty}\\text{EVA}_{t}}{(1+r)^t}\\)&gt;0\nCase 2: \\(\\dfrac{\\sum_{t=1}^{\\infty}\\text{EVA}_{t}}{(1+r)^t}\\)&lt;0"
  },
  {
    "objectID": "fin-mgmt/Lecture 3 - EVA/index.html#references",
    "href": "fin-mgmt/Lecture 3 - EVA/index.html#references",
    "title": "Economic Value Added (EVA)",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#mergers-and-acquisitions",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#mergers-and-acquisitions",
    "title": "Mergers and Acquisitions",
    "section": "Mergers and Acquisitions",
    "text": "Mergers and Acquisitions\n\nJuly, 2008: Anheuser-Busch agreed to an acquisition by Belgian-based beer giant InBev for $70 per share in cash\nIn fact, Anheuser-Busch’s board flatly rejected an initial $65 per share offer, preferring to remain independent\nWhat happens next?\n\nInBev’s managers faced the daunting task of integrating Anheuser’s organization and brands into their global company and generating enough value from the transaction to justify the price they paid\nApart from the deal itself, new financing and investment decisions are brought into action\n\nThis lecture discusses Mergers and Acquisitions, often referred to as M&A, and its importance to Financial Managers"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#historical-context-merger-waves",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#historical-context-merger-waves",
    "title": "Mergers and Acquisitions",
    "section": "Historical Context: Merger Waves",
    "text": "Historical Context: Merger Waves\n\nThe takeover market is characterized by merger waves: peaks of heavy activity followed by quiet troughs of few transactions. Some stylized facts on M&A activity from an historical perspective:\nFor example, M&A activity is greater during economic expansions than during contractions and correlates with bull markets:\n\nThe same economic activities that drive expansions most likely also drive peaks in merger activity\nFor example, lower interest rates, existence of investment opportunities may affect economic activity and, at the same time, foster M&A activity\n\nTo that matter, we can look at the time-series of economic booms and bursts and analyze which characteristics took place in each M&A “wave”"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#historical-context-merger-waves-continued",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#historical-context-merger-waves-continued",
    "title": "Mergers and Acquisitions",
    "section": "Historical Context: Merger Waves (continued)",
    "text": "Historical Context: Merger Waves (continued)"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#historical-context-merger-waves-continued-1",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#historical-context-merger-waves-continued-1",
    "title": "Mergers and Acquisitions",
    "section": "Historical Context: Merger Waves (continued)",
    "text": "Historical Context: Merger Waves (continued)\n\nThe periods of the greatest takeover activity occurred in the 1960s, 1980s, 1990s, and 2000s. Each merger wave was characterized by a typical type of deal:\n\n1960: “conglomerate wave” \\(\\rightarrow\\) firms typically acquired firms in unrelated businesses. The rationale was that managerial expertise was portable across business lines and that the conglomerate business form offered great financial advantages\n1980: “hostile, bust-up” takeovers \\(\\rightarrow\\) acquirers purchased poorly performing conglomerates and sold off its individual business units for more than the purchase price\n1990: “strategic” or “global” deals \\(\\rightarrow\\) more likely to be friendly and to involve companies in related businesses; these mergers often were designed to create strong firms on a scale that would allow them to compete globally\n2000: consolidation \\(\\rightarrow\\) in many industries such as telecommunications and software. This wave also saw private equity firms, such as KKR, TPG, BlackRock, and Cerberus playing a bigger role in acquiring larger firms, such as Hertz. This wave eventually ended with the 2008 economic contraction"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#types-of-mergers",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#types-of-mergers",
    "title": "Mergers and Acquisitions",
    "section": "Types of Mergers",
    "text": "Types of Mergers\n\nFrom an historical perspective, M&A activity was quite different across each wave. What actually characterizes mergers?\nWhile we tend to talk about merger waves and mergers in general, the term “merger,” as commonly used, encompasses several types of transactions that vary by the relation between the target and the acquirer and the method of payment used in the transaction, which are generally included in a document called term-sheet\n\n\nRelationship between target and acquirer:\n\nIf the target and acquirer are in the same industry, the merger is typically called a horizontal merger\nIf the target’s industry buys or sells to the acquirer’s industry, it is called a vertical merger\nFinally, if the target and acquirer operate in unrelated industries, the deal is a conglomerate merger"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#types-of-mergers-continued",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#types-of-mergers-continued",
    "title": "Mergers and Acquisitions",
    "section": "Types of Mergers (continued)",
    "text": "Types of Mergers (continued)\n\nMethod of payment:\n\nDeals also vary based on whether the target shareholders receive stock or cash as payment for target shares:\n\nWhen they receive stock, the deal is often called a stock swap, because target shareholders are swapping their old stock for new stock in either the acquirer or the newly created merged firm\nThe consideration paid to target shareholders can be very complex, including debt instruments, options, and mixes of any of these with cash and/or stock. Commonly, however, target shareholders receive stock, cash, or a mix of the two"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#market-reaction-to-takeovers",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#market-reaction-to-takeovers",
    "title": "Mergers and Acquisitions",
    "section": "Market Reaction to Takeovers",
    "text": "Market Reaction to Takeovers\n\nAbstracting from the fundamental question of takeovers, what happens after the deal has been announced (or, alternatively, put in place)?\nA bidder is unlikely to acquire a target company for less than its current market value. Instead, most acquirers pay a substantial acquisition premium, which is the percentage difference between the acquisition price and the pre-merger price of the target firm.\nBased on historical data from U.S. markets over 1985-2005:\n\nThe acquisition premium over the pre-merger price was around +43%\nThe market reaction to the acquisition announcement on the target stocks was +15%\nThe market reaction to the acquisition announcement on the acquired stocks was +1%"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#market-reaction-to-takeovers-continued",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#market-reaction-to-takeovers-continued",
    "title": "Mergers and Acquisitions",
    "section": "Market Reaction to Takeovers (continued)",
    "text": "Market Reaction to Takeovers (continued)\n\nAlthough acquirer shareholders see an average gain of 1%, in half of the transactions, the bidder price decreases. This raises up some questions:\n\nWhy do acquirers pay a premium over the market value for a target company?\nAlthough the price of the target company rises on average upon the announcement of the takeover, why does it rise less than the premium offered by the acquirer?\nIf the transaction is a good idea, why does the acquirer not consistently experience a large price increase?\n\nIn what follows, we’ll discuss each of these questions in detail"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#reasons-to-acquire-a-company",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#reasons-to-acquire-a-company",
    "title": "Mergers and Acquisitions",
    "section": "Reasons to acquire a company",
    "text": "Reasons to acquire a company\n\nLet’s focus on Question 1: why do acquirers pay a premium over the market value for a target company?\nFor most investors, an investment in the stock market is a zero-NPV investment.\nHow, then, can an acquirer pay a premium for a target and still satisfy the requirement that the investment is a positive-NPV investment opportunity?\n\n\nAnswer: an acquirer might be able to add economic value, as a result of the acquisition, that an individual investor cannot add! In general, these come from synergies, such as cost reductions, revenue enhancements strategies!"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#synergy-reasons-to-merge",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#synergy-reasons-to-merge",
    "title": "Mergers and Acquisitions",
    "section": "Synergy Reasons to Merge",
    "text": "Synergy Reasons to Merge\n\nEconomies of Scale and Scope\nVertical Integration\nExpertise and Learning Curve\nMonopoly Gains\nEfficiency Gains\nTax Savings from Operating Losses\nDiversification\nEarnings Growth"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#managerial-reasons-to-merge",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#managerial-reasons-to-merge",
    "title": "Mergers and Acquisitions",
    "section": "Managerial Reasons to Merge",
    "text": "Managerial Reasons to Merge\n\nMost of the reasons given so far are economically motivated, shareholder-driven incentives to merge. However, managers sometimes have their own reasons to merge. These may be exacerbated in the absence of corporate governance mechanisms\nRecall from our previous discussion that that the announcement gains for acquirers were negative in 50% of the time. Some explanations might include:\n\n\nConflicts of Interest\n\nLimited downside, infinite upside: a CEO that owns 1% of her firm’s stock bears 1% of every dollar lost on a bad acquisition, but enjoys 100% of the gains in compensation and prestige that come with being the CEO of a larger company and may have additional compensation\nManagers or controlling shareholders may also exploit their informational advantage by choosing to acquire their own firm when it is undervalued by the market"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#managerial-reasons-to-merge-continued",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#managerial-reasons-to-merge-continued",
    "title": "Mergers and Acquisitions",
    "section": "Managerial Reasons to Merge (continued)",
    "text": "Managerial Reasons to Merge (continued)\n\nMost of the reasons given so far are economically motivated, shareholder-driven incentives to merge. However, managers sometimes have their own reasons to merge. These may be exacerbated in the absence of corporate governance mechanisms\nRecall from our previous discussion that that the announcement gains for acquirers were negative in 50% of the time. Some explanations might include:\n\n\nOverconfidence\n\nManagerial hubris: which maintains that overconfident CEOs pursue mergers that have low chance of creating value because they truly believe that their ability to manage is great enough to succeed\nOverconfident managers believe they are doing the right thing for their shareholders, but irrationally overestimate their own abilities"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#valuing-an-acquisition",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#valuing-an-acquisition",
    "title": "Mergers and Acquisitions",
    "section": "Valuing an acquisition",
    "text": "Valuing an acquisition\n\nConceptually, we can separate the price paid for a target into its pre-bid market value plus a premium:\n\n\n\\[\n\\small \\text{Acquisition Price} = \\text{Target's pre-bid capitalization} + \\text{Acquisition Premium}\n\\]\n\nFrom the acquirer’s perspective, the benefits of the acquisition can be calculated as:\n\n\n\n\\[\n\\small \\text{Value Acquired} = \\text{Target Stand-Alone Value} + \\text{PV(Synergies)}\n\\]\n\\(\\rightarrow\\) Combining both, we can see that the takeover is a positive-NPV project only if:\n\\[\n\\small \\text{PV(Synergies)} &gt; \\text{Acquisition Premium} \\rightarrow NPV&gt;0\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#valuing-the-acquisition-continued",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#valuing-the-acquisition-continued",
    "title": "Mergers and Acquisitions",
    "section": "Valuing the acquisition (continued)",
    "text": "Valuing the acquisition (continued)\n\nAlthough the premium that is offered is a concrete number, the synergies are not:\n\nInvestors might well be skeptical of the acquirer’s estimate of their magnitude\nThe market reaction, from the acquirer’s perspective, will depend on the investor’s expectations on the present value of synergies created\n\nThe acquirer’s market reaction was, on average, +1%, with a median of 0%. Thus, the market, on average, believes that the premium is approximately equal to the synergies! This helps answering Question 3\nNote, however, that there is a large cross-section variation across deals:\n\nPositive reactions to bids are concentrated in smaller bidders\nDuring the 1990s, 87 large public acquirers announced bids that resulted in $1 billion or more in value reduction at announcement. This can be related to value-destroying operations driven by managerial incentives, and not synergies-related"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#making-the-offer",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#making-the-offer",
    "title": "Mergers and Acquisitions",
    "section": "Making the offer",
    "text": "Making the offer\n\nA tender offer is a public announcement of its intention to purchase a large block of shares for a specified price\nBidders can pay the tender offer value using cash, stock, or a combination of both:\n\nIn a cash transaction, the bidder simply pays for the target, including any premium, in cash\nIn a stock transaction, the bidder pays for the target by issuing new stock and giving it to the target shareholders\n\nThe determination of actual the price that will be offered to the shareholders of the target firm is determined by the Exchange Ratio:\n\n\n\\[\n\\small \\text{Exchange Ratio} = (\\text{# of acquirer shares received for each target share}) \\times \\text{Acquirer Stock Price}\n\\]\n\nIn what follows, we’ll determine how the Exchange Ratio can be estimated"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#exchange-ratio-in-stock-swap-mergers",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#exchange-ratio-in-stock-swap-mergers",
    "title": "Mergers and Acquisitions",
    "section": "Exchange Ratio in stock swap mergers",
    "text": "Exchange Ratio in stock swap mergers\n\nLet \\(A\\) be the premerger, or stand-alone, value of the acquirer, and \\(T\\) be the premerger (stand-alone) value of the target. Let \\(S\\) be the value of the synergies created by the merger.\nIf the acquirer has \\(N_A\\) shares before the merger, at a share price \\(P_A\\), and issues additional \\(x\\) shares to pay for the target, the acquirer’s share price would increase if:\n\n\n\\[\n\\small \\dfrac{A+T+S}{N_A+x}&gt;\\dfrac{A}{N_A}\\equiv P_A\n\\]\n\nTherefore, an acquirer would earn a positive NPV in the transaction if the value of the shares offered is less than the value of the target plus the synergies:\n\n\n\n\\[\n\\small x P_A &lt; T + S\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#exchange-ratio-in-stock-swap-mergers-continued",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#exchange-ratio-in-stock-swap-mergers-continued",
    "title": "Mergers and Acquisitions",
    "section": "Exchange Ratio in stock swap mergers (continued)",
    "text": "Exchange Ratio in stock swap mergers (continued)\n\nFrom the previous equation:\n\n\n\\[\n\\small x P_A &lt; T + S\n\\]\n\nIf we define the target firm pre-announcement value as \\(T = P_T \\times N_T\\), we can then rearrange the last equation to find the maximum exchange ratio that an acquirer can use to have positive NPV in the transaction:\n\n\n\n\\[\n\\small \\text{Exchange Ratio}=\\dfrac{x}{N_T}&lt;\\underbrace{\\dfrac{P_T}{P_A}}_{\\text{Difference in Price}}\\times\\underbrace{\\bigg(1+\\dfrac{S}{T}\\bigg)}_{\\text{Synergy %}}\n\\]\n\nFor example, if the value of synergies equals 20% of the value of the target, you would be willing to pay an exchange ratio 20% higher than the current price ratio"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#exercise-example-1",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#exercise-example-1",
    "title": "Mergers and Acquisitions",
    "section": "Exercise Example #1",
    "text": "Exercise Example #1\nTaggart Transcontinental and Phoenix-Durango have entered into a stock swap merger agreement whereby Taggart will pay a 30% premium over Phoenix-Durango’s premerger price. If Taggart’s premerger price per share was $15 and Phoenix-Durango’s was $30. Determine the exchange ratio that Taggart will offer.\n\\(\\rightarrow\\) Solution: define the ratio as:\n\\[\n\\text{Exchange Ratio}=\\dfrac{\\text{Target Price}\\times \\text{(1+Premium)}}{\\text{Acquirer Price}}=\\dfrac{30\\times(1+30\\%)}{15}=2.6\n\\]\n\nIn words, this means that for using a stock swap merger agreement, Taggart needs to offer 2.6 shares for each share in Durango"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#exercise-example-2",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#exercise-example-2",
    "title": "Mergers and Acquisitions",
    "section": "Exercise Example #2",
    "text": "Exercise Example #2\nAt the time Sprint announced plans to acquire Nextel in December 2004, Sprint stock was trading for $25 per share and Nextel stock was trading for $30 per share. If the projected synergies were $12 billion, and Nextel had 1.033 billion shares outstanding, what is the maximum exchange ratio Sprint could offer in a stock swap and still generate a positive NPV? What is the maximum cash offer Sprint could make?\n\n\\[\n\\small \\text{Exchange Ratio}&lt;\\dfrac{30}{25}\\bigg(1+\\dfrac{12}{31}\\bigg)= 1.665\n\\]\n\nThat is, Sprint could offer up to 1.665 shares of Sprint stock for each share of Nextel stock and generate a positive NPV. That would yield target shareholders \\(1.665 \\times 25 = 41.62\\)\nAlternatively, one could pay the target price plus the synergies per share:\n\n\n\n\\[\n\\small 30+\\dfrac{12}{1,033}=30+11.62=41.62\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#board-and-shareholder-approval",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#board-and-shareholder-approval",
    "title": "Mergers and Acquisitions",
    "section": "Board and Shareholder Approval",
    "text": "Board and Shareholder Approval\n\nFor a merger to proceed, both the target and the acquiring board of directors must approve the deal and put the question to a vote of the shareholders of the target (and, in some cases, the shareholders of the acquiring firm as well)\n\nIn a friendly takeover, there is no dispute: the target board of directors supports the deal and agrees on a price for the stock\nIn a hostile takeover, there is dispute: the target board of directors fights the takeover attempt. In this situation, the acquirer firm needs to garner enough shares (&gt;50%) to take control and replace the board of directors\n\n\n\nQuestion: if the shareholders of a target company receive a premium over the current market value of their shares, why would a board of directors ever oppose a takeover?\n\n\nAnswer: both rational (e.g., offer price may be too low, swap exchange is not advantageous in current prices) and irrational (e.g. managers self-interests) may play a role in here."
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#more-on-hostile-takeovers",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#more-on-hostile-takeovers",
    "title": "Mergers and Acquisitions",
    "section": "More on Hostile Takeovers",
    "text": "More on Hostile Takeovers\n\nMergers and Acquisitions can also be thought of a market for corporate control helps discipline managers from publicly traded firms to keep pushing for adding value to the shareholders:\n\nWhen managers poorly perform on a consistent basis \\(\\rightarrow\\) investors adjust expectations down \\(\\rightarrow\\) stock prices go down\nWhen managers consistently outperform \\(\\rightarrow\\) investors adjust expectations up \\(\\rightarrow\\) stock prices increase\n\nIt is interesting to think about what happens in situation 1:\n\nIf stock prices go down, the enterprise value of the firm decreases\nThe lump-sum value required for acquiring the firm is now cheaper\nThe firm is now more prone to being a target in a takeover!\n\nIncumbent managers are put into the spotlight, as a new shareholder can vote to replace them. Because of such credible threat of termination, managers have ex-ante incentives to maximize shareholder value!"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#approval-and-takeover-defenses",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#approval-and-takeover-defenses",
    "title": "Mergers and Acquisitions",
    "section": "Approval and Takeover Defenses",
    "text": "Approval and Takeover Defenses\n\nFor a hostile takeover to succeed, the acquirer must go around the target board and appeal directly to the target shareholders - something called tender offer\nThe acquirer will usually couple this with a proxy fight: the acquirer attempts to convince target shareholders to unseat the target board by using their proxy votes to support the acquirers’ candidates for election to the target board\nThere are a couple ways a target firm can adopt to stop or prevent this dynamic - all in which somewhat increase the takeover price - see details on (Berk and DeMarzo 2023), Chapter 28:\n\nPoison Pills\nStaggered Boards\nWhite Knights\nGolden Parachutes\nRecapitalization"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#who-gets-value-from-a-takeover---the-acquirer-side",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#who-gets-value-from-a-takeover---the-acquirer-side",
    "title": "Mergers and Acquisitions",
    "section": "Who gets value from a takeover - the acquirer side",
    "text": "Who gets value from a takeover - the acquirer side\n\nFrom Slide 9, we saw that acquirer firms barely experience any positive reactions to takeovers. Instead, the premium the acquirer pays is approximately equal to the value it adds, which means the target shareholders ultimately capture the value added by the acquirer.\nTo see that, suppose that you are one of the 1MM shareholders of a company, with 1 share\nShares are trading at $45. An acquirer believes that the same firm, under his management, could be worth $75\n\nThe acquirer makes a tender offer for the majority of the shares (50%+1) for $\\60\nIf fewer than 50% of the shareholders tender their shares, the deal is off.\n\nProfit for the acquirer: \\(\\small 1MM \\times (75-60)=7.5MM\\)"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#who-gets-value-from-a-takeover---the-target-side",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#who-gets-value-from-a-takeover---the-target-side",
    "title": "Mergers and Acquisitions",
    "section": "Who gets value from a takeover - the target side",
    "text": "Who gets value from a takeover - the target side\n\nQuestion: If a tender offer has an acquisition premium, why wouldn’t all shareholders of the firm promptly accept it?\n\nAll in all the deal is profitable, as the acquisition price ($60), is higher than the current market value ($45)\nBut if all shareholders tender their shares, as an individual shareholder, you could do better by not tendering your share, since the market value of the acquired firm will be worth $75!\nIf all shareholders think like this, no one will tender their shares \\(\\rightarrow\\) the deal is off and all the target shareholders are worse-off!\n\n\n\n\\(\\rightarrow\\) The only way to persuade shareholders it to increase the offer price up to $75. However, this would erode all economic profits from the acquirer, and he wouldn’t bother at all to invest time and effort in the acquisition\n\nThis is also referred to as the free-rider problem in M&A. How to circumvent that?"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#circumventing-the-free-rider-problem",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#circumventing-the-free-rider-problem",
    "title": "Mergers and Acquisitions",
    "section": "Circumventing the free-rider problem",
    "text": "Circumventing the free-rider problem\n\nBecause of the free-rider problem, shareholders would need to receive $75, which wouldn’t pay off for the acquirer, as the NPV of the deal would be zero\nTherefore, we would need to think of strategies that make the acquirer earn positive NPV in the transaction1!\n\n\nToeholds: one way for the acquirer to increase its profits is buying shares anonymously in the market. Regulations generally prevent acquirers from buying an unlimited amount of shares without conveying any information (e.g, disclose any participation \\(\\small \\geq 10\\%\\)), but we could:\n\nBuy 10% of the shares anonymously for $50\nDo a tender offer for the remaining 40% of the shares considering $75\n\nWith that, the acquirer profits would be \\(\\small (75-50)\\times 100,000=2,5MM\\). Why should target shareholders care? Because this creates a threat for the incumbent managers!\n\nSee (Berk and DeMarzo 2023) for the examples of Leveraged Buyouts and Freezeout mergers on page 1025."
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#regulatory-issues",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#regulatory-issues",
    "title": "Mergers and Acquisitions",
    "section": "Regulatory Issues",
    "text": "Regulatory Issues\n\nMergers need to be approved by regulators, although the extent to which these regulators block M&A activity varies from country to country\nIn Brazil, the regulatory agency in charge of deciding on M&A activity is the Conselho Administrativo de Defesa Econômica (CADE)1, which is responsible for oversighting M&A activity and prevent monopolist practices that may harm ultimate customers through:\n\nPreemptive: approve/deny M&A activity based on its expected effect to consumer welfare\nRepressive: judge matters related to monopolistic practices, such as overpricing, cartels, etc\nEducative: incentive studies aiming to better understand relevant competition issues\n\n\n\nImportant: not all M&A activity is harmful for the customers\n\n\n\\(\\rightarrow\\) See, for example, the Perdigão-Sadia (BRFoods) case\n\nMore information on how CADE operates: click here."
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#new-evidence-on-common-ownership-and-concentration",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#new-evidence-on-common-ownership-and-concentration",
    "title": "Mergers and Acquisitions",
    "section": "New evidence on common ownership and concentration",
    "text": "New evidence on common ownership and concentration\n\nWhy a regulatory agency cares so much about competition? Because it can affect customer’s welfare!\n\nFirms can use their monopolistic power to increase prices\nThey can also influence other market outcomes in such a way to maximize their profits\n\nLong story, short: from your Economics 101 class, ceteris paribus, fostering competition increases consumer welfare\n\n\nQuestion: if that is the case, and assuming that there are no gains from synergies that can be distributed over to customers, is having market-shares fairly distributed across competitors sufficient to ensure that customers are better-off?\n\\(\\rightarrow\\) In the next slides, you can see the distribution of ownership across big entrepreneurial firms in the U.S. (Schmalz2018?). Can you spot something odd?"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#new-evidence-on-common-ownership-and-concentration-continued",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#new-evidence-on-common-ownership-and-concentration-continued",
    "title": "Mergers and Acquisitions",
    "section": "New evidence on common ownership and concentration (continued)",
    "text": "New evidence on common ownership and concentration (continued)"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#new-evidence-on-common-ownership-and-concentration-continued-1",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#new-evidence-on-common-ownership-and-concentration-continued-1",
    "title": "Mergers and Acquisitions",
    "section": "New evidence on common ownership and concentration (continued)",
    "text": "New evidence on common ownership and concentration (continued)"
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#supplementary-reading",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#supplementary-reading",
    "title": "Mergers and Acquisitions",
    "section": "Supplementary Reading",
    "text": "Supplementary Reading\n\nSee Finance Reading: The Mergers and Acquisitions Process for a thorough discussion around the M&A process\nSee Evaluating M&A Deals: Introduction to the Deal NPV for a detailed discussion on how to use the valuation methods from previous lectures to assess the value of an M&A deal\n\n\n\\(\\rightarrow\\) All contents are available on eClass®."
  },
  {
    "objectID": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#references",
    "href": "fin-mgmt/Lecture 10 - Mergers and Acquisitions/index.html#references",
    "title": "Mergers and Acquisitions",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ."
  },
  {
    "objectID": "fin-mgmt/Course Overview/index.html#welcome-to-the-course",
    "href": "fin-mgmt/Course Overview/index.html#welcome-to-the-course",
    "title": "Course Overview - Financial Management",
    "section": "Welcome to the Course",
    "text": "Welcome to the Course\n\n\nOverview and Course Organization\nGrading and Evaluations\nNavigating through the syllabus\nHow you can get the best of this course\nOverall Q&A\nIntroduction to Corporate Finance"
  },
  {
    "objectID": "fin-mgmt/Course Overview/index.html#overview-and-course-organization",
    "href": "fin-mgmt/Course Overview/index.html#overview-and-course-organization",
    "title": "Course Overview - Financial Management",
    "section": "Overview and course organization",
    "text": "Overview and course organization\n\n\nBasic text-book\n\nWe will extensively follow Corporate Finance (Berk and DeMarzo 2023), as our text-book\nThroughout the syllabus, it will be referred as [BDM]\n\nOther mandatory reading\n\n[BMA] Principles of Corporate Finance (Brealey, Myers, and Allen 2020)\n[PH] Business Analysis and Valuation (Healy and Palepu 2012)\n[SS] Principles of Sustainable Finance (Schoenmaker and Schramade 2019)\n\nSupplementary Reading\n\nHarvard Business Review (selected cases) - access granted on e-Class®\nOther focused contents previously defined on the syllabus"
  },
  {
    "objectID": "fin-mgmt/Course Overview/index.html#grading-and-evaluations",
    "href": "fin-mgmt/Course Overview/index.html#grading-and-evaluations",
    "title": "Course Overview - Financial Management",
    "section": "Grading and Evaluations",
    "text": "Grading and Evaluations\n\nGrading will be composed of the following activities:\n\nMid-term Examination (35%)\nFinal Examination (40%)\n4x Quizzes throughout the semester (15%)\nPractical work (10%)\n\n\n\n\nYou can find the details of any of these activities in the official syllabus (available on eClass)\nIn case of any questions, feel free to reach out to lucas.macoris@fgv.br\n\n\n\n\n\n\n\n\n\nOffice-hours\n\n\nI also host office-hours (by appointment) on Thursdays, 5PM-6PM. In these sessions, I’ll be more than happy to help you with anything you need from this course. Use the Office-hour Appointments link at the bottom of this slide to schedule some time (or click here)."
  },
  {
    "objectID": "fin-mgmt/Course Overview/index.html#getting-the-best-of-this-course",
    "href": "fin-mgmt/Course Overview/index.html#getting-the-best-of-this-course",
    "title": "Course Overview - Financial Management",
    "section": "Getting the best of this course",
    "text": "Getting the best of this course"
  },
  {
    "objectID": "fin-mgmt/Course Overview/index.html#getting-the-best-of-this-course-1",
    "href": "fin-mgmt/Course Overview/index.html#getting-the-best-of-this-course-1",
    "title": "Course Overview - Financial Management",
    "section": "Getting the best of this course",
    "text": "Getting the best of this course\n\nHow you can get the best of this course\n\nFollow FGV-EAESP code of conduct\nBe organized: pay attention to pre-readings and deliverables!\nBe proactive: ask questions and participate in discussions, proactively study the list of exercises\nTake the lead on your learning: you are the ultimate responsible for your success!\n\n\n\nHow the professor can facilitate you getting the best of this course\n\nAll mandatory content will be provided in English, classes will be held in Portuguese\nProvide a safe and open space for questions, both in-person and remote\nMotivate students, both from the academic and practitioner standpoints\nProvide opportunities to extend knowledge beyond the mandatory readings"
  },
  {
    "objectID": "fin-mgmt/Course Overview/index.html#references",
    "href": "fin-mgmt/Course Overview/index.html#references",
    "title": "Course Overview - Financial Management",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nBrealey, R. A., S. C. Myers, and F. Allen. 2020. Principles of Corporate Finance. The Irwin/McGraw-Hill Series in Finance, Insurance, and Real Estate. McGraw-Hill Education. https://books.google.com.br/books?id=nsrHuwEACAAJ.\n\n\nHealy, P. M., and K. G. Palepu. 2012. Business Analysis Valuation: Using Financial Statements. Cengage Learning. https://books.google.com.br/books?id=Qp0IzgEACAAJ.\n\n\nSchoenmaker, D., and W. Schramade. 2019. Principles of Sustainable Finance. Oxford University Press. https://books.google.com.br/books?id=zoN8DwAAQBAJ."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Lecture Notes and Handouts",
    "section": "",
    "text": "{css, echo = FALSE} .justify {   text-align: justify !important }\n\nWithin this section, you’ll find an organized repository containing my lecture notes from FGV-EAESP, technical handouts, and illustrative code samples. Most, if not all, of these resources extensively leverage Quarto, an open-source scientific and technical publishing system that supports R, Python, Julia, and Observable. Quarto offers a seamless and scalable solution for sharing technical content, encompassing a blend of plain text and executable code, being applicable to static document formats like .pdf files and dynamic, interactive presentations facilitated by Reveal JS.\nFor comprehensive guidance on harnessing Quarto for presentations and technical content in a standardized and comprehensive manner, please refer to the official reference material.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinancial Management\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinancial Strategy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#introduction-to-corporate-finance",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#introduction-to-corporate-finance",
    "title": "Introduction to Corporate Finance",
    "section": "Introduction to Corporate Finance",
    "text": "Introduction to Corporate Finance\nWhat is a Corporation, anyway? Think about any business opportunity, like opening a small pizza vendor facility…\n\nOn the one hand, there is an endless variety of real assets needed to carry on such business\nOn the other hand, these assets simply do not come for free – there has to be some way to acquire them!\n\n\nBroadly speaking, these are:\n\nInvestment Decisions \\(\\rightarrow\\) acquisition of real assets - e.g, oven, machinery, powerplant\nFinancing Decisions: \\(\\rightarrow\\) sale of financial assets - bank financing, equity\n\nNot surprisingly, these decisions are intertwinned:\n\nWhile Investment decisions define what is the set of assets to be organized…\nFinancing decisions define how these assets will be acquired!"
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#examples-of-investment-and-financing-decisions",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#examples-of-investment-and-financing-decisions",
    "title": "Introduction to Corporate Finance",
    "section": "Examples of Investment and Financing Decisions",
    "text": "Examples of Investment and Financing Decisions"
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#investment-decisions",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#investment-decisions",
    "title": "Introduction to Corporate Finance",
    "section": "Investment Decisions",
    "text": "Investment Decisions\n\nInvestment decisions are generally thought of as a capital investment made today to generate returns in the future\nThe extent to when these returns are expected to happen depends on the specific investment\nExamples of Investment decisions:\n\n\nResearch and Development (R&D) on creating/optimizing new products/services\nMachinery, Property, Pland, and Equipment\nOpening new stores\nAcquiring an operation from a company\nExpanding trade credit to leverage sales\nStock up warehouses with inventory during high seasonality"
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#financing-decisions",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#financing-decisions",
    "title": "Introduction to Corporate Finance",
    "section": "Financing Decisions",
    "text": "Financing Decisions\n\nAfter you decide on what to invest: how are you going to fund it?\nOverall, a firm can raise money by two ways:\n\nA firm can borrow money from a lender \\(\\rightarrow\\) firm receives the money, but now has a promise to pay back the debt plus interest\nAlternatively, the firm can raise money from the shareholders \\(\\rightarrow\\) equity financing\n\nThe choice between the amount of equity vs is often referred to Capital Structure decisions\nWithin each type of financing, there are specific decisions that increase the complexity:\n\n\nWould the firm be better-off by having a 1-year or 20-year financing? Should it include collateral or not? Fixed or variable interest rate?\nFor equity financing, should the firm issue new equity or reinvest profits into its operations?\n\n\nQuestion: between investment and financing, which one is the most important?"
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#managing-investment-and-financing-decisions",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#managing-investment-and-financing-decisions",
    "title": "Introduction to Corporate Finance",
    "section": "Managing investment and financing decisions",
    "text": "Managing investment and financing decisions\n\nWe saw that any business opportunity must be accompained by an investment and a financing decision\nManaging these decisions is a hard task, especially if you have to conduct the front-end of the business altogether\nTo this point, there has to be someone to organize these flows on behalf of the firm…\n\n\n\n\n\nInvestment and Financing Decisions (Brealey, Myers, and Allen 2020)"
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#the-role-of-the-financial-manager",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#the-role-of-the-financial-manager",
    "title": "Introduction to Corporate Finance",
    "section": "The role of the Financial Manager",
    "text": "The role of the Financial Manager\n\n\n\n\nInvestment and Financing Decisions (Brealey, Myers, and Allen 2020)\n\n\n\nA Financial Manager1 stands between the firm and outside investors:\n\nOn the one hand, it helps managing the firm’s operations, particularly by helping to make good investment decisions\nOn the other hand, it deals with investors such as shareholders and financial institutions\n\n\n\nFollowing (Brealey, Myers, and Allen 2020), we use this term to refer to anyone (or a group of people) responsible for an investment or financing decision."
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#the-investment-trade-off",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#the-investment-trade-off",
    "title": "Introduction to Corporate Finance",
    "section": "The Investment Trade-off",
    "text": "The Investment Trade-off\n\n\n\n\nInvestment and Financing Decisions (Brealey, Myers, and Allen 2020)\n\n\n\nSuppose that firm has a proposed investment project (a real asset) and has cash on hand to finance the project\nThe Financial Manager has to decide whether to invest in the project:\n\nIf investing, cash goes to fund the project\nIf not investing, the firm can then pay out the cash to shareholders as an extra-dividend"
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#the-investment-trade-off-continued",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#the-investment-trade-off-continued",
    "title": "Introduction to Corporate Finance",
    "section": "The Investment Trade-off, continued",
    "text": "The Investment Trade-off, continued\n\nAssume that this financial manager acts on behalf of the shareholder’s interests. What do they want the financial manager to do?\nThe answer depends on the project’s rate of return:\n\nIf the return offered by project is higher than what the shareholders can get elsewhere investing on their own \\(\\rightarrow\\) shareholders would be better off with the project\nIf it offers a return rate that is lower than what the shareholders can achieve on their own \\(\\rightarrow\\) shareholders would be better off by having the cash on their hands\n\nThis decision is tied to an important concept called the opportunity cost of capital:\n\nWhenever a corporation invests in a project, its shareholders lose the opportunity to invest the cash on their own\nCorporations increase value by accepting all investment projects that earn more than the opportunity cost of capital."
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#ibovespa-and-interest-rate-dynamics",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#ibovespa-and-interest-rate-dynamics",
    "title": "Introduction to Corporate Finance",
    "section": "Ibovespa and interest rate dynamics",
    "text": "Ibovespa and interest rate dynamics"
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#how-to-take-actions-on-behalf-of-the-shareholders",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#how-to-take-actions-on-behalf-of-the-shareholders",
    "title": "Introduction to Corporate Finance",
    "section": "How to take actions on behalf of the shareholders",
    "text": "How to take actions on behalf of the shareholders\n\nShareholders differ in several dimensions:\n\nAge\nTastes\nConsumption Patterns\nWealth\nRisk Aversion\nInvestment Horizon\n\n\n\nQuestion: which criteria should the Financial Manager use to take investment and financing decisions?\n\nFortunately, there is a widely accepted criteria: maximize the current market value of shareholders’ investment!"
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#shareholder-value-maximization",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#shareholder-value-maximization",
    "title": "Introduction to Corporate Finance",
    "section": "Shareholder Value Maximization",
    "text": "Shareholder Value Maximization\n\nMaximizing shareholder wealth is a sensible goal when the they have access to well-functioning financial markets:\n\nFinancial markets allow them to share risks and transport savings across time\nIt also gives them the flexibility to manage their own savings and investment plans\n\nWhy it works like this? Assume that the Financial Manager acts on behalf of the stockholders of the firm. A plausible assumption is that:\n\n\nShareholders want to be as rich as possible…\nAnd transform such wealth into his/her specific consumption pattern via borrowing/lending\nFinally, shareholders need to manage the risk of his/her chosen consumption pattern\n\n\nNote that points 2 and 3 can be done by the shareholder\nHow then can the financial manager help the firm’s stockholders? Increasing their wealth!"
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#the-separation-of-ownership-and-control",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#the-separation-of-ownership-and-control",
    "title": "Introduction to Corporate Finance",
    "section": "The separation of ownership and control",
    "text": "The separation of ownership and control\n\nWhen we introduced the figure of the Financial Manager and the existence of financial markets, we implicitly discussed the separation of ownership and control\nIn other words, the shareholders of the firm cannot fully control what the managers do\nIs this a problem? Up to now, we are assuming that the Financial Manager should look after the interests of the shareholders\nAlthough this separation is necessary, there are reasons to think that managers could pursue their own objectives:\n\n\nMaximize their bonuses\nPass attractive, but risky projects to increase job safety\nOverinvest to show hard work\nTake on too much risk as the downside will be ultimately borne out by the shareholders"
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#the-agency-problem",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#the-agency-problem",
    "title": "Introduction to Corporate Finance",
    "section": "The Agency Problem",
    "text": "The Agency Problem\n\n\n\n\n\n\n\nDefinition\n\n\nThe Agency Problem is a coordination issue present in corporate decisions. Between the manager and the shareholders of the firm:\n\nThe manager (or the agent) does not have incentives to maximize shareholder value; and\nShareholders (the principals) need to incur in monitoring costs to constrain value-destroying decisions\n\n\nThat sets up the stage for Corporate Governance: the design of actions that aims to align the interests of the different stakeholders of the firm and reduce Agency Costs\n\n\n\n\n\n\n\nExamples of internally enforced Corporate Governance mechanisms:\n\n\n\n\n\nThe creation of a Board of Directors\nCompensation plans based on metrics that are tied to maximizing shareholder value (stock options, restricted stock units, variable compensation, etc)\nVoluntary adoption of Corporate Governance principles, such as ESG practices"
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#the-agency-problem-continued",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#the-agency-problem-continued",
    "title": "Introduction to Corporate Finance",
    "section": "The Agency Problem, continued",
    "text": "The Agency Problem, continued\n\n\n\n\n\n\n\nDefinition\n\n\nThe Agency Problem is a coordination issue present in corporate decisions. Between the manager and the shareholders of the firm:\n\nThe manager (or the agent) does not have incentives to maximize shareholder value; and\nShareholders (the principals) need to incur in monitoring costs to constrain value-destroying decisions\n\n\nThat sets up the stage for Corporate Governance: the design of actions that aims to align the interests of the different stakeholders of the firm and reduce Agency Costs\n\n\n\n\n\n\n\nExamples of externally enforced Corporate Governance mechanisms:\n\n\n\n\n\nLegal and regulatory requirements - SEC (U.S), Comissão de Valores Mobiliários (CVM) (Brazil)\nHostile Takeovers and Market Monitoring\nShareholder Pressure, also called the “Wall Street walk”"
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#supplementary-reading",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#supplementary-reading",
    "title": "Introduction to Corporate Finance",
    "section": "Supplementary Reading",
    "text": "Supplementary Reading\n\nAfter-class reading (available on e-Class):\n\nCarbon Credit Markets\nWhat Every Leader Needs to Know About Carbon Credits\n\n\n\nMiscellaneous (social media, video content, etc):\n\nBarbarians at the Gate: movie from 1993 describing some corporate governance conflicts in the 80’s - link\nSucession: series reflecting a troublesome family business intertwined with a series of corporate governance scandals - link"
  },
  {
    "objectID": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#references",
    "href": "fin-mgmt/Lecture 1 - Introduction to Corporate Finance/index.html#references",
    "title": "Introduction to Corporate Finance",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBrealey, R. A., S. C. Myers, and F. Allen. 2020. Principles of Corporate Finance. The Irwin/McGraw-Hill Series in Finance, Insurance, and Real Estate. McGraw-Hill Education. https://books.google.com.br/books?id=nsrHuwEACAAJ."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#outline",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#outline",
    "title": "Financial Analysis",
    "section": "Outline",
    "text": "Outline\n\n\nThis lecture will extensively use the following text-books:\n\n\n(Healy and Palepu 2012): Business Analysis Valuation: Using Financial Statements\n(Berk and DeMarzo 2023): Corporate Finance\n\n\n(Healy and Palepu 2012) provides a more comprehensive discussion around the use of financial indications, although the content pretty much resembles (Berk and DeMarzo 2023)\nIt is important to be familiarized with financial reporting concepts, such as understanding Balance-Sheet, Income Statements, and Cash-Flow Statements. For a summarized discussion on these reports, refer to (Berk and DeMarzo 2023), Chapter 2.\nIn the Appendix, you’ll find additional content (non-mandatory, but recommended) that can be used to get a deeper understanding and/or comprehend applications of financial indicators"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#financial-analysis",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#financial-analysis",
    "title": "Financial Analysis",
    "section": "Financial Analysis",
    "text": "Financial Analysis\n\nWe saw in our previous lecture that:\n\nFinance Managers have to deal with investment and financing decisions\nThe extent of their actions in these two areas can either create or destroy value\n\nHow to analyze the effect of their actions on the firm value?\nTo this point, the goal of financial analysis is to assess the performance of a firm:\n\nIs the firm getting too risky?\nIs the profitability aligned with its industry peers?\nIs the firm growing its business?\nAre profits trending upwards or downwards?\n\nProblem: not all information about the firm’s actual and past performance is disclosed"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#analyzing-financial-statements",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#analyzing-financial-statements",
    "title": "Financial Analysis",
    "section": "Analyzing Financial Statements",
    "text": "Analyzing Financial Statements\n\nInvestors often use accounting statements to evaluate a firm in one of two ways:\n\nCompare the firm with itself by analyzing how the firm has changed over time\nCompare the firm to other similar firms using a common set of financial ratios\n\nDespite being a high-level view of complexity that is the firm’s business, such information can shed light on some areas that dictate the firm’s performance\nAccounting statements are the inputs of a series of analysis that can be done to assess financial performance\nTo that point, we can mix and match information from accounting statements in order to create meaningful financial indicators that guide our analysis"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#what-drives-firm-performance",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#what-drives-firm-performance",
    "title": "Financial Analysis",
    "section": "What drives firm performance?",
    "text": "What drives firm performance?"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#introducing-financial-analysis",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#introducing-financial-analysis",
    "title": "Financial Analysis",
    "section": "Introducing Financial Analysis",
    "text": "Introducing Financial Analysis\n\nObjective: evaluate the effectiveness of the firm’s policies in each of the previously mentioned areas\nHigh-level view: while ratio analysis may not give an analyst all the answers regarding the firm’s performance, it will help the analyst frame questions for further probing.\nOn the investment side, we have:\n\nManaging Revenue and Expenses\nManaging Working Capital and Fixed Assets\n\nOn the financing side, we have:\n\nManaging Liabilities and Equity\nManaging Payout\n\nFor each of these areas, we can use financial statements and create indicators that shed light on important factors that drive firm’s performance!"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#liquidity-and-financial-strength",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#liquidity-and-financial-strength",
    "title": "Financial Analysis",
    "section": "Liquidity and Financial Strength",
    "text": "Liquidity and Financial Strength\n\nOur starting point will be looking at the capacity that a firm has to honor its liabilities\nFinancial analysts often use the information in the firm’s balance sheet to assess its financial solvency and liquidity\nBefore we dive into the calculations, let’s look at two important definitions:\n\n\n\n\n\n\n\n\nSolvency - Definition\n\n\nThe ability of a company to meet its long-term debts and financial obligations. Solvency portrays the ability of a business (or individual) to pay off its financial obligations.\n\n\n\n\n\n\n\n\n\nLiquidity - Definition\n\n\nLiquidity refers to the ease with which an asset, or security, can be converted into cash without affecting its market price. In other words, liquidity describes the degree to which an asset can be quickly bought or sold in the market at a price reflecting its true value."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#solvency-measures",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#solvency-measures",
    "title": "Financial Analysis",
    "section": "Solvency Measures",
    "text": "Solvency Measures\n\n\n\n\n\n\n\nThe amount of Debt vs. Equity can shed light on the solvency of a firm\nThe higher the % of Equity, all things equal, the higher its solvency\nWhat happens if we change the % of Equity and Debt?"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#solvency-measures-1",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#solvency-measures-1",
    "title": "Financial Analysis",
    "section": "Solvency Measures",
    "text": "Solvency Measures\n\n\n\n\n\n\n\nFrom Case 1 \\(\\rightarrow\\) Case 2: firm increased its leverage\nCase 2 may face higher difficulty in paying back its debt \\(\\rightarrow\\) increased risk of insolvency"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#solvency-ratios",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#solvency-ratios",
    "title": "Financial Analysis",
    "section": "Solvency Ratios",
    "text": "Solvency Ratios\n\nThe easiest solvency indicator to begin with compares the amount of equity relative to all liabilities a firm has at a given point in time\nThe Liabilities-to-Equity indicator measures the overall solvency of a given firm, and is calculated as:\n\n\n\\[\n\\text{Liabilities to Equity}=\\dfrac{\\text{Total Liabilities}}{\\text{Equity}}\n\\]\n\nInterpretation: for each \\(\\small \\$1\\) of equity, how much dollars (or cents) a firm has in liabilities?\n\n\nDespite being a helpful guide on analyzing how the firm’s leverage decisions are reflected in the balance sheet, this indicator may not reflect the\nFor example, a lot of liabilities might just be related to accounts that are part of the business cycle: wages payable, taxes payable, accounts payable (suppliers), etc"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#solvency-ratios-continued",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#solvency-ratios-continued",
    "title": "Financial Analysis",
    "section": "Solvency Ratios, continued",
    "text": "Solvency Ratios, continued\n\nThe Debt-to-Equity focuses the on the Debt component, which is generally more descriptive of the firm’s financing decisions as it only includes interest-bearing liabilities:\n\n\n\\[\n\\text{Debt to Equity}=\\dfrac{\\text{Short-Term Debt + Long-Term Debt}}{\\text{Equity}}\n\\]\n\nInterpretation: for each \\(\\small \\$1\\) of equity, how much dollars (or cents) a firm has in interest-bearing liabilities?\n\n\nThis indicator is very useful in understanding the mix of debt and equity a firm has ultimately decided to adopt so as to fund its projects\nImportant: whenever assessing which accounts should be classified as debt, one might need to adjust the calculations to include other items that are also interest bearing, such as leases1\n\n\nSee IFRS 16 note on leases here"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#interpreting-solvency-ratios",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#interpreting-solvency-ratios",
    "title": "Financial Analysis",
    "section": "Interpreting Solvency Ratios",
    "text": "Interpreting Solvency Ratios\n\nIs less Debt always a good idea? No! There are several potential benefits of having debt:\n\n\nLess costly: debt is typically cheaper than equity because the firm promises predefined payment terms to debt holders\nTax-shield: in most countries, interest on debt financing is tax deductible \\(\\rightarrow\\) lower taxes\nDebt holders as monitors debt can impose discipline on the firm’s management and motivate it to reduce wasteful expenditures\n\n\nAll-in-all, the optimal level of debt needs to trade-off:\n\n\nThe benefits of having debt in the financing mix\nThe inherent risks related to having a high amount of debt and not being able to honor it\n\n\nFirms with low business risk can rely more on debt than more volatile firms"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#liquidity-measures",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#liquidity-measures",
    "title": "Financial Analysis",
    "section": "Liquidity Measures",
    "text": "Liquidity Measures\n\n\n\n\n\n\n\nRegardless of having enough resources to pay off its obligations, can the firm pay on time?\nLiquidity evaluates the risk related to a firm’s current liabilities\nIn general, ratios attempt to measure the firm’s ability to repay its current liabilities using its the current assets\n\n\n\nAll liquidity indicators seek, to some extent, answering the following question: if a firm had to liquidate some of its assets to honor its current liabilities, would the firm be able to do so?\nThese indicators help to understand, for example, why some firms need to hold substantial amounts of liquid assets, such as cash, instead of investing it in operating assets"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#liquidity-measures-1",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#liquidity-measures-1",
    "title": "Financial Analysis",
    "section": "Liquidity Measures",
    "text": "Liquidity Measures\n\n\n\n\n\n\n\nFrom Case 1 \\(\\rightarrow\\) Case 2: firm decreased its liquidity\nCase 2, even if it is a profitable business, can run out of cash due to a liquidity mismatch!"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#liquidity-ratios",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#liquidity-ratios",
    "title": "Financial Analysis",
    "section": "Liquidity Ratios",
    "text": "Liquidity Ratios\n\nCurrent Ratio: measures overall liquidity\n\n\n\\[\n\\text{Current Ratio}=\\dfrac{\\text{Current Assets}}{\\text{Current Liabilities}}\n\\]\n\nQuick Ratio: focus on the more liquid components of current assets\n\n\n\n\\[\n\\text{Quick Ratio}=\\dfrac{\\text{Cash + Short-Term Investments + Accounts Receivable}}{\\text{Current Liabilities}}\n\\]\n\nCash Ratio: focus only on the portion that is straighforward to convert into cash:\n\n\n\n\\[\n\\text{Cash Ratio}=\\dfrac{\\text{Cash + Marketable Securities}}{\\text{Current Liabilities}}\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#interpreting-liquidity-ratios",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#interpreting-liquidity-ratios",
    "title": "Financial Analysis",
    "section": "Interpreting Liquidity Ratios",
    "text": "Interpreting Liquidity Ratios\n\nIf a Liquidity Index is higher than 1 \\(\\rightarrow\\) the firm has, to some extent, enough resources to pay its current obligations\nIf a Liquidity Index lower than 1 \\(\\rightarrow\\) the firm may not have enough resources when its current debt is finally due\nNote that a the firm can face a short-term liquidity problem even with a Current Ratio exceeding one when some of its current assets are not easy to liquidate\nFor these reasons, the Quick Ratio and the Cash Ratio are more conservative:\n\n\n\\[\n\\underbrace{\\text{Cash Ratio} &gt;&gt; \\text{Quick Ratio} &gt;&gt; \\text{Current Ratio}  }_{\\text{From highest-to-lowest in the degree of conservativeness}}\n\\]\n\nThe higher the liquidity index, the better? Not necessarily!"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check",
    "title": "Financial Analysis",
    "section": "Concept Check",
    "text": "Concept Check\n\n\nAssume the following data: Current Assets = \\(\\small\\$450\\); Current Liabilities = \\(\\small\\$250\\); Inventory = \\(\\small\\$200\\); Cash = \\(\\small\\$50\\); Account Receivables = \\(\\small\\$200\\). Calculate and interpret the Quick Ratio.\n\n1.0\n2.0\n1.2\n0.4\n\n\n\n\nA: using the formula, we have:\n\n\n\\[\n\\text{Quick Ratio}=\\dfrac{\\text{Cash and ST Investments + Account Receivables}}{\\text{Current Liabilities}}=\\dfrac{50+200}{250}=1\n\\]\n\n\n\nA Quick Ratio of \\(\\small\\1\\) means that for each dollar due in the short-term, the firm has the same dollar value in assets."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-1",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-1",
    "title": "Financial Analysis",
    "section": "Concept Check",
    "text": "Concept Check\n\nUsing the same information as before, calculate the Current Ratio.\n\n\nA: using the formula, we have:\n\\[\n\\text{Current Ratio}=\\dfrac{\\text{Current Assets}}{\\text{Current Liabilities}}\\rightarrow\\dfrac{450}{250}=1.8\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-2",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-2",
    "title": "Financial Analysis",
    "section": "Concept Check",
    "text": "Concept Check\n\nWhat can you say about the firm’s liquidity in this case as the Current Ratio is \\(\\small&gt;1\\) and the Quick Ratio is equal to \\(\\small1\\)? Can we asset that the firm has a zero risk of incurring in liquidity issues?\n\n\nA: we can’t assert that the firm has not liquidity risk\n\nFirst, the Current Ratio does not take into consideration the fact that Inventories are, in general, less liquid, and by definition, cannot be guaranteed to be liquidated at book value.\nIn addition to that, even if the Quick Ratio is equal to one, one might think about situations where a firm cannot recover the totality of its account receivables, which could also hinder the firms’ ability to honor its current obligations.\n\n\nOverall, these two points are examples of (potential) accounting distortions that affect the interpretation of financial ratios."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-3",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-3",
    "title": "Financial Analysis",
    "section": "Concept Check",
    "text": "Concept Check\n\n\nConsidered alone, which of the following would increase a company’s current ratio?\n\nAn increase in accounts payable.\nAn increase in net fixed assets.\nAn increase in accrued liabilities.\nAn increase in notes payable.\nAn increase in accounts receivable."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-4",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-4",
    "title": "Financial Analysis",
    "section": "Concept Check",
    "text": "Concept Check\n\n\nConsidered alone, which of the following would increase a company’s Current Ratio?\n\nAn increase in accounts payable.\nAn increase in net fixed assets.\nAn increase in accrued liabilities.\nAn increase in notes payable.\nAn increase in accounts receivable.\n\n\nA: any positive changes on a company’s Current should come from a increase in Current Assets or a decrease in Current Liabilities. The only alternative where we see such pattern is when there is an increase in Account Receivable."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#calculate-and-interpret-all-ratios-using-the-data-below",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#calculate-and-interpret-all-ratios-using-the-data-below",
    "title": "Financial Analysis",
    "section": "Calculate and interpret all ratios using the data below",
    "text": "Calculate and interpret all ratios using the data below"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#solvency-ratios-1",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#solvency-ratios-1",
    "title": "Financial Analysis",
    "section": "Solvency Ratios",
    "text": "Solvency Ratios\n\nLiabilities-to-Equity\n\n\n\\[\n\\dfrac{(\\text{S.T Liabilities + L.T Liabilities})}{Equity} =\\dfrac{(24,500+52,500)}{150,000}=\\dfrac{77,000}{150,000}\\approx 0.51\n\\]\n\nDebt-to-Equity\n\n\n\n\\[\n\\dfrac{(\\text{S.T Debt + L.T Debt})}{Equity} =\\dfrac{(10,00+50,000)}{150,000}=\\dfrac{60,000}{150,000}= 0.40\n\\]\n\nInterpretation: for each $0.4 in debt, the firm has $1 or 2.5 times the amount, to cover"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#liquidity-ratios-1",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#liquidity-ratios-1",
    "title": "Financial Analysis",
    "section": "Liquidity Ratios",
    "text": "Liquidity Ratios\n\nCalculating the Liquidity Ratios:\n\n\n\\[\n\\text{Current Ratio}\\small=\\dfrac{\\text{Current Assets}}{\\text{Current Liabilities}}=\\dfrac{77,000}{24,500}\\approx3.14\n\\]\n\\[\n\\text{Quick Ratio}=\\small \\dfrac{\\text{Cash+S.T Inv + Acc. Rec.}}{\\text{Current Liabilities}}=\\dfrac{15,000+4,500+7,500}{24,500}=\\dfrac{27,000}{24,500}\\approx1.1\n\\]\n\\[\n\\text{Cash Ratio}=\\small \\dfrac{\\text{Cash + S.T Financials}}{\\text{Current Liabilities}}=\\dfrac{15,000+4,500}{24,500}=\\dfrac{19,500}{24,500}\\approx0.8\n\\]\n\nInterpretation: firm is heavily dependent upon the firm’s capability to quickly liquidate its inventory at the book price!"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#integrating-different-financial-statements",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#integrating-different-financial-statements",
    "title": "Financial Analysis",
    "section": "Integrating different Financial Statements",
    "text": "Integrating different Financial Statements\n\nWe’ll now move towards profitability ratios\nIn order to understand profitability, we need to understand the relationship between the firms’ assets and its income generation:"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#one-indicator-to-rule-them-all",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#one-indicator-to-rule-them-all",
    "title": "Financial Analysis",
    "section": "One indicator to rule them all",
    "text": "One indicator to rule them all\n\nAt the end of the day, shareholders will ask: for each dollar invested, how much does the firm return back to its shareholders? The Return on Equity (ROE) measures the percent return to the shareholders of the firm:\n\n\n\\[\nROE=\\dfrac{\\text{Net Income}}{\\text{Equity}}\n\\]\n\nROE is a comprehensive indicator of a firm’s performance as it gauges how well managers are employing the funds invested by the firm’s shareholders to generate returns\nUsing the numbers from our example, we have:\n\n\n\n\\[\nROE=\\dfrac{\\text{Net Income}}{\\text{Equity}}=\\dfrac{150,000}{500,000}=30\\%\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#analyzing-roe",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#analyzing-roe",
    "title": "Financial Analysis",
    "section": "Analyzing ROE",
    "text": "Analyzing ROE\n\nIn the long-run, firms are expected to earn a ROE that is higher than its cost of capital:\n\nConsistently generating ROE above average, absent any barriers to entry, makes entry attractive for competitors\n\\(\\uparrow\\) Competition \\(\\rightarrow\\) ROE tends to go down to a “normal”, benchmark level1\n\nShort-term deviations from such “benchmark” level of ROE can occur due to industry conditions and competitive strategy, accounting distortions, among other factors\nDrawbacks:\n\nROE doesn’t explain the whole story: it shows how well managers are employing shareholders’ capital, but it cannot explain what were the drivers of such performance\nAccounting distortions: ROE comparisons can be misleading if, for some reason, different firms are exposed to different accounting treatments\n\n\nROE levels can vary significantly across industries - click here for an industry summary compiled in Aswath Damodaran’s website."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#decomposing-roe",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#decomposing-roe",
    "title": "Financial Analysis",
    "section": "Decomposing ROE",
    "text": "Decomposing ROE\n\nSay that you want to understand how the firm has generated the \\(\\small 30\\%\\) ROE previously shown. The indicator can be broken down in two factors:\n\nThe profit margin it generates from its assets\nHow big the firm’s asset base is relative to shareholders’ investment\n\n\n\n\\[\nROE=\\dfrac{\\text{Net Income}}{\\text{Equity}}=\\underbrace{\\dfrac{\\text{Net Income}}{\\text{Assets}}}_{ROA}\\times \\underbrace{\\dfrac{\\text{Assets}}{\\text{Equity}}}_{Leverage}\n\\]\n\nThe Return on Assets (ROA) tells us how much profit a company is able to generate for each dollar of assets invested, irrespective of the mix of financing\nFinancial leverage indicates how many dollars of assets the firm is able to deploy for each dollar invested by its shareholders"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#roa-and-financial-leverage",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#roa-and-financial-leverage",
    "title": "Financial Analysis",
    "section": "ROA and Financial Leverage",
    "text": "ROA and Financial Leverage\n\nUsing the numbers from our example, we have:\n\n\n\\[\nROA = \\dfrac{\\text{Net Income}}{\\text{Assets}}=\\dfrac{150,000}{750,000}=20\\%\n\\]\n\n\n\\[\nLeverage= \\dfrac{\\text{Assets}}{\\text{Equity}}=\\dfrac{750,000}{500,000}=1.5\n\\]\n\nTherefore, we can compute the ROE as:\n\n\n\n\\[\nROE= ROA \\times Leverage = 20\\% \\times 1.5 = 30\\%\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#the-effects-of-financial-leverage",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#the-effects-of-financial-leverage",
    "title": "Financial Analysis",
    "section": "The effects of financial leverage",
    "text": "The effects of financial leverage"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#analyzing-the-roe-decomposition",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#analyzing-the-roe-decomposition",
    "title": "Financial Analysis",
    "section": "Analyzing the ROE decomposition",
    "text": "Analyzing the ROE decomposition\n\nOur ROE decomposition yielded the following results:\n\nThe Return on Assets is \\(\\small20\\%\\)\nFirm’s Leverage is \\(\\small1.5\\)\n\nWe can see that leverage acts in this case by exacerbating the shareholder’s return, either upwards or downwards\n\nAs the firm uses debt in its mix, after paying out the interest expenses, all the remaining net income is accrued to the shareholders\nTherefore, using debt leverages the returns since the the value of assets being employed in the operation are higher than what the shareholders are employing\n\nNote, however, that there is a catch: if the firm’s ROA is negative, it means that the shareholder’s ROE will be even lower than in the case where the company was financed using \\(\\small100\\%\\) Equity!"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-in",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-in",
    "title": "Financial Analysis",
    "section": "Concept check-in",
    "text": "Concept check-in\n\nDefine if the statements below are TRUE or FALSE:\n\nSince ROE is a high-level view of the return to the shareholders, it can be safely used to compare firms from different industries\nLooking at the ROA portion of ROE, it can inform about the value that managers get from the assets, but it does not point to the operational levers that generated such performance\nHigher leverage always helps by leveraging up the returns to the shareholders\nCalculating a given industry’s ROE can shed light on the industry’s equilibrium return, and it is useful for benchmarking\nIf a firm opts to perform a sale-and-leaseback operation, it will never affect its calculated ROE"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-in-1",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-in-1",
    "title": "Financial Analysis",
    "section": "Concept check-in",
    "text": "Concept check-in\n\nDefine if the statements below are TRUE or FALSE:\n\nSince ROE is a high-level view of the return to the shareholders, it can be safely used to compare firms from different industries\nLooking at the ROA portion of ROE, it can inform about the value that managers get from the assets, but it does not point to the operational levers that generated such performance\nHigher leverage always helps by leveraging up the returns to the shareholders\nCalculating a given industry’s ROE can shed light on the industry’s equilibrium return, and it is useful for benchmarking\nIf a firm opts to perform a sale-and-leaseback operation, it will never affect its calculated ROE"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#the-dupont-decomposition",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#the-dupont-decomposition",
    "title": "Financial Analysis",
    "section": "The DuPont Decomposition",
    "text": "The DuPont Decomposition\n\nYou can drill down the ROE determinants even deeper by explanding the ROA component:\n\n\n\\[\n\\small ROE=ROA\\times Leverage=\\dfrac{\\text{Net Income}}{\\text{Assets}}\\times \\dfrac{\\text{Assets}}{\\text{Equity}}=\\bigg[\\bigg(\\dfrac{\\text{Net Income}}{\\text{Sales}}\\times \\dfrac{\\text{Sales}}{\\text{Assets}}\\bigg)\\bigg]\\times \\dfrac{\\text{Assets}}{\\text{Equity}}\n\\]\n\nTherefore, one can view ROE as a composite of three determinants:\n\n\n\n\\[\n\\small ROE=\\underbrace{\\dfrac{\\text{Net Income}}{\\text{Sales}}}_{\\text{Net Profit Margin}}\\times \\underbrace{\\dfrac{\\text{Sales}}{\\text{Assets}}}_{\\text{Asset Turnover}}\\times \\underbrace{\\dfrac{\\text{Assets}}{\\text{Equity}}}_{Leverage}\n\\]\n\nThe Net Profit Margin indicates how much a firm keeps for each dollar of sales it makes\nThe Asset Turnover indicates how many sales it gets for each dollar of assets\nThe Leverage indicates how big is the firm’s asset base relative to the shareholder’s equity"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#applying-the-duponts-roe-decomposition",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#applying-the-duponts-roe-decomposition",
    "title": "Financial Analysis",
    "section": "Applying the DuPont’s ROE Decomposition",
    "text": "Applying the DuPont’s ROE Decomposition\n\n\nUsing the numbers from our example, we have:\n\n\n\n\\[\n\\small ROE=\\underbrace{\\dfrac{150,000}{1,000,000}}_{\\text{Net Profit Margin}}\\times \\underbrace{\\dfrac{1,000,000}{750,000}}_{\\text{Asset Turnover}}\\times\\underbrace{\\dfrac{750,000}{500,000}}_{\\text{Leverage}}=0.15\\times 1.33 \\times 1.5 = 30\\%\n\\]\n\n\n\nSuch decomposition greatly improve our understanding of how strategic, investment, and financing decisions made by the firm affect its ratios:\n\n\nBy nailing ROE down to specific determinants, one can understand how managers are generating return to the shareholders\nUnderstanding and comparing these specific drivers to benchmarks help us understanding the areas where the managers are over(under)performing\n\n\n\n\\(\\rightarrow\\) See Leverage Example.xlsx (available on eClass®) for a detailed explanation."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-5",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-5",
    "title": "Financial Analysis",
    "section": "Concept Check",
    "text": "Concept Check\n\n\nBonner Corp.’s sales last year were \\(\\small\\$430,500\\), and its year-end total assets were \\(\\small\\$360,000\\). The average firm in the industry has a total assets turnover ratio (TATO) of \\(\\small2.2\\). Bonner’s new CFO believes the firm has excess assets that can be sold so as to bring the TATO down to the industry average without affecting sales. By how much must the assets be reduced to bring the TATO to the industry average, holding sales constant?\n\n\\(\\small\\$164,318\\)\n\\(\\small\\$172,979\\)\n\\(\\small\\$182,083\\)\n\\(\\small\\$191,188\\)\n\\(\\small\\$200,747\\)"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-6",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-6",
    "title": "Financial Analysis",
    "section": "Concept Check",
    "text": "Concept Check\n\nUsing that the assets turnover formula with the actual firm’s sales and the target TATO, we have that:\n\n\n\\[\n\\small \\text{Target Asset Turnover}=\\dfrac{Sales}{Assets}\\rightarrow 2.2=\\dfrac{430,500}{Assets}\\rightarrow Assets=\\dfrac{430,500}{2.2}\\approx 195,682\n\\]\n\nThus, the amount that needs to be reduced in terms of assets is given by:\n\n\n\n\\[\n\\small \\Delta Assets = \\text{Actual Assets}-\\text{Target Assets}\\rightarrow 360,000-195,681.8\\approx 164,318\n\\]\n\nTherefore, if we believe that freeing up assets is not going to affect sales, Bonner Corp needs to reduce \\(\\small\\$164,318\\) to get to the desired industry average."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-7",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-7",
    "title": "Financial Analysis",
    "section": "Concept Check",
    "text": "Concept Check\n\nAfter figuring out that the amount of assets to be liquidated is around \\(\\small\\$164,318\\), the CFO was given two options:\n\nA mix of selling Properly, Plant and Equipment, worth \\(\\small\\$120,000\\) in book value, and liquidating \\(\\small\\$44,318\\) in inventories, to be paid as dividends\nLiquidating \\(\\small\\$130,000\\) in short-term financial investments and \\(\\small\\$34,318\\) in cash, to be paid as dividends\n\nWhich one should get Bonner Corp. closer to the desired TATO and why?\n\n\nA: in most cases, Option 2 should lead Bonner Corp. closer to its objective, since short-term financial investments and cash are more liquid than real assets. Therefore, not only they are liquidated faster, but they also are more easily liquidated by its intrinsic value."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-8",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-8",
    "title": "Financial Analysis",
    "section": "Concept Check",
    "text": "Concept Check\n\nThe CEO insists on the fact that Bonner Corp. should stick with Option 1 with the argument that holding Cash and Short-Term investments due to a precautionary savings motivation makes the company less risky, and that they are pretty sure that the assets can be sold at book value. Which assumption of the strategy described to bring the TATO to the industry benchmark can be violated if the CFO chooses Option 1?\n\n\nA: even assuming that PPE and Inventories can be liquidated at book value, the assumption that sales will not be affected by the strategy can not hold. Since PPE and Inventories are, in general, actively used in the firm’s operations, reducing the amount of operational assets can negatively impact sales."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-9",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-9",
    "title": "Financial Analysis",
    "section": "Concept Check",
    "text": "Concept Check\n\nWhich of the following statements is CORRECT?\n\nThe modified DuPont equation provides information about how operations affect the ROE, but the equation does not include the effects of debt on the ROE.\nSuppose a firm’s total assets turnover ratio falls from \\(\\small1.0\\) to \\(\\small0.9\\), but at the same time its profit margin rises from \\(\\small9\\%\\) to \\(\\small10\\%\\) and its debt increases from \\(\\small40\\%\\) of total assets to \\(\\small60\\%\\). Without additional information, we cannot tell what will happen to the ROE.\nSuppose a firm’s total assets turnover ratio falls from \\(\\small1.0\\) to \\(\\small0.9\\), but at the same time its profit margin rises from \\(\\small9\\%\\) to \\(\\small10\\%\\), and its debt increases from \\(\\small40\\%\\) of total assets to \\(\\small60\\%\\). Under these conditions, the ROE will decrease.\nSuppose a firm’s total assets turnover ratio falls from \\(\\small1.0\\) to \\(\\small0.9\\), but at the same time its profit margin rises from \\(\\small9\\%\\) to \\(\\small10\\%\\) and its debt increases from \\(\\small40\\%\\) of total assets to \\(\\small60\\%\\). Under these conditions, the ROE will increase.\nOther things held constant, an increase in the debt ratio will result in an increase in the profit margin on sales."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-10",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-10",
    "title": "Financial Analysis",
    "section": "Concept Check",
    "text": "Concept Check\n\nFrom our formula, we have that:\n\n\n\\[\n\\small ROE=\\underbrace{\\dfrac{\\text{Net Income}}{\\text{Sales}}}_{\\text{Net Profit Margin}}\\times \\underbrace{\\dfrac{\\text{Sales}}{\\text{Assets}}}_{\\text{Asset Turnover}}\\times \\underbrace{\\dfrac{\\text{Assets}}{\\text{Equity}}}_{Leverage}\n\\]\n\nIf the Asset Turnover went from \\(\\small1\\) to \\(\\small0.9\\) \\(\\rightarrow\\) a decrease of \\(\\small-10\\%\\) (\\(\\Delta A.T\\))\nIf the Profit Margin went up from \\(\\small9\\%\\) to \\(\\small10\\%\\) \\(\\rightarrow\\) an increase of \\(\\small11.11\\%\\) (\\(\\Delta Margin\\))\n\n\n\\(\\small 0.9\\times 1.11=1\\), so the ROA components offset each other\nIf \\(\\small ROA\\) stays the same (and \\(\\small&gt;0\\)) and we increase our leverage, we know that \\(\\small ROE\\) will increase"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-11",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#concept-check-11",
    "title": "Financial Analysis",
    "section": "Concept Check",
    "text": "Concept Check\n\nFirst Scenario:\n\n\n\\[\n\\dfrac{Debt}{Assets}=40\\% \\rightarrow Equity = 60\\% \\rightarrow\\dfrac{Assets}{Equity}\\approx1.66\n\\\\\nROE_{1}=1\\times9\\%\\times1.66=15\\%\n\\]\n\nSecond Scenario:\n\n\n\n\\[\n\\dfrac{Debt}{Assets}=60\\% \\rightarrow Equity = 40\\% \\rightarrow\\dfrac{Assets}{Equity}\\approx2.5\n\\\\ ROE_{2}=0.9\\times10\\%\\times2.5=22.5\\%\n\\]\n\nTherefore, statement D is correct."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#a-deep-dive-on-the-firms-net-profit-margin",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#a-deep-dive-on-the-firms-net-profit-margin",
    "title": "Financial Analysis",
    "section": "A deep-dive on the firm’s net profit margin",
    "text": "A deep-dive on the firm’s net profit margin\n\nA firm’s net profit margin, or return on sales (ROS), shows the profitability of the company’s operating activities, and is the first component of a firm’s ROE:\n\n\n\\[\n\\text{Net Profit Margin}=\\dfrac{\\text{Net Income}}{\\text{Total Sales}}\n\\]\n\nFurther decomposition of a firm’s ROS allows an analyst to assess the efficiency of the firm’s operating management:\n\nBy analyzing each line item of an income statement, one can shed light on what were the specific cost items that drove the net profit margin result\nIn order to do that, a popular tool is to calculate indexes in which all the line items are expressed as a percentage of sales revenues - this is also known as vertical analysis"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#the-anatomy-of-an-income-statement",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#the-anatomy-of-an-income-statement",
    "title": "Financial Analysis",
    "section": "The anatomy of an income statement",
    "text": "The anatomy of an income statement\n\n\n\nEach line item can be thought of an index that represents the % of sales\nTherefore, the Net Profit Margin is thus a composite effect of these impacts on firm’s total sales!"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#gross-profit-margin",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#gross-profit-margin",
    "title": "Financial Analysis",
    "section": "Gross Profit Margin",
    "text": "Gross Profit Margin\n\n\n\n\n\n\nDefinition\n\n\nThe Gross Profit Margin is the difference between a firm’s sales and cost of sales is gross profit, and is an indication of the extent to which revenues exceed direct costs associated with sales:\n\\[\n\\small\\text{Gross Profit Margin}=\\dfrac{\\text{Sales - Cost of Sales}}{\\text{Total Sales}}\n\\]\n\n\n\n\nUsing the numbers from our example, we have:\n\n\n\\[\n\\small\\text{Gross Profit Margin}=\\dfrac{\\text{Sales - Cost of Sales}}{\\text{Total Sales}}=\\dfrac{1,000,000-600,000}{1,000,000}=40\\%\n\\]\n\nFactors that influence Gross Profit Margins:\n\nThe price premium that a firm’s products or services operate\nThe efficiency of the firm’s operations"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#net-operating-profit-after-taxes-nopat",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#net-operating-profit-after-taxes-nopat",
    "title": "Financial Analysis",
    "section": "Net Operating Profit After Taxes (NOPAT)",
    "text": "Net Operating Profit After Taxes (NOPAT)\n\nAfter deducting direct costs, there are still indirect operating costs (or revenues) to be taken into consideration, not to mention the effects of taxes. To take both direct and indirect costs as well as the effects of taxes into account, we can canculate a firm’s NOPAT\n\n\n\n\n\n\n\n\nDefinition\n\n\nThe Net Operating Profit after Taxes (also known as NOPAT) takes into account all non-financial components. The NOPAT margin is calculated as:\n\\[\n\\text{NOPAT Margin}=\\dfrac{(\\text{Sales}-\\text{COGS}-\\text{SG&A}-\\text{Depr./Am.}\\pm\\text{Other Non. Fin.})\\times(1-\\text{Tax Rate})}{\\text{Total Sales}}\n\\]\n\n\n\n\nUsing our example (and assuming a \\(\\small40\\%\\) tax rate), we have:\n\n\n\n\\[\n\\small \\text{NOPAT Margin}=\\dfrac{(1,000,000-600,000-125,000)\\times(1-40\\%)}{1,000,000}=16.5\\%\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#understanding-nopat",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#understanding-nopat",
    "title": "Financial Analysis",
    "section": "Understanding NOPAT",
    "text": "Understanding NOPAT\n\nThe NOPAT margin provides a comprehensive indication of the operating performance of a company because it reflects all operating costs and eliminates the effects of debt policy:\nIn special, Selling, General, and Administrative Expenses (SG&A) expenses are influenced by the operating activities it has to undertake to implement its competitive strategy:\n\nFirms with differentiation strategies have to undertake activities to achieve that differentiation (e.g, R&D-intensive firms)\nSimilarly, a company that attempts to build a brand-equity is likely to have higher selling and administration costs\n\nSG&A expenses are also influenced by the efficiency of the firm’s overhead activities\n\n\n\n\n\n\n\n\nA word of caution\n\n\nAs we saw, although NOPAT is a comprehensible indicator for the operating performance of a firm, it can mask some important differences when it comes to account treatments!"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#the-nopat-and-the-tax-rate",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#the-nopat-and-the-tax-rate",
    "title": "Financial Analysis",
    "section": "The NOPAT and the Tax Rate",
    "text": "The NOPAT and the Tax Rate\n\nQuestion: in the NOPAT calculation, why do we use a Tax Rate (that is generally given) instead of just collecting the Tax Expenses from the Income Statement?\nNote that the tax expense in the Income Statement comes from the taxable income, which is, in turn, composed of Operating as well as Financing components\nIf you simply deduct Tax Expenses, you are potentially considering the tax-shield that comes from Net Interest Expenses into the calculation:\n\nFirms with more debt in its capital structure, all else equal, will pay lower taxes\nFirms that had previous losses can compensate future profits paying less taxes\n\nTherefore, Tax Expenses can be a source of distortions due to different financing decisions when comparing different firms and/or the same firm over time!\nAs such, a more apples-to-apples comparison between firms that have very different capital structure may involve insulating the analysis from financials and taxes!"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#the-ebit-margin",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#the-ebit-margin",
    "title": "Financial Analysis",
    "section": "The EBIT margin",
    "text": "The EBIT margin\n\n\n\n\n\n\nDefinition\n\n\nThe Earnings before Interest and Taxes (also known as EBIT) takes into account all non-financial components, disregarding any effects from taxes and interest income/expenses. The EBIT margin is calculated as:\n\\[\n\\text{EBIT Margin}=\\dfrac{(\\text{Sales}-\\text{COGS}-\\text{SG&A}-\\text{Depr./Am.}\\pm\\text{Other Non. Fin.})}{\\text{Total Sales}}=\\dfrac{275,000}{1,000,000}=27.5\\%\n\\]\n\n\n\n\nThe EBIT margin takes into account:\n\nThe direct costs of the activity (Cost of Goods Sold)\nThe indirect costs of the activity (SG&A Expenses)\n\nIt disregards the effects of financial income/expenses in the calculation, as well as taxes\nAs such, such indicator can be used to compare firms with distinct financing structures!"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#from-ebitda-to-ebitda",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#from-ebitda-to-ebitda",
    "title": "Financial Analysis",
    "section": "From EBITDA to EBITDA",
    "text": "From EBITDA to EBITDA\n\nApart from taxes, another criticism around NOPAT (and EBIT) is that they still take into consideration a significant non-cash item: Depreciation and Amortization expenses\nThe Earnings Before Interest, Taxes, Depreciation, and Amortization adds back Depreciation/Amortization expenses and does not deduct taxes:\n\n\n\n\n\n\n\n\nDefinition\n\n\nThe Earnings before Interest and Taxes (also known as EBIT) takes into account all non-financial components, disregarding any effects from taxes and interest income/expenses. The EBIT margin is calculated as:\n\\[\n\\small \\text{EBITDA Margin}=\\dfrac{\\text{Net Income} + \\text{Interest Expenses} + \\text{Taxes}+ \\text{Depreciation/Amortization}}{\\text{Total Sales}}\n\\]\nNote that you can also calculate the EBITDA margin by summing up only the relevant accounts:\n\\[\n\\small\\text{EBITDA Margin}=\\dfrac{\\text{Sales} -\\text{COGS}- \\text{SG&A} \\pm\\text{ Other Op.}}{\\text{Total Sales}}\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#understanding-ebitda",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#understanding-ebitda",
    "title": "Financial Analysis",
    "section": "Understanding EBITDA",
    "text": "Understanding EBITDA\n\nUsing the numbers from our example, we have:\n\n\n\\[\n\\small\\text{EBITDA Margin}=\\dfrac{\\text{Net Income} +\\text{Int. Expenses} +\\text{Taxes}+ \\text{Depr./Am.}}{\\text{Total Sales}}=\\dfrac{325,000}{1,000,000}=32.5\\%\n\\]\n\\[\n    \\small\\text{EBITDA Margin}=\\dfrac{\\text{Sales} -\\text{COGS}- \\text{SG&A} \\pm\\text{ Other Op.}}{\\text{Total Sales}}= \\dfrac{325,000}{1,000,000}=32.5\\%\n\\]\n\nEBITDA is useful to contrast operations by disregarding differences stemming from:\n\n\nNon-cash items (Depreciation and Amortization)\nTaxes (different firms may have different effective tax rates)\nFinancing Policy (firms may have financial investment and also benefit from tax-shields through higher debt levels)"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#market-valuation-ratios",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#market-valuation-ratios",
    "title": "Financial Analysis",
    "section": "Market-valuation Ratios",
    "text": "Market-valuation Ratios\n\nWe saw a series of Liquidity, Solvency, and Profitability indices that could shed light on specific aspects of firm’s performance\nNotwithstanding, these indices are not deterministic: although they provide valuable information, they cannot fully determine how much a firm is worth\nIdeally, the balance sheet would provide us with an accurate assessment of the true value of the firm’s equity. Unfortunately, this is unlikely to be the case:\n\nMany of the assets listed on the balance sheet are valued based on their historical cost rather than their true, intrinsic value\nMore importantly, a substantial amount of the firm’s valuable assets are not captured on the balance sheet (expertise of its employees, reputation, relationships with customers and suppliers, among others)\n\nFor these reasons, we should expect that Market Value of Equity \\(\\neq\\) Book Value of Equity"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#market-valuation-ratios---mve-and-enterprise-value",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#market-valuation-ratios---mve-and-enterprise-value",
    "title": "Financial Analysis",
    "section": "Market-valuation Ratios - MVE and Enterprise Value",
    "text": "Market-valuation Ratios - MVE and Enterprise Value\n\n\n\n\n\n\nDefinition\n\n\nThe Market Value of Equity (MVE), or simply the value of the firm’s, is defined as the number of shares outstanding times the firm’s market price per share - often referred to as Market Capitalization:\n\\[\n\\text{Market Value of Equity}= \\text{Shares Outstanding} \\times \\text{Market Price per Share}\n\\]\nNote: Shares Outstanding refer to the total number of shares issued by the company, and can include publicly traded (float) insider stocks and closely held stocks.\n\nIt does not depend on the historical cost of the firm’s assets\nRather, it depends on what investors expect those assets to produce in the future\n\n\n\n\n\n\nWe also can calculate the firm’s Enterprise Value (known as the “cost to take over the business”) as:\n\n\n\n\\[\n\\small \\text{Enterprise Value}=\\text{Market Value of Equity} + Debt - Cash\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#market-valuation-ratios---the-mb-ratio",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#market-valuation-ratios---the-mb-ratio",
    "title": "Financial Analysis",
    "section": "Market-valuation Ratios - the M/B Ratio",
    "text": "Market-valuation Ratios - the M/B Ratio\nWe can also calculate indices that contrast market and book measurements:\n\n\n\n\n\n\n\nDefinition\n\n\nThe Market-to-Book Ratio (or simply M/B Ratio) highlights the differences in fundamental firm characteristics as well as the value added by management:\n\\[\n\\text{M/B Ratio}=\\dfrac{\\text{Market Value of Equity}}{\\text{Book Value of Equity}}\n\\]\nWhere market capitalization is defined as before, and the Book Value of Equity is the accounting value of the firm’s equity (i.e, Assets - Liabilities).\n\n\n\n\nSome common patterns on M/B Ratios:\n\nA successful firm’s Market-to-Book ratio typically exceeds 1.\nLow Market-to-Book ratios \\(\\rightarrow\\) also known as value stocks\nHigh Market-to-Book ratios \\(\\rightarrow\\) also known as growth stocks"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#market-valuation-ratios---the-pe-ratio",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#market-valuation-ratios---the-pe-ratio",
    "title": "Financial Analysis",
    "section": "Market-valuation Ratios - the P/E Ratio",
    "text": "Market-valuation Ratios - the P/E Ratio\n\n\n\n\n\n\nDefinition\n\n\nThe Price-Earnings Ratio (or simply P/E Ratio) is a simple measure that is used to assess whether a stock is over- or under-valued based on the idea that the value of a stock should be proportional to the level of earnings it can generate for its shareholders\n\\[\n\\text{P/E Ratio} = \\dfrac{\\text{Market Capitalization}}{\\text{Net Income}}\\equiv \\dfrac{\\text{Share Price}}{\\text{Earnings per Share}}\n\\]\n\n\n\n\nThis indicator tends to be highest for industries with high expected growth rates\nBecause the P/E Ratio considers the value of the firm’s equity, it is sensitive to the firm’s choice of leverage\nWe can avoid this limitation by instead assessing the market value of the underlying business using valuation ratios based on the firm’s enterprise value1\n\nSee (Berk and DeMarzo 2023) Comment on mismatched ratios, p. 77"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#sustainable-growth-rate",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#sustainable-growth-rate",
    "title": "Financial Analysis",
    "section": "Sustainable Growth Rate",
    "text": "Sustainable Growth Rate\n\nWe have studied factors that, ultimately, show how a firm’s profitability and {growth}{.blue} can be broke down in terms of investment and financing decisions\n\n\nQuestion: what could be considered a sustainable growth rate of a business? We can shed light on this question by relating a firm’s long-term ROE to its payout policy:\n\nROE shows how much the firm can generate in terms of cash for its shareholders\nPayout indicates how much of the profits have been paid out as dividends\n\n\n\n\n\n\n\n\n\nDefinition\n\n\nThe Sustainable Growth Rate is an estimate for a given firm’s growth rate using only funds from its own operations and not issuing any new debt/equity. It can be estimated as:\n\\[\ng = ROE \\times \\text{% Retailed Profits} \\equiv ROE \\times \\bigg(1-\\dfrac{\\text{Divivends Paid}}{\\text{Net Income}}\\bigg) \\equiv ROE \\times (1-\\text{Payout Ratio})\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#wrapping-up-on-financial-indicators",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#wrapping-up-on-financial-indicators",
    "title": "Financial Analysis",
    "section": "Wrapping up on Financial Indicators",
    "text": "Wrapping up on Financial Indicators\n\nWe have studied how Liquidity, Solvency, and Profitability can shed light on the firm’s future profits and growth rates\nAlso, Market Valuation indices can help us relate how investors see (and price) these businesses based on the available information\nAlthough these indices provide valuable information for us, there are some caveats:\n\nIndices are backward-looking: it does not mean that a firm will continue to deliver the same indices year-over-year\nThey may suffer from accounting distortions\nFinally, they only provide a high-level view of a firm performance, and does not necessarily go in lockstep with market evaluations about the firm\n\n\n\n\\(\\rightarrow\\) Practice these concepts using the Financial Statement Analysis HBS Case (available on eClass®)"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#criticism-around-traditional-financial-ratio-analysis",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#criticism-around-traditional-financial-ratio-analysis",
    "title": "Financial Analysis",
    "section": "Criticism around traditional Financial Ratio analysis",
    "text": "Criticism around traditional Financial Ratio analysis\n\nFinancial reporting serves as an important role as means of communication between managers and stakeholders - in special, investors\nNotwithstanding, traditional financial reporting falls short on understanding the value driven from intangibles, which have been becoming a more important portion of the asset base\nThe most concrete proposal for this shortcoming is called Integrated Reporting:\n\nHow an organization creates integrated value\nAdds Intellectual, Social , Relationship, Human, and Natural dimensions to the analysis\n\nIntegrated Reporting is facilitated by the recent widespread in ESG data (e.g, gender in boards, wages, carbon emissions, etc)\n\n\n\\(\\rightarrow\\) Full content on Integrated Reporting: see (Schoenmaker and Schramade 2019), available on eClass®"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#the-role-of-credit-ratings",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#the-role-of-credit-ratings",
    "title": "Financial Analysis",
    "section": "The role of Credit Ratings",
    "text": "The role of Credit Ratings\n\nWho might be interested in getting a full view of the firm’s financial prospects?\nBorrowers which seek credit in capital markets need to signal their financial conditions to potential creditholders\nA rating agency is responsible for translating firm’s financials into creditworthiness\nAgencies provide a grading on a given firm’s creditworthiness, which can guide financing decisions (for example, a investor fund may only invest in business that are rated as AAA, which stands for the highest creditworthiness firms)\n\n\nRating firms were at the focus of the 2008 global financial crisis (see the movie The Big Short)\nCredit ratings are affected (and have the potential to affect) business outcomes"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#appendix-i-decomposing-profitability-an-alternative-approach",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#appendix-i-decomposing-profitability-an-alternative-approach",
    "title": "Financial Analysis",
    "section": "Appendix I: decomposing profitability: an alternative approach",
    "text": "Appendix I: decomposing profitability: an alternative approach\n\nDecomposing ROE using DuPont’s approach provide us with key insights on specific factors that can drive shareholder’s profitability. Notwithstanding, it has some limitations:\n\nIn the computation of ROA, the denominator includes the assets claimed by all providers of capital to the firm, but the numerator includes only the earnings available to equity holders\nThe assets themselves include both operating assets and financial assets such as cash and short-term investments\nNet income includes income from operating activities as well as interest income and expense, which are consequences of financing decisions\nFinally, financial leverage ratio used above does not recognize the fact that a firm’s cash and short-term investments are in essence “negative debt” because they can be used to pay down the debt on the company’s balance sheet\n\nWe can use the terms discussed in this lecture to further decompose ROE"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#decomposing-profitability-step-1",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#decomposing-profitability-step-1",
    "title": "Financial Analysis",
    "section": "Decomposing profitability: Step 1",
    "text": "Decomposing profitability: Step 1\nStarting from the general equation:\n\\[\n\\small\nROE=\\dfrac{\\text{Net Income}}{\\text{Equity}}\n\\]\nUsing the fact that Net Income is the difference between NOPAT and Net Interest Expenses after Taxes, we have:\n\\[\n\\small\nROE=\\dfrac{\\text{Net Income}}{\\text{Equity}}=\\dfrac{\\text{NOPAT-Net Interest Expenses after Taxes}}{\\text{Equity}}\n\\]\nWhich we can disentangle into two terms:\n\\[\n\\small\nROE=\\dfrac{\\text{Net Income}}{\\text{Equity}}=\\dfrac{\\text{NOPAT}}{\\text{Equity}}-\\dfrac{\\text{Net Interest Expenses after Taxes}}{\\text{Equity}}\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#decomposing-profitability-step-2",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#decomposing-profitability-step-2",
    "title": "Financial Analysis",
    "section": "Decomposing profitability: Step 2",
    "text": "Decomposing profitability: Step 2\nBy now, let’s focus on the first term, highlighted with a \\(\\star\\) :\n\\[\n\\small\nROE=\\underbrace{\\dfrac{\\text{NOPAT}}{\\text{Equity}}}_{\\star}-\\dfrac{\\text{Net Interest Expenses after Taxes}}{\\text{Equity}}\n\\] We can divide and multipy by Net Assets, which is the sum of Long Term Assets and Operating working Capital:\n\\[\n\\small\n\\dfrac{\\text{NOPAT}}{\\text{Equity}}\\times \\dfrac{\\text{Net Assets}}{\\text{Net Assets}}\\equiv \\dfrac{\\text{NOPAT}}{\\text{Net Assets}}\\times \\dfrac{\\text{Net Assets}}{\\text{Equity}}\n\\] Finally, since Net Assets are just the sum of Equity and Net Debt, we can subtstitue the terms into the Net Assets in the numerator:\n\\[\n\\small\n\\dfrac{\\text{NOPAT}}{\\text{Net Assets}}\\times \\dfrac{\\text{Net Assets}}{\\text{Equity}}=\\dfrac{\\text{NOPAT}}{\\text{Net Assets}}\\times \\bigg[\\bigg(\\dfrac{\\text{Equity + Net Debt}}{\\text{Equity}}\\bigg)\\bigg]\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#decomposing-profitability-step-2-1",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#decomposing-profitability-step-2-1",
    "title": "Financial Analysis",
    "section": "Decomposing profitability: Step 2",
    "text": "Decomposing profitability: Step 2\n\nSimplifying, we have:\n\n\n\\[\n\\small\n\\dfrac{\\text{NOPAT}}{\\text{Net Assets}}\\times \\bigg[\\bigg(\\dfrac{\\text{Equity}}{\\text{Equity}}+\\dfrac{\\text{Net Debt}}{\\text{Equity}}\\bigg)\\bigg]=\\dfrac{\\text{NOPAT}}{\\text{Net Assets}}\\times \\bigg(1+\\dfrac{\\text{Net Debt}}{\\text{Equity}}\\bigg)\n\\]\n\nLets hold off this term for on and focus on the other term of the ROE decomposition."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#decomposing-profitability-step-3",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#decomposing-profitability-step-3",
    "title": "Financial Analysis",
    "section": "Decomposing profitability: Step 3",
    "text": "Decomposing profitability: Step 3\nNow, let’s focus on the second component of the ROE decomposition:\n\\[\n\\small\nROE=\\dfrac{\\text{NOPAT}}{\\text{Equity}}-\\underbrace{\\dfrac{\\text{Net Interest Expenses after Taxes}}{\\text{Equity}}}_{\\star}\n\\]\nUsing the same rationale,divide and multipy by Net Debt, which is the difference between all Interest-bearing liabilities and Cash + Marketable securities:\n\\[\n\\small\n\\dfrac{\\text{Net Interest Expenses after Taxes}}{\\text{Equity}}\\times \\dfrac{\\text{Net Debt}}{\\text{Net Debt}}\\equiv \\dfrac{\\text{Net Interest Expenses after Taxes}}{\\text{Net Debt}} \\times \\dfrac{\\text{Net Debt}}{\\text{Equity}}\n\\] This is the second term of our ROE equation, and we’ll use it to simplify the equation even further."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#decomposing-profitability-step-4",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#decomposing-profitability-step-4",
    "title": "Financial Analysis",
    "section": "Decomposing profitability: Step 4",
    "text": "Decomposing profitability: Step 4\n\nJoining steps 2-3 altogether, our ROE can be expressed as:\n\n\n\\[\n\\small\nROE= \\dfrac{\\text{NOPAT}}{\\text{Net Assets}}\\times \\bigg(1+\\dfrac{\\text{Net Debt}}{\\text{Equity}}\\bigg)- \\dfrac{\\text{Net Interest Expenses after Taxes}}{\\text{Net Debt}} \\times \\dfrac{\\text{Net Debt}}{\\text{Equity}}\n\\]\n\nSimplifying and rearranging, we have:\n\n\n\n\\[\n\\small\nROE= \\underbrace{\\dfrac{\\text{NOPAT}}{\\text{Net Assets}}}_{\\text{Operating ROA}} + \\bigg[\\underbrace{\\dfrac{\\text{Net Debt}}{\\text{Equity}}}_{\\text{Net Financial Leverage}}\\times\\bigg(\\underbrace{\\dfrac{\\text{NOPAT}}{\\text{Net Assets}}- \\dfrac{\\text{Net Interest Expenses after Taxes}}{\\text{Net Debt}}\\bigg)}_{\\text{Spread}}\\bigg]\n\\]\n\nFor more details on the calculations, see (Healy and Palepu 2012). The next slide explains each term of the new ROE decomposition"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#understanding-the-roe-decomposition---operating-roa",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#understanding-the-roe-decomposition---operating-roa",
    "title": "Financial Analysis",
    "section": "Understanding the ROE decomposition - Operating ROA",
    "text": "Understanding the ROE decomposition - Operating ROA\n\n\\[\n\\small\n\\dfrac{\\text{NOPAT}}{\\text{Net Assets}}\n\\]\n\nThe first term yields us the Operating ROA:\n\nThis is a measure of how profitably a company is able to deploy its operating assets to generate operating profits\nThis would be a company’s ROE if it were financed entirely with equity\n\nAnalyzing the Operating ROA in isolation is very important, as it focus on the operational portion of the firms’ profits and assets. Therefore, we get a more clear view on how the operating assets of a firm are being put in production\nIn other words, absent any effects coming from financing decisions, how well are the firm’s operating activities?"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#understanding-the-roe-decomposition---spread",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#understanding-the-roe-decomposition---spread",
    "title": "Financial Analysis",
    "section": "Understanding the ROE decomposition - Spread",
    "text": "Understanding the ROE decomposition - Spread\n\\[\n\\small\n\\bigg(\\dfrac{\\text{NOPAT}}{\\text{Net Assets}}- \\dfrac{\\text{Net Interest Expenses after Taxes}}{\\text{Net Debt}}\\bigg)\n\\]\n\nThe second term yields us the Spread:\n\nSpread is the incremental economic effect from introducing debt into the capital structure.\nThis economic effect of borrowing is positive as long as the return on operating assets is greater than the cost of borrowing\n\nConsequently, firms that do not earn adequate operating returns to pay for interest cost reduce their ROE by borrowing debt"
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#understanding-the-roe-decomposition---net-financial-leverage",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#understanding-the-roe-decomposition---net-financial-leverage",
    "title": "Financial Analysis",
    "section": "Understanding the ROE decomposition - Net Financial Leverage",
    "text": "Understanding the ROE decomposition - Net Financial Leverage\n\n\\[\n\\small\n\\dfrac{\\text{Net Debt}}{\\text{Equity}}\n\\]\n\nThe third (and final) term of the decomposition is the Net Financial Leverage:\n\nBoth the positive and negative effects of introducing debt into the capital structure are magnified by the extent to which a firm borrows relative to its equity base\nThe ratio of net debt to equity provides a measure of this net financial leverage.\nA firm’s spread times its net financial leverage, therefore, provides a measure of the financial leverage gain (or loss) to the shareholders."
  },
  {
    "objectID": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#references",
    "href": "fin-mgmt/Lecture 2 - Financial Analysis/index.html#references",
    "title": "Financial Analysis",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nHealy, P. M., and K. G. Palepu. 2012. Business Analysis Valuation: Using Financial Statements. Cengage Learning. https://books.google.com.br/books?id=Qp0IzgEACAAJ.\n\n\nSchoenmaker, D., and W. Schramade. 2019. Principles of Sustainable Finance. Oxford University Press. https://books.google.com.br/books?id=zoN8DwAAQBAJ."
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#decomposing-asset-turnover",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#decomposing-asset-turnover",
    "title": "Working Capital Management",
    "section": "Decomposing Asset Turnover",
    "text": "Decomposing Asset Turnover\nIn this lecture, we’ll focus on the Asset Turnover component:\n\\[\n\\small ROE=\\dfrac{\\text{Net Income}}{\\text{Sales}}\\times \\underbrace{\\dfrac{\\text{Sales}}{\\text{Assets}}}_{\\text{Asset Turnover}}\\times \\dfrac{\\text{Assets}}{\\text{Equity}}\n\\]\n\nSince firms invest considerable resources in their assets, using them productively is critical to overall profitability\nThere are two primary areas of investment management that are relevant to Asset Turnover:\n\nWorking capital Management\nManagement of long-term assets\n\nIn this lecture, we’ll extensively follow (Berk and DeMarzo 2023) to study Working Capital Management"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#working-capital-management",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#working-capital-management",
    "title": "Working Capital Management",
    "section": "Working Capital Management",
    "text": "Working Capital Management\n\n\nDefinition: working capital is the difference between a firm’s current assets and current liabilities. It does not distinguish between operating and financing components:\n\nOperating: Accounts Receivable, Inventory, and Accounts Payable, Cash (excluding non-excess cash), etc\nFinancing components: Excess Cash, Marketable securities, and Notes Payable"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#why-working-capital-is-so-important",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#why-working-capital-is-so-important",
    "title": "Working Capital Management",
    "section": "Why Working Capital is so important?",
    "text": "Why Working Capital is so important?\n\nOur general definition for Net Working Capital is Current Assets - Current Liabilities\nIf this number is positive, is this good or bad? It depends!\n\nHaving current assets higher than current liabilities tells us something about the firm’s liquidity…\nHowever, within these accounts, we’ll see items such as Cash, Inventories, and Accounts Receivable…\n\nNotably, there are opportunity costs associated with investing in inventories and accounts receivable, and from holding cash\n\nFor example, excess funds invested in these accounts could instead be used to pay down debt or returned to shareholders in the form of a dividend or share repurchase.\nTo this point, Working Capital Management aims to manage working capital efficiently and thereby minimize these opportunity costs!"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#why-working-capital-is-so-important-an-example",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#why-working-capital-is-so-important-an-example",
    "title": "Working Capital Management",
    "section": "Why Working Capital is so important? An example…",
    "text": "Why Working Capital is so important? An example…\n\nLet’s get back to the pizza vendor example that we had in our first class…\n\nWe know that, to run a food-business like this, we need fixed investments on Equipments, Machinerys, Buildings, etc\nBut after these are set in place, we need to worry about the flow of the operations!\n\nExamples:\n\nWe need money to buy the core ingredients and to make up some inventory\nWe also need to have a cushion to pay salaries for delivery/production/waiter workers\nThere will also be electric, water, sewage bills, along with other fixed costs that we’ll need to cushion\n\nNote that the operation cannot generate cash if we don’t have these assets in place\nThese are certain expenditures that we will continuously have as the operation flows – hence working capital"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#working-capital-another-example",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#working-capital-another-example",
    "title": "Working Capital Management",
    "section": "Working Capital, another example",
    "text": "Working Capital, another example\n\nA certain amount of investment in working capital is generally necessary for the firm to run its normal operations\nTo make this point clear, think about any mass retailer, such as Arezzo (ticker: ARZZ3):\n\n\nTo ensure that consumers are buying, retailers might need to provide a flexible payment method \\(\\rightarrow\\) increased investment on Accounts Receivable\nTo ensure that the goods are delivered on time, we might need to have inventories in place \\(\\rightarrow\\) increased investment on Inventories\nHowever, as ARZZ3 is a big retailer, it also may have good contracting terms with its suppliers to pay its inventory later \\(\\rightarrow\\) increased Accounts Payable\n\n\nOften, the interaction between these three accounts yields an important part of operating working capital needed for the business!"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#the-cash-and-operating-cycle",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#the-cash-and-operating-cycle",
    "title": "Working Capital Management",
    "section": "The Cash and Operating Cycle",
    "text": "The Cash and Operating Cycle\n\nThe level of working capital reflects the length of time between when cash goes out of a firm at the beginning of the production process and when it comes back in:\n\n\nA company first buys inventory from its suppliers, in the form of either raw materials or finished goods\nAfter receiving the inventory, even if the inventory is in the form of finished goods, it may sit on the shelf for some time\nFinally, when the inventory is ultimately sold, the firm may extend credit to its customers, delaying when it will receive the cash\n\n\nThe main components of net working capital are cash, inventory, receivables, and payables. To that matter, we can use the most important parts of a firm’s net working capital to define the cash and operating cycle"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#cash-and-operating-cycle-dynamics",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#cash-and-operating-cycle-dynamics",
    "title": "Working Capital Management",
    "section": "Cash and Operating Cycle Dynamics",
    "text": "Cash and Operating Cycle Dynamics\n\n\n\n\n\nDefinition: A firm’s cash cycle is the length of time between when the firm pays cash to purchase its initial inventory and when it receives cash from the sale of the output produced from that inventory"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#cash-cycle-dynamics-continued",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#cash-cycle-dynamics-continued",
    "title": "Working Capital Management",
    "section": "Cash Cycle Dynamics, continued",
    "text": "Cash Cycle Dynamics, continued\n\nThe longer the firm’s cash cycle, the more working capital it has, and the more cash it needs to carry to conduct its daily operations\nBecause of the characteristics of different industries, working capital levels vary significantly\n\n\nPolicies that increase the firm’s Cash Cycle:\n\nIf a firm extends the credit terms to its clients from 30 to 60 days \\(\\rightarrow\\) increased investment in Accounts Receivable\nIf the firm takes more time to produce its finished goods \\(\\rightarrow\\) the higher is the time that inventories are stocked \\(\\rightarrow\\) increased investment in Inventories\n\n\n\nPolicies that decrease the firm’s Cash Cycle:\n\nIf a firm receives more time to pay for its raw materials \\(\\rightarrow\\) increased liabilities in Accounts Payable"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#cash-conversion-cycles-by-industry",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#cash-conversion-cycles-by-industry",
    "title": "Working Capital Management",
    "section": "Cash Conversion Cycles by Industry",
    "text": "Cash Conversion Cycles by Industry\n\n\nCompanyIndustryReceivable DaysInventory DaysPayable DaysCash Conversion CycleCoca-ColaBeverages4381297-173T-MobileTelecommunications3624155-95Sirius XMCable and Satellite142108-92Molson Coors BrewingBrewing2634143-83AppleComputer Hardware2419106-63Amazon.comInternet Retail114376-22Southwest AirlinesAirlines121131-8Chipotle Mexican GrillRestaurants2211-7WalmartSuperstores442442KrogerGrocery Stores525236MicrosoftSoftware66227217Macy'sDepartment Stores414012421StarbucksRestaurants12291923FedExAir Freight4642327NordstromDepartment Stores4835730Bristol-Myers SquibbPharmaceuticals887010157NikeFootwear34903788ONE GasGas Utilities73473090Lennar CorporationHomebuilding537621359Tiffany & Co.Luxury Goods2053258494Brown-FormanDistillers and Vintners83548122509U.S. Firms-50707941"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#determining-the-cash-cycle",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#determining-the-cash-cycle",
    "title": "Working Capital Management",
    "section": "Determining the Cash Cycle",
    "text": "Determining the Cash Cycle\n\nSome practitioners measure the cash cycle by calculating the cash conversion cycle. The cash conversion cycle (CCC) is defined as\n\n\n\\[\n\\small CCC= \\text{Accounts Receivable Days}+\\text{Inventory Days}- \\text{Accounts Payable Days}\n\\] Where:\n\\[\n\\small\n\\text{Accounts Receivable Days}= \\dfrac{\\text{Accounts Receivable}}{\\text{Average Daily Sales}}\n\\]\n\\[\n\\small\n\\text{Inventory Days}= \\dfrac{\\text{Inventory}}{\\text{Average Daily COGS}}\n\\]\n\\[\n\\small\n\\text{Accounts Payable Days}= \\dfrac{\\text{Accounts Payable}}{\\text{Average Daily COGS}}\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#estimating-the-cash-conversion-cycle",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#estimating-the-cash-conversion-cycle",
    "title": "Working Capital Management",
    "section": "Estimating the Cash Conversion Cycle",
    "text": "Estimating the Cash Conversion Cycle\n\nSuppose that a firm has \\(\\small \\$100,000\\) in account receivables, \\(\\small\\$50,000\\) in inventory, and \\(\\small\\$25,000\\) in accounts payable. It has reported sales of about \\(\\small \\$1,000,000\\) and its Cost of Goods Sold (COGS) represent \\(\\small 40\\%\\) of total revenues. Calculate the Cash Conversion Cycle. Assume a 365-day year.\nNow, we’ll calculate the Cash Conversion Cycle of the firm by looking at its individual components\n\n\n\\(\\rightarrow\\) See CCC Dynamics.xlsx, available on eClass®, for a detailed explanation on cash conversion cycle dynamics using this lecture’s example."
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#step-1-inventory-days",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#step-1-inventory-days",
    "title": "Working Capital Management",
    "section": "Step 1: Inventory Days",
    "text": "Step 1: Inventory Days\n\nThe firm has \\(\\small\\$50,000\\) in Inventory\nIt had reported COGS of \\(\\small 40\\% \\times 1,000,0000=\\$400,000\\)\nThe Inventory Conversion is then:\n\n\n\\[\n\\small\nIC=\\dfrac{Inventory}{\\dfrac{COGS}{365}}\\rightarrow \\dfrac{50,000}{\\dfrac{400,000}{365}}\\rightarrow \\dfrac{50,000}{1,096}\\approx \\text{45.6 days}\n\\]\n\nInterpretation: if the firm had \\(\\small \\$400,000\\) in product costs yearly, this translates to a daily average of \\(\\small \\$1,096\\). If there’s \\(\\small \\$50,000\\) in Inventories, it means that it takes, on average, \\(\\small \\$50,000/\\$1,096=45.6\\) days for Inventories to be finished."
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#step-2-accounts-receivable-days",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#step-2-accounts-receivable-days",
    "title": "Working Capital Management",
    "section": "Step 2: Accounts Receivable Days",
    "text": "Step 2: Accounts Receivable Days\n\nThe firm has \\(\\small\\$100,000\\) in Receivables\nIt had reported Sales of \\(\\small \\$1,000,0000\\)\nThe Accounts Receivable Conversion is then:\n\n\n\\[\n\\small\nACC=\\dfrac{Receivables}{\\dfrac{Sales}{365}}\\rightarrow \\dfrac{100,000}{\\dfrac{1,000,000}{365}}\\rightarrow \\dfrac{100,000}{2,739}\\approx \\text{36.5 days}\n\\]\n\nInterpretation: if the firm had \\(\\small\\$1,000,000\\) in Sales yearly, this translates to a daily average of \\(\\small\\$2,739\\). If there’s \\(\\small100,000\\) in Receivables, it means that it takes, on average, \\(\\small \\$100,000/\\$2,739=36.5\\) days for a given client to pay the firm, after it has bought the product."
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#step-3-accounts-payable-days",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#step-3-accounts-payable-days",
    "title": "Working Capital Management",
    "section": "Step 3: Accounts Payable Days",
    "text": "Step 3: Accounts Payable Days\n\nThe firm has \\(\\small\\$25,000\\) in Payables\nIt had reported COGS of \\(\\small 40\\% \\times \\$1,000,0000=\\$400,000\\)\nThe Accounts Payable Conversion is then:\n\n\n\\[\n\\small\nAPC=\\dfrac{Payables}{\\dfrac{COGS}{365}}\\rightarrow \\dfrac{25,000}{\\dfrac{400,000}{365}}\\rightarrow \\dfrac{25,000}{1,096}\\approx \\text{22.8 days}\n\\]\n\nInterpretation: if the firm had \\(\\small\\$400,000\\) in COGS yearly, this translates to a daily average of \\(\\small\\$1,096\\) If there’s \\(\\small\\$25,000\\) in Payables, it means that it takes, on average, \\(\\small \\$25,000/\\$1,096=22.8\\) days for the firm to pay its suppliers, after it bought the Inventory."
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#step-3-putting-all-together",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#step-3-putting-all-together",
    "title": "Working Capital Management",
    "section": "Step 3: putting all together",
    "text": "Step 3: putting all together\n\nRecall that the Cash Conversion Cycle is defined as:\n\n\n\\[\n\\small CCC= \\text{Accounts Receivable Days}+\\text{Inventory Days}- \\text{Accounts Payable Days}\n\\]\n\nUsing our formula, we have:\n\n\n\n\\[\n\\small CCC= \\text{36.5}+\\text{45.6}- \\text{22.8}\\approx \\text{59.3 days}\n\\]\n\nIt means that, on average, it takes roughly 60 days for the firm to receive from its clients after it paid for its inventories.\n\nIf CCC&gt;0 \\(\\rightarrow\\) the firm needs to invest in Working Capital to be able to pay its obligations before receiving from clients\nIf CCC&lt;0 \\(\\rightarrow\\) the firm receives from clients before it actually had to pay its suppliers"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#changing-the-accounts-payable",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#changing-the-accounts-payable",
    "title": "Working Capital Management",
    "section": "Changing the Accounts Payable",
    "text": "Changing the Accounts Payable\n\nSuppose that the firm has made an arrangement with its suppliers to be able to pay in 40 days. What is the effect of this policy?\n\n\nFirst, we can see that the new CCC is \\(36.5+45.6-40\\approx 42.1\\) days. Why is this important? Because it reduces the net working capital requirement!\nIn order to see that, recall that, on average, the firm pays \\(\\$400,000/365\\approx \\$1,096\\) to its suppliers on a daily basis. Therefore, as there’s an increase from 22.8 \\(\\rightarrow\\) 40 days, the new level of accounts payable is:\n\n\n\\[\n\\small\nAPC=\\dfrac{Payables}{\\dfrac{COGS}{365}}\\rightarrow 40=\\dfrac{Payables}{1,096}\\rightarrow Payables=  43,835\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#changing-the-accounts-payable-continued",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#changing-the-accounts-payable-continued",
    "title": "Working Capital Management",
    "section": "Changing the Accounts Payable, continued",
    "text": "Changing the Accounts Payable, continued\n\nTherefore, the change in the Net Working Capital is simply given by the difference in Accounts Payable:\n\n\n\\[\n\\small\n\\Delta Payables=43,835-25,000=18,835\n\\]\n\nThe company was able to free-up $18,835 of resources that can be either paid off to shareholders as dividends or reinvested in other activities or even financial instruments\nAlso, if we distributed this value, as we’re keeping the Sales numbers constant, but being able to do so with a lower level of assets, we are increasing the firms’s Operating Asset Turnover, and hence, the ROIC"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#practical-example-arezzo-arzz3",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#practical-example-arezzo-arzz3",
    "title": "Working Capital Management",
    "section": "Practical Example, Arezzo (ARZZ3)",
    "text": "Practical Example, Arezzo (ARZZ3)"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#understanding-working-capital-nuances",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#understanding-working-capital-nuances",
    "title": "Working Capital Management",
    "section": "Understanding Working Capital nuances",
    "text": "Understanding Working Capital nuances\n\nWorking capital (or Net Working Capital) is the difference between a firm’s current assets and current liabilities\nIt does not distinguish between operating and financing components:\n\nOperating: Accounts Receivable, Inventory, and Accounts Payable, Cash (excluding non-excess cash), etc\nFinancing components: Excess Cash, Marketable securities, and Notes Payable.\n\nTo what it concerns us the most, Operating Working Capital is the portion that relates to the cash that is needed to run the operating part\nThink, for example, about our last lecture on Economic Value Added - EVA. Ideally, we would like to know the amount of capital that has been invested in the Operation, excluding any financial effects."
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#important-watchout-for-definitions",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#important-watchout-for-definitions",
    "title": "Working Capital Management",
    "section": "Important: watchout for definitions!",
    "text": "Important: watchout for definitions!\n\n\n\n\n\n\n\nImportant\n\n\nIn Chapter 2 of Berk and DeMarzo, Working Capital is defined as Current Assets - Current Liabilities. In Chapter 26, however the authors state that “[…] working capital management involves short-term asset accounts such as cash, inventory, and accounts receivable, as well as short-term liability accounts such as accounts payable”.\n\n\n\n\nWhy do we have these distinctions? It is important to understand that, within working capital, we have Financing and Operating Accounts\nWe can use the operating accounts to calculate the Operating Working Capital, which is formally defined as Current Operating Assets - Current Operating Liabilities\n\n\n\n\\(\\rightarrow\\) Berk and DeMarzo uses Accounts Payable, Receivables, and Inventories (the main components) to calculate the Cash Conversion Cycle, but keep in mind that Operating Working Capital contains all current operating accounts!"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#cash-and-operating-cycle-dynamics-1",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#cash-and-operating-cycle-dynamics-1",
    "title": "Working Capital Management",
    "section": "Cash and Operating Cycle Dynamics",
    "text": "Cash and Operating Cycle Dynamics\n\n\n\n\n\n\n\nAs of now, we defined what Working Capital is and how to measure it\nQuestion: which policies a Financial manager can adopt to manage working capital?"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#working-capital-policies",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#working-capital-policies",
    "title": "Working Capital Management",
    "section": "Working Capital Policies",
    "text": "Working Capital Policies\n\nWe saw that investment in working capital are necessary, but incur in an opportunity cost:\n\nBy increasing inventories, not only there are increasing costs of maintaning a physical structure, but also we are losing the opportunity to invest and earn additional income\nHolding cash as a precautionary motive precludes the firm from being able to earn interest\nHaving a high level of account receivables also precludes the firm from earning interest on the money\n\nBecause of these reasons, aiming to optimize working management entails the creation of policies that seek to:\n\nProvide all the resources that the firm needs in order to run its operations; and\nAt the same time, minimize the (opportunity) costs related to it\n\nIn what follows, we will look at the most common working capital policies, understand how their work, and estimate their effects"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#trade-credit",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#trade-credit",
    "title": "Working Capital Management",
    "section": "Trade Credit",
    "text": "Trade Credit\n\nThe credit that the firm is extending to its customer is known as trade credit\n\nA firm would, of course, prefer to be paid in cash at the time of purchase\nHowever, a “cash-only” policy may cause it to lose its customers to competition\n\nTrade credit is, in essence, a loan from the selling firm to its customer\n\nThe price discount represents an interest rate\nFirms offer favorable interest rates on trade credit as a price discount to their customers\n\nUnderstanding the terminology: what is “2/10, Net 30”? It means that the buying firm will receive a \\(\\small 2\\%\\) discount if it pays for the goods within \\(\\small 10\\) days; otherwise, the full amount is due in \\(\\small 30\\) days\nAs a Financial Manager, it is important to understand that Trade Credit has a cost (or a discount): by taking an offer to pay later, what is the additional cost that the firm needs to incur?"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#calculating-the-cost-of-trade-credit",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#calculating-the-cost-of-trade-credit",
    "title": "Working Capital Management",
    "section": "Calculating the Cost of Trade Credit",
    "text": "Calculating the Cost of Trade Credit\n\nSuppose that a firm purchases goods from its supplier on terms of 5/15, Net 40. What is the effective annual cost to your firm if it chooses not to take advantage of the trade discount offered?\nBecause the discount is \\(\\small5\\%\\), for a \\(\\small\\$100\\) bill, a firm faces the following options:\n\nPay \\(\\small\\$95\\) in \\(\\small 15\\) days; or\nPay \\(\\small\\$100\\) in \\(\\small40\\) days\n\nGiven that the difference is \\(\\small(40-15)=25\\) days, it means that the cost is \\(\\small 100/95-1\\approx 5.26\\%\\) during the period. In order to see this in annual terms, we have:\n\n\n\\[\n\\dfrac{100}{95}\\approx 1.0526\\rightarrow(1.0526)^{365/25}-1\\approx111.5\\%!\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#if-trade-credit-is-costly-why-it-exists",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#if-trade-credit-is-costly-why-it-exists",
    "title": "Working Capital Management",
    "section": "If Trade Credit is costly, why it exists?",
    "text": "If Trade Credit is costly, why it exists?\n\nIn our previous example, by not taking the discount, the firm is effectively paying \\(\\small111.5\\%\\), at an annual rate, to borrow the money.\nIf the firm can obtain a bank loan at a lower interest rate, it would be better off borrowing at the lower rate and using the cash proceeds of the loan to take advantage of the discount offered by the supplier\nBut if that is the case, why we see so much trade credit in the market?\n\n\nIt is simple and convenient \\(\\rightarrow\\) lower transaction costs\nIt is flexible\nSometimes, it is the only funding resource that a firm can obtain\n\n\nTrade Credit can also be seen from a Receivables perspective: although firms would be better-off by adopting a cash-in-advance polocy for its receivables, extending trade credit to its suppliers can be thought of as a strategic move"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#reasons-to-provide-trade-credit",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#reasons-to-provide-trade-credit",
    "title": "Working Capital Management",
    "section": "Reasons to provide Trade Credit",
    "text": "Reasons to provide Trade Credit\n\nScreening different client types: rather than lowering the price for all clients, we may offer specific credit terms that are attractive to customers with bad credit, but unattractive to customers with good credit:\n\nBad credit clients would take the discount since they would not have any better offer elsewhere; and\nGood credit clients would not be interested in taking the discount and would pay the full price\n\nCustomer-Supplier relationships:\n\nSometimes, the supplier may have more information about the customer than the bank has, and thus provide more interesting terms\nIt could also threaten to cut off future supplies\nFinally, seizing the inventory collateral in case of default is likely to be much more valuable to the supplier than the bank"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#float",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#float",
    "title": "Working Capital Management",
    "section": "Float",
    "text": "Float\n\nFloat refers to the length of a firm’s receivables and payables is the delay between the time a bill is paid and the cash is actually received. This delay, or processing float, will impact a firm’s working capital requirements.\nIn the past, Float seemed to be much more relevant due to the alternatives that were in place:\n\nCash\nElectronic Check processing\nTransfer Deposits (in Brazil, DOC and TED)\n\nAs technology disrupts the financial marketplace, float reduced significantly\n\n\nQuestion: what happens with the introduction of PIX?"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#credit-policy",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#credit-policy",
    "title": "Working Capital Management",
    "section": "Credit Policy",
    "text": "Credit Policy\n\nA firm can better manage its receivables by establishing a clear credit policy:\n\n\nEstablishing Credit Standards:\n\nWhat is the criteria to extend credit?\nInternal evaluation versus the use of credit rating agencies\n\nEstablishing Credit Terms:\n\nAfter creating the criteria, what will be the credit terms?\nThis is also highly influenced by competition and industry standards\n\nEstablishing a Collection Policy:\n\nIn case of default, what should the firm do?\nPolicies range from seizing inventory, interest expenses, legal actions\nDebt Collection business (collection agencies)"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#monitoring-accounts-receivable---average-receivable-days",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#monitoring-accounts-receivable---average-receivable-days",
    "title": "Working Capital Management",
    "section": "Monitoring Accounts Receivable - Average Receivable Days",
    "text": "Monitoring Accounts Receivable - Average Receivable Days\n\nAfter establishing a Credit Policy, a firm must monitor its accounts receivable to analyze whether its credit policy is working effectively. There are two common ways of doing so:\nMonitoring the average receivable days: the accounts receivable days is the average number of days that it takes a firm to collect on its sales.\n\nA firm can compare this number to the payment policy specified in its credit terms to judge the effectiveness of its credit policy\nIf the credit terms specify “Net 30” and the accounts receivable days outstanding is \\(\\small 50\\) days, the firm can conclude that its customers are paying \\(\\small20\\) days late, on average\n\nNote that this has the benefit of being feasible only with balance-sheet data. Notwithstanding, it conveys little information about the specificities of the due payments"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#monitoring-accounts-receivable---aging-schedules",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#monitoring-accounts-receivable---aging-schedules",
    "title": "Working Capital Management",
    "section": "Monitoring Accounts Receivable - Aging Schedules",
    "text": "Monitoring Accounts Receivable - Aging Schedules\n\nA way to monitor Accounts Receivable is through Aging Schedules: they provide more detailed information regarding the specifics of due payments:\n\n\n\n\n\n\n\n\nIf, for example, the Credit Policy established \\(\\small 30\\) days, it means that the dollar percentage due is \\(\\small 21.9+12.5+4.4=38.8\\%\\)"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#payables-management",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#payables-management",
    "title": "Working Capital Management",
    "section": "Payables Management",
    "text": "Payables Management\n\nWhat to do with the liabilities that we have with suppliers?\n\nA firm should choose to borrow using accounts payable only if trade credit is the cheapest source of funding\nIf not, a firm can finance its working capital needs via other sources or pay in advance\n\nA firm should strive to keep its money working for it as long as possible without developing a bad relationship with its suppliers or engaging in unethical practices\nIn addition, a firm should always pay on the latest day allowed, considering the option taken:\n\nIf the discount period is \\(\\small 10\\) days and the firm is taking the discount, payment should be made on day \\(\\small 10\\), not on day \\(\\small 2\\)\nIf the discount is not taken and the terms are 2/10, Net 30, the full payment should be made on day \\(\\small 30\\), not on day \\(\\small 16\\)"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#payables-management---accounts-payable-outstanding",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#payables-management---accounts-payable-outstanding",
    "title": "Working Capital Management",
    "section": "Payables Management - Accounts Payable Outstanding",
    "text": "Payables Management - Accounts Payable Outstanding\n\nSimilar to the situation with its accounts receivable, a firm should monitor its accounts payable to ensure that it is making its payments at an optimal time\nOne way is to the accounts payable days outstanding and compare it to the credit terms\nSuppose that a firm has an average accounts payable balance of \\(\\small\\$250,000\\). Its average daily cost of goods sold is \\(\\small \\$14,000\\), and it receives terms of 2/15, Net 40, from its suppliers. The firm chooses to forgo the discount. Is the firm managing its accounts payable well?\n\n\n\nA: the account payable days is \\(\\small 250,000/14,000= 17.9\\) days. There are two cases:\n\nIf the firm made payment three days earlier, it could take advantage of the \\(\\small2\\%\\) discount\nIf for some reason it chooses to forgo the discount, it should not be paying the full amount until the fortieth day\n\nImportant Note: these are average terms and do not refer to specific payments."
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#payables-management---stretching-accounts-payable",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#payables-management---stretching-accounts-payable",
    "title": "Working Capital Management",
    "section": "Payables Management - Stretching Accounts Payable",
    "text": "Payables Management - Stretching Accounts Payable\n\nSome firms ignore the payment due period and pay later, in a practice referred to as stretching the accounts payable.\nFor example, given terms of 2/10, Net 30, for example, a firm may choose to not pay until \\(\\small 45\\) days have passed.\nExample: what is the effective annual cost of credit terms of 1/15, Net 40, if the firm stretches the accounts payable to \\(\\small 60\\) days?\n\n\n\nA: the interest rate per period is \\(\\small 1/99=1.01\\%\\). If the firm delays the payment until the 60th day, it has used the funds for 45 days beyond the agreed period. Therefore, the annual effective rate is \\(\\small (1.0101)^{365/45}-1=8.49\\%\\), as opposed to what has been established before, which is \\(\\small (1.0101)^{365/15}-1=27.39\\%\\)\n\n\nThere may be direct and indirect costs of streching the accounts payable (e.g, retaliation, cash-on delivery and cash-before delivery policies, reduced supply, lower credit ratings)"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#inventory-management",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#inventory-management",
    "title": "Working Capital Management",
    "section": "#3 Inventory Management",
    "text": "#3 Inventory Management\n\nIf holding inventory is costly, why do firms ever keep high levels of inventory?\n\n\nBenefits of holding inventory\n\nIf a firm holds too little inventory, stock-outs, the situation when a firm runs out of goods, may occur, leading to lost sales\nLikewise, disappointed customers may switch to one of the firm’s competitors\nIt might be optimal for firms to hold inventory because of the seasonality in demand\n\nCosts of holding inventory\n\nAcquisition Costs\nOrder Costs\nCarrying Costs\n\n\n\n\\(\\rightarrow\\) For a detailed discussion, see (Berk and DeMarzo 2023), Chapter 26."
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#cash-management",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#cash-management",
    "title": "Working Capital Management",
    "section": "#4 Cash Management",
    "text": "#4 Cash Management\n\nIf firms could perfectly tap into resources at any time needed, there would be no need for holding cash:\n\nFirms could invest \\(\\small 100\\%\\) of its money in operations\nIn case of a mismatch, raise money at a fair rate to make up the differences\n\nOn the one hand, holding cash is costly, as we liquid assets may earn below-average returns\nOn the other hand, firm’s may face a much higher cost if there’s ever need for raising cash quickly (e.g, financial distress periods)\n\n\nBecause of these reasons, firms may hold cash for some specific reasons:\n\nTransactions Balance: day-to-day operations\nPrecautionary Motives: uncertainty regarding the firms cash flows\nCompensating Balance: credit agreements"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#supplementary-material",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#supplementary-material",
    "title": "Working Capital Management",
    "section": "Supplementary Material",
    "text": "Supplementary Material\n\nHarvard Notes\n\nDell’s Working Capital\nTrade Credit\nCredit Analysis Basics\n\nWhat to do with excess cash? See more on the Money Market Industry (in special, the American Money Market Funds)\nA note on The Economist about the Debt Collection industry: access here\n\n\n\\(\\rightarrow\\) All contents are available on eClass®."
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#references",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#references",
    "title": "Working Capital Management",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ."
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#evaluating-cash-flow-streams",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#evaluating-cash-flow-streams",
    "title": "Investment Decision Rules",
    "section": "Evaluating Cash Flow Streams",
    "text": "Evaluating Cash Flow Streams\n\nFinancial Managers are often faced with a decision regarding a stream of cash flows stemming from an investment opportunity\nThere are multiple ways to evaluate an investment opportunity:\n\nCompare the discounted value of the cash-flows among alternatives\nAnalyze the rate of return of the alternatives\nVerify the amount of time that a project takes to repay its investment\n\nAs we’ll see throughout this (and the next) lectures, we’ll compare the net present value rule to other investment rules that firms sometimes use and explain why the net present value rule is superior"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-net-present-value-npv",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-net-present-value-npv",
    "title": "Investment Decision Rules",
    "section": "The Net Present Value (NPV)",
    "text": "The Net Present Value (NPV)\nThe present value of any given stream of cash-flows is given by:\n\\[\nPV= \\dfrac{FV}{(1+i)^n}\n\\]\nwhere \\(FV\\) is the future value of the cash-flows (in periods other than the one in analysis), \\(r\\) is the discount rate, and \\(n\\) is the number of periods.\n\nFor a general case with \\(n\\) periods, we have:\n\n\n\\[\nPV= \\dfrac{FC_1}{(1+r)^1}+ \\dfrac{FC_2}{(1+r)^2}+...+\\dfrac{FC_t}{(1+r)^n} \\equiv \\sum_{t=0}^{t=n}\\dfrac{FC_t}{(1+r)^t}\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-net-present-value-npv-rule",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-net-present-value-npv-rule",
    "title": "Investment Decision Rules",
    "section": "The Net Present Value (NPV) rule",
    "text": "The Net Present Value (NPV) rule\n\nWe begin our discussion of investment decision rules by considering a take-it-or-leave-it decision involving a single, stand-alone project.\nNPV Investment Rule: When making an investment decision, take the alternative with the highest NPV. Choosing this alternative is equivalent to receiving its NPV in cash today:\n\nIn the case of a stand-alone project, we must choose between accepting or rejecting the project.\nTherefore, the NPV rule then says we should compare the project’s NPV to zero (the NPV of doing nothing) and accept the project if its NPV is positive\n\nLet’s use this rationable to analyze how the NPV rule applies to the Cia Amazônia case. For these exercises, we’ll assume a discount rate (i.e, a cost of capital) of 15%."
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#applying-net-present-value-npv-rule",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#applying-net-present-value-npv-rule",
    "title": "Investment Decision Rules",
    "section": "Applying Net Present Value (NPV) rule",
    "text": "Applying Net Present Value (NPV) rule\n\nWe’ll work on a case from Cia Amazonia in the following lectures. For now, assume that its cash-flows were:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear 0\nYear 1\nYear2\nYear3\nYear 4\nYear 5\n\n\n\n\nFree Cash Flow\n-$219,600\n$46,592\n$69,266\n$80,218\n$101,293\n$130,684\n\n\n\n\nTherefore, the NPV is given by:\n\n\n\n\\[\n\\small\nNPV= \\dfrac{-219,000}{(1+15\\%)^0}+\\dfrac{46,592}{(1+15\\%)^1}+\\dfrac{69,266}{(1+15\\%)^2}+\\dfrac{80,218}{(1+15\\%)^3}+\\dfrac{101,293}{(1+15\\%)^4}+\\dfrac{130,684}{(1+15\\%)^5}=48,922.22\n\\] Caution: in Microsoft Excel’s NPV formula, the first period is, by default, \\(t=1\\), and not \\(t=0\\)"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#bridging-npv-and-irr",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#bridging-npv-and-irr",
    "title": "Investment Decision Rules",
    "section": "Bridging NPV and IRR",
    "text": "Bridging NPV and IRR\n\nThe NPV of a project depends on the appropriate cost of capital.\nThere may be some uncertainty regarding the project’s cost of capital. Therefore, it is helpful to compute an NPV profile: a graph of the project’s NPV over a range of discount rates, \\(r\\):\n\nFor each \\(r\\) in a sequence of discount rates, compute the \\(NPV\\) of the project using the NPV formula\nStore the results\nPlot it in a chart using all the different \\(r\\) in the x-axis and the estimated NPV in the y-axis\n\nAs you’ll see in the next slide, it seems that for \\(\\small r=22.61\\%\\), the NPV of the project is exactly zero: this is the Internal Rate of Return (IRR) of the project!\nThe IRR of a project provides useful information regarding the sensitivity of the project’s NPV to errors in the estimate of its cost of capital"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#npv-profile-for-cia-amazônia",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#npv-profile-for-cia-amazônia",
    "title": "Investment Decision Rules",
    "section": "NPV Profile for Cia Amazônia",
    "text": "NPV Profile for Cia Amazônia"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#analyzing-the-npv-rule",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#analyzing-the-npv-rule",
    "title": "Investment Decision Rules",
    "section": "Analyzing the NPV rule",
    "text": "Analyzing the NPV rule\n\nLooking at the results, we can see that:\n\n\nFor any value of \\(\\small r\\) that is lower than \\(\\small r=22.6%\\), the NPV of the project is positive\nFor any value of \\(\\small r\\) that is greater than \\(\\small r=22.6%\\), the NPV of the project is negative\n\n\nFrom a practical perspective, the decision to accept the project is correct as long as our estimate of \\(\\small r=15%\\) is within \\(\\small 22.6\\%-15\\%=7.6\\%\\) of the true cost of capital\nIn general, the difference between the cost of capital and the IRR is the maximum estimation error in the cost of capital that can exist without altering the original decision.\nBut wait…how do we know that \\(\\small r=22.6\\%\\) is the value that sets the NPV exactly to zero?\nThis value is also known as the internal rate of return, and it is defined as the \\(\\small r\\) that satisfies \\(\\small NPV = 0\\)"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-internal-rate-of-return-irr",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-internal-rate-of-return-irr",
    "title": "Investment Decision Rules",
    "section": "The Internal Rate of Return (IRR)",
    "text": "The Internal Rate of Return (IRR)\n\nFormally, the IRR and it is defined as the \\(\\small r\\) that satisfies:\n\n\n\\[\n\\small\nr_{IRR} = r \\text{ such that } \\sum_{t=0}^{t=n}\\dfrac{FC_t}{(1+r)}=0\n\\]\n\nApplying this concept to our case, we have:\n\n\n\n\\[\n\\small\n0= \\dfrac{-219,000}{(1+i)^0}+\\dfrac{46,592}{(1+i)^1}+\\dfrac{69,266}{(1+i)^2}+\\dfrac{80,218}{(1+i)^3}+\\dfrac{101,293}{(1+i)^4}+\\dfrac{130,684}{(1+i)^5}\n\\]\n\n\n\\[\n\\small\n\\rightarrow \\dfrac{46,592}{(1+i)^1}+\\dfrac{69,266}{(1+i)^2}+\\dfrac{80,218}{(1+i)^3}+\\dfrac{101,293}{(1+i)^4}+\\dfrac{130,684}{(1+i)^5} = 219,000\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-internal-rate-of-return-irr-1",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-internal-rate-of-return-irr-1",
    "title": "Investment Decision Rules",
    "section": "The Internal Rate of Return (IRR)",
    "text": "The Internal Rate of Return (IRR)\n\nSetting \\(a={(1+i)^{-1}}\\), we have:\n\n\n\\[\n\\small\n(46,592\\times a) + (69,266\\times a^2) + (80,218\\times a^3) + (101,293\\times a^4) + (130,684\\times a^5) -219,000 = 0\n\\]\n\nTheoretically, this is a \\(5th\\)-order polynomial, and we need to find \\(\\small a=\\dfrac{1}{(1+i)}\\) that satisfies this equation.\nIn practice, we generally do this in Excel by using:\n\nUse the \\(IRR(\\cdot)\\) (or \\(TIR(\\cdot)\\)) formula\nGoal-seek functions: set the sum of the discounted cash-flows to zero by varying the discount rate, \\(r\\)."
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-irr-rule",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-irr-rule",
    "title": "Investment Decision Rules",
    "section": "The IRR rule",
    "text": "The IRR rule\n\nIRR Investment Rule: Take any investment opportunity where the IRR exceeds the opportunity cost of capital. Turn down any opportunity whose IRR is less than the opportunity cost of capital:\n\nOne interpretation of the internal rate of return is the average return earned by taking on the investment opportunity\nIf the average return on the investment opportunity (i.e., the IRR) is greater than the return on other alternatives in the market with equivalent risk and maturity (i.e., the project’s cost of capital), one should undertake the investment opportunity\n\nCaution: the IRR investment rule will give the correct answer (that is, the same answer as the NPV rule) in many—but not all situations\nThe IRR rule is only guaranteed to work for a stand-alone project if all of the project’s negative cash flows precede its positive cash flows\nIn what follows, we’ll review the many IRR pitfalls"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-pitfall-1-delayed-investments",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-pitfall-1-delayed-investments",
    "title": "Investment Decision Rules",
    "section": "IRR Pitfall #1: delayed investments",
    "text": "IRR Pitfall #1: delayed investments\n\nSay that you have the following cash-flow streams:\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear 0\nYear 1\nYear2\nYear3\n\n\n\n\nFree Cash Flow\n$1,000,000\n-$500,000\n-$500,000\n-$500,000\n\n\n\n\nTherefore, the \\(\\small IRR\\) is found by setting when:\n\n\n\n\\[\n\\small\nNPV=0 \\rightarrow +1,000,000-\\dfrac{500,000}{(1+i)^1}-\\dfrac{500,000}{(1+i)^2}-\\dfrac{500,000}{(1+i)^3}=0\n\\]\n\nIf you use the \\(\\small IRR(\\cdot)\\) function in Excel, you’ll find that \\(\\small r\\approx0.23\\). However, if you calculate the \\(\\small NPV\\) using \\(\\small r=10\\%\\), you’ll find a negative value."
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#npv-profile-when-inflows-precede-outflows",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#npv-profile-when-inflows-precede-outflows",
    "title": "Investment Decision Rules",
    "section": "NPV profile when inflows precede outflows",
    "text": "NPV profile when inflows precede outflows"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-pitfall-1-delayed-investments-1",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-pitfall-1-delayed-investments-1",
    "title": "Investment Decision Rules",
    "section": "IRR Pitfall #1: delayed investments",
    "text": "IRR Pitfall #1: delayed investments\n\nAt a \\(\\small 10\\%\\) discount rate, the NPV is negative, so signing the deal would destroy value. Why?\nFor most investment projects, expenses occur initially and cash is received later. In this case, the project gets cash upfront and incurs the costs later\n\nIt is like we are receiving cash today in exchange for a future liability — and when you borrow money you prefer as low a rate as possible\nIn this case, the IRR is best interpreted as the rate that we are paying rather than earning\n\n\n\n\\(\\rightarrow\\) The optimal rule is to borrow money so long as this rate is less than his cost of capital!"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-pitfall-2-multiple-irr-solutions",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-pitfall-2-multiple-irr-solutions",
    "title": "Investment Decision Rules",
    "section": "IRR Pitfall #2: multiple IRR solutions",
    "text": "IRR Pitfall #2: multiple IRR solutions\n\nSay that you have the following cash-flow streams:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear 0\nYear 1\nYear2\nYear3\nYear4\n\n\n\n\nFree Cash Flow\n$550,000\n-$500,000\n-$500,000\n-$500,000\n+$1,000,000\n\n\n\n\nTherefore, the \\(\\small IRR\\) is found by setting when:\n\n\n\n\\[\n\\small\nNPV=0 \\rightarrow +550,000-\\dfrac{500,000}{(1+i)^1}-\\dfrac{500,000}{(1+i)^2}-\\dfrac{500,000}{(1+i)^3}+\\dfrac{1,000,000}{(1+i)^4}=0\n\\]\n\nAs it is described in the next slide, when there is more than a one-time change in the sign of the cash-flows, the IRR will yield multiple solutions!\nIn cases like this, the NPV rule should be used."
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#npv-profile-when-there-are-multiple-changes-in-the-sign-of-cash-flows",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#npv-profile-when-there-are-multiple-changes-in-the-sign-of-cash-flows",
    "title": "Investment Decision Rules",
    "section": "NPV profile when there are multiple changes in the sign of cash-flows",
    "text": "NPV profile when there are multiple changes in the sign of cash-flows"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-pitfall-2-multiple-irr-solutions-1",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-pitfall-2-multiple-irr-solutions-1",
    "title": "Investment Decision Rules",
    "section": "IRR Pitfall #2: multiple IRR solutions",
    "text": "IRR Pitfall #2: multiple IRR solutions\n\nIn this case, there are two IRRs — that is, there are two values of \\(\\small r\\) that set the NPV equal to zero: \\(\\small7.16\\%\\) and \\(\\small33.67\\%\\). Because there is more than one IRR, we cannot apply the IRR rule\n\nIf \\(r\\leq \\small7.16\\%\\) or \\(r\\geq\\small33.67\\%\\), we should undertake the opportunity\nOtherwise, he should turn it down\n\nNotice that even though the IRR rule fails in this case, the two IRRs are still useful as bounds on the cost of capital:\n\nIf the cost of capital estimate is wrong, and it is actually smaller than \\(\\small7.16\\%\\) or larger than \\(\\small33.67\\%\\), the decision not to pursue the project will change\nEven if we are uncertain whether the actual cost of capital is \\(\\small10\\%\\), as long as he believes it is within these bounds, he can have a high degree of confidence in the decision to reject the deal!"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#pitfall-3-non-existent-irr",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#pitfall-3-non-existent-irr",
    "title": "Investment Decision Rules",
    "section": "Pitfall #3: non-existent IRR",
    "text": "Pitfall #3: non-existent IRR\n\nSay that you have the following cash-flow streams:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear 0\nYear 1\nYear2\nYear3\nYear4\n\n\n\n\nFree Cash Flow\n$750,000\n-$500,000\n-$500,000\n-$500,000\n+$1,000,000\n\n\n\n\nTherefore, the \\(\\small IRR\\) is found by setting when:\n\n\n\n\\[\n\\small\nNPV=0 \\rightarrow +750,000-\\dfrac{500,000}{(1+i)^1}-\\dfrac{500,000}{(1+i)^2}-\\dfrac{500,000}{(1+i)^3}+\\dfrac{1,000,000}{(1+i)^4}=0\n\\]\n\nIf you try to do this in excel, you’ll see that there is no \\(\\small r\\) that satisfies the IRR rule - in words, \\(\\small NPV&gt;0\\) for any \\(r\\geq0\\)!\nIn cases like this, the NPV rule should be used."
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#npv-profile-when-there-is-no-irr-that-satisfies",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#npv-profile-when-there-is-no-irr-that-satisfies",
    "title": "Investment Decision Rules",
    "section": "NPV profile when there is no IRR that satisfies",
    "text": "NPV profile when there is no IRR that satisfies"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-rule-overall-thoughts",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-rule-overall-thoughts",
    "title": "Investment Decision Rules",
    "section": "IRR rule: overall thoughts",
    "text": "IRR rule: overall thoughts\n\nGiven the mathematical caveats of the IRR rule expression, one needs to be cautious when using it to evaluate a project (or compare across projects):\n\nIf a project has positive cash flows that precede negative ones, it is important to look at the project’s NPV profile in order to interpret the IRR\nIf there is more than a one-time-change in the sign of the cash-flow streams (say, for example, - + + + -), there will be a case of multiple IRRs that satisfy the \\(\\small NPV=0\\) condition!\nDepending on the magnitude of the cash-flows, there may be no \\(r\\) that satisfies \\(\\small NPV=0\\)\nEven if these caveats are not applicable, the IRR rule will yield the same solution as of the NPV rule (i.e, accept/reject, or indicate the rank the best investment projects)\nFinally, the IRR has also an implicit reinvestment assumption - see the Appendix for a detailed discussion\n\nBecause of these reasons, it is always recommended to use the NPV rule!"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#comparing-irrs-for-multiple-projects-practice",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#comparing-irrs-for-multiple-projects-practice",
    "title": "Investment Decision Rules",
    "section": "Comparing IRRs for multiple projects: practice",
    "text": "Comparing IRRs for multiple projects: practice\n\nThe complications around the use of IRR as a general rule for investment decisions are clearer when, instead of comparing a single project to an outside option of not investment at all, we have multiple potential projects:\n\n\n\n\n\nProject\nYear 0\nYear 1\nYear 2\n\n\n\n\nA\n-$375\n-$300\n$900\n\n\nB\n-$22,222\n$50,000\n-$28,000\n\n\nC\n$400\n$400\n-$1,056\n\n\nD\n-$4,300\n$10,000\n-$6,000\n\n\n\nQuestion: Which of these projects have an IRR close to 20%? For which of these projects does the IRR rule provide the correct decision? For this, we’ll analyze an NPV profile for each of these projects - see the example in (Berk and DeMarzo 2023)"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#comparing-irrs-for-multiple-projects",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#comparing-irrs-for-multiple-projects",
    "title": "Investment Decision Rules",
    "section": "Comparing IRRs for multiple projects",
    "text": "Comparing IRRs for multiple projects"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-payback-rule",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-payback-rule",
    "title": "Investment Decision Rules",
    "section": "The Payback rule",
    "text": "The Payback rule\n\nManagers are often concerned about the length of time, considering the initial investment, by which a given investment is going to pay off in terms of free cash-flows\nThe payback investment rule states that you should only accept a project if its cash flows pay back its initial investment within a prespecified period\nTo apply the payback rule, you first calculate the amount of time it takes to pay back the initial investment, called the payback period\nWith that, the payback rule:\n\n\nAccept the project if the payback period is less than or equal to a pre-specified length of time\nReject otherwise"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#payback-rule-cia-amazônia-example",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#payback-rule-cia-amazônia-example",
    "title": "Investment Decision Rules",
    "section": "Payback rule, Cia Amazônia example",
    "text": "Payback rule, Cia Amazônia example\n\nRecall that our free-cash flow analysis for Cia Amazônia yields the following results:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear 0\nYear 1\nYear 2\nYear 3\nYear 4\nYear 5\n\n\n\n\nFCF\n-$219,600\n$46,592\n$69,266\n$80,218\n$101,293\n$130,684\n\n\n\n\nWith that, for each period, we calculate the cumulative sum of the cash-flows:\n\nYear 1: \\(\\small -219,600+46,592 =\\) -173,008\nYear 2: \\(\\small -173,008+69,266 =\\) -103,742\nYear 3: \\(\\small -103,742+80,218 =\\) -23,524\nYear 4: \\(\\small -23,524+101,293 =\\) +77,769\n\nAs we can see, the payback occurs in Year 4, because it is the first year when the cumulative cash-flows from the project (considering the investment) are positive\nIf we were to set a payback period of, for example, 4 (four) years, in this case, we would accept the project"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#payback-pitfalls",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#payback-pitfalls",
    "title": "Investment Decision Rules",
    "section": "Payback pitfalls",
    "text": "Payback pitfalls\n\nThe payback rule is arguably the easiest rule that one can have in order to decide on whether to accept or reject a project\nNaturally, it comes with a series of pitfalls that may hinder its straightforward application\nIn what follows, we’ll detail some of the most important ones that make the case for the use of NPV as our “ground truth”:\n\n\nThe Payback ignores the time value of money and the project’s cost of capital\nIt also ignores the cash-flow realizations after the payback period\nFinally, it relies on an ad-hoc decision around the payback period threshold"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#payback-pitfall-1",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#payback-pitfall-1",
    "title": "Investment Decision Rules",
    "section": "Payback pitfall #1",
    "text": "Payback pitfall #1\nPitfall #1: the payback rule ignores the time value of money and the project’s cost of capital\n\n\nConsider the Cia Amazônia project:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 1\nYear 0\nYear 1\nYear 2\nYear 3\nYear 4\nYear 5\n\n\n\n\nFCF\n-$219,600\n$46,592\n$69,266\n$80,218\n$101,293\n$130,684\n\n\n\n\n\n\nThe traditional payback rule is calculated based of a simple sum of the cash-flows, which totally abstract away from differences in the time value of money and risk!\nA way to overcome that would be to use the discounted payback, which sums over the cumulative discounted cash-flows. Assuming a 15% discount rate:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 1\nYear 0\nYear 1\nYear 2\nYear3\nYear 4\nYear 5\n\n\n\n\nFCF\n-$219,600\n$40,514\n$52,375\n$52,744\n$57,914\n$64,973"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#payback-pitfall-1-continued",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#payback-pitfall-1-continued",
    "title": "Investment Decision Rules",
    "section": "Payback pitfall #1 (continued)",
    "text": "Payback pitfall #1 (continued)\n\n\n\n\n\n\n\n\n\n\n\n\nProject 1\nYear 0\nYear 1\nYear 2\nYear 3\nYear 4\nYear 5\n\n\n\n\nFCF\n-$219,600\n$40,514\n$52,375\n$52,744\n$57,914\n$64,973\n\n\n\n\n\nThe payback rule using the discounted flow then yields:\n\nYear 1: \\(\\small -219,600+40,514 =\\) -179,085\nYear 2: \\(\\small -179,085+52,375 =\\) -126,710\nYear 3: \\(\\small -126,710+52,744 =\\) -73,965\nYear 4: \\(\\small -73,965+57,914 =\\) -16,050\nYear 5: \\(\\small -16,050+64,973 =\\) +48,922\n\nTherefore, using the discounted cash-flows, the payback period is actually 5 (five), and not 4 (four) years. If our payback period threshold was 4 years, we would reject the project!"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#payback-pitfall-2",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#payback-pitfall-2",
    "title": "Investment Decision Rules",
    "section": "Payback pitfall #2",
    "text": "Payback pitfall #2\nPitfall #2: the payback rule ignores the cash-flow realizations after the payback period\n\n\nWe saw that using the discounted cash-flows from the project could address the first pitfall. Notwithstanding, there is still an issue with the way that we look at the cash-flows\nIn order to see that, consider two variations of projects:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 1\nYear 0\nYear 1\nYear2\nYear3\nYear 4\nYear 5\n\n\n\n\nFCF\n-$219,600\n$46,592\n$69,266\n$80,218\n$101,293\n$130,684\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 2\nYear 0\nYear 1\nYear2\nYear3\nYear 4\nYear 5\n\n\n\n\nFCF\n-$219,600\n$46,592\n$69,266\n$80,218\n$150,000\n$500,000\n\n\n\n\nIf you were to calculate the payback for these two projects, it would yield the same result (4 years), but Project 2 generates substantially higher free cash flows in the subsequent periods!\nAs the payback rule is just concerned around the timing of cash-flows, it ignores these differences"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#payback-pitfall-3",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#payback-pitfall-3",
    "title": "Investment Decision Rules",
    "section": "Payback pitfall #3",
    "text": "Payback pitfall #3\nPitfall #3: it relies on an ad-hoc decision around the payback period threshold\n\nIn our example, we’ve set a payback period of 4 years, in such a way that:\n\nIf the payback of the project was equal to or less than 4 years, we would accept the project\nIf the payback of the project was higher than 4 years, we would reject the project\n\nWhy did the come up with 4 years to begin with?\nNote that the choice of a given threshold for the payback rule might abstract away from good projects that have long-term returns:\n\nInnovative and disruptive industries may have NPV&gt;0 projects where the payback period is in the long-term\nTheoretically, if project owners within a company know that the payback rule is applied, they may have incentives to exacerbate short-term gains in spite of long-term value!"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#wrapping-up-on-payback",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#wrapping-up-on-payback",
    "title": "Investment Decision Rules",
    "section": "Wrapping up on payback",
    "text": "Wrapping up on payback\n\nQuestion: if the payback rule has so many caveats, why do we even bother using it?\nDespite these failings, along with the IRR, the payback rule is widespread in the context of business decision-making\nWhy? The answer probably relates to its simplicity:\n\nThis rule is typically used for small investment decisions\nThe payback rule also provides budgeting information regarding the length of time capital will be committed to a project\nFinally, if the required payback period is short (one or two years), then most projects that satisfy the payback rule will have a positive NPV\n\nAs a consequence, firms might save effort by first applying the payback rule, and only if it fails take the time to analyze differences in NPV"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#choosing-between-projects",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#choosing-between-projects",
    "title": "Investment Decision Rules",
    "section": "Choosing between projects",
    "text": "Choosing between projects\n\nSo far, we were concerned about making a decision around a binary outcome: we either accepted or rejected a stand-alone project\nWhat if we must choose one project from a much higher set of projects?\nFor example, a manager may be evaluating alternative package designs for a new product. When choosing any one project excludes us from taking the others, we are facing mutually exclusive investments\nFor cases like this, we can still stick with the NPV rule:\n\n\nNPV Decision rule for mutually exclusive projects: because the NPV expresses the value of the project in terms of cash today, picking the project with the highest NPV leads to the greatest increase in wealth. Therefore, you should pick the project with the highest NPV"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#choosing-between-projects-the-npv-rule",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#choosing-between-projects-the-npv-rule",
    "title": "Investment Decision Rules",
    "section": "Choosing between projects, the NPV rule",
    "text": "Choosing between projects, the NPV rule\nA small commercial property is for sale near your university. Given its location, you believe a student-oriented business would be very successful there. You have researched several possibilities and come up with the following cash flow estimates (including the cost of purchasing the property). Which investment should you choose?\n\n\n\n\n\n\n\n\n\n\n\nProject\nInvestment\nFirst Year Cash-Flow\nGrowth Rate\nCost of Capital\n\n\n\n\nBook Store\n$300,000\n$63,000\n3.00%\n8%\n\n\nCoffee Shop\n$400,000\n$80,000\n3.00%\n8%\n\n\nMusic Store\n$400,000\n$104,000\n0.00%\n8%\n\n\nElectronics Store\n$400,000\n$100,000\n3.00%\n11%"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#choosing-between-projects-the-npv-rule-1",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#choosing-between-projects-the-npv-rule-1",
    "title": "Investment Decision Rules",
    "section": "Choosing between projects, the NPV rule",
    "text": "Choosing between projects, the NPV rule\n\nCalculating the NPV for each project assuming a constant growth in perpetuity:\n\n\nBook Store: \\(\\small -300,000 + \\dfrac{63,000}{8\\%-3\\%}=960,000\\)\nCoffee Shop: \\(\\small -400,000 + \\dfrac{80,000}{8\\%-3\\%}=1,200,000\\)\nMusic Store: \\(\\small -400,000 + \\dfrac{104,000}{8\\%}=900,000\\)\nElectronics Store: \\(\\small -400,000 + \\dfrac{100,000}{11\\%-3\\%}=850,000\\)\n\n\nThus, all of the alternatives have a positive NPV. But, because we can only choose one, the coffee shop is the best alternative."
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#comments-on-the-npv-rule-for-mutually-exclusive-projects",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#comments-on-the-npv-rule-for-mutually-exclusive-projects",
    "title": "Investment Decision Rules",
    "section": "Comments on the NPV rule for mutually exclusive projects",
    "text": "Comments on the NPV rule for mutually exclusive projects\n\nAs we saw before, the NPV rule should also be considered the ground-truth for establishing a decision among mutually exclusive projects\nImportantly, different investment projects being considered can vary across several dimensions:\n\n\nInvestment levels\nCash-flow profiles\nCost of Capital (which is inherently tied to the riskiness of the project)\n\n\nBecause of this, the NPV rule provides flexibility when assessing a rank-order of investment projects that vary across several dimensions"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-and-mutually-exclusive-projects",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-and-mutually-exclusive-projects",
    "title": "Investment Decision Rules",
    "section": "IRR and mutually exclusive projects",
    "text": "IRR and mutually exclusive projects\n\nOne might be tempted to use the IRR rule to assess mutually exclusive projects when the assumptions of the IRR rule about the cash-flow profiles are satisfied\nAs a consequence, extending the IRR investment rule to the case of mutually exclusive projects would imply picking the project with the highest IRR\nUnfortunately, one cannot confidently use the IRR rule to choose among different projects whenever the alternatives differ in terms of:\n\n\nThe scale of investment\nThe timing of the cashflows\nTheir riskiness\n\n\nIn what follows, we’ll redo our exercise to consider these three caveats"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-caveat-1-scale-of-investment",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-caveat-1-scale-of-investment",
    "title": "Investment Decision Rules",
    "section": "IRR caveat #1: scale of investment",
    "text": "IRR caveat #1: scale of investment\n\nProblem: whenever projects differ in terms of scale of investment, simply contrasting the IRRs may not provide the correct answers\nBecause the IRR is a return metric, you really cannot tell how much monetary value will actually be created without knowing the scale of the investment!\n\n\nBook Store: \\(\\small -300,000 + \\dfrac{63,000}{IRR-3\\%}\\rightarrow IRR = 24\\%\\)\nCoffee Shop: \\(\\small -400,000 + \\dfrac{80,000}{IRR-3\\%}\\rightarrow IRR = 23\\%\\)\n\n\nBook Store has a higher IRR. However, Coffee Shop has a higher scale of investment level (\\(\\small \\$400,000\\) vs. \\(\\small \\$300,000\\)), generating a higher NPV!\nWhat if we increase the size of the smaller project? One needs to think if this possibility is plausible from a business perspective and, if that is the case, if the IRR can be maintained (e.g, market saturation)"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-caveat-2-timing-of-cashflows",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-caveat-2-timing-of-cashflows",
    "title": "Investment Decision Rules",
    "section": "IRR caveat #2: timing of cashflows",
    "text": "IRR caveat #2: timing of cashflows\n\nProblem: even when projects have the same scale, the IRR may lead you to rank them incorrectly due to differences in the timing of the cash flows\nEarning a very high annual return is much more valuable if you earn it for several years than if you earn it for only a few days\nBecause the IRR expresses the average compensation at a given interval, it is insensitive to how many periods these returns may actually be realized!\nFor example, consider the following projects:\n\n\nInvest \\(\\small \\$100\\) today, earn \\(\\small\\$150\\) in one year\nInvest \\(\\small\\)$100$ today, earn \\(\\small (1.5^5)\\times100=759.4\\) in \\(\\small 5\\) years\n\n\nThe IRR for both cases is \\(\\small 50\\%\\), but the second one has a much higher monetary value because of its capacity to generate a \\(\\small 50\\%\\) over a higher horizon!"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-caveat-3-riskiness-of-projects",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-caveat-3-riskiness-of-projects",
    "title": "Investment Decision Rules",
    "section": "IRR caveat #3: riskiness of projects",
    "text": "IRR caveat #3: riskiness of projects\n\nTo know whether the IRR of a project is attractive, we must compare it to the project’s cost of capital, which is determined by the project’s risk\nThus, an IRR that is attractive for a safe project need not be attractive for a risky project\nIn order to see that, consider the last project of the previous exercise (i.e, Electronics Store):\n\n\n\\[\n\\small -400,000 + \\dfrac{100,000}{11\\%-3\\%}=850,000\n\\]\n\nElectronics Store is the project with the lowest NPV, even with higher cash-flows over time, because its cost of capital is higher!\nDespite having a higher IRR, it is not sufficiently profitable to be as attractive as the safer alternatives"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#choosing-projects-with-resource-constraints",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#choosing-projects-with-resource-constraints",
    "title": "Investment Decision Rules",
    "section": "Choosing projects with resource constraints",
    "text": "Choosing projects with resource constraints\n\nIn principle, the firm should take on all positive-NPV investments it can identify…\nIn practice, there are often limitations on the number of projects the firm can undertake\n\n\nWhen projects are mutually exclusive, the firm can only take on one of the projects even if many of them are attractive\nSo far, we’ve assumed that all projects have similar effects on the resource constraint. In the book-store example, all potential projects fully exhaust the resource constraint (i.e, there is only one property available that will be 100% used for the chosen project)\nWhat if different projects demand different amounts of a particular scarce resource? For example, different products may consume different proportions of a firm’s production capacity, or might demand different amounts of managerial time and attention"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#choosing-projects-with-resource-constraints-continued",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#choosing-projects-with-resource-constraints-continued",
    "title": "Investment Decision Rules",
    "section": "Choosing projects with resource constraints, continued",
    "text": "Choosing projects with resource constraints, continued\n\nConsider the following list of projects:\n\n\n\n\n\nProject\nNPV $\nInvestment\n\n\n\n\nI\n110\n100\n\n\nII\n70\n50\n\n\nIII\n60\n50\n\n\n\n\nIf there were no budget constraints, we would invest in all three projects since they all have \\(\\small NPV&gt;0\\)\nIf now you have a budget constraint of \\(\\small \\$100\\), how to maximize the gains from the investment?\n\n\nNote that although project I has the highest NPV, it consumes all the budget constraint, making impossible to invest in projects II and III"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-profitability-index",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-profitability-index",
    "title": "Investment Decision Rules",
    "section": "The Profitability Index",
    "text": "The Profitability Index\n\nOne can identify projects that efficiently generate NPV by using the profitability index:\n\n\n\\[\n\\small \\text{Profitability Index}=\\dfrac{\\text{NPV Generated}}{\\text{Resource Consumed}}\n\\]\n\nApplying this to our example, we have:\n\n\n\n\n\n\nProject\nNPV $\nInvestment\nProfitability Index\n\n\n\n\nI\n110\n100\n1.1\n\n\nII\n70\n50\n1.4\n\n\nIII\n60\n50\n1.2\n\n\n\n\nStarting with the project with the highest index, we move down the ranking, taking all projects until the resource is consumed - in this case, taking projects \\(\\small II\\) and \\(\\small III\\) yields a combined NPV of \\(\\small 70+60=130\\)"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#profitability-index-practical-example",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#profitability-index-practical-example",
    "title": "Investment Decision Rules",
    "section": "Profitability Index: practical example",
    "text": "Profitability Index: practical example\nNetIt, a large networking company, has put together a project proposal to develop a new home networking router. The expected NPV of the project is $17.7 million, and the project will require 50 software engineers. NetIt has a total of 190 engineers available and there are competing projects:\n\n\n\nProject\nNPV\nEngineering Headcount\n\n\n\n\nRouter\n17.7\n50\n\n\nProject A\n22.7\n47\n\n\nProject B\n8.1\n44\n\n\nProject C\n14\n40\n\n\nProject D\n11.5\n61\n\n\nProject E\n20.6\n58\n\n\nProject F\n12.9\n32\n\n\nTotal\n107.5\n332"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#profitability-index-practical-example-continued",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#profitability-index-practical-example-continued",
    "title": "Investment Decision Rules",
    "section": "Profitability Index: practical example (continued)",
    "text": "Profitability Index: practical example (continued)\n\nHow should NetIt prioritize these projects? We can calculate the Profitability Index, rank the projects from highest-to-lowest, and choose the projects up to a point where all the resources are exhausted:\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject\nNPV\nEng. Count\nProf. Index\nRank\nConsumed\n\n\n\n\nProject A\n22.7\n47\n0.4830\n1\n47\n\n\nProject F\n12.9\n32\n0.4031\n2\n79\n\n\nProject E\n20.6\n58\n0.3552\n3\n137\n\n\nRouter\n17.7\n50\n0.3540\n4\n187\n\n\nProject C\n14\n40\n0.3500\n5\n-\n\n\nProjectD\n11.5\n61\n0.1885\n6\n-\n\n\nProject B\n8.1\n44\n0.1841\n7\n-"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#profitability-index-practical-example-continued-1",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#profitability-index-practical-example-continued-1",
    "title": "Investment Decision Rules",
    "section": "Profitability Index: practical example (continued)",
    "text": "Profitability Index: practical example (continued)\n\nRanking the projects from highest-to-lowest, we have:\n\nProject A, with \\(\\small NPV= 22.7\\)\nProject F, with \\(\\small NPV= 12.9\\)\nProject E, with \\(\\small NPV= 20.6\\)\nRouter, with \\(\\small NPV= 17.7\\)\n\nTherefore, the combined NPV is \\(\\small 22.7+12.9+20.6+17.7=73.9\\). There is no other combination of projects that will create more value without using more engineers than we have\nNote, however, that the resource constraint forces NetIt to forgo three otherwise valuable projects (C, D, and B) with a total NPV of \\(\\small \\$33.6\\) million"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#shortcomings-of-the-profitability-index",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#shortcomings-of-the-profitability-index",
    "title": "Investment Decision Rules",
    "section": "Shortcomings of the Profitability Index",
    "text": "Shortcomings of the Profitability Index\n\nAlthough the profitability index is simple to compute and use, for it to be completely reliable, two conditions must be satisfied:\n\nThe set of projects taken following the profitability index ranking completely exhausts the available resource\n\n\nSay that we have a new project, with \\(\\small NPV=120,000\\) that uses \\(\\small 3\\) engineers\nIts index is \\(\\small 0.12/3=0.04\\), the lowest from all the projects\nHowever, because there are still \\(\\small 3\\) engineers available, we could pursue it!\n\n\nThere is only a single relevant resource constraint\n\n\nIn practice, there are more scarce resources that need to be considered altogether: budget limitations, production inputs, physical space, etc\nFor dealing with a multiple resource problem, we can use linear programming techniques through Solver and/or other softwares"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#supplementary-reading",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#supplementary-reading",
    "title": "Investment Decision Rules",
    "section": "Supplementary Reading",
    "text": "Supplementary Reading\n\nSee Present Value: A Note on Personal Applications (UV5136) for a detailed discussion on how the diffent applications of the Net Present Value rule\n\n\n\\(\\rightarrow\\) All contents are available on eClass®."
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-and-the-reinvestment-assumption",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-and-the-reinvestment-assumption",
    "title": "Investment Decision Rules",
    "section": "IRR and the reinvestment assumption",
    "text": "IRR and the reinvestment assumption\n\nLet’s go back to our Cia Amazônia example where the IRR rule coincided with the NPV rule:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear 0\nYear 1\nYear2\nYear3\nYear 4\nYear 5\n\n\n\n\nFree Cash Flow\n-$219,600\n$46,592\n$69,266\n$80,218\n$101,293\n$130,684\n\n\n\n\nAnd we know that when \\(\\small r=22.61\\%\\), \\(\\small NPV=0\\):\n\n\n\n\\[\n\\small\n-219,000+\\dfrac{46,592}{(1+22.61\\%)^1}+\\dfrac{69,266}{(1+22.61\\%)^2}+\\dfrac{80,218}{(1+22.61\\%)^3}+\\dfrac{101,293}{(1+22.61\\%)^4}+\\dfrac{130,684}{(1+22.61\\%)^5}=0\n\\]\n\nNotably, \\(r\\) is just adjusting each cash-flow stream to accomodate the time-value of money in each given period.\nIf that is true, then moving \\(\\small FC_{1}\\) to \\(\\small FC_{4}\\) to \\(\\small t=5\\) using the approriate \\(\\small r\\) should not change our \\(\\small IRR\\) calculation"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-and-the-reinvestment-assumption-1",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#irr-and-the-reinvestment-assumption-1",
    "title": "Investment Decision Rules",
    "section": "IRR and the reinvestment assumption",
    "text": "IRR and the reinvestment assumption\n\nRearranging our cash-flows from periods \\(\\small t=2\\) to \\(\\small t=4\\) to the last period, we now have a new cash-flow stream:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear 0\nYear 1\nYear2\nYear3\nYear 4\nYear 5\n\n\n\n\nFree Cash Flow\n-$219,600\n$0\n$0\n$0\n$0\n$608,405\n\n\n\n\nAgain, if we use \\(\\small IRR(\\cdot)\\) in Excel, \\(\\small r=22.61\\%\\) as expected, since we’ve just reorganized all intermediate cash-flows to the last period. But what this is telling us?\nRecall that the IRR is an average of the project’s profitability. Behind the scenes, we are assuming that the intermediate cash-flows could have been reinvested at the same \\(r=\\small 22.61\\%\\) rate. This may not hold true if, for example:\n\nA project does not have enough room to expand and receive reinvestments\nThe rate of return of new investments is not the same as of the actual project"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-modified-irr-m-tir",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-modified-irr-m-tir",
    "title": "Investment Decision Rules",
    "section": "The Modified IRR (M-TIR)",
    "text": "The Modified IRR (M-TIR)\n\nAs outlined in the previous example, one of the main problems with the IRR is the assumption that the obtained positive cash flows are reinvested at the same rate at which they were generated\nThe Modified IRR (M-TIR) considers that the proceeds from the positive cash flows of a project will be reinvested using a different rate of return. Frequently, the external rate of return is set equal to the company’s cost of capital.\nUsing \\(\\small MIRR(\\cdot)\\) - or \\(\\small MTIR(\\cdot)\\) - in Excel, one can specify different financing rates (i.e, the cost of capital) and reinvestment rates"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-modified-irr-m-tir-in-the-cia-amazônia-case",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#the-modified-irr-m-tir-in-the-cia-amazônia-case",
    "title": "Investment Decision Rules",
    "section": "The Modified IRR (M-TIR) in the Cia Amazônia case",
    "text": "The Modified IRR (M-TIR) in the Cia Amazônia case\n\nLet’s say that, in our example, we would be able to reinvest the proceeds of the intermediate cash-flows at a 15% rate. If that is the case, then we would have the following cash-flow stream:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear 0\nYear 1\nYear2\nYear3\nYear 4\nYear 5\n\n\n\n\nFree Cash Flow\n-$219,600\n$0\n$0\n$0\n$0\n$540,094\n\n\n\n\nIf we now use the \\(IRR(\\cdot)\\) function in Excel, our new estimate for \\(\\small r\\) is \\(19.72\\%\\)!\nAlternatively, if we kept the original cash-flow stream, but applied the \\(MIRR(\\cdot)\\) function and set the reinvestment rate to be \\(15\\%\\), we would have gotten the exact same result!\nIn other words, one needs to be very cautious when interpreting IRR estimates"
  },
  {
    "objectID": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#references",
    "href": "fin-mgmt/Lecture 6 - Investment Decision Rules/index.html#references",
    "title": "Investment Decision Rules",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ."
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#revisiting-the-free-cash-flow",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#revisiting-the-free-cash-flow",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Revisiting the Free Cash Flow",
    "text": "Revisiting the Free Cash Flow\n\n(+) Revenues\n(-) Costs\n(-) Depreciation\n(=) EBIT\n(-) Tax Expenses\n(=) Unlevered Net Income\n(+) Depreciation\n(-) CAPEX\n(-) \\(\\Delta\\) NWC\n(=) Free Cash Flow\n\nThis is the standard estimate of a Free Cash Flow, which is the amount of incremental cash that a project can actually bring to the firm!"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#introducing-different-financing-decisions",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#introducing-different-financing-decisions",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Introducing different Financing Decisions",
    "text": "Introducing different Financing Decisions\n\nSo far, we’ve assumed that this project was financed only through equity\nHow does the financing decision of a firm can affect both the cost of capital and the set of cash flows that we ultimately discount?\nThere are three main methods that can consider leverage decisions and market imperfections:\n\nThe Weighted-Average Cost of Capital (WACC) method\nThe Adjusted Present Value (APV) method\nThe Flow-to-Equity (FTE) method\n\nWhile their details differ, when appropriately applied each method produces the same estimate of an investment’s (or firm’s) value"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#underlying-assumptions",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#underlying-assumptions",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Underlying assumptions",
    "text": "Underlying assumptions\n\nThe project has average risk: in essence, the market risk of the project is equivalent to the average market risk of the firm’s investments. In that case, the project’s cost of capital can be assessed based on the risk of the firm\nThe firm’s debt-equity ratio is constant: we consider a firm that adjusts its leverage to maintain a constant debt-equity ratio in terms of market values\nCorporate taxes are the only imperfection: we assume that the main effect of leverage on valuation is due to the corporate tax shield. Other effects, such as issuance costs, personal costs, and bankruptcy costs, are abstracted away\n\n\n\n\n\n\n\n\nImportant\n\n\nWe will be applying each method to a single example in which we have made a number of simplifying assumptions. Although we will only cover the Weighted Average Cost of Capital (WACC), the Appendix contains a detailed discussion on the Adjusted Present Value (APV) and the Flow-to-Equity (FTE) methods. While the assumptions discussed herein are restrictive, they are also a reasonable approximation for many projects and firms."
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-weighted-average-cost-of-capital-wacc",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-weighted-average-cost-of-capital-wacc",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "The Weighted Average Cost of Capital (WACC)",
    "text": "The Weighted Average Cost of Capital (WACC)\n\nRecall that our definition of free cash flow measures the after-tax cash flow of a project before regardless how it is financed\nIn a perfect capital markets, choosing debt of equity shouldn’t change the value of the firm. However, because interest expenses are tax deductible, leverage reduces the firm’s total tax liability, enhancing its value!\nWe can directly incorporate market imperfections using the WACC method:\n\n\n\\[\nr_{\\text{WACC}}=\\underbrace{\\dfrac{E}{D+E}}_{\\text{% of Equity}}\\times r_e+ \\underbrace{\\dfrac{D}{D+E}}_{\\text{% of Debt}}\\times r_{D}\\times (1-\\tau)\n\\]\nwhere \\(E\\) is the market-value of Equity, \\(D\\) is the market-value of debt, \\(r_e\\) is the cost of equity, \\(r_d\\) is the cost of debt, and \\(\\tau\\) is the marginal tax rate"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-weighted-average-cost-of-capital-wacc-continued",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-weighted-average-cost-of-capital-wacc-continued",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "The Weighted Average Cost of Capital (WACC), continued",
    "text": "The Weighted Average Cost of Capital (WACC), continued\n\nBecause the WACC incorporates the tax savings from debt, we can compute the levered value of an investment by looking at its stream of cash flows discounted by \\(r_{\\text{WACC}}\\):\n\n\n\\[\nV^{L}= \\dfrac{FCF_1}{(1+r_{\\text{WACC}})}+ \\dfrac{FCF_2}{(1+r_{\\text{WACC}})^2}+...+\\dfrac{FCF_n}{(1+r_{\\text{WACC}})^n}\n\\]\n\nIn what follows, we’ll be using an example taken from (Berk and DeMarzo 2023), Chapter 18, to see how the WACC and the other methods can be applied in practice for the RFX project that is being studied by AVCO’s company"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#practical-application-wacc",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#practical-application-wacc",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Practical Application: WACC",
    "text": "Practical Application: WACC\n\n\n\n\n\n\\(\\rightarrow\\) See accompaining Excel document for the calculations"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#practical-application-wacc-continued",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#practical-application-wacc-continued",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Practical Application: WACC (continued)",
    "text": "Practical Application: WACC (continued)\n\nAs said before, we’ll be assuming that the market risk of the RFX project is expected to be similar to that for the company’s other lines of business\nBecause of that, we can use Avco’s equity and debt to determine the weighted average cost of capital for the new project:\n\n\n\n\n\n\n\n\\(\\rightarrow\\) Important: because market values reflect the true economic claim of each type of financing, while calculating the WACC, market value weights for each financing element (equity, debt, etc.) must be used, and not historical, book values"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#practical-application-wacc-continued-1",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#practical-application-wacc-continued-1",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Practical Application: WACC (continued)",
    "text": "Practical Application: WACC (continued)\n\nUsing our example, we can calculate the WACC as1:\n\n\n\\[\n\\small\nr_{\\text{WACC}}=\\underbrace{\\dfrac{300}{300+300}}_{\\text{% of Equity}}\\times 10\\%+ \\underbrace{\\dfrac{300}{300+300}}_{\\text{% of Debt}}\\times 6\\%\\times (1-25\\%)=7.25\\%\n\\]\n\nNow, using \\(\\small r_{\\text{WACC}}=7.25\\%\\), we can calculate the value of the project, including the tax shield from debt, by calculating the present value of its future free cash flows:\n\n\n\n\\[\n\\small\nV^{L}=\\sum_{T=1}^{T=4}\\dfrac{21}{(1+7.25\\%)^t}=70.73\n\\]\n\nBecause the investment in \\(\\small t=0\\) is \\(\\small 29\\), NPV is \\(\\small 70.73-29=\\$41.73\\) million.\n\n\nNote that we’re using Net Debt \\(\\small (320-20)\\) to weigh in the debt potion in the capital structure"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#general-thoughts-on-the-wacc-method",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#general-thoughts-on-the-wacc-method",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "General thoughts on the WACC method",
    "text": "General thoughts on the WACC method\n\nThis is the method that is most commonly used in practice for capital budgeting purposes. An important advantage of the WACC method is that you do not need to know how this leverage policy is implemented in order to make the capital budgeting decision\nAfter calculating \\(r_{\\text{WACC}}\\), the rate can then be used throughout the firm assuming that:\n\nThis rate represents the company-wide cost of capital for new investments that are of comparable risk to the rest of the firm\nPursuing the project will not alter the firm’s Debt-to-Equity ratio\n\nIf the firm’s Debt-to-Equity ratio is not constant anymore, we cannot reliably use the WACC method anymore\n\n\n\\(\\rightarrow\\) Refer to the Appendix for a detailed discussion on the Adjusted Present Value (APV) method"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#projects-based-cost-of-capital",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#projects-based-cost-of-capital",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Project’s Based Cost of Capital",
    "text": "Project’s Based Cost of Capital\n\nWe began the last section with three simplifying assumptions:\n\nThe project has average risk: in essence, the market risk of the project is equivalent to the average market risk of the firm’s investments. In that case, the project’s cost of capital can be assessed based on the risk of the firm\nThe firm’s debt-equity ratio is constant: we consider a firm that adjusts its leverage to maintain a constant debt-equity ratio in terms of market values\nCorporate taxes are the only imperfection: we assume that the main effect of leverage on valuation is due to the corporate tax shield. Other effects, such as issuance costs, personal costs, and bankruptcy costs, are abstracted away\n\nQuestion: what if we now relax Assumptions 1 and 2?"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#projects-based-cost-of-capital-1",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#projects-based-cost-of-capital-1",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Project’s Based Cost of Capital",
    "text": "Project’s Based Cost of Capital\n\nRelaxing hypothesis about the project’s risk and leverage does have a lot of practical relevance:\n\nSpecific projects often differ from the average investment made by the firm. Therefore, a given project may well be much riskier than the average firm\nFurthermore, acquisitions of real estate or capital equipment are often highly levered, whereas investments in intellectual property are not. Thus, depending on the specific investment being made, the leverage policy used may differ substantially from the firm’s average leverage policy\n\nTo take into account the differences in the project relative to the average firm’s risk and leverage, we will proceed by:\n\nEstimating \\(r_U\\), the unlevered cost of capital, based on a sample of comparable projects;\n(Re)lever the result based on the specific leverage policy adopted"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-unlevered-cost-of-capital",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-unlevered-cost-of-capital",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "The (un)levered cost of capital",
    "text": "The (un)levered cost of capital\n\nHow can we estimate the cost of capital for a project based on a sample of comparable firms? If the comparable firms are 100% equity, their cost of equity, (\\(\\small r_E\\)) can be used to assess our project’s \\(\\small r_E\\) - all in all, the firm’s underlying business risk is fully reflect in the cost of equity\nThe situation is a bit more complicated if the comparable firms have debt. In this case, the cash flows generated by the firm’s assets are used to pay both debt and equity holders\nConsequently, the returns of the firm’s equity (which are being measured using the \\(\\beta\\) from its equity returns) alone are not representative of the underlying assets risk!\n\nIn fact, because of the firm’s leverage, the equity will often be much riskier\nThus, the cost of equity of a sample of levered firm will not be a good estimate of the cost of equity our project!\n\n\n\n\\(\\rightarrow\\) Whenever we are evaluating a project’s cost of capital based on a sample of firm comparables that have debt, we cannot directly use the cost of equity!"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-unlevered-cost-of-capital-1",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-unlevered-cost-of-capital-1",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "The (un)levered cost of capital",
    "text": "The (un)levered cost of capital\n\n\n\n\n\n\nRecall that a firm’s asset cost of capital or unlevered cost of capital (\\(\\small r_U\\)) is the expected return required by the firm’s investors to hold the firm’s underlying assets, and is a weighted average of the firm’s equity and debt costs of capital:\n\n\n\\[\\small r_U = \\frac{E}{E+D}\\times r_E + \\frac{D}{E+D} \\times r_D\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-unlevered-cost-of-capital-continued",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-unlevered-cost-of-capital-continued",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "The (un)levered cost of capital, continued",
    "text": "The (un)levered cost of capital, continued\n\nWhy we should unlever the return on equity1? Note that whenever we measure the cost of equity, we are measuring the expected returns based on the equity risk, which has some implications if a given firm has debt:\n\nBecause debt payments are given, equityholders are referred to as residual claimants - they’ll receive their compensation only after the debtholders receive their payments\nAs such, if a given firm has debt in its financing structure, this makes the equity to be riskier - all in all, an equityholder may not receive anything after paying out debtholders!\n\nBut if you are evaluating a project based on a comparable firm that has debt, you want to consider only the risk of the underlying business, but not the risk due to financial leverage!\nAs a consequence, unlevering the required return makes the comparison to be relative to the investments of a company, regardless of the financing structure!\n\nA similar rationale can be applied to the equity \\(\\small \\beta\\) - See (Berk and DeMarzo 2023) and the Appendix for a detailed discussion."
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#step-1-projects-based-cost-of-capital",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#step-1-projects-based-cost-of-capital",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Step 1: Project’s Based Cost of Capital",
    "text": "Step 1: Project’s Based Cost of Capital\n\nThe first step involves estimating \\(r_U\\) not based on the firm’s unlevered cost of capital, but rather a set of comparable projects that share similar risks. Suppose that our project relates to a new plastics manufacturing division that faces different market risks than the firm’s main packaging business:\n\n\n\n\n\n\n\n\n\n\n\nFirm\nEquity Cost of Capital\nDebt Cost of Capital\nD/(D+E)\n\n\n\n\n1\n12%\n6%\n40%\n\n\n2\n10.7%\n5.5%\n25%\n\n\n\n\nWe can estimate \\(r_U\\) for the plastics division by looking at other single-division plastics firms that have similar business risks. For example, suppose two firms are comparable to the plastics division and have the following characteristics:"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#step-1-projects-based-cost-of-capital-1",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#step-1-projects-based-cost-of-capital-1",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Step 1: Project’s Based Cost of Capital",
    "text": "Step 1: Project’s Based Cost of Capital\n\n\n\n\n\n\n\n\n\n\nFirm\nEquity Cost of Capital\nDebt Cost of Capital\nD/(D+E)\n\n\n\n\n1\n12%\n6%\n40%\n\n\n2\n10.7%\n5.5%\n25%\n\n\n\n\nBased on this, we calculate each firm’s \\(r_U\\) and get the average:\n\\(\\small r_U^1= 0.6 \\times 12\\% + 0.4 \\times 6\\% = 9.6\\%\\)\n\\(\\small r_U^2= 0.75 \\times 10.7\\% + 0.25 \\times 5.5\\% = 9.4\\%\\)\nIn this way, a reasonable estimate for \\(r_U\\) of our project is around \\(\\small 9.5\\%\\). If we wanted to use the APV approach to calculate the value of the project, we could use this estimate\nIf we wanted to use the WACC or FTE methods, however, we still need to estimate \\(r_E\\), which will depend on the incremental debt the firm will take on as a result of the project"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#step-2-projects-based-cost-of-capital",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#step-2-projects-based-cost-of-capital",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Step 2: Project’s Based Cost of Capital",
    "text": "Step 2: Project’s Based Cost of Capital\n\nRecall that our expression for the unlevered cost of capital, \\(r_U\\), was:\n\n\n\\[\n\\small r_U= \\dfrac{E}{E+D}\\times r_E + \\dfrac{D}{E+D}\\times r_D\n\\]\n\nRearranging terms, we have:\n\n\n\n\\[\n\\small  \\dfrac{E}{E+D}\\times r_E = r_U - \\dfrac{D}{E+D}\\times r_D \\\\\n\\small r_E=\\dfrac{E+D}{E}\\times r_U - \\dfrac{D}{E}\\times r_D \\\\\n\\small r_E = r_U+ \\dfrac{D}{E}\\times r_U - \\dfrac{D}{E}\\times r_D \\\\\n\\small r_E = r_U+ \\dfrac{D}{E}( r_U -  r_D)\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#step-2-projects-based-cost-of-capital-1",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#step-2-projects-based-cost-of-capital-1",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Step 2: Project’s Based Cost of Capital",
    "text": "Step 2: Project’s Based Cost of Capital\n\nOur last equation shows us that:\n\n\n\\[\n\\small r_E = r_U+ \\dfrac{D}{E}( r_U -  r_D)\n\\]\n\nIn words, the project’s cost of capital depends on:\n\nThe unlevered cost of capital, \\(r_U\\)\nThe specific debt-to-equity ratio that the project will use\n\nSuppose that the firm will use a debt-to-equity ratio of 1, and the cost of debt remains at \\(\\small6\\%\\). Then, we can calculate \\(\\small r_E\\) as:\n\n\n\n\\[\n\\small r_E = 9.5\\% + \\dfrac{0.5}{0.5}(9.5\\% - 6\\%) = 13\\%\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#step-2-projects-based-cost-of-capital-2",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#step-2-projects-based-cost-of-capital-2",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Step 2: Project’s Based Cost of Capital",
    "text": "Step 2: Project’s Based Cost of Capital\n\nWe can finally plug the estimate of \\(\\small r_E\\) to estimate the project’s WACC, assuming that the tax-rate if 25%:\n\n\n\\[\n\\small r_{\\text{WACC}}=50\\% \\times 13\\% + 50\\% \\times 6\\% \\times(1-25\\%)= 8.75\\%\n\\]\n\nBased on these estimates, Avco should use a WACC of \\(\\small8.75\\%\\) for the plastics division, compared to the WACC of \\(\\small7.25\\%\\) that has been previously estimated based on the firm’s overall\nIntuition: because the project had a higher unlevered risk (\\(\\small9.5\\%\\) versus \\(\\small8\\%\\)), after applying the adopted leverage policy, will also have a higher cost of capital"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#common-misconception-i-determining-the-incremental-leverage-of-a-project",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#common-misconception-i-determining-the-incremental-leverage-of-a-project",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Common Misconception I: determining the incremental leverage of a project",
    "text": "Common Misconception I: determining the incremental leverage of a project\n\nTo determine the equity or weighted average cost of capital for a project, we need to know the amount of debt to associate with the project\nHow to determine the correct \\(\\small D/(D+E)\\) ratio to use in our estimations?\n\n\nSuppose a project involves buying a new warehouse, and the purchase of the warehouse is financed with a mortgage for \\(\\small90\\%\\) of its value\nHowever, if the firm has an overall policy to maintain a \\(\\small40\\%\\) debt-to-value ratio, it will reduce debt elsewhere in the firm once the warehouse is purchased in an effort to maintain that ratio\n\n\n\\(\\rightarrow\\) In that case, the appropriate debt-to-value ratio to use when evaluating the warehouse project is \\(\\small40\\%\\), not \\(\\small90\\%\\)! For capital budgeting purposes, the project’s financing is the change in the firm’s total debt (net of cash) with the project versus without the project!"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#common-misconception-ii-relevering-the-wacc",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#common-misconception-ii-relevering-the-wacc",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Common Misconception II: (re)levering the WACC",
    "text": "Common Misconception II: (re)levering the WACC\n\nSuppose that a firm has a debt-to-value ratio of \\(\\small25\\%\\), a debt cost of capital of \\(\\small5.33\\%\\), an equity cost of capital of \\(\\small12\\%\\), and a tax rate of \\(\\small25\\%\\). The current WACC is:\n\n\n\\[\n\\small r_{\\text{WACC}}=0.75 \\times 12\\% + 0.25\\times 5.33\\% \\times (1- 25\\%) = 10\\%\n\\]\n\nWhat happens to WACC if the firm increases its debt-to-value ratio to \\(\\small50\\%\\)? It is tempting to do:\n\n\n\n\\[\n\\small  r_{\\text{WACC}}=0.5 \\times 12\\% + 0.5\\times 5.33\\% \\times (1- 25\\%) = 8\\%\n\\]\n\nNote, however, that this is wrong, because we’re keeping \\(\\small r_E\\) and \\(\\small r_D\\) fixed! Since these are the cost of equity and debt, we should expect these to increase with leverage, as the risk of both shareholders and debt holders increase!"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#common-misconception-relevering-the-wacc",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#common-misconception-relevering-the-wacc",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Common misconception: (re)levering the WACC",
    "text": "Common misconception: (re)levering the WACC\n\nWhen the firm increases leverage, the risk of its equity and debt will increase, increasing \\(\\small r_E\\) and \\(\\small r_D\\)! To compute the new WACC correctly, we must first determine the firm’s unlevered cost of capital:\n\n\n\\[\n\\small\nr_U = 0.75 \\times 12\\% + 0.25\\times 5.33\\% = 10.33\\%\n\\]\n\nIf \\(\\small r_D\\) has risen to \\(\\small 6.67\\%\\) with the change in leverage, then:\n\n\n\n\\[\n\\small\nr_E = 10.33\\% + \\dfrac{0.5}{0.5}\\times(10.33\\%-6.67\\%)=14\\%\n\\]\n\nFinally, the correct new WACC is:\n\n\n\n\\[\n\\small\nr_{\\text{WACC}}=0.5 \\times 14\\% + 0.5\\times 6.67\\% \\times (1- 25\\%) = 9.5\\%\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#industry-betas-for-estimating-a-projects-cost-of-capital",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#industry-betas-for-estimating-a-projects-cost-of-capital",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Industry betas for estimating a project’s cost of capital",
    "text": "Industry betas for estimating a project’s cost of capital\n\nUsing a single comparable firm is often not a good idea, as there might be a lot of noise in the estimation of the results\nHowever, it is possible to combine estimates of asset betas for multiple firms in the same industry to reduce our estimation error and improve the estimation accuracy:\n\n\nFor example, instead of using only one comparable firm to find the unlevered cost of capital, we may use the average (or median) of several firms that are thought of as comparable peers\nAs you imagine, unlevered betas within an industry are much more stable than the pure equity betas: large differences in the firms’ equity betas are mainly due to differences in leverage, whereas the firms’ asset betas are much more similar, suggesting that the underlying businesses in this industry have similar market risk\n\n\n\\(\\rightarrow\\) See Damodaran (here) industry betas for the U.S."
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#comparing-the-main-methods-for-valuing-levered-firms-and-projects",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#comparing-the-main-methods-for-valuing-levered-firms-and-projects",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Comparing the main methods for valuing levered firms and projects",
    "text": "Comparing the main methods for valuing levered firms and projects\n\nThere are three that we could use value a project’s cash flows when there is debt financing:\n\nThe Weigthed Average Cost of Capital (WACC) method\nThe Adjusted Present Value (APV) method\nThe Flow-to-Equity (FTE) method\n\nStarting from the same assumptions, all methods yield the same results. However:\n\n\nWACC is the method that is the easiest to use when the firm will maintain a fixed debt-to-value ratio over the life of the investment\nFor alternative leverage policies, the APV method is usually the most straightforward approach\nThe FTE method is typically used only in complicated settings for which the values of other securities in the firm’s capital structure or the interest tax shield are difficult to determine"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#other-effects-of-financing",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#other-effects-of-financing",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Other Effects of Financing",
    "text": "Other Effects of Financing\n\nPreviously, we assumed that: Corporate taxes were the only imperfection. In words, we assumed that the main effect of leverage on valuation is due to the corporate tax shield. Other effects, such as issuance costs, personal costs, and bankruptcy costs, were abstracted away\nThe three methods that we saw determine the value of an investment incorporating the tax shields associated with leverage. What if we have more than one market imperfection?:\n\n\nIssuing Costs\nSecurity Mispricing\nFinancial Distress and Bankruptcy Costs\n\n\nIn order to for these three potential imperfections, we generally use the APV method, since it is the most flexible way of adjusting the estimates that stem from the use of leverage, although we can also adjust the other methods with some underlying assumptions"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#supplementary-reading",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#supplementary-reading",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Supplementary Reading",
    "text": "Supplementary Reading\n\nSee Note on Cash Flow Valuation Methods: Comparison of WACC, FTE, CCF and APV Approaches for a detailed discussion on the valuation methods\nSee (Berk and DeMarzo 2023), Chapters 14 to 17, to understand how to account our valuation for the most common market imperfections, such as taxes, financial distress costs, and asymmetric information\n\n\n\\(\\rightarrow\\) All contents are available on eClass®."
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#levered-firms-when-debt-to-equity-is-not-constant",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#levered-firms-when-debt-to-equity-is-not-constant",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Levered Firms when Debt-to-Equity is not constant",
    "text": "Levered Firms when Debt-to-Equity is not constant\n\nQuestion: how to ensure that the Debt-to-Equity will remain constant when implementing new projects?\n\nThus far, we have simply assumed the firm adopted a policy of keeping its debt-equity ratio constant\nNevertheless, keeping the Debt-to-Equity ratio constant has implications for how the firm’s total debt will change with new investment - we’ll refer to this as Debt-Capacity"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#debt-capacity",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#debt-capacity",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Debt Capacity",
    "text": "Debt Capacity\n\nDebt Capacity refers to the the amount of debt that a firm needs to raise in order to keep its debt-to-equity ratio constant. Why is that important?\n\n\nWACC is a weighted average based on the proportions of Equity and Debt\nBecause of that, any changes in these proportions affect the WACC\nTherefore, after calculating \\(r_\\text{WACC}\\), to ensure that you can use it over the years, you need to ensure that the firm maintains the same debt-to-equity ratio\n\n\nYou can find the the debt capacity for a given period \\(t\\) by:\n\n\n\\[\nD_t=d\\times V^L_t\n\\]\nWhere \\(d\\) is the debt-to-value ratio, which is the proportion of (market-value) debt over the market value of the firm or project (debt + equity)"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#debt-capacity-1",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#debt-capacity-1",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Debt Capacity",
    "text": "Debt Capacity\n\nYou can estimate the value of the levered firm, \\(V^L_t\\), over each period \\(t\\) by summing up the discounted stream of cash-flows remaining:\n\n\n\\[\nV^L_t=\\dfrac{FCF_{t+1}+V^L_{t+1}}{(1+r_{\\text{WACC}})}\n\\] where \\(V^L_{t+1}\\) refers to the continuation value – see the accompanying Excel spreadsheet for a comprehensive example\n\nWhile the WACC does not require you to know exactly the debt capacity of the project, this component is essential when calculating the value of the project using other methods, such as the APV, as we’ll see in the next set of slides"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-apv-adjusted-present-value-apv-method",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-apv-adjusted-present-value-apv-method",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "The APV Adjusted Present Value (APV) method",
    "text": "The APV Adjusted Present Value (APV) method\n\nOur previous method estimated the value of a levered firm, \\(\\small V^L\\), by considering the interest-tax shields into the cost of capital calculation, \\(r_{\\text{WACC}}\\)\nWhat if we wanted to gauge the impact of the interest tax-shields separately from the actual value of the unlevered project?\nThe Adjusted Present Value (APV) method does it so by calculating two components: \\(V^U\\), which is the present value of the unlevered project (i.e, no debt) and the present value of the interest tax-shields stemming from the financing decision:\n\n\n\\[\n\\small\nV^{L}_{APV}=V^U+ PV(\\text{Interest Tax Shield})\n\\]\n\nThe APV method incorporates the value of the interest tax shield directly, rather than by adjusting the discount rate as in the WACC method"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-apv-method-in-practice",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-apv-method-in-practice",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "The APV method in practice",
    "text": "The APV method in practice\n\nThe first step in the APV method is to calculate the value of these free cash flows using the project’s cost of capital if it were financed without leverage \\(\\rightarrow V^U\\)\nWhat is the project’s unlevered cost of capital?\n\nBecause the RFX project has similar risk to Avco’s other investments, its unlevered cost of capital is the same as for the firm as a whole\nWe can calculate the unlevered cost of capital using Avco’s pre-tax WACC, the average return the firm’s investors expect to earn\n\n\n\n\\[\nr_U = \\dfrac{E}{E+D}\\times r_e + \\dfrac{D}{E+D}\\times r_d=\\text{Pre-Tax WACC}\n\\]\n\nNote that this formula is the same as of the \\(r_{WACC}\\), but we’re not including the tax-shield effect, \\((1-\\tau)\\), into account!"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "The APV method in practice (continued)",
    "text": "The APV method in practice (continued)\n\nTo understand why the firm’s unlevered cost of capital equals its pre-tax WACC, note that the pre-tax WACC represents investors’ required return for holding the entire business (equity and debt)\nSo long as the firm’s leverage choice does not change the overall risk of the firm, the pre-tax WACC must be the same whether the firm is levered or unlevered!\nApplying it to our case, we have:\n\n\n\\[\nr_U = 0.5\\times 10\\% + 0.5\\times 6\\% = 8\\%\n\\]\n\nWith that, our estimate for \\(V^U\\) is:\n\n\n\n\\[\n\\small\nV^{U}=\\sum_{T=1}^{T=4}\\dfrac{21}{(1+8\\%)^t}=69.55\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued-1",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued-1",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "The APV method in practice (continued)",
    "text": "The APV method in practice (continued)\n\nThe value of the unlevered project, \\(V^U\\), does not include the value of the tax shield provided by the interest payments on debt\nKnowing the project’s debt capacity for the future, we can explicitly calculate the the present value of the interest tax-shields. First, determine the amount of interest expenses at each period \\(t\\):\n\n\n\\[\n\\small \\text{Interest Expenses}_t= r_D\\times D_{t-1}\n\\]\n\nAfter that, assuming a corporate tax rate of \\(\\tau\\), the interest tax-shield is just:\n\n\n\n\\[\n\\small \\text{Interest Tax-Shield}= \\text{Interest Expenses}_t\\times \\tau\n\\]"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued-2",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued-2",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "The APV method in practice (continued)",
    "text": "The APV method in practice (continued)\n\n\n\n\n\n\nTo compute the present value of the interest tax shield, we need to determine the appropriate cost of capital. Which rate shall we use? Note that:\n\nIf the project does well, its value will be higher \\(\\rightarrow\\) more debt \\(\\rightarrow\\) more interest tax-shield\nIf the project performs poorly, its value will be lower \\(\\rightarrow\\) less debt \\(\\rightarrow\\) less interest tax-shield\n\nBecause the interest tax-shield fluctuates with the risk of the project, in this specific case should discount it using the same rate1, \\(r_U\\)!\n\nThe decision on which rate to use is heavily dependent upon the project’s characteristics - see (Berk and DeMarzo 2023) for a detailed discussion on the most common cases."
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued-3",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued-3",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "The APV method in practice (continued)",
    "text": "The APV method in practice (continued)\n\nUsing \\(\\small r_U=8\\%\\) and evaluating the present value of the interest tax-shield, we have:\n\n\n\\[\n\\small PV(\\text{Interest Tax-Shield})=\\dfrac{0.53}{(1+8\\%)}+\\dfrac{0.41}{(1+8\\%)^2}+\\dfrac{0.28}{(1+8\\%)^3}+\\dfrac{0.15}{(1+8\\%)^4}=1.18\n\\]\n\nNow, to determine the value of the levered firm, \\(V^L\\), we add the value of the interest tax shield to the unlevered value of the project:\n\n\n\n\\[\n\\small V^L=V^U+PV(\\text{Interest Tax-Shield})= 69.55+1.18=70.73\n\\]\n\nWhich is exactly the same value that we’ve found using the WACC method!"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#general-thoughts-on-the-apv-method",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#general-thoughts-on-the-apv-method",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "General thoughts on the APV method",
    "text": "General thoughts on the APV method\n\nIn the APV method, we separately calculated the value of the unlevered firm and the value stemming from the tax-shields\nIn this case, the APV method is more complicated than the WACC method because we must compute two separate valuations\nNotwithstanding, the APV method has some advantages:\n\nIt can be easier to apply than the WACC method when the firm does not maintain a constant debt-equity ratio\nIt also provides managers with an explicit valuation of the tax shield itself\n\nThere could be cases where the value of the project heavily depends on the tax-shield, and not on the operating gains themselves \\(\\rightarrow\\) if taxes change, the value of the project may be severely affected!"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#exercise-apv",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#exercise-apv",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Exercise: APV",
    "text": "Exercise: APV\nConsider again Avco’s acquisition from previous examples. The acquisition will contribute \\(\\small \\$4.25\\) million in free cash flows the first year, which will grow by \\(\\small 3\\%\\) per year thereafter. The acquisition cost of \\(\\small \\$80\\) million will be financed with \\(\\small \\$50\\) million in new debt initially. Compute the value of the acquisition using the APV method, assuming Avco will maintain a constant debt-equity ratio for the acquisition.\n\\(\\rightarrow\\) Taken from (Berk and DeMarzo 2023), p. 689\n\nSolution Rationale: proceed in the following steps to compute the value using the APV method:\n\nCalculate \\(V^U\\) - the value of the unlevered project\nCalculate the present value of the tax-shields\nSum them up\n\n\nNote that, because the project will grow at a \\(\\small3\\%\\) rate, debt capacity will also grow at the same rate. Therefore, the growth-rate of the interest tax-shield is also \\(\\small3\\%\\)"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#exercise-apv-1",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#exercise-apv-1",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Exercise: APV",
    "text": "Exercise: APV\n\nCalculating \\(V^U\\): this is just the value of a growing perpetuity for the unlevered cash-flows:\n\n\n\\[\n\\small V^U= \\dfrac{FCFC}{r-g}=\\dfrac{4.25}{8\\%-3\\%}= 85\n\\]\n\nNow, if the firm will start with \\(\\small\\$50\\) million in debt, interest expenses are \\(\\small 50\\times6\\%=3\\) million. The present value of the interest tax-shield is:\n\n\n\n\\[\n\\small \\dfrac{25\\%\\times 3}{8\\%-3\\%}=\\dfrac{0.75}{5\\%}=15\n\\]\n\nTherefore, \\(\\small V^L=V^U+PV(\\text{Tax-Shield})=85+15=100\\)"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-flow-to-equity-fte-method",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-flow-to-equity-fte-method",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "The Flow-to-Equity (FTE) Method",
    "text": "The Flow-to-Equity (FTE) Method\n\nIn the WACC and APV methods, we value a project based on its free cash flow, which is computed ignoring interest and debt payments\nWhat if we take these into consideration and value the cash flows that pertain only to shareholders? The Flow to Equity method does this by:\n\nExplicitly calculating the free cash flow available to equity holders after taking into account all payments to and from debt holders\nThe cashflow to equity holders are then discounted using the equity cost of capital\n\nDespite this difference in implementation, the FTE method produces the same assessment of the project’s value as the WACC or APV methods"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-flow-to-equity-fte-method-continued",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#the-flow-to-equity-fte-method-continued",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "The Flow-to-Equity (FTE) Method, continued",
    "text": "The Flow-to-Equity (FTE) Method, continued\n\nIn order to implement the FTE method, we need to compute the Free Cash Flow to Equity (FCFE), which shows the available proceeds for the shareholders of the firm after paying out all costs, considering all working capital and CAPEX investments, deducting interest expenses and considering the firm’s net borrowing activity:\n\n\n\\[\n\\small FCFE = FCF - (1-\\tau)\\times (\\text{Interest Expenses})\\pm \\text{Net Borrowing}\n\\]\n\nCompared to our previous case, there will be two differences:\n\nFirst, we deduct interest expenses before calculating taxes\nWe add the proceeds from the firm’s net borrowing activity.\n\nThese will be positive when the firm increases its net debt\nOn the other hand, these will be negative when the firm reduces its net debt"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#previous-estimation-of-free-cash-flow-for-wacc-and-apv",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#previous-estimation-of-free-cash-flow-for-wacc-and-apv",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Previous Estimation of Free Cash Flow (for WACC and APV)",
    "text": "Previous Estimation of Free Cash Flow (for WACC and APV)"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#estimation-of-free-cash-flow-to-equity",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#estimation-of-free-cash-flow-to-equity",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Estimation of Free Cash Flow to Equity",
    "text": "Estimation of Free Cash Flow to Equity\n\n\n\n\n\n\\(\\rightarrow\\) See accompaining Excel document for the calculations"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#estimation-of-free-cash-flow-to-equity-explanation",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#estimation-of-free-cash-flow-to-equity-explanation",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Estimation of Free Cash Flow to Equity, explanation",
    "text": "Estimation of Free Cash Flow to Equity, explanation\n\nFrom the previous table, you can see that we have made two major changes relative to our regular Free Cash Flow estimation:\n\nWe explicitly included interest expenses - as calculated in our previous class - before calculating taxes. As a consequence, our taxable income was lower, and so does the tax expense for each year\nBecause we’re measuring the cash flows to equity holders and not all the claimants of the firm, we need to include all dynamics in debt levels (inclusions or deductions). We can do it by considering changes in debt levels from one period to the other:\n\n\n\n\\[\n\\small \\text{Net Borrowing}_t= Debt_t-Debt_{t-1}\n\\]\n\nAs we did in our previous class when calculating the amount of necessary Debt that the firm needed in order to keep the debt-to-equity ratio constant, we can use the calculated debt capacity to calculate the increases/decreases in net debt for each period"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#valuing-equity-cash-flows",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#valuing-equity-cash-flows",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Valuing Equity Cash Flows",
    "text": "Valuing Equity Cash Flows\n\nYou now have the cash flows that pertain exclusively to the shareholders of the firm. Now what?\nThe project’s free cash flow to equity shows the expected amount of additional cash the firm will have available to pay dividends (or conduct share repurchases) each year\nBecause these cash flows represent payments to equity holders, they should be discounted at the project’s equity cost of capital.\nGiven that the risk and leverage of the RFX project are the same as for Avco overall, we can use Avco’s equity cost of capital of (\\(r_e=10\\%\\)):\n\n\n\\[\n\\small NPV(FCFE)=6.37 + \\dfrac{11.47}{1.10}+ \\dfrac{11.25}{1.10^2} +\\dfrac{11.02}{1.10^3}+ \\dfrac{10.77}{1.10^4}=41.73\n\\]\n… which yields exactly the same NPV as of the previous methods!"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#overall-thoughts-on-the-fte-method",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#overall-thoughts-on-the-fte-method",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Overall thoughts on the FTE method",
    "text": "Overall thoughts on the FTE method\n\nSteps to compute the value using the FTE method:\n\nCompute the Free Cash Flow to Equity by directly including interest expenses and net debt\nCalculate the project’s cost of equity, \\(r_e\\)\nDiscount the cash flows using \\(r_e\\)\n\nApplying the FTE method was simplified in our example because the project’s risk and leverage matched the firm’s, and the firm’s equity cost of capital was expected to remain constant.\nJust as with the WACC, however, this assumption is reasonable only if the firm maintains a constant debt-equity ratio. If the debt-equity ratio changes over time, the risk of equity—and, therefore, its cost of capital—will change as well"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#overall-thoughts-on-the-fte-method-continued",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#overall-thoughts-on-the-fte-method-continued",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Overall thoughts on the FTE method, continued",
    "text": "Overall thoughts on the FTE method, continued\n\nLimitations: the FTE method carries the same limitations as of the APV method: we need to compute the project’s debt capacity to determine interest and net borrowing before we can make the capital budgeting decision. Because of that, the WACC method is easier to apply\n\n\nBenefits: whenever we have a complex capital structure, using the FTE has some advantages over the other two methods:\n\nThe APV and WACC methods estimate the the firm’s enterprise value, and need a separate valuation of the other components to separate the value of equity\nIn constrast, the FTE method can be used to estimate the equity value directly\nFinally, by emphasizing a project’s implications for the firm’s payouts to equity, the FTE method may be viewed as a more transparent method for discussing a project’s benefit to shareholders—a managerial concern."
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#other-effects-of-financing-issuing-costs",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#other-effects-of-financing-issuing-costs",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Other Effects of Financing: Issuing Costs",
    "text": "Other Effects of Financing: Issuing Costs\n\nWhen a firm takes out a loan or raises capital by issuing securities, the banks that provide the loan or underwrite the sale of the securities charge fees\nThe fees associated with the financing of the project are a cost that should be included as part of the project’s required investment, reducing the NPV of the project\n\n\n\\[\n\\small NPV = V^L - \\text{Investment} - \\text{Issuance Costs}\n\\]\n\nThis calculation presumes the cash flows generated by the project will be paid out. If instead they will be reinvested in a new project, and thereby save future issuance costs, the present value of these savings should also be incorporated and will offset the current issuance costs."
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#other-effects-of-financing-security-mispricing",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#other-effects-of-financing-security-mispricing",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Other Effects of Financing: Security Mispricing",
    "text": "Other Effects of Financing: Security Mispricing\n\nWith perfect capital markets, all securities are fairly priced and issuing securities is a zero-NPV transaction. However, there are situations where the pricing is more (or less) relative to the true value!\nEquity mispricing: if management believes that the equity will sell at a price that is less than its true value, this mispricing is a cost of the project for the existing shareholders. It can be deducted from the project NPV in addition to other issuance costs\nLoan mispricing: a firm may pay an interest rate that is too high if news that would improve its credit rating has not yet become public\n\nWith the WACC, we could adjust it using the higher interest rate\nWith the APV, we must add to the value of the project the NPV of the loan cash flows when evaluated at the “correct” rate that corresponds to their actual risk"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#other-effects-of-financing-financial-distress",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#other-effects-of-financing-financial-distress",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "Other Effects of Financing: Financial Distress",
    "text": "Other Effects of Financing: Financial Distress\n\nOne consequence of debt financing is the possibility of financial distress and agency costs:\n\nWhen the debt level - and, therefore, the probability of financial distress - is high, the expected free cash flow will be reduced by the expected costs associated with financial distress and agency problems\nFinancial distress costs therefore tend to increase the sensitivity of the firm’s value to market risk, further raising the cost of capital for highly levered firms\n\nHow to adjust for potential financial distress and agency costs?\n\n\nOne approach is to adjust our free cash flow estimates to account for the costs, and increased risk, resulting from financial distress\nAn alternative method is to first value the project ignoring these costs, and then add the present value of the incremental cash flows associated with financial distress and agency problems separately"
  },
  {
    "objectID": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#references",
    "href": "fin-mgmt/Lecture 8 - Valuation with Leverage/index.html#references",
    "title": "Capital Budgeting and Valuation with Leverage",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ."
  },
  {
    "objectID": "fin-mgmt.html#about-the-course",
    "href": "fin-mgmt.html#about-the-course",
    "title": "Financial Management",
    "section": "About the course",
    "text": "About the course\nThe Financial Administration course is the second training program in the financial field within the EAESP Business Administration undergraduate track.\nIt proposes to present, in a context of risk \\(\\times\\) return and from a long-term perspective, the fundamental decisions of Corporate Finance: investment, financing, and shareholder compensation. It particularly addresses the fundamentals of capital investment analysis, working capital management, economic-financial analysis of companies, cash generation capacity and its influence on the objective of maximizing shareholder wealth. These approaches will consider the fundamental factors of ESG (Environmental, Social and Corporate Governance) policies and their consequences for companies.\n\n\n\n\n\n\nFor students\n\n\n\nBelow you find the persistent links to all lectures of the course. As they are continuously updated with fixes and new implementations, you might expect some changes from time to time in the contents of each file.\nThese slides leverage Quarto, an open-source scientific and technical publishing system. They contain both static and dynamic content that will be displayed in your internet browser. Some useful tips for using these slides:\n\nHit F for full-screen mode\nIf you are interest in getting a .pdf version of the slides, hit E to switch to print mode and then Ctrl + P"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#outline",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#outline",
    "title": "Equity Financing",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\n(Berk and DeMarzo 2023)\n(Brealey, Myers, and Allen 2020)\n\nStudy review and practice: I strongly recommend using Prof. Henrique Castro (FGV-EAESP) materials. Use this link to the corresponding exercises related to this lecture:\n\nMultiple Choice Exercises\nNumeric Exercises\nOpen-ended Exercises\n\n\n\n\\(\\rightarrow\\) For coding replications, whenever applicable, please follow this page or hover on the specific slides with coding chunks."
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#thinking-about-investment-opportunities",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#thinking-about-investment-opportunities",
    "title": "Equity Financing",
    "section": "Thinking about investment opportunities",
    "text": "Thinking about investment opportunities\n\nSuppose you, as a sole entrepreneur, owns a local shoes retailer and you identify opportunities to expand your business to boost sales\n\nAlthough you clearly understand the investment opportunity, you run short of resources to fund it. Therefore, as a single entrepreneur, you have little capacity for growth\nFurthermore, you may not want to hold a large fraction (or the totality) of your wealth in a single asset\n\nBecause of these reasons, moving from a sole entrepreneur to a corporation provides several benefits:\n\nBy incorporating, businesses can gain access to capital\nFounders can reduce the risk of their portfolios by selling someof their equity and diversifying\n\nQuestion: how to raise capital for growth?"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#raising-equity-financing-for-private-companies",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#raising-equity-financing-for-private-companies",
    "title": "Equity Financing",
    "section": "Raising Equity Financing for Private Companies",
    "text": "Raising Equity Financing for Private Companies\n\nOften, a private company must seek outside sources that can provide additional capital for growth. This can be done mainly using three financing sources:\n\nEquity Financing (using external resources)\nDebt Financing\nReinvesting retained profits (using internal resources)\n\nDepending on the option chosen, the inclusion of outside capital will affect the control of the company in a different way!\nAt this point, we’ll focus specifically on Equity Financing"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#how-equity-financing-takes-place",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#how-equity-financing-takes-place",
    "title": "Equity Financing",
    "section": "How Equity Financing Takes Place",
    "text": "How Equity Financing Takes Place\n\n\n\n\n\n\nDefinition\n\n\nEquity Financing is the money that helps firms to grow through equity participation (i.e, acquiring a share of the company). Although the initial capital that is required to start a business is usually provided by the entrepreneur, this source of funding can also come from external investors\n\n\n\n\nThere are several ways in which Equity Financing can be done, each with specific characteristics, such the funding amount (in $) and the firms’ stage in the business life cycle\nAs in (Berk and DeMarzo 2023), the usual types of Equity Financing come from:\n\nAngel Investors\nVenture Capital\nPrivate Equity\nInstitutional Investors\nCorporate Investors"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#angel-investors",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#angel-investors",
    "title": "Equity Financing",
    "section": "Angel Investors",
    "text": "Angel Investors\n\n\n\n\n\n\n\nDefinition\n\n\nIndividual investors who buy equity in small private firms. Angel investors are often rich, successful entrepreneurs themselves who are willing to help new companies get started in exchange for a share of the business.\n\n\n\n\nThe angel investment is usually necessary to kick off the business and the angel market is more well organized these days. It often occurs at such an early stage in the business when it is difficult to assess a value for the firm\nAngel investors often circumvent this uncertainty problem by holding either a convertible note or a SAFE (simple agreement for future equity) rather than equity:\n\nAngel investors may have a share price discount over new investors\nThis allows angels and entrepreneurs to agree on terms without agreeing on a value for the firm, postponing the valuation decision until the firm is more mature and attractive to venture capitalists"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#angel-investors-continued",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#angel-investors-continued",
    "title": "Equity Financing",
    "section": "Angel Investors, continued",
    "text": "Angel Investors, continued\n\nIn general, angel investment comes in place by an individual, and not an entity - click here for a list of famous angel investors\nHowever, in recent years, there’s the emerging figure of angel groups, groups of angel investors who pool their money and decide as a group which investments to fund\n\n\nThe typical size of an angel investment ranges from several hundred thousand dollars for individual investors to a few million dollars for angel groups\nCrowdfunding platforms may also help tunneling angel investments to early-stage startups\n\nIn this format, a firm raises very small amounts of money from a large number of people. Investment levels can be minute, in some cases less than $100\nIt is important to keep in mind that crowdfunding has its own specificities and is currently regulated\n\n\n\n\\(\\rightarrow\\) Examples: Angelist, Kickstarter"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#venture-capital",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#venture-capital",
    "title": "Equity Financing",
    "section": "Venture Capital",
    "text": "Venture Capital\n\n\n\n\n\n\nDefinition\n\n\nA limited partnership that specializes in raising money to invest in the private equity of young firms. One of the general partners who work for and run a venture capital firm.\n\n\n\n\nVenture Capital firms offer limited partners advantages over investing directly in startups themselves as angel investors:\n\nLimited partners are more diversified\nThey also benefit from the expertise of the general partners\nGeneral partners usually charge substantial fees: in general, these funds receive return over the profits the firm make (carried interest) plus a an annual management fee of about 2% of the fund’s committed capital\nOften, they demand greater control over the company’s ownership (e.g., board seats): there is evidence that monitoring these firms pay off (Bernstein, Giroud, and Townsend 2016)"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#venture-capital-1",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#venture-capital-1",
    "title": "Equity Financing",
    "section": "Venture Capital",
    "text": "Venture Capital"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#private-equity",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#private-equity",
    "title": "Equity Financing",
    "section": "Private Equity",
    "text": "Private Equity\n\n\n\n\n\n\nDefinition\n\n\nOrganized very much like a venture capital firm, but it invests in the equity of existing privately held firms rather than start-up companies. Private equity firms also might initiate their investment by finding a publicly traded firm and purchasing the outstanding equity, thereby taking the company private in a transaction called a leveraged buyout (LBO).\n\n\n\n\nIn most cases, the private equity firms use debt as well as equity to finance the purchase\nPrivate equity firms share the advantages of Venture Capital firms, and also charge similar fees. One key difference between them is the magnitude of the investment:\n\nThe transaction volume in 2021 (the peak of the private equity market) was over $1 trillion (versus $133 billion from Venture Capital)\nThe average deal size was over $1 billion (versus $21 million from Venture Capital)"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#private-equity-1",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#private-equity-1",
    "title": "Equity Financing",
    "section": "Private Equity",
    "text": "Private Equity\n\n\n\n\n\n\nExamples of Brazilian Private Equity funds: Kinea, Vinci Partners, Opportunity, Pátria"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#institutional-investors",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#institutional-investors",
    "title": "Equity Financing",
    "section": "Institutional Investors",
    "text": "Institutional Investors\n\n\n\n\n\n\nDefinition\n\n\nInstitutional investors such as mutual funds, pension funds, insurance companies, endowments, and foundations manage large quantities of money. They are major investors in many different types of assets, so, not surprisingly, they are also active investors in private companies.\n\n\n\n\nInstitutional investors may invest directly in private firms or they may invest indirectly by becoming limited partners in venture capital firms\nExamples include the retirement funds (CalPERS - California Public Employees Retirement System), Previ, among others\nThese funds are responsible for managing large amounts of money, and are often seen as key players in the corporate governance of a company"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#have-you-ever-watched-billions",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#have-you-ever-watched-billions",
    "title": "Equity Financing",
    "section": "Have you ever watched Billions?",
    "text": "Have you ever watched Billions?"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#more-on-institutional-investors",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#more-on-institutional-investors",
    "title": "Equity Financing",
    "section": "More on institutional investors",
    "text": "More on institutional investors\n\nSome institutional investors are famous for their position on ETF (Exchange Traded funds)\nBlackRock, Vanguard, and State Street are the three largest ETF providers had more than 1 trillion dollars in ETF managed funds as of 2024 (see here for a comprehensive list)\n\n\n\n\n\n\n\n\nCuriosity\n\n\nWhile some of these investors are considered active, there could be cases where we can find passive institutional investors. For example, Exchange Traded Funds (ETFs), which are funds that aim to match an index, like the S&P500 or the Ibovespa indices, are generally thought of as passive, because they’ll need to hold a specific portfolio of assets in any situation.\n\n\n\n\nInstitutional Investors are often in the scrutiny of regulatory agencies due to their relevance across many assets and industries"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#corporate-investors",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#corporate-investors",
    "title": "Equity Financing",
    "section": "Corporate Investors",
    "text": "Corporate Investors\n\n\n\n\n\n\nDefinition\n\n\nA corporation that invests in private companies.\n\n\n\n\nMany established corporations purchase equity in younger, private companies\n\nMost of the other types of investors in private firms that we have considered so far are primarily interested in the financial return that they will earn on their investments\nCorporate investors, by contrast, might invest for corporate strategic objectives in addition to the desire for investment returns.\n\nThese are also known as a corporate partner, strategic partner, or strategic investor\nExamples of Corporate Investors include Google and Intel Capital, which invest in highly specialized companies. Other recent examples include the ramp-up in investment from big-techs into generative-AI startups"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#venture-capital-investing",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#venture-capital-investing",
    "title": "Equity Financing",
    "section": "Venture Capital investing",
    "text": "Venture Capital investing\n\nWhen a company founder decides to sell equity to outside investors for the first time, it is common practice for private companies to issue preferred stock rather than common stock to raise capital:\n\nFor mature companies, it usually has a preferential dividend and seniority in any liquidation and, sometimes, has special voting rights\nFor young companies, it has seniority in any liquidation but typically does not pay regular cash dividends and often contains a right to convert to common stock\n\nIf the company goes bad, the preferred stockholders have a senior claim on the assets of the firm relative to any common stockholders (who are often the employees of the firm)\nIf things go well, then these investors will convert their preferred stock and receive all the rights and benefits of common stockholders"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#financing-rounds",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#financing-rounds",
    "title": "Equity Financing",
    "section": "Financing Rounds",
    "text": "Financing Rounds\n\nEach time the firm raises money is referred to as a funding round, and each round will have its own set of securities with special terms and provisions - each called rounds\nAfter a potential initial “seed round”, it is common to name the securities alphabetically, starting with Series A, Series B, and so on\nThese rounds mount up to the total shares outstanding of the firm\nIn what follows, we’ll see an example of how different funding rounds interact to generate the final shares outstanding"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#example-real-networks",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#example-real-networks",
    "title": "Equity Financing",
    "section": "Example: Real Networks",
    "text": "Example: Real Networks\n\nReal Networks, which was founded by Robert Glaser in 1993, with an investment of approximately 1 million by Glaser\nIn April 1995, Glaser’s 1 million initial investment represented 13,713,439 shares of Series A preferred stock, implying an initial price of \\(\\approx\\) $0.07 per share\nThe company’s first round of outside equity funding was a Series B preferred stock. Real Networks sold 2,686,567 shares of Series B preferred stock at $0.67 per share\nAt the price the new shares were sold for, Glaser’s shares were worth 9.2 million and represented 83.6% of the outstanding shares:"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#example-real-networks-continued",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#example-real-networks-continued",
    "title": "Equity Financing",
    "section": "Example: Real Networks, continued",
    "text": "Example: Real Networks, continued\n\nBased on this example, we may want to assess the firm’s value in two distinct periods of time:\n\nPre-Money Valuation\n\n\nAt the issuance of new equity, the value of the firm’s prior shares outstanding at the price in the funding round\nIn the Real Netkworks example, \\(\\small \\$9.2\\) million\n\n\nPost-Money Valuation\n\n\nAt the issue of new equity, the value of the whole firm (old plus new shares) at the price at which the new equity sold\nIn the Real Netkworks example, \\(\\small \\$11\\) million"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#adding-new-funding-series",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#adding-new-funding-series",
    "title": "Equity Financing",
    "section": "Adding new funding series",
    "text": "Adding new funding series\n\nOver the next few years, Real Networks raised three additional rounds of external equity financing in addition to the Series B funding round\n\n\n\n\nEach of these rounds, along with the shares outstanding prior to the issuance, made the totality of the firm’s outstanding shares!"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#exercise-equity-financing-dynamics",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#exercise-equity-financing-dynamics",
    "title": "Equity Financing",
    "section": "Exercise: Equity Financing Dynamics",
    "text": "Exercise: Equity Financing Dynamics\n\nYou founded your own firm two years ago. Initially, you contributed \\(\\small \\$100,000\\) of your money and, in return, received \\(\\small 1,500,000\\) shares of stock\nSince then, you have sold an additional \\(\\small 500,000\\) shares to angel investors\nYou are now considering raising even more capital from a venture capitalist. The venture capitalist has agreed to invest \\(\\small \\$6\\) million with a post-money valuation of \\(\\small\\$10\\) million for the firm\n\n\nQuestion: Assuming that this is the venture capitalist’s first investment in your company, what percentage of the firm will she end up owning? What percentage will you own? What is the value of your shares?"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#exercise-equity-financing-dynamics---solution",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#exercise-equity-financing-dynamics---solution",
    "title": "Equity Financing",
    "section": "Exercise: Equity Financing Dynamics - Solution",
    "text": "Exercise: Equity Financing Dynamics - Solution\n\nBecause the Venture Capitalist will invest \\(\\small \\$6\\) million out of the \\(\\small \\$10\\) million post-money valuation, her ownership percentage is \\(\\small 6/10=60\\%\\)\nConsequently, the pre-money valuation is \\(\\small \\$10 − \\$6 = \\$4\\) million. As there are \\(\\small 2\\) million pre-money shares outstanding, this implies a share price of \\(\\frac{\\$4,000,000}{2,000,000}= \\small \\$2 \\text{ per share}\\)\nThus, the Venture Capitalist will receive \\(\\small \\frac{\\$6,000,000}{\\$2}=3\\) million shares for her investment, and after this funding round, there will be a total of \\(\\small 5,000,000\\) shares outstanding\n\n\n\\(\\rightarrow\\) You will own \\(\\frac{1,500,000}{5,000,000} = \\small 30\\%\\) of the firm and your post-money valuation is \\(\\small\\$3\\) million"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#venture-capital-financing-terms",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#venture-capital-financing-terms",
    "title": "Equity Financing",
    "section": "Venture Capital Financing Terms",
    "text": "Venture Capital Financing Terms\n\nWe saw that outside investors generally receive preferred stock that is convertible at a later stage. When things go well, these securities will ultimately convert to common stock and so all investors are treated equally. But what happens when they don’t? There are a handful of contract terms that can help mitigating this risk:\n\nLiquidation Preference: specifies a minimum amount that must be paid to these security holders before any payments to common stockholders in the event of a liquidation, sale, or merger of the company\nSeniority: investors in later rounds can demand seniority over investors in earlier rounds to ensure that they are repaid first\nAnti-Dilution Protection: if things are not going well and the firm raises new funding at a lower price than in a prior round, this protection lowers the price at which early investors can convert their shares to common, increasing their ownership percentage\nBoard seats: appoint members to secure control rights"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#exit-strategies",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#exit-strategies",
    "title": "Equity Financing",
    "section": "Exit strategies",
    "text": "Exit strategies\n\nLet’s get back to the Real Networks case. As investors in Series E were willing to pay $8.99 for a share of preferred stock, the valuation of existing preferred stock (earlier rounds) had increased significantly \\(\\rightarrow\\) early investors had substantial capital gains\nBecause Real Networks was still a private company, however, investors could not liquidate their investment by selling their stock in the public stock markets!\nHow can investors realize the return from their investment in terms of an exit strategy? This happens mainly through:\n\nAcquisitions: often, large corporations acquire startups by purchasing the outstanding stock of the private company, allowing all investors to cash out their investment and gains\nGoing public: a firm can also become a public traded company through a Initial Public Offering, allowing shareholders to publicly negotiate their shares\n\nIn the next session, we’ll focus on the latter exit strategy"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#the-initial-public-offering-ipo",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#the-initial-public-offering-ipo",
    "title": "Equity Financing",
    "section": "The Initial Public Offering (IPO)",
    "text": "The Initial Public Offering (IPO)\n\n\n\n\n\n\nDefinition\n\n\nThe process of selling stock to the public for the first time.\n\n\n\n\nWhy going public? It allows greater liquidity and better access to capital at the cost of more external monitoring and more demand for transparency\nEquity offers can be distinguished in terms of:\n\nPrimary Offering: new shares available in a public offering that raise new capital\nSecondary Offering: shares sold by existing shareholders in an equity offering\n\n\n\n\\(\\rightarrow\\) In the former, there is new money on the table; in the latter, money is just “changing hands”!\n\nAn IPO process has important characteristics and agents - let’s dive into some of the key concepts behind this exit strategy"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#the-participants-of-an-ipo---the-underwriter",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#the-participants-of-an-ipo---the-underwriter",
    "title": "Equity Financing",
    "section": "The participants of an IPO - the underwriter",
    "text": "The participants of an IPO - the underwriter\n\nAn IPO entails several market agents that are relevant for the offer to be completed\nAn important piece of this is the Underwriter: an investment banking firm that manages a security issuance and designs its structure\nThey can work on different schemes according to its exposure:\n\nBest-Efforts Basis: For smaller IPOs, a situation in which the underwriter does not guarantee that the stock will be sold, but instead tries to sell the sock for the best possible price. Often, such deals have an all-or-none clause: either all of the shares are sold on the IPO or the deal is called off.\nFirm Commitment: An agreement between an underwriter and an issuing firm in which the underwriter guarantees that it will sell all of the stock at the offer price. Most common.\nAuction: A method of selling new issues directly to the public rather than setting a price itself and then allocating shares to buyers, the underwriter in an auction IPO takes bids from investors and then sets the price that clears the market."
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#the-initial-public-offering---underwriting-participants",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#the-initial-public-offering---underwriting-participants",
    "title": "Equity Financing",
    "section": "The Initial Public Offering - Underwriting participants",
    "text": "The Initial Public Offering - Underwriting participants\n\nLead Underwriter: The primary investment banking firm responsible for managing a security issuance\nSyndicate: A group of underwriters who jointly underwrite and distribute a security issuance\n\n\nUnderwriters market the IPO and help the company with all the necessary filings\nMore importantly, they actively participate in determining the offer price\nIn many cases, the underwriter will also commit to making a market in the stock after the issue, thereby guaranteeing that the stock will be liquid"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#the-initial-public-offering-ipo---filings",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#the-initial-public-offering-ipo---filings",
    "title": "Equity Financing",
    "section": "The Initial Public Offering (IPO) - Filings",
    "text": "The Initial Public Offering (IPO) - Filings\n\nRegistration Statement\n\n\nA legal document that provides financial and other information about a company to investors prior to a security issuance\nCompany managers work closely with the underwriters to prepare this registration statement\n\n\nPreliminary Prospectus\n\n\nPart of the registration statement prepared by a company prior to an IPO that is circulated to investors before the stock is offered\n\n\nFinal Prospectus\n\n\nPart of the final registration statement prepared by a company prior to an IPO that contains all the details of the offering, including the number of shares offered and the offer price"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#the-initial-public-offering-ipo---stages",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#the-initial-public-offering-ipo---stages",
    "title": "Equity Financing",
    "section": "The Initial Public Offering (IPO) - Stages",
    "text": "The Initial Public Offering (IPO) - Stages\n\nValuation: an assessment of the company’s value done prior to the IPO. This is either done by:\n\nEstimating the present value of the estimated future cash flows; or\nExamining comparable operations (e.g, recent IPOs) based on characteristics\n\nRoad Show: occurs during an IPO\n\nSenior management and its underwriters travel around promoting the company and explaining their rationale for an offer price\nDirected towards the underwriters’ largest customers, mainly institutional investors such as mutual funds and pension funds\n\nBook Building: process made by underwriters during and IPO for coming up with an offer price based on customers’ expressions of interest\n\nProvides an early indication of the demand for the IPO\nIf demand appears to be weak in the target price range, the firm may choose to withdraw from the IPO process"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#the-initial-public-offering-ipo---spread",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#the-initial-public-offering-ipo---spread",
    "title": "Equity Financing",
    "section": "The Initial Public Offering (IPO) - Spread",
    "text": "The Initial Public Offering (IPO) - Spread\n\nIf underwriters put effort on making the IPO process to run seamlessly, how they are paid for?\n\n\n\n\n\n\n\n\nDefinition\n\n\nThe spread is the fee a company pays to its underwriters, expressed in a percentage of the issue price of a share of stock.\n\n\n\n\nConsider the final offer price is \\(\\small \\$12.50\\) per share. If the company paid the underwriters a spread of \\(\\small\\$0.875\\) per share, or exactly \\(\\small 7\\%\\) of the issue price, the underwriters bought the stock for \\(\\small\\$11.625\\) per share and then resold it to their customers for \\(\\small\\$12.50\\) per share\nWhen an underwriter provides a firm commitment, it is potentially exposing itself to the risk that the banking firm might have to sell the shares at less than the offer price and take a loss\n\n\n\n\\(\\rightarrow\\) Research shows that about \\(\\small75\\%\\) of IPOs experience an increase in share price on the first day (only \\(\\small9\\%\\) experience a decrease)"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#the-initial-public-ipo-offering---additional-terms-and-conditions",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#the-initial-public-ipo-offering---additional-terms-and-conditions",
    "title": "Equity Financing",
    "section": "The Initial Public (IPO) Offering - additional terms and conditions",
    "text": "The Initial Public (IPO) Offering - additional terms and conditions\n\nOver-allotment allocation (greenshoe provision): In an IPO, an option that allows the underwriter to issue more stock, usually amounting to some % of the original offer size, at the IPO offer price\nLockup: A restriction that prevents existing shareholders from selling their shares for some period, usually 180 days, after an IPO\n\n\n\\(\\rightarrow\\) Specific terms and conditions can be found in the IPO filings (Preliminary/Final Prospectus) outlining all the rules that the specific IPO will folow"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#exercise---the-initial-public-offering-ipo",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#exercise---the-initial-public-offering-ipo",
    "title": "Equity Financing",
    "section": "Exercise - The Initial Public Offering (IPO)",
    "text": "Exercise - The Initial Public Offering (IPO)\n\nRAX House is a private company considering going public. It has assets of \\(\\small 585\\) million and liabilities of \\(\\small 415\\) million. The firm’s cash flow from operations was \\(\\small 137\\) million for the previous year. After the IPO, RAX House will have \\(\\small 118\\) million shares outstanding\nThe industry average cash flow per share multiple is 3.0, and the average book value per share is 2.3. Based on these multiples, estimate the IPO price.\n\n\nSolution\n\nThe firm’s book value of equity is \\(\\small 585 - 415 = 170\\) million. With \\(\\small 118\\) million shares outstanding, the book value per share is \\(\\frac{170}{118} = \\small 1.44\\) per share. Given the industry average of \\(\\small 2.3\\), the estimated IPO price would be \\(\\small 1.44 \\times 2.3 = \\small \\$3.31\\)\nThe firm’s cash flow from operations was \\(\\small 137\\) million, thus cash flow per share is \\(\\frac{137}{118} =\\small \\$1.16\\) Given the industry average multiple of \\(\\small 3.0\\), the estimated IPO price would be \\(\\small 1.16 \\times 3.0 = \\$3.48\\)"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#ipo-puzzles-1",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#ipo-puzzles-1",
    "title": "Equity Financing",
    "section": "IPO Puzzles",
    "text": "IPO Puzzles\n\nResearch conducted using historical IPOs show some important stylized facts\n\nOn average, IPOs appear to be underpriced: The price at the end of trading on the first day is often substantially higher than the IPO price\nThe number of issues is highly cyclical: When times are good, the market is flooded with new issues; when times are bad, the number of issues dries up\nThe costs of an IPO are very high, and it is unclear why firms willingly incur them\nThe long-run performance of a newly public company (three to five years from the date of issue) is poor. That is, on average, a three-to five-year buy and hold strategy appears to be a bad investment\n\nIn what follows, we’ll discuss each of these puzzles"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#ipo-puzzles---underpricing",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#ipo-puzzles---underpricing",
    "title": "Equity Financing",
    "section": "IPO Puzzles - Underpricing",
    "text": "IPO Puzzles - Underpricing\n\nResearch has found that about \\(\\small 75\\%\\) of first-day returns are positive: the average first-day return in the United States is 17%\nThe underwriters benefit from the under pricing because it allows them to manage their risk:\n\n\nIf the stock is underpriced and there’s market correction upwards \\(\\rightarrow\\) stock goes up, there’s positive momentum on the market, and the underwriter may sell shares at profit\nIf the stock is overvalued and the firm has some level of commitment, it might incur in a loss if stock prices go down in the first trading day\n\n\nConsequently, underwriters set the issue price so that the average first-day return is positive:\n\n\nWho bears the cost of underpricing? The pre-IPO shareholders!\nWhy? In effect, these owners are selling stock in their firm for less than they could get in the aftermarket!"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#ipo-puzzles---underpricing-continued",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#ipo-puzzles---underpricing-continued",
    "title": "Equity Financing",
    "section": "IPO Puzzles - Underpricing (continued)",
    "text": "IPO Puzzles - Underpricing (continued)\n\nAlthough IPO returns are attractive, not all investors can earn these returns:\n\nWhen an IPO goes well, the demand for the stock exceeds the supply. Thus the allocation of shares for each investor is rationed (goes down) \\(\\rightarrow\\) you get only a fraction of the shares you requested\nWhen an IPO goes bad, demand at the issue price is weak, so all initial orders are filled completely\n\n\n\n\\(\\rightarrow\\) The typical investor will have their investment in good IPOs rationed while fully investing in bad IPOs - this is also known as the “winners curse”\n\n\n\n\n\n\n\n\nDefinition\n\n\nWinner’s Curse: refers to a situation in when the high bidder, by virtue of being the high bidder, has likely overestimated the value of the item being bid on. You “win” (get all the shares you requested) when demand for the shares by others is low and the IPO is more likely to perform poorly."
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#ipo-puzzles---cyclicality-and-recent-trends",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#ipo-puzzles---cyclicality-and-recent-trends",
    "title": "Equity Financing",
    "section": "IPO Puzzles - Cyclicality and Recent Trends",
    "text": "IPO Puzzles - Cyclicality and Recent Trends\n\nThe number of issues is highly cyclical:\n\n\nWhen times are good, the market is flooded with new issues\nWhen times are bad the number of issues dries up"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#ipo-puzzles---costs-of-an-ipo",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#ipo-puzzles---costs-of-an-ipo",
    "title": "Equity Financing",
    "section": "IPO Puzzles - Costs of an IPO",
    "text": "IPO Puzzles - Costs of an IPO\n\nA typical spread is roughly 7% of the issue price, which is by most standards large, especially considering the additional cost to the firm associated with underpricing\nWhy? One possible explanation is that by charging lower fees, an underwriter may risk signaling that it is not the same quality as its higher priced competitors"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#ipo-puzzles---long-run-underperformance",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#ipo-puzzles---long-run-underperformance",
    "title": "Equity Financing",
    "section": "IPO Puzzles - Long-Run Underperformance",
    "text": "IPO Puzzles - Long-Run Underperformance\n\nAlthough shares of IPOs generally perform very well immediately following the public offering, it has been shown that newly listed firms subsequently appear to perform relatively poorly over the following three to five years after their IPOs\nAn important note is that underperformance is not unique to an initial public issuance of equity:\n\nUnderperformance is also found in subsequent issuances as well (“seasoned offerings”)\nThis raises the possibility that this phenomenum might not result from the issue of equity itself, but rather from the conditions that motivated the equity issuance in the first place\n\nRecent studies also cast doubt on the interpretation of this long-run underperformance: is it really due to the IPO, or is the underperformance relative to the characteristics of the firm that has chosen to go public?"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#the-seasoned-equity-offering-seo",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#the-seasoned-equity-offering-seo",
    "title": "Equity Financing",
    "section": "The Seasoned Equity Offering (SEO)",
    "text": "The Seasoned Equity Offering (SEO)\n\n\n\n\n\n\nDefinition\n\n\nWhen a public company offers new shares for sale - also known as follow-on offer in Brazil.\n\n\n\n\nPublic firms use SEOs to raise additional equity\nWhen a firm issues stock using a SEO, it follows many of the same steps as for an IPO\nThe main difference is that a market price for the stock already exists, so the price-setting process is not necessary anymore\nThere are two types of seasoned equity offerings:\n\nA Cash Offer is a type of SEO in which a firm offers the new shares to investors at large.\nOn the other hand, a Rights Offer is a type of SEO in which a firm offers the new shares only to existing shareholders\n\nIt is important to note that Rights Offers protect existing shareholders from underpricing!"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#rights-offer-and-underpricing",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#rights-offer-and-underpricing",
    "title": "Equity Financing",
    "section": "Rights Offer and Underpricing",
    "text": "Rights Offer and Underpricing\nSuppose a firm holds \\(\\small \\$ 100\\) in cash and has \\(\\small 50\\) shares outstanding. Each share is worth \\(\\small\\$2\\).\n\nOption 1: it announces a cash offer for \\(\\small 50\\) shares at \\(\\small\\$1\\) per share\n\nOnce this offer is complete, the company will have \\(\\small 150\\) in cash and \\(\\small 100\\) shares outstanding\nThe price per share is now \\(\\small\\$1.50\\)\nThe new shareholders therefore receive a \\(\\small \\$0.50\\) benefit at the expense of the old shareholders\n\n\n\nOption 2: it announces a rights offer, where shareholders have the right to purchase an additional share for $1\n\nIf all shareholders chose to exercise their rights, then after the sale, the value of the firm is \\(\\small \\$150\\) with $ \\(\\small 100\\) shares outstanding and a price of \\(\\small \\$1.50\\) per share\nIn this case, however, the \\(\\small \\$0.50\\) benefit accrues to existing shareholders, which exactly offsets the drop in the stock price\nThe firm can continue to issue equity without imposing a loss on its current shareholders!"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#the-seasoned-equity-offering-seo-price-reaction",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#the-seasoned-equity-offering-seo-price-reaction",
    "title": "Equity Financing",
    "section": "The Seasoned Equity Offering (SEO) price reaction",
    "text": "The Seasoned Equity Offering (SEO) price reaction\n\nResearchers have found that, on average, the market greets the news of an SEO with a price decline. This is consistent with an Adverse Selection argument:\n\n\nA company concerned about protecting its existing shareholders will tend to sell only at a price that correctly values or overvalues the firm, investors infer from the decision to sell that the company is likely to be overvalued\nAs a consequence, the price drops with the announcement of the SEO\n\n\nAlthough not as costly as IPOs, seasoned offerings are still expensive. Underwriting fees amount to 5% of the proceeds of the issue - rights offers have lower costs than cash offers\n\n\n\\(\\rightarrow\\) Can SEOs go bad? Definitely! The recent case of Via Varejo/Casas Bahia shows how prior shareholders were severely diluted and had substantial losses in the first-trading day after the offer - click here and here to access relevant news"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#spacs-a-new-way-to-go-public",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#spacs-a-new-way-to-go-public",
    "title": "Equity Financing",
    "section": "SPACs: a new way to go public",
    "text": "SPACs: a new way to go public\n\nIn recent years, an alternative process by which a private company can initially raise capital in public markets gained traction, called Special Purpose Acquisition Company (or SPAC)\n\nA SPAC is a shell company created solely for the purpose of finding a target private firm and taking it public by merging it with the SPAC\nAfter the merger transaction, referred to as a deSPAC, the SPAC shell company is renamed to match the target\n\nThe result is that the target receives cash from the SPAC and has shares that are publicly traded. In other words, the outcome for the target is the same as an IPO, but without the SEC’s rigorous listing requirements or the long IPO sales process!\n\n\n\\(\\rightarrow\\) Lower requirements are not always desirable: see here the fraudulent case of the electic vehicle startup Nikola involved in a SPAC merger"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#practice",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#practice",
    "title": "Equity Financing",
    "section": "Practice",
    "text": "Practice\n\nTake a look at an example of an IPO Prospectus for Raízen S.A, made in 2021 by a syndicated group of underwriters, where BTG Pactual was the lead underwriter - access on eClass®\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPractice using the following links:\n\nMultiple-choice Questions\nNumeric Questions"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#references",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#references",
    "title": "Equity Financing",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nBernstein, Shai, Xavier Giroud, and Richard R. Townsend. 2016. “The Impact of Venture Capital Monitoring.” The Journal of Finance 71 (4): 1591–1622. https://doi.org/10.1111/jofi.12370.\n\n\nBrealey, R. A., S. C. Myers, and F. Allen. 2020. Principles of Corporate Finance. The Irwin/McGraw-Hill Series in Finance, Insurance, and Real Estate. McGraw-Hill Education. https://books.google.com.br/books?id=nsrHuwEACAAJ."
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#outline",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#outline",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\n(Berk and DeMarzo 2023)\n(Brealey, Myers, and Allen 2020)\n\nStudy review and practice: I strongly recommend using Prof. Henrique Castro (FGV-EAESP) materials. Below you can find the links to the corresponding exercises related to this lecture:\n\nMultiple Choice Exercises - click here\nNumeric Exercises - click here\n\n\n\n\\(\\rightarrow\\) For coding replications, whenever applicable, please follow this page or hover on the specific slides with coding chunks"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#risk-return-insights-from-history-1",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#risk-return-insights-from-history-1",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Risk & Return: Insights from History",
    "text": "Risk & Return: Insights from History\n\nThere are a couple of observations based on historical data:\n\nSmall stocks had the highest long-term return, while T-Bills had the lowest\nSmall stocks had the largest fluctuations in price, while T-Bills had the lowest\nHigher risk requires a higher return\n\nMore realistic investment horizons and different timeframes can greatly influence each investment’s risk and return over time"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#risk-return-insights-from-history-continued",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#risk-return-insights-from-history-continued",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Risk & Return: Insights from History (continued)",
    "text": "Risk & Return: Insights from History (continued)"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#risk-return-insights-from-history-continued-1",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#risk-return-insights-from-history-continued-1",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Risk & Return: Insights from History (continued)",
    "text": "Risk & Return: Insights from History (continued)"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Measures of Risk and Return",
    "text": "Measures of Risk and Return\n\nIf an investment is risky, it means that the return of its investment is not guaranteed:\n\n\nThink about each possible return that a given investment can deliver. Each \\(R\\) has some likelihood (or probability) of occurring\nThis information is summarized with a probability distribution, which assigns a probability, \\(P_{R}\\) , that each possible return, \\(R\\), will occur\n\n\n\nFor example, assume that a given stock currently trades for $100 per share. In one year, there is a 25% chance the share price will be 140, a 50% chance it will be 110, and a 25% chance it will be 80. The likelihood is then summarized as:\n\n\n\n\\[\n\\small\nP(R)=\n\\begin{cases}\n25\\% \\text{, for } R = 140, \\\\\n50\\% \\text{, for } R = 110, \\\\\n25\\% \\text{, for } R = 80, \\\\\n\\end{cases}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return---expected-returns",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return---expected-returns",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Measures of Risk and Return - Expected Returns",
    "text": "Measures of Risk and Return - Expected Returns\n\nApart from the probability of occurrence, financial analysts are mostly interested in answering the following question: what is the expected return from a given investment?\n\n\n\n\n\n\n\n\nDefinition\n\n\nExpected Return is the average of all possible returns, weighted by its return probability:\n\\[\nE[R] = \\sum_{R} P_R \\times R\n\\]\n\n\n\n\nIn our simple example, we can compute expected returns as:\n\n\n\n\\[\\small E[R_{BFI}] = 25\\%\\times(−0.20) + 50\\%\\times(0.10) + 25\\%\\times(0.40) = 10\\%\\]"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return---standard-deviation",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return---standard-deviation",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Measures of Risk and Return - Standard Deviation",
    "text": "Measures of Risk and Return - Standard Deviation\n\nLook at our previous example on BMA\n\nWe know that \\(E[R]=10\\%\\)\nNotwithstanding, looking at individual returns, there is significant variation from this average (or expected value)\nIn other words, individual returns \\(R\\) can vary significantly from the expected value!\n\n\n\n\n\n\n\n\n\nDefinition\n\n\nStardard Deviation (Variance) of returns is the average deviation (average squared deviation) from the expected return:\n\\[Var(R) = E[(R-E[R])^2]   = \\sum_{R} P_R \\times (R-E[R])^2 \\]\n\\[SD(R) = \\sqrt{Var(R)}\\]"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return---standard-deviation-1",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return---standard-deviation-1",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Measures of Risk and Return - Standard Deviation",
    "text": "Measures of Risk and Return - Standard Deviation\n\nUsing the previous example from BMA, we have that the variance of the returns can be computed as:\n\n\n\\[\\text{Var}(R_{BMA})  = \\sigma^2_{BMA}=0.25 × (−0.2 − 0.1)^2  + 0.5 × (0.1 − 0.1)^2 + 0.25 × (0.4 − 0.1)^2  = 0.045\\]\n\nTherefore, the standard deviation of returns is simply:\n\n\n\n\\[SD(R_{BFI}) = \\sigma_{BMA}= \\sqrt{0.045} = 21.2\\%\\]\n\nIn words: on average, we know that BMA return is 10%. Notwithstanding, the actual return can vary significantly - the average deviation from this expected return of 10% is \\(\\pm 21\\%\\)."
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return---caveats",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return---caveats",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Measures of Risk and Return - Caveats",
    "text": "Measures of Risk and Return - Caveats\n\nExpected Returns and the Variance (or the Standard Deviation) of returns are often used to characterize an asset in terms of risk and return. There are, notwithstanding, some caveats when we consider real cases of asset returns:\n\nRiskier assets, heavier tails: the likelihood of extreme negative or positive returns is bigger\nThe standard Deviation of an assets changes over time\nAssets that are considered to be riskier tend to remain riskier than others\n\n\n\n\n\n\n\n\n\nImportant\n\n\nStandard Deviation/Variance are correct measures of total risk only if the returns are normally distributed]{.blue}!\n\n\n\n\nFinally, keep in mind that the underlying assumption for using historical data is that past returns are good enough in predicting future returns, which may not hold!"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return---a-global-perspective",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return---a-global-perspective",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Measures of Risk and Return - a global perspective",
    "text": "Measures of Risk and Return - a global perspective\n\nAs we saw, there seems to be a positive relationship between risk and return:\n\nHigher returns are generally associated with higher risk\nLower returns, on the other hand, are less risky, and therefore more certain\n\nThis assumption can also be externalized to different asset classes and/or markets:\n\nStock market returns are higher and riskier than Treasury Bills\nBrazilian stock market returns are historically higher riskier than the U.S. stock market returns\nBrazilian Treasury Bill returns are historically higher and riskier than the U.S. Treasury Bill returns"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#look-at-the-difference-in-variationa-cross-asset-classes",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#look-at-the-difference-in-variationa-cross-asset-classes",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Look at the difference in variationa cross asset classes!",
    "text": "Look at the difference in variationa cross asset classes!"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#historical-returns-1",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#historical-returns-1",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Historical Returns",
    "text": "Historical Returns\n\nOur first example was a case where we knew the potential returns and their corresponding probabilities. With that, we were able to estimate the assets’ expected return\nIn real-world applications, however, we do not know the potential returns and their probabilities. Because of that, the distribution of past returns can be helpful when we seek to estimate the distribution of returns investors may expect in the future\n\n\n\n\n\n\n\n\nDefinition\n\n\nTherealized (or historical) return is the return that actually occurs over a particular time period. Suppose you invest in a stock on date \\(t\\) for price \\(P_t\\) . If the stock pays a dividend, \\(Div_{t +1}\\) , on date \\(t + 1\\), and you sell the stock at that time for price \\(P_{t +1}\\), then the realized return from your investment in the stock from \\(t\\rightarrow t + 1\\) is:\n\\[R_{t+1} = \\frac{\\overbrace{Div_{t+1}}^{\\text{Dividend yield}} + \\overbrace{P_{t+1}}^{\\text{Capital Gain}}}{P_t} - 1\\]"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#a-note-on-future-value-calculations",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#a-note-on-future-value-calculations",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "A note on future value calculations",
    "text": "A note on future value calculations\n\nSometimes, we are interested in the compounded return over a longer period of time. Recall that the future value of an investment is given by:\n\n\n\\[\nPV=\\dfrac{FV}{(1+r)^n}\\rightarrow FV = PV \\times (1+r)^n\n\\]\n\nSay that you observe a stock’s return for four periods: \\(R_1,R_2,R_3\\), and \\(R_4\\). How would you calculate the final price over the whole period (i.e, in \\(t=4\\))? The most straightforward way is to use the future value formula with \\(PV\\) equal to the price of a given stock in \\(t=3\\):\n\n\n\n\\[\nFV= PV\\times(1+r)^n \\rightarrow P_4 = P_3\\times(1+R_4)^1\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#a-note-on-future-value-calculations-continued",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#a-note-on-future-value-calculations-continued",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "A note on future value calculations, continued",
    "text": "A note on future value calculations, continued\n\nBy the same rationale, note that \\(P_3\\) is simply \\(P_2\\times(1+R_3)\\). Iteratively, we have:\n\n\n\\[\n\\small P_4 = P_3\\times(1+R_4)\\\\\n\\small P_4 = P_2\\times(1+R_3)\\times(1+R_4)\\\\\n\\small P_4 = P_1\\times(1+R_2)\\times\\times(1+R_3)\\times(1+R_4)\\\\\n\\small P_4 = \\underbrace{P_0}_{\\text{Initial Price}}\\times\\underbrace{(1+R_1)\\times(1+R_2)\\times(1+R_3)\\times(1+R_4)}_{\\text{Total Return for the Period}}\n\\]\n\\(\\rightarrow\\) In words: the compounded return of a given asset over a period of time is just the product of all individual \\((1+R_t)\\)"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#calculating-realized-annual-returns",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#calculating-realized-annual-returns",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Calculating realized annual returns",
    "text": "Calculating realized annual returns\n\nUsing the rationale presented before, if a stock pays dividends at the end of each quarter (with realized returns \\(R_{Q1}\\), \\(R_{Q2}\\), \\(R_{Q3}\\), and \\(R_{Q4}\\), then its annual realized return, \\(R_{Annual}\\), is computed as follows:\n\n\n\\[(1 + 𝑅_{Annual})   = (1+𝑅_{Q1})\\times(1+𝑅_{Q2})\\times(1+ 𝑅_{Q3})\\times (1+𝑅_{Q4})\\]\n\nTherefore, \\(R_{Annual}\\) (in percentage terms) is found subtracting by \\(1\\) from the previous formula\n\n\n\n\n\n\n\n\n\nImportant\n\n\nYou should know whether the return is calculated adjusted by dividends (they usually are, but always ask). For example, Yahoo! Finance provides Open, High, Low, Close, and Adjusted Close trading prices for each asset that is being tracked, where Adjusted Close is defined by the closing price adjusted for dividends and stock splits. If you use R, Python, or any API to pull this data, ensure to use the information adjusted by dividends and splits!"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#example-annual-historic-returns",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#example-annual-historic-returns",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Example: Annual Historic Returns",
    "text": "Example: Annual Historic Returns\n\nCompute the annual returns over \\(2011\\rightarrow 2012\\) for this MSFT:\n\n\n\n\n\nDate\nPrice ($)\nDividends ($)\nReturn\n\n\n\n\n12/31/2011\n58.69\n-\n-\n\n\n1/31/2012\n61.44\n0.26\n5.13%\n\n\n4/30/2012\n63.94\n0.26\n4.49%\n\n\n7/31/2012\n48.5\n0.26\n-23.74%\n\n\n10/31/2012\n54.88\n0.29\n13.75%\n\n\n12/31/2012\n53.31\n0\n-2.86%\n\n\n\n\n\n\\[𝑅_{2012}=(1.0513)\\times(1.0449)\\times(0.7626)\\times(1.1375)\\times(0.9714)−1=−7.43\\%\\]"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#example-annual-historic-returns-without-dividends",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#example-annual-historic-returns-without-dividends",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Example: Annual Historic Returns, without dividends",
    "text": "Example: Annual Historic Returns, without dividends\n\n\n\nDate\nPrice ($)\nDividends ($)\nReturn\n\n\n\n\n12/31/2015\n6.73\n-\n-\n\n\n3/31/2016\n5.72\n-\n-15.01%\n\n\n6/30/2016\n4.81\n-\n-15.91%\n\n\n9/30/2016\n5.20\n-\n8.11%\n\n\n12/31/2016\n2.29\n-\n-55.96%\n\n\n\n\\[  𝑅_{2016}=(0.8499)\\times(0.8409)\\times(1.0811)\\times(0.4404)−1=−65.9\\%\\]\n\nAs the firm did not pay dividends in 2016, you can compute the annual return as:\n\n\n\\[\\small \\frac{2.29}{6.73}-1 = -65.9\\%\\]"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#average-annual-return-and-variance",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#average-annual-return-and-variance",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Average Annual Return and Variance",
    "text": "Average Annual Return and Variance\n\n\n\n\n\n\nDefinition\n\n\n\nThe average annual return of an investment during some historical period is simply the average of the realized returns for each year.\n\\[\\overline{R}  =  \\frac{1}{T} (𝑅_1  + 𝑅_2  + ⋯ + 𝑅_𝑇)  =  \\frac{1}{T} \\sum_{t=1}^{T} R_t\\]\nNow, using the estimate for \\(\\overline{R}\\), we can calculate the yearly variance and standard deviation as:\n\\[\\small Var[R] =  \\frac{1}{T-1} \\sum_{t=1}^{T} (R_t- \\overline{R})^2 \\]\n\\[\\small SD(R) = \\sqrt{Var(R)}\\]\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nWarning: because you are using a sample of historical returns (instead of the population) there is a T-1 in the variance formula. As \\(T\\rightarrow \\infty\\), then \\(T-1 \\rightarrow T\\) and the sample variance approximates the population one."
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return---practice",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return---practice",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Measures of Risk and Return - Practice",
    "text": "Measures of Risk and Return - Practice\n\nLet’s use this rationale to calculate the expected returns from Petrobrás S.A (ticker: PETR3):\n\nUse the download link at the bottom of the page to download PETR3 monthly returns\nPlot the monthly returns over time. What do you see?\nCalculate the sample expected return by calculating the average monthly return over the period and add it to the chart\nCalculate the monthly volatility (or standard deviation of monthly returns)\n\n\n\n\n\n\n\n\n\nData Exercise\n\n\nBecause you don’t know the actual probabilities, you need to replace your expectation operator, \\(E(\\cdot)\\), by a sample analogue - in our case, we calculate the expected returns as the sample average of past returns and the volatility as the sample standard deviation.\n\n\n Download Data"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#data-exercise-continued",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#data-exercise-continued",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Data Exercise, continued",
    "text": "Data Exercise, continued"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#how-acurate-our-average-return-estimate-is",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#how-acurate-our-average-return-estimate-is",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "How acurate our average return estimate is?",
    "text": "How acurate our average return estimate is?\n\nAs you saw, we can use a security’s historical average return to estimate its actual expected return\nHowever, the historical average return is just an estimate of the expected return:\n\nIf you change your sample (e.g, use different periods), the average return will change\nThe historical average return may not be a good predictor of future return\n\nA way to assess such uncertainty is to estimate how much the average return is expected to change due to different samples\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe standard error of the average return is given by:\n\\[SE(R) = \\frac{SD(R)}{\\sqrt{\\text{Number of Observations}}}\\]"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#data-exercise---standard-error",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#data-exercise---standard-error",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Data Exercise - Standard Error",
    "text": "Data Exercise - Standard Error\n\nFrom 2020 to 2024 (year-to-date), we have 62 months, and an sample standard deviation of returns of 11.72%\nAssume that the returns from PETR3 follow a normal distribution. You know from our statistics course that, for a normally-distributed random variable, a 95% confidence interval is corevered by \\(\\pm 1.96\\) standard errors:\n\n\n\\[E[R] \\pm 1.96\\times SE = 1.12 \\pm 1.96\\times \\dfrac{11.72}{\\sqrt{62}}=[-1.8\\%,+4.04\\%]\\]\n\n\n\nThis means that, with 95% confidence interval, the expected return for PETR3 during this period ranges from -1.8% to 4.04%"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#standard-error---another-example",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#standard-error---another-example",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Standard Error - another example",
    "text": "Standard Error - another example\n\nFrom 1926 to 2017, the average return of the S&P 500 was 12.0%, with a standard deviation of 19.8%\n\n\n\\[E[R] \\pm 1.96\\times SE = 12\\% \\pm 1.96\\times\\frac{19.8\\%}{\\sqrt{92}}= 12\\% \\pm 4.05\\%\\]\n\nThis means that, with 95% confidence interval, the expected return of the S&P 500 during this period ranges from 7.9% and 16.1%\nThe longer the period, the more accurate you are. But even with 92 years of data, you are not very accurate to predict the expected return of the S&P500."
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#compound-annual-growth-rate-cagr",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#compound-annual-growth-rate-cagr",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Compound Annual Growth Rate (CAGR)",
    "text": "Compound Annual Growth Rate (CAGR)\n\n\n\n\n\n\n\nLet’s get back to our example. Note that there are heavy tails, both negative and positive\nNotwithstanding, a simple average return estimation gives the same weights to observations with small and big changes\n\n\nBecause of that, some analysts prefer to use a geometric average instead of arithmetic average, also called Compound Annual Growth Rate or CAGR"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#compound-annual-growth-rate-cagr-1",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#compound-annual-growth-rate-cagr-1",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Compound Annual Growth Rate (CAGR)",
    "text": "Compound Annual Growth Rate (CAGR)\n\nCAGR is nothing more than the geometric average (instead of arithmetic):\n\n\n\\[\\small CAGR = [(1+R_1)\\times(1+R_2)\\times ...\\times (1+R_T)]^{\\frac{1}{T}}-1\\]\n\nUsing our example, the geometric return of PETR3 is:\n\n\n\n\\[\\small CAGR = \\bigg[\\prod_{i=1}^{62}(1+R_i)\\bigg]^{\\frac{1}{62}}-1\\approx 0.38\\%\\]\n\nRemember the (arithmetic) average was 1.12%. Alternatively, we could have calculated the CAGR using Initial and Final Prices: \\(\\small CAGR = \\bigg[\\frac{\\text{Final Price}}{\\text{Initial Price}}\\bigg]^\\frac{1}{T}-1\\)"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#trade-off-between-risk-and-return",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#trade-off-between-risk-and-return",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Trade-off between Risk and Return",
    "text": "Trade-off between Risk and Return\n\nWe saw that whenever average returns are higher, they’re generally riskier\nRelatedly, investors are assumed to be risk averse:\n\nTo assume risk, they need extra return for that risk\nIn other words, they demand excess returns relative to safer assets!\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\n\nExcess Return is difference between:\n\nThe average return for an investment with risk; and\nThe average return of a risk-free asset\n\n\n\n\n\n\n\nThe relationship is expected to be positve - higher excess returns should be accompained with higher volatility\nIn practice the association is not 100% linear as one might expect!"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#the-tradeoff-between-risk-and-return-in-u.s.-securities",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#the-tradeoff-between-risk-and-return-in-u.s.-securities",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "The tradeoff between risk and return in U.S. securities",
    "text": "The tradeoff between risk and return in U.S. securities"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#for-individual-stocks-there-is-no-clear-relationship",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#for-individual-stocks-there-is-no-clear-relationship",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "For individual stocks, there is no clear relationship!",
    "text": "For individual stocks, there is no clear relationship!"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#diversification",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#diversification",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Diversification",
    "text": "Diversification\n\nAs of now, we focused on a single asset case to analyze the risk and return\n\nIn practice, investors hold a portfolio of assets\nHow can we assess the portfolio average returns and its standard deviation?\n\nWhy? Diversification! To see that, can think of risk in terms of two components:\n\nFirm-specific risk\n\nGood or bad news about the company itself. For example, a firm might announce that it has been successful in gaining market share within its industry.\nAlso called firm-specific, idiosyncratic, unique, or diversifiable risk\n\nMarket-wide risk\n\nNews about the economy as a whole, affects all assets\nThis type of risk is common to all firms\nAlso called systematic, undiversifiable, or market risk"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#firm-specific-versus-systematic-risk",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#firm-specific-versus-systematic-risk",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Firm-specific versus systematic Risk",
    "text": "Firm-specific versus systematic Risk\n\nWhen investors hold a portfolio of assets, the portfolio risk is lower than the weighted-average of the individual asset’s risks\nWhy diversification works that way? The rationale behind the argument:\n\nWhen many stocks are combined in a large portfolio, the firm-specific risks for each stock will average out and be diversified\nThe systematic risk, however, will affect all firms and will not be averaged out due to diversification!\n\nIn practice, individual firms are affected by both market-wide risks and firm-specific risks\nWhen firms carry both types of risk, only the idiosyncratic risk will be diversified by forming a portfolio!\nThis also explains why you haven’t found a clear risk \\(\\times\\) return relationship between individual stocks, but when the firm-specific risk is eliminated (through portfolio formation), risk \\(\\times\\) return comparison only consider the different exposure to systematic risks!"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#firm-specific-versus-systematic-risk-continued",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#firm-specific-versus-systematic-risk-continued",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Firm-specific versus systematic Risk, continued",
    "text": "Firm-specific versus systematic Risk, continued\n\nTo build on the previous point, consider two types of firms:\n\nType S firms are affected only by systematic risk\n\nThere is a 50% chance the economy will be strong and they will earn a return of 40%\nThere is a 50% change the economy will be weak and their return will be −20%\nBecause all these firms face the same systematic risk, holding a large portfolio of type S firms will not diversify the risk\n\nType I firms are affected only by firm-specific risks\n\nTheir returns are equally likely to be 35% or −25%, based on factors specific to each firm’s local market\nBecause these risks are firm specific, if we hold a portfolio of many type I firms, risk is diversified!"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#diversification---which-risks-are-mitigated",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#diversification---which-risks-are-mitigated",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Diversification - which risks are mitigated?",
    "text": "Diversification - which risks are mitigated?\n\nConsider again Type I firms, which are affected only by firm-specific risk. Because each individual Type I firm is risky, should investors expect to earn a risk premium when investing in type I firms?\nThe short answers is no, because the risk premium for diversifiable risk is zero, so investors are not compensated for holding firm-specific risk!\n\nThe reason is that they can mitigate this part of risk through diversification\nDiversification eliminates this risk for free, implying that all investors should have a diversified portfolio. Otherwise, the investor is not rational\n\n\n\n\\(\\rightarrow\\) The key takeaway here is that the risk premium of a security is determined by solely by its systematic risk and does not depend on its diversifiable risk"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return---practice-1",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measures-of-risk-and-return---practice-1",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Measures of Risk and Return - Practice",
    "text": "Measures of Risk and Return - Practice\n\nLet’s use this rationale to understand how increasing the portfolio can reduce its volatility\n\nUse the download link at the bottom of the page to download a sample of daily returns for 10 selected stocks from Ibovespa\nStart from left to right and create a portfolio that consists of 100% of the first stock. Calculate the volatility of the returns. For \\(n\\geq2\\), assign equal-weights to each stock\nIterate until you have a portfolio comprised of \\(10\\) different stocks with 10% each\nPlot the annual volatility of each portfolio. How much were you able to reduce in terms of variance? How does this compare to an average volatility of the 10 stocks?\n\n\n\n\n\n\n\n\n\nData Exercise\n\n\nDownload the data using the following button and proceed with the practical exercise.\n\n\n Download Data"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#limits-to-diversification",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#limits-to-diversification",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Limits to diversification",
    "text": "Limits to diversification\n\nHow much volatility can we shrink through diversification? The volatility will therefore decline until only the systematic risk remains - e.g, exposure to macroeconomic events."
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#diversification---wrapping-up",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#diversification---wrapping-up",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Diversification - wrapping-up",
    "text": "Diversification - wrapping-up\n\nAll in all, in a world where diversification exists, the Standard Deviation is not a good measure for risk anymore:\n\nIt is a measure a stock’s total risk, which includes diversifiable and non-diversifiable risks\nHowever, if you are diversified (and rationally, you should be), you are not incurring the total risk, only the systematic risk.\n\nDoes that mean that standard deviation (or the variance) should never be used?\n\nNo! Although the standard deviation of individual stocks contained in a portfolio is not a good measure of risk, the standard deviation of the returns of a portfolio itself is still a good measure for the portfolio’s risk\nTherefore, when comparing two portfolios, you are inherently comparing two “assets” that are already diversified, so the portfolio with the higher standard deviation is riskier!"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measuring-systematic-risk-1",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measuring-systematic-risk-1",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Measuring Systematic Risk",
    "text": "Measuring Systematic Risk\n\nAs we saw before, when it comes to diversification benefits, only the idiosyncratic risk can be diversified, whereas the systematic risk remains in place\nAs a consequence, if you assume that diversification is possible, the standard deviation is not a good measure for risk anymore, as it mixes both types of risk: the one we can get rid out with diversification, and the one which we cannot get rid\nTo measure the systematic risk of a stock, we need to quantity of the variability of its return is due to:\n\nSystematic Risk , or the portion that is not eliminated through diversification\nIdiosyncratic Risk, or the portion that can be eliminated through diversification\n\nQuestion: how can we decompose the risk into these components and extract the systematic part of a given stock’s risk?"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measuring-systematic-risk-continued",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measuring-systematic-risk-continued",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Measuring Systematic Risk, continued",
    "text": "Measuring Systematic Risk, continued\n\nTo determine how sensitive a given stock \\(S\\) is to systematic risk, we can look at the average change in the return for each 1% change in the return of a portfolio that fluctuates solely due to systematic risk - which we’ll call \\(R_M\\) for now:\n\n\n\\[\n\\beta=\\dfrac{\\Delta \\overline R_S}{\\Delta \\overline{R}_{M}}\n\\]\n\nIn other words, \\(\\beta\\) measures the expected % change in the excess return of a security for a 1% change in the excess return of the market portfolio"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measuring-systematic-risk---estimating-beta",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#measuring-systematic-risk---estimating-beta",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Measuring Systematic Risk - estimating \\(\\beta\\)",
    "text": "Measuring Systematic Risk - estimating \\(\\beta\\)\n\nSuppose the market portfolio tends to increase by +47% when the economy is strong and decline by -25% when the economy is weak. What is the beta of a Type S (i.e, with only systematic risk) firm whose return is +40% on average when the economy is strong and −20% when the economy is weak?\n\nIf the Market increases by +47%, then Type S increases by +40% \\(\\rightarrow \\beta = \\frac{40}{47}=0.85\\)\nIf the Market decreases by -25%, then Type S decreases by -20% \\(\\rightarrow \\beta = \\frac{20}{25}=0.8\\)\nIf the Market changes from -25% to +47% = 72%, Type S changes from -20% to 40% \\(\\rightarrow \\beta = \\frac{60}{72}=0.833\\)\n\nImportant: it does not mean that the stock has three \\(\\beta\\)’s. Rather, it just means that we have three estimates for the stock’s sensitivity to systematic risk (\\(\\beta\\))\nAlso, note that, using the the same setting as before, the \\(\\beta\\) of a Type I firm that bears only idiosyncratic, firm-specific risk is zero: \\(\\beta=\\frac{0}{72}=0\\)"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#market-risk-premium",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#market-risk-premium",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Market Risk Premium",
    "text": "Market Risk Premium\n\nLet’s say that you decide to hold the exact market portfolio (e.g, buy an ETF that mimicks the S&P500, Dow Jones Index, or even the Ibovespa Index for a brazilian setting). By definition, because your portfolio is exactly the market portfolio, then \\(\\beta=1\\)\nYou know that, rationally, the higher the risk, the higher the return you need to earn in order to justify holding this portfolio (and not holding, for example, risk-free assets)\nThe question that remains is…how much are you earning, in addition to a risk-free portfolio, for bearing systematic risk?\n\n\n\n\n\n\n\n\nDefinition\n\n\nThe Market (or Equity) Risk Premium (or MRP) is the reward investors expect to earn for holding a portfolio with a \\(\\beta\\) of 1 - i.e, the market portfolio:\n\\[\\text{MRP} = \\underbrace{E[R_m]}_{\\text{Return of the Market portfolio}} - \\underbrace{R_{F}}_{\\text{Return of a risk-free asset}}\\]"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#market-risk-premium-continued",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#market-risk-premium-continued",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Market Risk Premium, continued",
    "text": "Market Risk Premium, continued\n\nNote that MRP is an excess return: it is the return that investors receive net of what they would have earned if they invested in risk-free assets - investors are risk-averse and dislike risk\n\nTherefore, in order to invest in risky assets, investors demand an extra return\nFlipping the argument, a risky asset will have to pay an extra return for its additional risk in order to attract investors!\n\n\n\n\\[E[R_m] =  R_{F} + MRP\\]\n\\(\\rightarrow\\) Key Takeaway: the return of the market portfolio is simply the sum of the risk-free return and the premium for bearing systematic risk!\n\nThere is some heterogeneity in the market risk premium across countries\nFurthermore, the Market Risk premium also changes over time due to macroeconomic conditions (for example, changes in SELIC)"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#market-risk-premium---continued",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#market-risk-premium---continued",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Market Risk Premium - continued",
    "text": "Market Risk Premium - continued\n\nEstimating the historical Market excess returns is fairly straightforward:\n\nDefine what the market portfolio is: it can be, for example, a representative index of a country’s stock exchange, such as the S&P500 (U.S) or Ibovespa (Brazil)\nDefine what is the corresponding risk-free asset - in the U.S. case, we generally use the return on Treasury Bills and Treasury-Bonds - similar to Tesouro Direto in Brazil\n\nA word of caution: again, note that this is not the same as of the expected Market Risk Premium, which is forward-looking! In practice, we often compute the historical average return and using this number as the best estimate of the expected returns\n\n\n\n\n\n\n\n\nMarket Risk Premium - Sources\n\n\n\n\nYou can find the Market Risk Premium estimation\n\nUnited States and other countries (A. Damodaran, NYU Stern): access here\nBrazil (CEQEF-FGV): access here"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#expected-returns-for-individual-stocks",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#expected-returns-for-individual-stocks",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Expected Returns for individual stocks",
    "text": "Expected Returns for individual stocks\n\nConsider an investment with \\(\\beta = 1.5\\)\n\nThis investment has 50% more risk than the market portfolio\nEvery 1% change in the market portfolio leads to a 1.5% percent change in the investment’s price\n\nQuestion: what is the expected return for investing in this specific stock? Based on these figures, we can compute the expected return for this investment, adjusted by the level or risk it provides:\n\n\n\\[E[R] = R_{rf} + \\beta \\times (E[R_m] - R_{rf})\\]\n\nWe will discuss more about this equation later when discussing the CAPM (or Capital Asset Pricing Model)"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#example-beta-and-the-cost-of-capital",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#example-beta-and-the-cost-of-capital",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Example: \\(\\beta\\) and the Cost of Capital",
    "text": "Example: \\(\\beta\\) and the Cost of Capital\n\nAssume the economy has a 60% chance that the market return will be 15% next year and a 40% chance the market return will be 5% next year. Assume the risk-free rate is 6%. If a company’s beta is 1.18, what is its expected return next year?\n\nFirst, compute \\(E[R_m]\\):\n\n\n\n\\[ E[R_m] = 60\\% \\times 15\\% + 40\\% \\times 5\\%  = 11\\%\\]\n\nSecond, compute \\(E[R]\\):\n\n\n\n\\[E[R] = 6\\% + 1.18 \\times (11\\% - 6\\%) = 11.9\\%\\]\n\\(\\rightarrow\\) Key Takeaway: because the stock riskier than the market portfolio, the risk-premium is also higher!"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#beta-for-selected-stocks-against-ibovespa",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#beta-for-selected-stocks-against-ibovespa",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "\\(\\beta\\) for selected stocks (against Ibovespa)",
    "text": "\\(\\beta\\) for selected stocks (against Ibovespa)"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#practice",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#practice",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "Practice",
    "text": "Practice\n\n\n\n\n\n\n\nImportant\n\n\nPractice using the following links:\n\nMultiple-choice Questions\nNumeric Questions"
  },
  {
    "objectID": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#references",
    "href": "fin-strat/Lecture 2 - Capital Markets and the Pricing of Risk/index.html#references",
    "title": "Capital Markets and the Pricing of Risk",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nBrealey, R. A., S. C. Myers, and F. Allen. 2020. Principles of Corporate Finance. The Irwin/McGraw-Hill Series in Finance, Insurance, and Real Estate. McGraw-Hill Education. https://books.google.com.br/books?id=nsrHuwEACAAJ."
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#outline",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#outline",
    "title": "Estimating the Cost of Capital",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\n(Berk and DeMarzo 2023)\n(Brealey, Myers, and Allen 2020)\n\nStudy review and practice: I strongly recommend using Prof. Henrique Castro (FGV-EAESP) materials. Below you can find the links to the corresponding exercises related to this lecture:\n\nMultiple Choice Exercises - click here\nNumeric Exercises - click here\n\n\n\n\\(\\rightarrow\\) For coding replications, whenever applicable, please follow this page or hover on the specific slides with coding chunks."
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#from-our-last-lecture-you-saw-that",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#from-our-last-lecture-you-saw-that",
    "title": "Estimating the Cost of Capital",
    "section": "From our last lecture, you saw that…",
    "text": "From our last lecture, you saw that…\n\nDetermining how much to expect an asset to reward investors is tied to portfolio choices\nThe required return is the expected return that is necessary to compensate for the risk investment \\(i\\) will contribute to the portfolio:\n\n\n\\[\n\\small E[R_i]  &gt;  R_f + \\beta_i^P  \\times (E[R_p] - R_f)\n\\]\n\nIt is is equal to the risk-free interest rate…\n…plus the risk premium of the current portfolio, P…\n… scaled by \\(i\\)’s sensitivity to \\(P\\), denoted by \\(\\beta_i^P\\)\n\n\nIn such a way, this enables us to “price” the required returns for investing in any asset based on the amount of required returns that are needed to improve the performance of an efficient portfolio!"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#putting-it-into-practice",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#putting-it-into-practice",
    "title": "Estimating the Cost of Capital",
    "section": "Putting it into practice",
    "text": "Putting it into practice\n\nOnce we can identify the efficient portfolio, we can compute the expected return of any security based on its beta with the efficient portfolio according to the equation just shown\nHowever, we face a practical issue: in order to identify the efficient portfolio we must know the expected returns (\\(E[\\cdot]\\)), volatilities (\\(\\sigma\\)), and correlations between all securities\nTo answer this question, we’ll look at the Capital Asset Pricing Model - also known as CAPM:\n\nIt allows us to identify the efficient portfolio of risky assets without having any knowledge of the expected return of each security\nInstead of that, the CAPM makes some identifying assumptions regarding the investor behavior"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#about-the-capm",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#about-the-capm",
    "title": "Estimating the Cost of Capital",
    "section": "About the CAPM",
    "text": "About the CAPM\n\nAs we’ll see throughout the lecture, CAPM is very practical and straightforward to implement, and the the CAPM-based approach is very robust\n\nIt imposes a disciplined process on managers to identify the cost of capital\nIt makes the capital budgeting process less subject to managerial manipulation than if managers could set project costs of capital (i.e., the opportunity cost for the firm’s equity) without clear justification\nIt is often the model many investors use to evaluate risk\nAll in all, it gets managers to think about risk in the correct way: instead of thinking about total risk, the CAPM shows us that we only the market risk (non-diversifiable) should be the concern"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-capm-assumptions",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-capm-assumptions",
    "title": "Estimating the Cost of Capital",
    "section": "The CAPM Assumptions",
    "text": "The CAPM Assumptions\n\nThere are basically three simplifying assumptions around investor behavior that the CAPM establishes:\n\nInvestors can buy and sell all securities at competitive market prices without incurring taxes or transactions costs can borrow and lend at the risk-free interest rate\nInvestors hold only efficient portfolios of traded securities.\nInvestors have homogeneous expectations regarding the volatilities, correlations, and expected returns of securities. There is no information asymmetry.\n\nWith these assumptions, we are able to identify the efficient portfolio without having knowledge of the expected returns, volatilities, and correlations between all available investments"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-capm-assumptions-1",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-capm-assumptions-1",
    "title": "Estimating the Cost of Capital",
    "section": "The CAPM Assumptions",
    "text": "The CAPM Assumptions\n\nThere are basically three simplifying assumptions around investor behavior that the CAPM establishes:\n\nInvestors can buy and sell all securities at competitive market prices without incurring taxes or transactions costs can borrow and lend at the risk-free interest rate\nInvestors hold only efficient portfolios of traded securities.\nInvestors have homogeneous expectations regarding the volatilities, correlations, and expected returns of securities. There is no information asymmetry.\n\nWith these assumptions, we are able to identify the efficient portfolio without having knowledge of the expected returns, volatilities, and correlations between all available investments"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-capm-assumptions-2",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-capm-assumptions-2",
    "title": "Estimating the Cost of Capital",
    "section": "The CAPM Assumptions",
    "text": "The CAPM Assumptions\n\n\n\n\n\n\nAssumption I\n\n\nInvestors can buy and sell all securities at competitive market prices (without incurring taxes or transactions costs) and can borrow and lend at the risk-free interest rate\n\n\n\n\nThis assumption states that there are no market imperfections that would prevent investors adjust their portfolios according to their needs\n\n\nAll investors have access to the same set of securities (i.e there is no such investment that is only for accredited investors)\nAll investors are able to buy and sell securities at the same conditions\nFinally, they’re all able to access a risk-free investment, \\(R_f\\), both for investment and lending\n\n\nThis guarantees that if a given investor is not holding an optimized portfolio anymore, he/she can adjust his/her holdings so as to hold the optimized portfolio again"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-capm-assumptions-continued",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-capm-assumptions-continued",
    "title": "Estimating the Cost of Capital",
    "section": "The CAPM Assumptions, continued",
    "text": "The CAPM Assumptions, continued\n\n\n\n\n\n\nAssumption II\n\n\nInvestors hold only efficient portfolios of traded securities—portfolios that yield the maximum expected return for a given level of volatility. All in all, for a given level of volatility assumed, an investor will always select the portfolio with the highest risk \\(\\times\\) return relationship\n\n\n\n\nThis assumption states that all investors behave so as to choose the portfolio with the highest return for a given level of risk that they are willing to accept\nAs we saw, investors will seek to choose a given portfolio \\(P\\) such that has the highest Sharpe Ratio - which we called the tangent portfolio\nAlthough risk preferences may change, investors will seek to combine a risky portfolio \\(P\\) with the risk-free rate so as to adjust for their preferences:\n\n\nInvestors that have preferences towards risk will weight in more on the tangent portfolio and less on the risk-free asset\nOn the other hand, investors that are risk-adverse will weight in more on the risk-free asset"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-capm-assumptions-continued-1",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-capm-assumptions-continued-1",
    "title": "Estimating the Cost of Capital",
    "section": "The CAPM Assumptions, continued",
    "text": "The CAPM Assumptions, continued\n\n\n\n\n\n\nAssumption III\n\n\nInvestors have homogeneous expectations regarding the volatilities, correlations, and expected returns of securities.\n\n\n\n\nPlausibly, there are many investors in the world, and each may have his or her own estimates of the volatilities, correlations, and expected returns of the available securities…\nBut investors don’t come up with these estimates arbitrarily:\n\nThey base them on historical patterns and other information (including market prices)\nIf all investors use publicly available information sources, then their estimates are likely to be similar\n\n\n\n\\(\\rightarrow\\) As a consequence, it is not unreasonable to consider a special case in which all investors have the same estimates concerning future investments and returns"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#wrapping-up-the-assumptions",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#wrapping-up-the-assumptions",
    "title": "Estimating the Cost of Capital",
    "section": "Wrapping up the assumptions",
    "text": "Wrapping up the assumptions\n\nThink about your simplifying assumptions now…\n\nIf investors have homogeneous expectations (Assumption #3), they will identify the same efficient portfolio (the one with the highest Sharpe Ratio)\nWe know that all investors will invest in the efficient portfolio of risky assets (Assumption #2)\nFinally, as they can borrow and lend at the same rate \\(R_f\\) (Assumption #1), they can adjust the risk according to their preferences\n\nIf these are valid, under the CAPM assumptions, we can identify the efficient portfolio: it is equal to the market portfolio!\nA Market Portfolio contains all traded securities in a economy according to their shares relative to the total!"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-capital-market-line",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-capital-market-line",
    "title": "Estimating the Cost of Capital",
    "section": "The Capital Market Line",
    "text": "The Capital Market Line\n\nWhen the CAPM assumptions hold, the market portfolio is the efficient portfolio, so the tangent portfolio that we discussed before is actually the market portfolio\nThe tangent line goes that through the market portfolio is called the Capital Market Line (CML):\n\nIt contains all possible choices of portfolios that investors would pick\nAccording to the CAPM, these portfolios will always be a combination of the risk-free rate, \\(R_f\\), and the market portfolio\n\nTherefore, investors should always choose a portfolio on the Capital Market Line, by holding some combination of the risk-free security and the market portfolio"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-capital-market-line-1",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-capital-market-line-1",
    "title": "Estimating the Cost of Capital",
    "section": "The Capital Market Line",
    "text": "The Capital Market Line"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#pricing-the-risk-premium-under-the-capm",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#pricing-the-risk-premium-under-the-capm",
    "title": "Estimating the Cost of Capital",
    "section": "Pricing the Risk Premium under the CAPM",
    "text": "Pricing the Risk Premium under the CAPM\n\nWe can now identify the efficient portfolio: it is equal to the market portfolio\nWhat does that mean for us in terms of determining expected equity returns? Recall that, from our previous class, the required return for a given stock \\(i\\) should be:\n\n\n\\[\n\\small E[R_i]  &gt;  R_f + \\beta_i^P  \\times (E[R_p] - R_f)\n\\]\n\nUntil now, we were agnostic on what \\(P\\) was. Under the CAPM, we can change \\(R_p\\) to \\(R_m\\) (which is the return of the market portfolio):\n\n\n\n\\[\n\\small E[R_i] =  R_f + \\beta_i^M  \\times (E[R_m] - R_f)\n\\]\n\nConsequently, the \\(\\beta\\) of a security measures its volatility due to market risk relative to the market as a whole, and thus captures the security’s sensitivity to market risk"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#pricing-the-risk-premium-under-the-capm-continued",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#pricing-the-risk-premium-under-the-capm-continued",
    "title": "Estimating the Cost of Capital",
    "section": "Pricing the Risk Premium under the CAPM, continued",
    "text": "Pricing the Risk Premium under the CAPM, continued\n\nThe CAPM implies that there is a linear relationship between a stock’s sensitivity to the market (\\(\\beta\\)) and its expected return:\n\n\n\\[\n\\small E[R_i] =  \\underbrace{R_f + \\beta_i^M  \\times (E[R_m] - R_f)}_{\\text{This is a 1st order equation: }a+bx}\n\\]\n\nThis relationship has also a name: the Security Market Line (SML). It is the line along which all individual securities should lie when plotted according to their expected return and \\(\\beta\\)\n\nContrast this result with the Capital Market Line shown before, where there is no clear relationship between an individual stock’s volatility and its expected return\nDue to the linear nature between the expected return and \\(\\beta\\), a stock’s expected return is due only to the fraction of its volatility that is common with the market\n\n\n\n\n\\(\\rightarrow\\) Therefore, the distance of each stock to the right of the capital market line must be due to its diversifiable risk!"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#capital-market-line-shows-no-clear-relationship-between-risk-and-return",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#capital-market-line-shows-no-clear-relationship-between-risk-and-return",
    "title": "Estimating the Cost of Capital",
    "section": "Capital Market Line shows no clear relationship between risk and return…",
    "text": "Capital Market Line shows no clear relationship between risk and return…"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-security-market-line-makes-the-relationship-clear-when-focusing-only-on-the-sensitivy-to-the-market-risk-beta",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-security-market-line-makes-the-relationship-clear-when-focusing-only-on-the-sensitivy-to-the-market-risk-beta",
    "title": "Estimating the Cost of Capital",
    "section": "The Security Market Line makes the relationship clear when focusing only on the sensitivy to the market risk (\\(\\beta\\))!",
    "text": "The Security Market Line makes the relationship clear when focusing only on the sensitivy to the market risk (\\(\\beta\\))!"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#beta-of-a-portfolio",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#beta-of-a-portfolio",
    "title": "Estimating the Cost of Capital",
    "section": "Beta of a portfolio",
    "text": "Beta of a portfolio\n\nBecause the security market line applies to all tradable investment opportunities, we can apply it to portfolios as well\nHow can we find the \\(\\beta\\) of a full portfolio of assets? In order to see that, assume that you have several assets \\(i\\) in portfolio \\(P\\)\nThus, the return of the portfolio can be writen as \\(R_p=\\sum_{i}x_i R_i\\), where \\(x_i\\) is the weight of each asset \\(i\\) in the portfolio\nThe \\(\\beta\\) of this “asset” is then:\n\n\n\\[\n\\small \\beta_{p}=\\dfrac{Cov(R_p,R_m)}{\\sigma^2_m}=\\dfrac{Cov(\\sum_{i}x_i R_i,R_m)}{\\sigma^2_m}=\\sum_i x_i\\dfrac{Cov(R_i,R_m)}{\\sigma^2_m}=\\sum_i x_i\\times \\beta_i\n\\] \\(\\rightarrow\\) Therefore, the \\(\\beta\\) of a portfolio is simply the weighted average of the indidivual betas!"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#estimating-the-equity-cost-of-capital-1",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#estimating-the-equity-cost-of-capital-1",
    "title": "Estimating the Cost of Capital",
    "section": "Estimating the Equity Cost of Capital",
    "text": "Estimating the Equity Cost of Capital\n\nBased on the CAPM model we’ve just seen, the cost of capital of any investment opportunity equals the expected return of the available investments with the same beta (those that are along the Capital Market Line):\nYou can know use this rationale to estimate how much an investor should need to earn in order to invest in a given stock \\(i\\):\n\n\n\\[R_i = R_f + \\beta \\times (E[R_m] - R_f)\\]\n\nIn what follows, we’ll see how this can be used in practce"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#estimating-the-equity-cost-of-capital-example",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#estimating-the-equity-cost-of-capital-example",
    "title": "Estimating the Cost of Capital",
    "section": "Estimating the Equity Cost of Capital, example",
    "text": "Estimating the Equity Cost of Capital, example\n\nSuppose you estimate that Disney (ticker: DIS) has a volatility of 20% and a \\(\\beta\\) of 1.29. A similar process for Chipotle (ticker: CMG) yields a volatility of 30% and a \\(\\beta\\) of 0.55. If the risk-free interest rate is 3% and you estimate the market’s expected return to be 8%, calculate the equity cost of capital for DIS and CMG.\n\n\n\\[R_{DIS}=3\\%+1.29 \\times (8\\%−3\\%) = 3\\% + 6.45\\% =9.45\\%\\]\n\\[R_{GMG}=3\\%+0.55 \\times(8\\%−3\\%)=3\\%+2.75\\%=5.75\\%\\]\n\n\n\\(\\rightarrow\\) Because market risk cannot be diversified, it is the market risk that determines the cost of capital. Therefore, DIS has a higher cost of equity capital than CMG, even though it is less volatile"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#estimating-the-equity-cost-of-capital-in-practice",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#estimating-the-equity-cost-of-capital-in-practice",
    "title": "Estimating the Cost of Capital",
    "section": "Estimating the Equity Cost of Capital in practice",
    "text": "Estimating the Equity Cost of Capital in practice\n\nOk, now you now the dynamics behind the pricing of securities under the CAPM\nHowever, no one really told you from where the numbers came from\nRecall that, under the CAPM, we need to have estimates related to the market portfolio:\n\nIt is is equal to the risk-free interest rate, \\(R_f\\)…\nThe expected return on the market portfolio, \\(E[R_m]\\)…\nAnd a stock’s s sensitivity to the market portfolio, denoted by \\(\\beta\\)\n\nWe will proceed by understanding each of these components and how to get estimates for them"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#first-step-defining-the-market-portfolio",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#first-step-defining-the-market-portfolio",
    "title": "Estimating the Cost of Capital",
    "section": "First step: defining the Market Portfolio",
    "text": "First step: defining the Market Portfolio\n\nFirst and foremost, what is the definition of a market portfolio? It is the total supply of securities, with the proportions of each security corresponding to the proportion of the total market that each security represents\nThus, the market portfolio contains more of the largest stocks and less of the smallest stocks. In order to see that, note that the market capitalization of one firm is simply the total market value of a firm’s outstanding shares:\n\n\n\\[\\small MV_i = \\text{# of shares outstanding} \\times \\text{Price per share} = N_i \\times P_i\\]\n\nWe then calculate the portfolio weights of each security (a “Value-Weighted” Portfolio): a portfolio in which each security is held in proportion to its market capitalization:\n\n\n\n\\[\\small x_i = \\frac{MV_i}{Total\\; MV}= \\frac{MV_i}{\\sum{MV}}\\]"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-market-portfolio-and-its-proxies",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-market-portfolio-and-its-proxies",
    "title": "Estimating the Cost of Capital",
    "section": "The Market Portfolio and its proxies",
    "text": "The Market Portfolio and its proxies\n\nIf we were to look at real capital markets, there are a couple of portfolios (which are generally called indexes that aim to mimick the dynamics of a market portfolio. Examples of market portfolios indexes include:\n\nS&P500: A value-weighted portfolio of the 500 largest U.S. stocks\nDow Jones Industrial Average (DJIA): a price-weighted portfolio of 30 large industrial stocks (holds an equal number of shares of each stock).\nIbovespa: around 90 BR stocks following an algorithm that focuses on liquidity\n\n\n\nCaution: indexes like the S&P500 and Ibovespa are not considered the market portfolio, but rather, they are proxies for the market portfolios - in other words, they are reasonable approximations of the market portfolio for a given set universe of securities"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#cost-of-equity-components-the-risk-free-rate",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#cost-of-equity-components-the-risk-free-rate",
    "title": "Estimating the Cost of Capital",
    "section": "#1 Cost of Equity Components: the risk-free rate",
    "text": "#1 Cost of Equity Components: the risk-free rate\n\nThe first ingredient of CAPM is risk-free rate, which is the interest rate that investors can earn while having zero to limited volatility\nSuggestions on how to pick the Risk-Free (\\(R_f\\)) rate to be used:\n\nThe yield on U.S. Treasury securities\nSurveys suggest most practitioners use 10- to 30-year treasuries\nHighest quality assets\n\nOften, we use a short-term risk-free rate to evaluate a short-term investment, and a long-term rate when evaluating a long-term investment"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#cost-of-equity-components-the-market-risk-premium",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#cost-of-equity-components-the-market-risk-premium",
    "title": "Estimating the Cost of Capital",
    "section": "#2 Cost of Equity Components: the market risk premium",
    "text": "#2 Cost of Equity Components: the market risk premium\n\nAnother component of the Cost of Equity is the difference between \\(E[R_m]\\) and \\(R_f\\) (the market risk premium)\nWays to estimate the market risk premium:\n\nEstimate the risk premium (\\(E[R_m] − R_f\\)) using the historical average excess return of the market over the risk-free interest rate\nNotice that, even with long periods, we often have large standard errors\nImplicitly, you are assuming that the past is a good proxy for the future"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#note-criticism-around-using-historical-data",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#note-criticism-around-using-historical-data",
    "title": "Estimating the Cost of Capital",
    "section": "Note: criticism around using historical data",
    "text": "Note: criticism around using historical data\n\nWhen estimating the Cost of Equity, using historical data has two problems:\n\nStandard errors of the estimates are often large\nThey are backward looking, meaning that they reflect past, and not future, information. Therefore, we cannot be sure they are representative of current expectations\n\nAn alternative is to use a “discount rate” that is consistent with the current level of the market index in consideration: assume that, for a given firm \\(i\\), current prices can be modeled as future dividends (Gordon model). If that is true, then we have that:\n\n\n\\[\nP_i=\\dfrac{D_1}{r-g}\\rightarrow r = \\dfrac{D_1}{P_i}+g\\equiv \\text{Dividend Yield}+ \\text{Growth rate on Dividends}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#note-criticism-around-using-historical-data-example",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#note-criticism-around-using-historical-data-example",
    "title": "Estimating the Cost of Capital",
    "section": "Note: Criticism around using historical data, example",
    "text": "Note: Criticism around using historical data, example\n\nLet’s say that, after aggregating all firms, Ibovespa’s current dividend yield is 2%. Also, both earnings and dividends per share are expected to grow 6% per year. If that is true, then the discount rate that would make sense to justify the actual level of the Ibovespa index is the discount rate, \\(r\\):\n\n\n\\[\nR_m = \\text{Dividend yield} + \\text{Growth Rate on Dividends}= 2\\% + 6\\% = 8\\%\n\\]\n\nWhile this model is highly inaccurate for an individual firm, the assumption of constant expected growth is more reasonable when considering the overall market\nFollowing such methods, researchers generally report estimates in the 3%–5% range for the future equity risk premium"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#cost-of-equity-components-beta-estimation",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#cost-of-equity-components-beta-estimation",
    "title": "Estimating the Cost of Capital",
    "section": "#3 Cost of Equity Components: \\(\\beta\\) estimation",
    "text": "#3 Cost of Equity Components: \\(\\beta\\) estimation\n\nAs of now, you were able to get a sense on how to get reasonable estimates for \\(R_f\\) and the Market Risk Premium, \\(E[R_m]-R_f\\)\nAll that is left is to estimate the stocks’s sensitivity to the returns of the market portfolio, \\(\\beta\\). Recall that, a new asset \\(i\\) should be enhance the performance of a portfolio if:\n\n\n\\[\n\\small \\underbrace{\\frac{E[R_i] - R_f}{\\sigma_{i} \\times Corr(R_i,R_m)}}_{\\text{Sharpe Ratio of } i} &gt; \\underbrace{\\frac{E[R_m] - R_f}{\\sigma_{m}}}_{\\text{Sharpe Ratio of Market}}\n\\]\n\nWith that, we saw that the expected return from an asset \\(i\\) should be:\n\n\n\n\\[\n\\small R_i - R_f = \\underbrace{\\frac{\\sigma_{i} \\times corr(R_i,R_m)}{\\sigma_{m}}}_{\\beta^M_i}  \\times (E[R_m] - R_f)\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#cost-of-equity-components-beta-estimation-continued",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#cost-of-equity-components-beta-estimation-continued",
    "title": "Estimating the Cost of Capital",
    "section": "#3 Cost of Equity Components: \\(\\beta\\) estimation, continued",
    "text": "#3 Cost of Equity Components: \\(\\beta\\) estimation, continued\n\nBecause \\(\\small Corr(R_i,R_m)=\\frac{Cov(R_i,R_m)}{\\sigma_i\\sigma_m}\\), we have that:\n\n\n\\[\n\\small (R_i - R_f)=\\frac{\\sigma_{i} \\times Cov(R_i,R_m)}{\\sigma_i \\sigma_m\\sigma_{m}}  \\times (E[R_p] - R_f)\\rightarrow  (R_i - R_f)=  \\underbrace{\\frac{Cov(R_i,R_m)}{\\sigma^2_m}}_{\\text{OLS formula for slope}}\\times (E[R_p] - R_f)\n\\]\n\nWe can then estimate \\(\\beta\\) using an Ordinary Least Squares regression:\n\n\n\n\\[\n\\small (R_i - R_f) = \\alpha_i + \\beta_i \\times (R_m - R_f) + \\epsilon_i\n\\]\n\n\\(\\epsilon_i\\) is the error term (or the residual). It represents the deviations from the best-fitting line and is, by definition, zero on average (or else we could improve the fit), and represent firm-specific risk that is diversifiable and that averages out in a large portfolio"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#example-looking-at-petr3.sa-beta-1.02",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#example-looking-at-petr3.sa-beta-1.02",
    "title": "Estimating the Cost of Capital",
    "section": "Example: looking at PETR3.SA (\\(\\beta\\) = 1.02)",
    "text": "Example: looking at PETR3.SA (\\(\\beta\\) = 1.02)"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#understanding-the-alpha-term-inside-the-ols-estimation",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#understanding-the-alpha-term-inside-the-ols-estimation",
    "title": "Estimating the Cost of Capital",
    "section": "Understanding the \\(\\alpha\\) term inside the OLS estimation",
    "text": "Understanding the \\(\\alpha\\) term inside the OLS estimation\n\nRecall that our OLS specification was:\n\n\n\\[\n\\small (R_i - R_f) = \\alpha_i + \\beta_i \\times (R_m - R_f) + \\epsilon_i\n\\]\n\n\\(\\alpha_i\\) is the constant term. It measures the historical performance of the security relative to the expected return predicted by the security market line\nIt is the distance that the stock’s average return is above or below the SML. Thus, we can say \\(\\alpha_i\\) is a risk-adjusted measure of the stock’s historical performance.\nAccording to the CAPM, \\(\\alpha_i\\) should not be significantly different from zero\n\n\nIf \\(\\alpha&gt;0\\) constistently, it would mean that a security delivers a constant positive return and, by definition, independent from the market returns\nIf that is the case, investors would buy the security up to a point where price adjusts so that \\(\\alpha\\) goes to zero (recall Assumption #1)!"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#beta-estimation-practice",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#beta-estimation-practice",
    "title": "Estimating the Cost of Capital",
    "section": "Beta Estimation, practice",
    "text": "Beta Estimation, practice\n\nAssuming that the market volatility, \\(\\sigma_m\\), is 10%, estimate the \\(\\beta\\) for each of these three securities:\n\n\n\n\n\nPortfolio\nWeight\nVolatility (\\(\\sigma\\))\nCorrelation with M\n\n\n\n\nHEC Corp\n0.21\n13%\n0.42\n\n\nGreen Midget\n0.31\n20%\n0.68\n\n\nAlive And Well\n0.48\n12%\n0.54\n\n\n\n\n\n\n\\(\\beta_{H} = \\frac{Sd(R_i) \\times Corr(R_i,R_m)}{Sd(R_m)} = \\frac{0.13 \\times 0.42}{0.10} = \\small 0.546\\)\n\\(\\beta_{G} = \\frac{Sd(R_i) \\times Corr(R_i,R_m)}{Sd(R_m)} = \\frac{0.20 \\times 0.68}{0.10} = \\small 1.36\\)\n\\(\\beta_{A} = \\frac{Sd(R_i) \\times Corr(R_i,R_m)}{Sd(R_m)} = \\frac{0.12 \\times 0.54}{0.10} = \\small 0.648\\)"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#beta-estimation-practice-1",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#beta-estimation-practice-1",
    "title": "Estimating the Cost of Capital",
    "section": "Beta Estimation, practice",
    "text": "Beta Estimation, practice\n\nSuppose you have estimated Tikyberd’s beta to be 0.8 with a 95% confidence interval of 0.65 to 0.95. Assuming the risk-free rate is 2% and the market is expected to return 12%, what range would you estimate for Tikyberd’s equity cost of capital?\n\n\n\\[E[R_i]=r_f + β_i (E[R_M] −R_f) =2\\% + 0.65 (12\\% − 2\\%) = 8.5\\%\\]\n\\[E[R_i]=r_f + β_i (E[R_M] −R_f) = 2\\% + 0.95 (12\\% − 2\\%) = 11.5\\%\\]\n\\(\\rightarrow\\) Therefore, the range for the expected returns from Tikyberd is \\([8.5\\%,11.5\\%]\\)"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-debt-cost-of-capital",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-debt-cost-of-capital",
    "title": "Estimating the Cost of Capital",
    "section": "The Debt Cost of Capital",
    "text": "The Debt Cost of Capital\n\nIn the previous slides, we saw how to use the CAPM to estimate the cost of capital of a firm’s equity\nWhat about a firm’s debt — in other words, how to estimate the expected return required by a firm’s creditors?\nIn what follows, we’ll see some approaches for estimating the Debt Cost of Capital. Recall that the Cost of Capital of a given firm will then be a weighted average of the Equity and Debt costs\nWe’ll look at two different methods:\n\nUsing Debt Yields\nUsing the CAPM to generate debt betas"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#approach-1-debt-yields",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#approach-1-debt-yields",
    "title": "Estimating the Cost of Capital",
    "section": "Approach #1: Debt Yields",
    "text": "Approach #1: Debt Yields\n\nRecall that the Yield to Maturity (YTM) is the IRR an investor will earn from holding the bond to maturity and receiving its promised payments\n\nIf there is little risk the firm will default, yield to maturity is a reasonable estimate of investors’ expected rate of return\nOn the other hand, if there is significant risk of default, yield to maturity will overstate investors’ expected return"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#approach-1-debt-yields-continued",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#approach-1-debt-yields-continued",
    "title": "Estimating the Cost of Capital",
    "section": "Approach #1: Debt Yields, continued",
    "text": "Approach #1: Debt Yields, continued\n\nHow can we adjust for potential losses due to defaults? We can estimate the expected returns from debt, \\(r_d\\), as:\n\n\n\\[\n\\small r_d=\\text{YTM} - \\text{Prob. of Default}\\times \\text{Expected Loss given Default}\n\\]\n\nIn order to see that, assume that the average loss rate for unsecured debt is 60%\nDuring average times, the annual default rate for B-rated bonds is 5.5%.\nIn this case, the expected return to B-rated bondholders during average times is \\(\\small 0.055 \\times 0.60 = 3.3\\%\\) below the bond’s quoted yield:"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#approach-2-debt-betas",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#approach-2-debt-betas",
    "title": "Estimating the Cost of Capital",
    "section": "Approach #2: Debt Betas",
    "text": "Approach #2: Debt Betas\n\nWe can also use the CAPM to estimate debt cost through identifying the sensitivity of a given firm’s debt returns to the market returns\n\nDebt betas are difficult to estimate because corporate bonds are traded infrequently, so there’s little variation in bond returns\nOne approximation is to use estimates of betas of bond indices by rating category"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#debt-betas-practice",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#debt-betas-practice",
    "title": "Estimating the Cost of Capital",
    "section": "Debt Betas, practice",
    "text": "Debt Betas, practice\n\nIn early 2013, auto parts retailer Autozone had outstanding 10-year bonds with a Yield-to-Maturity of 3% and a BBB rating. If corresponding risk-free rates were 1.5% and the market risk premium is 8%, estimate the expected return of Autozone’s debt.\n\n\nApproach #1: using the average estimates in Table and an expected loss rate of 60%, we have:\n\\[\n\\small r_d = 3\\% - 0.5\\% \\times 0.6 = 2.7\\%\n\\]\n\n\nApproach #2: alternatively, we can estimate the bond’s expected return using CAPM and an estimated beta of 0.10:\n\\[\n\\small r_d = 1.5\\% + 0.10(8\\%) = 2.3\\%\n\\]\n\\(\\rightarrow\\) Both estimates are rough approximations and they both suggest that the expected return of Autozone’s debt is below its yield-to-maturity of 3%"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#a-projects-cost-of-capital",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#a-projects-cost-of-capital",
    "title": "Estimating the Cost of Capital",
    "section": "A Project’s Cost of Capital",
    "text": "A Project’s Cost of Capital\n\nWhen we started studying the CAPM, the focus was to price the required returns of a company as a whole by means of the sensitivity of its returns with the market\nWhat about investment opportunities within a firm (or, alternatively, that are not publicly traded)? Because a new project is not itself a publicly traded security, we cannot use historical risks of equity and debt to estimate beta and the cost of capital\nFurthermore, the decision to invest in specific projects using the firm’s cost of capital might ignore the project’s risk:\n\nIf, for example, a company has an overall \\(\\beta\\) of 0.75, it is, on average, less sensitive to changes in market returns\nHowever, this company can operate in multiple industries at the same time - e.g, Ultrapar - Grupo Ultra\nFinally, there may be some specific projects, such as Research and Development (R&D), that are likely riskier than the average risk of a project within the company"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#estimating-a-projects-cost-of-capital-1",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#estimating-a-projects-cost-of-capital-1",
    "title": "Estimating the Cost of Capital",
    "section": "Estimating a project’s Cost of Capital",
    "text": "Estimating a project’s Cost of Capital\n\nWhenever we are evaluating a new project, the ultimate decision is assert whether or not to undertake a project:\n\nIf the expected return is higher than the opportunity cost for investing in that project, we should undertake the project\nIf the expected return is lower than the opportunity cost for investing in that project, we should reject the project\n\nHow can we do this as the cost of capital that is required to make this decision is specific to a project, which does not have historical data available?\nThere are several ways in which we can use information to estimate such return\nThese methods range in the complexity in which we assume the project to have, as well as the source of funds that are used to fund the project"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#approach-1-all-equity-comparables",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#approach-1-all-equity-comparables",
    "title": "Estimating the Cost of Capital",
    "section": "Approach #1: All-Equity Comparables",
    "text": "Approach #1: All-Equity Comparables\n\nThe simplest setting of a theoretical comparable firm is to find an all-equity financed firm (a firm with no debt) in a single line of business that is comparable to the project\nRemember that:\n\nAssets = Equity + Debt\nWhen debt is zero: Assets = Equity.\n\nBased on the information from this firm, use the comparable firm’s equity \\(\\beta\\) and cost of capital as estimates to estimate the project’s cost of capital\n\nThe rationale here is that your project is an asset by itself. If you find an all-equity firm, the equity \\(\\beta\\) will also be the beta of its assets\nConsequently, you can use this estimate as the \\(\\beta\\) estimate of your project"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#a-projects-cost-of-capital-practice",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#a-projects-cost-of-capital-practice",
    "title": "Estimating the Cost of Capital",
    "section": "A Project’s Cost of Capital, practice",
    "text": "A Project’s Cost of Capital, practice\n\nYou have just invented a new low-cost, long-lasting rechargeable battery for use in electric cars. You are working on your business plan, and believe your firm will face similar market risk to Seguin Inc, which has a \\(\\beta\\) of 1.3. To develop your financial plan, estimate the cost of capital of financing your firm assuming a risk-free rate of 2.5% and a market risk premium of 6.5%.\n\n\n\nUsing Seguin’s beta as the estimate of the project beta (i.e., assuming the market risk of your project is the same as this company’s):\n\n\n\n\\[\nr_{\\text{Project}} = 2.5\\% + 1.3 \\times 6.5\\% = 10.95\\%\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#introducing-leverage",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#introducing-leverage",
    "title": "Estimating the Cost of Capital",
    "section": "Introducing Leverage",
    "text": "Introducing Leverage\n\nWhat if the firm that we have used as a comparable peer is not 100% Equity? As a result, the situation is a bit more complicated if the comparable firm has debt\nIn that case, the cash flows generated by the firm’s assets are used to pay both debt and equity holders\nConsequently, the returns of the firm’s equity (which are being measured using the \\(\\beta\\) from its equity returns) alone are not representative of the underlying assets risk!\n\nIn fact, because of the firm’s leverage, the equity will often be much riskier\nThus, the beta of a levered firm’s equity will not be a good estimate of the beta of its assets and of our project"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#approach-2-leveraged-comparable-firms",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#approach-2-leveraged-comparable-firms",
    "title": "Estimating the Cost of Capital",
    "section": "Approach #2: Leveraged Comparable Firms",
    "text": "Approach #2: Leveraged Comparable Firms\n\nHow can we compute the cost of capital of a given project whenever the comparable firm has debt? A method to solve this issue is to unlever the \\(\\beta\\) of the comparable firm\nTo understand that, recall that a firm’s asset cost of capital or unlevered cost of capital is the expected return required by the firm’s investors to hold the firm’s underlying assets, and is a weighted average of the firm’s equity and debt costs of capital:\n\n\n\\[r_U = \\frac{E}{E+D}\\times r_E + \\frac{D}{E+D} \\times r_D\\]\n\nAnalogously, because the beta of a portfolio is the weighted-average of the betas:\n\n\n\n\\[\\beta_U = \\frac{E}{E+D}\\times \\beta_E + \\frac{D}{E+D} \\times \\beta_D\\]"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#approach-2-leveraged-comparable-firms-continued",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#approach-2-leveraged-comparable-firms-continued",
    "title": "Estimating the Cost of Capital",
    "section": "Approach #2: Leveraged Comparable Firms, continued",
    "text": "Approach #2: Leveraged Comparable Firms, continued\n\nWhy we should unlever the return on equity (or, alternatively, the equity \\(\\beta\\))? Note that whenever we measure the cost of equity, we are measuring the expected returns based on the equity risk, which has some implicaitons if a given firm has debt:\n\nBecause debt payments are given, equityholders are referred to as residual claimants - they’ll receive their compensation only after the debtholders receive their payments\nAs such, if a given firm has debt in its financing structure, this makes the equity to be riskier - all in all, an equityholder may not receive anything after paying out debtholders!\n\nBut if you are evaluating a project based on a comparable firm that has debt, you want to consider only the risk of the underlying business, but not the risk due to financial leverage!\nAs a consequence, unlevering the \\(\\beta\\) or the required return makes the comparison to be relative to the investments of a company, regardless of the financing structure!\n\n\n\\(\\rightarrow\\) See the Appendix for the calculations around how to lever (or unlever) \\(\\beta\\) and expected returns"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#a-projects-cost-of-capital-practice-1",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#a-projects-cost-of-capital-practice-1",
    "title": "Estimating the Cost of Capital",
    "section": "A Project’s Cost of Capital, practice",
    "text": "A Project’s Cost of Capital, practice\n\nSuppose that your firm is launching a new product and you identify Company X as a firm with comparable investments. X’s equity has a market capitalization of 77 billion and a beta of 0.75. X also has 57 billion of AA-rated debt outstanding, with an average yield of 4.1%. Estimate the cost of capital of your firm’s investment given a risk-free rate of 2.5% and a market risk-premium of 6%.\n\n\n\\(\\rightarrow\\) Solution: Company’s X equity cost of capital is:\n\n\n\\[\nr_E = 2.5\\% + 0.75 \\times 6\\% = 7\\%\n\\]\n\n\nAs a result, Company’s X unlevered cost of capital is (using the yield as debt cost):\n\n\n\\[r_U =\\frac{77}{77+57} \\times  7\\% + \\frac{57}{77+57} \\times 4.1\\% = 5.76\\%\\]"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#a-projects-cost-of-capital-practice-2",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#a-projects-cost-of-capital-practice-2",
    "title": "Estimating the Cost of Capital",
    "section": "A Project’s Cost of Capital, practice",
    "text": "A Project’s Cost of Capital, practice\n\nAssuming that the debt \\(\\beta\\) is zero, We can also use X’s unlevered Beta and CAPM:\n\n\n\\[\n\\small \\beta_u =\\frac{77}{77+57} \\times  0.75 + \\frac{57}{77+57} \\times 0 = 0.43\n\\]\n\nTherefore, the unlevered cost of capital is:\n\n\n\n\\[\n\\small r_u = 2.5\\% + 0,43 \\times 6\\% = 5.08\\%\n\\]\n\nIn the first case, we assumed that the expected return debt is equal to its promised yield of 3.2% (which overstates the return)\nIn the second, we assumed the debt has a beta of zero, which implies an expected return equal to the risk-free rate of 3% according (which underestimates the cost of debt\nThe truth is somewhere between the two results"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#introducing-cash-and-net-debt",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#introducing-cash-and-net-debt",
    "title": "Estimating the Cost of Capital",
    "section": "Introducing Cash and Net Debt",
    "text": "Introducing Cash and Net Debt\n\nSometimes, firms maintain large cash balances in excess of their operating needs. Holding cash is a risk-free asset, and as so, reduces the average risk of the firm’s assets\nTherefore, we should exclude cash holdings when computing the asset’s risk, as we’re interested in the risk of the firm’s underlying business operations, separate from its cash position\nIn that case, we can measure the leverage of the firm in terms of its net debt:\n\n\n\\[\n\\small \\text{Net Debt} = Debt - \\text{Excess Cash and Short-Term Investments}\n\\]\n\nIntuition: if a firm holds $1 in cash and has $1 in risk-free debt, then the interest earned on the cash will equal the interest paid on the debt\nThe cash flows from each source cancel each other, just as if the firm held no cash and no debt!"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#a-projects-cost-of-capital-practice-3",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#a-projects-cost-of-capital-practice-3",
    "title": "Estimating the Cost of Capital",
    "section": "A Project’s Cost of Capital, practice",
    "text": "A Project’s Cost of Capital, practice\n\nIn mid-2022, Garmin had a market capitalization of $18.8 billion, $100 million in debt, and $1.6 billion in cash. If its estimated equity beta was 0.93, estimate the beta of Garmin’s underlying business\n\n\n\nIn other words, we want to estimate the beta of Garmin’s assets, which comprise the whole business, not just the equity portion! Note that Garmin’s Net Debt is $ \\(\\small (0.1-1.6)=-1.5\\) billion, and the enterprise value is simply \\(\\small \\text{Equity + Net Debt = } 18.8-1.6=17.3\\). Assuming that cash and debt are both risk-free investments, we can estimate \\(\\beta_U\\) as:\n\n\n\n\\[\n\\small \\beta_U = \\frac{E}{E+D}\\times \\beta_E + \\frac{D}{E+D} \\times \\beta_D =  \\frac{18.8}{18.8-1.5}\\times 0.93 + \\frac{-1.5}{18.8-1.5} = 1.01\n\\]\n\nIf a firm has more cash than debt, its net debt will be negative. In this case, its unlevered beta and cost of capital will exceed its equity beta and cost of capital, as the risk of the firm’s equity is mitigated by its cash holdings!"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#industry-betas-for-estimating-a-projects-cost-of-capital",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#industry-betas-for-estimating-a-projects-cost-of-capital",
    "title": "Estimating the Cost of Capital",
    "section": "Industry betas for estimating a project’s cost of capital",
    "text": "Industry betas for estimating a project’s cost of capital\n\nUsing a single comparable firm is often not a good idea, as there might be a lot of noise in the estimation of the results\nHowever, it is possible to combine estimates of asset betas for multiple firms in the same industry to reduce our estimation error and improve the estimation accuracy:\n\n\nFor example, instead of using only one comparable firm to find the unlevered \\(\\beta\\), we may use the average (or median) of several firms that are thought of as comparable peers\nAs you imagine, unlevered betas within an industry are much more stable than the pure equity betas: large differences in the firms’ equity betas are mainly due to differences in leverage, whereas the firms’ asset betas are much more similar, suggesting that the underlying businesses in this industry have similar market risk\n\n\n\\(\\rightarrow\\) See accompanying excel notebook with an exercise on Industry betas\n\\(\\rightarrow\\) See Damodaran (here) industry betas for the U.S."
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#project-risk-and-financing-some-caveats",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#project-risk-and-financing-some-caveats",
    "title": "Estimating the Cost of Capital",
    "section": "Project Risk and Financing, some caveats",
    "text": "Project Risk and Financing, some caveats\n\nWe saw that we should evaluate a project’s cost of capital by comparing it with the unlevered assets of firms in the same line of business\nHowever, even firm asset betas reflect the market risk of the average project in the firm. Individually, projects can differ in risk:\n\nFor instance, think about a multi-divisional firm, like 3M, where each division will likely have its own level of market risk.\nEven within a firm with a single line of business, some projects obviously have different market risk characteristics from the firm’s other activities, like R&D versus product line expansion projects\nFinally, Operating Leverage may also play a role - see (Berk and DeMarzo 2023) for a detailed discussion"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#financing-with-equity-and-debt",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#financing-with-equity-and-debt",
    "title": "Estimating the Cost of Capital",
    "section": "Financing with Equity and Debt",
    "text": "Financing with Equity and Debt\n\nThus far, presumed the project we are evaluating is all-equity financed:\n\nWhenever we have another comparable firm that is 100% equity-financed and has similar risk, we can use it to estimate expected returns\nIf the comparable firm has equity and debt, we can unlever the cost of capital (or the \\(\\beta\\)) so as to compare the underlying business\n\nWhat if the firm in which we wish to estimate the cost of capital is not 100% equity anymore?\nFor cases like this, we will turn our attention to the Weighted Average Cost of Capital (also known as WACC), the weighted average of the cost of capital from all the firm’s claimants:\n\n\nIt takes into account the cost of equity, \\(r_E\\), which is generally estimated using the CAPM\nIt also takes into account the fact that debtholders should also expect a return, \\(r_D\\) (which is generally lower than \\(r_E\\)) for financing the firm"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-weighted-average-cost-of-capital-wacc",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#the-weighted-average-cost-of-capital-wacc",
    "title": "Estimating the Cost of Capital",
    "section": "The Weighted Average Cost of Capital (WACC)",
    "text": "The Weighted Average Cost of Capital (WACC)\n\nIn perfect capital markets (which we’ll define later in the course), the WACC of the firm’s assets can be estimated as:\n\n\n\\[\nr_{\\text{WACC}} = \\frac{E}{D+E}\\times r_E + \\frac{D}{D+E} \\times r_D\n\\]\n\n\n\nBecause interest expense is often tax deductible (using, for example, a marginal tax-rate \\(\\tau\\)), the WACC is less than the expected return of the firm’s assets - this will make the “after-tax” WACC decrease in comparison to the “pre-tax” above:\n\n\n\n\\[\nr_{\\text{WACC}} = \\frac{E}{D+E}\\times r_E + \\frac{D}{D+E} \\times r_D\\times \\underbrace{(1-\\tau)}_{\\text{Tax-shield}}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#pre-tax-wacc",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#pre-tax-wacc",
    "title": "Estimating the Cost of Capital",
    "section": "Pre-tax WACC",
    "text": "Pre-tax WACC\n\nThe pre-tax WACC is simply the weighted average of the different costs of capital:\n\n\n\\[\nr_{\\text{WACC}} = \\frac{E}{D+E}\\times r_E + \\frac{D}{D+E} \\times r_D\n\\]\n\nNote that it is the cost calculated without any adjustments that could arise due to tax-shiedls\n\n\nExpected return investors will earn by holding the firm’s assets - i.e, the whole business\nIn a world with taxes, it can be used to evaluate an all-equity project with the same risk as the comparable firm"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#after-tax-wacc",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#after-tax-wacc",
    "title": "Estimating the Cost of Capital",
    "section": "After-tax WACC",
    "text": "After-tax WACC\n\nThe after-tax WACC takes into account the fact that debt is generally tax-deductible, and therefore should have a lower effective cost than simply \\(r_D\\). If you assume that a company has a marginal tax-rate of \\(\\tau\\), the after-tax WACC is:\n\n\n\\[\nr_{\\text{WACC}} = \\frac{E}{D+E}\\times r_e + \\frac{D}{D+E} \\times r_d\\times (1-\\tau)\n\\]\n\nIn a world with taxes, it can be used to evaluate a project with the same risk and the same financing as the comparable firm\nWhy? Since the after-tax WACC takes into account the benefitial effects from tax-shields, it can only be applied to cases where both the risk and the financing structure of the company is equal to the comparable firm!"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#project-risk-and-financing-practice",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#project-risk-and-financing-practice",
    "title": "Estimating the Cost of Capital",
    "section": "Project Risk and Financing, practice",
    "text": "Project Risk and Financing, practice\n\nCavo Corp’s equity cost of capital is 15%, and its debt cost of capital is 7%. The corporate tax rate is 34%, and the firm has 100 million in debt outstanding and a market capitalization of 250 million. What is Cavo’s unlevered cost of capital (or pre-tax WACC) and its weighted average cost of capital (or after-tax WACC)?\n\n\nThe Unlevered Cost of Capital (\\(r_U\\)) or pre-tax WACC is:\n\n\n\\[\nr_U =  \\frac{250}{250+100}\\times 15\\% + \\frac{100}{250+100} \\times 7\\%  = 12.71\\%\n\\]\n\nThe after-taxWACC is:\n\n\n\n\\[r_{\\text{WACC}} =  \\frac{250}{250+100}\\times 15\\% + \\frac{100}{250+100} \\times 7\\% \\times (1-34\\%) = 12.03\\%\\]"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#practice",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#practice",
    "title": "Estimating the Cost of Capital",
    "section": "Practice",
    "text": "Practice\n\n\n\n\n\n\n\nImportant\n\n\nPractice using the following links:\n\nMultiple-choice Questions\nNumeric Questions"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#levered-and-unlevered-beta",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#levered-and-unlevered-beta",
    "title": "Estimating the Cost of Capital",
    "section": "Levered and Unlevered \\(\\beta\\)",
    "text": "Levered and Unlevered \\(\\beta\\)\n\nRecall that the unlevered beta, \\(\\beta_U\\), is simply:\n\n\n\\[\n\\small \\beta_U = \\frac{E}{E+D}\\times \\beta_E + \\frac{D}{E+D} \\times \\beta_D\n\\]\n\nPutting it into \\(\\beta_E\\) terms, we have:\n\n\n\n\\[\n\\small \\begin{align}\n&\\beta_U = \\frac{(E\\times \\beta_E +D\\times \\beta_D)}{E+D}\\\\\n&\\rightarrow E\\times \\beta_E =(E+D)\\times\\beta_U - D\\times \\beta_D\\\\\n&\\rightarrow\\beta_E =\\dfrac{(E+D)\\times\\beta_U - D\\times \\beta_D}{E}=\\beta_U+\\dfrac{D}{E}\\times \\beta_U-\\dfrac{D}{E}\\times \\beta_D\\\\\n&\\rightarrow \\beta_E= \\beta_U+\\dfrac{D}{E}\\times(\\beta_U-\\beta_D)\n\\end{align}\n\\]\n\nIn words, if leverage increases, the equity sensitivity should increase as well"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#levered-and-unlevered-cost-of-capital",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#levered-and-unlevered-cost-of-capital",
    "title": "Estimating the Cost of Capital",
    "section": "Levered and Unlevered Cost of Capital",
    "text": "Levered and Unlevered Cost of Capital\n\nNote that, as we can lever (or unlever) \\(\\beta\\), we can do the same with the required returns from equity, \\(r_E\\):\n\n\n\\[\n\\small r_U = \\frac{E}{E+D}\\times r_E + \\frac{D}{E+D} \\times r_D\n\\]\n\nPutting it into \\(r_E\\) terms and using the same rationale, we have:\n\n\n\n\\[\nr_E= r_U+\\dfrac{D}{E}\\times(r_U-r_D)\n\\]\n\nIn words, if leverage increases (through higher debt-to-equity ratios), the equity sensitivity should increase as well, and the required returns for equity should increase!"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#common-misconception-relevering-the-wacc",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#common-misconception-relevering-the-wacc",
    "title": "Estimating the Cost of Capital",
    "section": "Common misconception: (re)levering the WACC",
    "text": "Common misconception: (re)levering the WACC\n\nSuppose that a firm has a debt-to-value ratio of 25%, a debt cost of capital of 5.33%, an equity cost of capital of 12%, and a tax rate of 25%. The current WACC is\n\n\n\\[\n\\small r_{\\text{WACC}}=0.75 \\times 12\\% + 0.25\\times 5.33\\% \\times (1- 25\\%) = 10\\%\n\\]\n\nWhat happens to WACC if the firm increases its debt-to-value ratio to 50%? It is tempting to do:\n\n\n\n\\[\n\\small  r_{\\text{WACC}}=0.5 \\times 12\\% + 0.5\\times 5.33\\% \\times (1- 25\\%) = 8\\%\n\\]\n\nNote, however, that this is wrong, because we’re keeping \\(r_E\\) and \\(r_D\\) fixed! Since these are the cost of equity and debt, we should expect these to increase with leverage, as the risk of both shareholders and debt holders increase!"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#common-misconception-relevering-the-wacc-1",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#common-misconception-relevering-the-wacc-1",
    "title": "Estimating the Cost of Capital",
    "section": "Common misconception: (re)levering the WACC",
    "text": "Common misconception: (re)levering the WACC\n\nWhen the firm increases leverage, the risk of its equity and debt will increase, increasing \\(\\small r_E\\) and \\(\\small r_D\\)! To compute the new WACC correctly, we must first determine the firm’s unlevered cost of capital:\n\n\n\\[\n\\small\nr_U = 0.75 \\times 12\\% + 0.25\\times 5.33\\% = 10.33\\%\n\\]\n\nIf \\(r_D\\) has risen to \\(6.67\\%\\) with the change in leverage, then:\n\n\n\n\\[\n\\small\nr_E = 10.33\\% + \\dfrac{0.5}{0.5}\\times(10.33\\%-6.67\\%)=14\\%\n\\]\n\nFinally, the correct new WACC is:\n\n\n\n\\[\n\\small\nr_{\\text{WACC}}=0.5 \\times 14\\% + 0.5\\times 6.67\\% \\times (1- 25\\%) = 9.5\\%\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#references",
    "href": "fin-strat/Lecture 4 - Estimating the Cost of Capital/index.html#references",
    "title": "Estimating the Cost of Capital",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nBrealey, R. A., S. C. Myers, and F. Allen. 2020. Principles of Corporate Finance. The Irwin/McGraw-Hill Series in Finance, Insurance, and Real Estate. McGraw-Hill Education. https://books.google.com.br/books?id=nsrHuwEACAAJ."
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#outline",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#outline",
    "title": "Long-Term Financing through Debt",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\n(Berk and DeMarzo 2023)\n(Brealey, Myers, and Allen 2020)\n\nStudy review and practice: I strongly recommend using Prof. Henrique Castro (FGV-EAESP) materials. Below you can find the links to the corresponding exercises related to this lecture:\n\nMultiple Choice Exercises - click here\n\n\n\n\\(\\rightarrow\\) For coding replications, whenever applicable, please follow this page or hover on the specific slides with coding chunks."
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#raising-long-term-money-through-debt-financing",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#raising-long-term-money-through-debt-financing",
    "title": "Long-Term Financing through Debt",
    "section": "Raising long-term money through debt financing",
    "text": "Raising long-term money through debt financing\n\nFirms can also depart from the choice of issuing equity and instead fund heir long-term investment opportunities through debt financing\nIn this chapter, we’ll see discuss some specific types of debt:\n\nPublic Debt\nPrivate Debt\n\nEach type of debt has its own specific settings, and it is important to bear in mind characteristics such as the presence of covenants, the importance of ratings, and the eventual situation of repayment provisions"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#public-debt",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#public-debt",
    "title": "Long-Term Financing through Debt",
    "section": "Public Debt",
    "text": "Public Debt\n\nOne way that a firm can raise capital through through debt financing is through the issuance of Public Debt:\n\nIt refers to Corporate Bonds issued by corporations\nThey account for a significant amount of invested capital in the U.S.\n\nA key distinction of Public Debt is the fact that it a tradable security in a public market\nDue to its nature, whenever a firm has interest issuing Public Debt, it must follow a Prospectus (similar to the IPO case). In addition to that, the prospectus must include an indenture, a formal contract between the bond issuer and a trust company. The trust company represents the bondholders and makes sure that the terms of the indenture are enforced\nIn the case of default, the trust company represents the bondholders’ interests\n\n\n\\(\\rightarrow\\) Examples: Bonds, Notes, Debentures"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#types-of-corporate-debt",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#types-of-corporate-debt",
    "title": "Long-Term Financing through Debt",
    "section": "Types of Corporate Debt",
    "text": "Types of Corporate Debt\n\nHistorically, corporate bonds have been issued with a wide range of maturities:\n\nMost corporate bonds have maturities of 30 years or less…\n… although in the past there have been original maturities of up to 999 years (“perpetual bonds”)\n\nThe face value or principal amount of the bond is denominated in standard increments, most often $1,000\nThere are several types of Corporate Debt that can be publicly issued - in what follows, we’ll look at the most important ones and its their characteristics"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#unsecured-debt",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#unsecured-debt",
    "title": "Long-Term Financing through Debt",
    "section": "Unsecured Debt",
    "text": "Unsecured Debt\n\nUnsecured Debt is type of corporate debt that, in the event of bankruptcy, gives bondholders a claim to only the assets of the firm that are not already pledged as collateral on other debt. Examples include:\n\nNotes: notes typically are coupon bonds with maturities shorter than 10 years.\nDebentures: typically have longer maturities than notes.\n\nIn general, Notes and/or Debentures are used to finance long-term investment opportunities"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#secured-debt",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#secured-debt",
    "title": "Long-Term Financing through Debt",
    "section": "Secured Debt",
    "text": "Secured Debt\n\nAs opposed to Unsecured Debt, Secured Debt is a type of corporate debt in which specific assets are pledged as collateral. Examples include:\n\nMortgage Bonds: Real property is pledged as collateral that bondholders have a direct claim to in the event of bankruptcy. They are paid with the cash flow source generated by the asset\nAsset-Backed Bonds (ABS): specific assets are pledged as collateral that bondholders have a direct claim to in case of bankruptcy. Can be any kind of asset that a firm owns\n\n\n\n\\(\\rightarrow\\) Asset-backed securities (ABS) played a key role in the 2008 sub-prime financial crisis. Among other effects, the fact that any asset can be pledged as collateral can create issues if there are widespread distortions in the market value of these assets"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#asset-backed-securities-at-the-onset-of-the-subprime-financial-crisis",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#asset-backed-securities-at-the-onset-of-the-subprime-financial-crisis",
    "title": "Long-Term Financing through Debt",
    "section": "Asset-Backed Securities at the onset of the subprime Financial Crisis",
    "text": "Asset-Backed Securities at the onset of the subprime Financial Crisis\n\n\\(\\rightarrow\\) See “The Big Short” (link)"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#corporate-debt---terms-and-characteristics",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#corporate-debt---terms-and-characteristics",
    "title": "Long-Term Financing through Debt",
    "section": "Corporate Debt - Terms and Characteristics",
    "text": "Corporate Debt - Terms and Characteristics\n\nPublic Debt can vary significantly in terms of offer characteristics - the most important are:\n\n\nTranches: different classes of securities that comprise a single bond issue - it helps creating different offers for bond investors\nSeniority: a bondholder’s priority in claiming assets not already securing other debt. Most debenture issues contain covenants restricting the company from issuing new debt with equal or higher priority than existing debt\n\n\nSubordinated Debentures: Debt that, in the event of a default, has a lower priority claim to the firm’s assets than other outstanding debt\nSenior Debentures: Debt that, in the event of a default, has a higher priority claim to the firm’s assets than other outstanding debt\n\n\nIssuance Type: Domestic versus Foreign"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#private-debt",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#private-debt",
    "title": "Long-Term Financing through Debt",
    "section": "Private Debt",
    "text": "Private Debt\n\nAs opposed to Public Debt, Private Debt is a type of financing that is not publicly traded\n\nThe private debt market is larger than the public debt market…\n… and it has the advantage that it avoids the cost of registration but on the investor side has the disadvantage of being illiquid\n\nThe most common examples are Loans:\n\nSyndicated Bank Loan: A single loan that is funded by a group of banks rather than just a single bank\nLine of Credit: A credit commitment for a specific time period, typically two to three years, which a company can use as needed\nPrivate Placements: A bond issue that is sold to a small group of investors rather than the general public. Because a private placement does not need to be registered, it is less costly to issue than public debt"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#bond-covenants",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#bond-covenants",
    "title": "Long-Term Financing through Debt",
    "section": "Bond Covenants",
    "text": "Bond Covenants\n\nQuestion: how can debtholders constrain actions that might deviate from their interests?\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nCovenants are restrictive clauses in a bond contract that limit the issuers from undercutting their ability to repay the bonds. As a common feature in bonds, they may, for example:\n\n\nRestrict the ability of management to pay dividends\nRestrict the level of further indebtedness\nSpecify that the issuer must maintain a minimum amount of working capital\n\n\n\n\n\n\nPositive Covenants: the firm must do something (keep specific financial ratios, pay taxes and other obligations, among others\nNegative Covenants: the firm must not do something (sell specific assets, pay too much dividend, among others"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#repayment-provisions",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#repayment-provisions",
    "title": "Long-Term Financing through Debt",
    "section": "Repayment Provisions",
    "text": "Repayment Provisions\n\nA bond issuer typically repays its bonds by making coupon and principal payments as specified in the bond contract. However, the issuer of the bond has other options to repay:\n\nRepurchase a fraction of the outstanding bonds in the market\nMake a tender offer for the entire issue\nExercise a call provision\n\nIt may be optimal for bond issuers to repay bonds to optimize funding costs if, for example, interest rates have fallen (and the firm can refinance at a lower rate)\nIn what follows, we’ll see three different ways that a firm can repay its bond ahead of time"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#repayment-option-1-callable-bonds",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#repayment-option-1-callable-bonds",
    "title": "Long-Term Financing through Debt",
    "section": "Repayment Option #1: Callable Bonds",
    "text": "Repayment Option #1: Callable Bonds\n\nBonds that contain a call provision that allows the issuer to repurchase the bonds at a predetermined price\nIt allows the issuer of the bond the right (but not the obligation) to retire all outstanding bonds on (or after) a specific date, for the call price. The call price is generally set at or above, and expressed as a % of face value (e.g., 102% of face value)\nWhy an issuer might be interested in this option?\n\nA firm may choose to call a bond issue if interest rates have fallen\nThe issuer can lower its borrowing costs by exercising the call on the callable bond and then immediately refinancing the issue at a lower rate"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#repayment-option-1-callable-bonds-continued",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#repayment-option-1-callable-bonds-continued",
    "title": "Long-Term Financing through Debt",
    "section": "Repayment Option #1: Callable Bonds, continued",
    "text": "Repayment Option #1: Callable Bonds, continued\n\nHolders of callable bonds understand that the issuer will exercise the call option only when the coupon rate of the bond exceeds the prevailing market rate\nIf a bond is called, investors must reinvest the proceeds when market rates are lower than the coupon rate they are currently receiving\nThis makes callable bonds relatively less attractive to bondholders than identical non-callable bonds\nA callable bond will trade at a lower price (and therefore a higher yield) than an otherwise equivalent non-callable bond\n\n\n\\(\\rightarrow\\) If the call provision offers a cheaper way to retire the bonds, however, the issuer will forgo the option of purchasing the bonds in the open market and call the bonds instead"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#callable-bonds-exercise",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#callable-bonds-exercise",
    "title": "Long-Term Financing through Debt",
    "section": "Callable Bonds, Exercise",
    "text": "Callable Bonds, Exercise\nIBM has just issued a callable (at par) five-year, \\(\\small8\\%\\) coupon bond with annual coupon payments. The bond can be called at par in one year or anytime thereafter on a coupon payment date. It has a price of $\\(103\\) per $\\(100\\) face value. What is the bond’s yield to maturity and yield to call?\n\n\\(\\rightarrow\\) Solution: to calculate the YTM yield-to-maturity, we find \\(\\small i\\) such that the following equation is zero:\n\\[\n\\small -103+\\sum_{t=1}^{4}\\dfrac{8}{(1+i)^t}+\\dfrac{108}{(1+i)^5}=0\\rightarrow i=7.26\\%\n\\]\nOn the other hand, the yield-to-call is calculated by finding \\(\\small i\\) such that the bond is called at the first available opportunity:\n\\[\n\\small -103+\\dfrac{108}{(1+i)}=0\\rightarrow i=4.85\\%\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#repayment-option-2-sinking-fund",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#repayment-option-2-sinking-fund",
    "title": "Long-Term Financing through Debt",
    "section": "Repayment Option #2: Sinking Fund",
    "text": "Repayment Option #2: Sinking Fund\n\nA method of repaying a bond in which a company makes regular payments into a fund administered by a trustee over the life of the bond. These payments are then used to repurchase bonds\nThis allows the firm to retire some of the outstanding debt without affecting the cash flows of the remaining bonds\nThe trust can either repurchase the bonds in the market (if the price is below the face value) or by lottery at the par (if the price is above face value)"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#repayment-option-3-convertible-bond",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#repayment-option-3-convertible-bond",
    "title": "Long-Term Financing through Debt",
    "section": "Repayment Option #3: Convertible Bond",
    "text": "Repayment Option #3: Convertible Bond\n\nA corporate bond with a provision that gives the bondholder an option to convert each bond owned into a fixed number of shares of common stock.\n\n\nConversion Ratio: The number of shares received upon conversion of a convertible bond, usually stated per \\(\\small\\$1,000\\) of face value\nConversion Price: The face value of a convertible bond divided by the number of shares received if the bond is converted\n\n\nExample: assume a convertible bond with a \\(\\small\\$1,000\\) face value and a conversion ratio of \\(\\small 15\\). You have the following options: 1. If you convert the bond into stock, you will receive \\(\\small15\\) shares 2. If you do not convert, you will receive \\(\\small\\$1,000\\)\n\nBy converting, you essentially “pay” \\(\\small \\$1,000\\) for \\(\\small15\\) shares, implying a price per share of \\(\\small \\$66.67\\)\nIf the price of the stock exceeds \\(\\small \\$66.67\\), you will choose to convert; otherwise, you will take the cash"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#practice",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#practice",
    "title": "Long-Term Financing through Debt",
    "section": "Practice",
    "text": "Practice\n\nTake a look at an example of a Debenture Prospectus for Raízen S.A - access here\nRead more about the dispute between BTG Pactual and Americanas S.A on an accelerated repayment agreement close to when news about the firm’s accounting inconsistencies became public - access here\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPractice using the following links:\n\nMultiple-choice Questions"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#other-types-of-debt",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#other-types-of-debt",
    "title": "Long-Term Financing through Debt",
    "section": "Other Types of Debt",
    "text": "Other Types of Debt\n\nSovereign Debt: Debt issued by national governments. U.S. Treasury securities represents the single largest sector of the U.S. bond market. The same is true un Brazil.\nMunicipal Bonds (Munis): not common in Brazil, these bonds are issued by state and local governments\nAsset-Backed Securities (ABS): securities made up of other financial securities. Security’s cash flows come from the cash flows of the underlying financial securities that “back” it. This piece was in the center of the 2008 financial crisis"
  },
  {
    "objectID": "fin-strat/Lecture 6 - Debt Financing/index.html#references",
    "href": "fin-strat/Lecture 6 - Debt Financing/index.html#references",
    "title": "Long-Term Financing through Debt",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nBrealey, R. A., S. C. Myers, and F. Allen. 2020. Principles of Corporate Finance. The Irwin/McGraw-Hill Series in Finance, Insurance, and Real Estate. McGraw-Hill Education. https://books.google.com.br/books?id=nsrHuwEACAAJ."
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#outline",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#outline",
    "title": "Debt and Taxes",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\n(Berk and DeMarzo 2023)\n(Brealey, Myers, and Allen 2020)\n\nStudy review and practice: I strongly recommend using Prof. Henrique Castro (FGV-EAESP) materials. Below you can find the links to the corresponding exercises related to this lecture:\n\nMultiple Choice Exercises - click here\n\n\n\n\\(\\rightarrow\\) For coding replications, whenever applicable, please follow this page or hover on the specific slides with coding chunks."
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#a-recap-from-modigliani-and-miller",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#a-recap-from-modigliani-and-miller",
    "title": "Debt and Taxes",
    "section": "A recap from Modigliani and Miller",
    "text": "A recap from Modigliani and Miller\n\nIn our previous lecture, we saw the how the Modigliani and Miller propositions played a key role in explaining capital structure decisions:\n\nIn perfect capital markets, the total value of a firm’s securities is equal to the market value of the total cash flows generated and is not affected by its choice of capital structure\nMoreover, the cost of capital of levered equity increases with the firm’s market value debt equity ratio:\n\n\n\n\\[\n\\small r_E=r_U+\\dfrac{D}{E}(r_U-r_D)\n\\]\n\n\n\\(\\rightarrow\\) As a consequence, the weighted average cost of capital, WACC, should not be affected by the mix of debt and equity!"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#wacc-dynamics-perfect-capital-markets",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#wacc-dynamics-perfect-capital-markets",
    "title": "Debt and Taxes",
    "section": "WACC dynamics (Perfect Capital Markets)",
    "text": "WACC dynamics (Perfect Capital Markets)"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#why-should-i-bother-about-modigliani-and-miller",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#why-should-i-bother-about-modigliani-and-miller",
    "title": "Debt and Taxes",
    "section": "Why should I bother about Modigliani and Miller?",
    "text": "Why should I bother about Modigliani and Miller?\n\nYou may well think…but we are not in a perfect capital market, so why should I bother? That goes hand in hand with the Modigliani and Miller findings:\n\nIf capital markets were perfect (as in the Modigliani and Miller world), then the value of a firm should not be affected by any financial transaction…\n… this implies that if there is a financial transaction that appears to be profitable, it must be that it is exploiting some type of market imperfection!\n\nKnowing how these imperfections affect the firm value is important for business and policy considerations\n\nFor example, what happens if you remove the tax-shield from debt interest payments? What if the government decides to tax dividends differently?\nThese (and other) actions may create/remove market imperfections that will tweak the real-practice results away/closer to the Modigliani-Miller findings!"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#reconciling-modigliani-and-miller-with-debt",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#reconciling-modigliani-and-miller-with-debt",
    "title": "Debt and Taxes",
    "section": "Reconciling Modigliani and Miller with Debt",
    "text": "Reconciling Modigliani and Miller with Debt\nQuestion: if capital structure does not matter, why is debt so abundant?\n\nThe statement that capital structure decisions are irrelevant for the value of a firm are difficult to reconcile with the observation that firms invest significant resources, both in terms of managerial time and effort and investment banking fees, in managing their capital structures\nIn many instances, the choice of leverage is of critical importance to a firm’s value and future success:\n\nFirms issue large amounts of debt to pay dividends\nLarge funds buy companies issuing substantial amounts of debt through leveraged buyouts\n\n\n\n\\(\\rightarrow\\) Therefore, if capital structure does matter, then it must stem from a market imperfection!\n\nIn this lecture, we focus on one such imperfection — taxes. As we’ll see, when imperfections from taxes are present, managers can increase the value of a firm through leverage!"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#the-interest-tax-deduction",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#the-interest-tax-deduction",
    "title": "Debt and Taxes",
    "section": "The Interest Tax Deduction",
    "text": "The Interest Tax Deduction\n\nIn most cases, firms pay taxes on their profits only after interest payments are deducted\n\nInterest expense reduces the amount of corporate taxes\nAs a consequence, this creates an incentive to use debt\n\n\n\n\n\n\n\nWith Leverage\nWithout Leverage\n\n\n\n\nEBIT\n$2,800\n$2,800\n\n\nInterest Expenses\n-$400\n$0\n\n\nIncome before tax\n$2,400\n$2,800\n\n\nTaxes (35%)\n-$840\n-$980\n\n\nNet income\n$1,560\n$1,820\n\n\n\n\n\n\nInvestors receive \\(\\small \\$1560 + \\$400\\) (with leverage) versus \\(\\small \\$1820 + 0\\) (without leverage). Therefore, in the presence of taxes, leverage increases firm value (equity value + debt value) because the total cash flow received by investors increases!"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#the-interest-tax-deduction-continued",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#the-interest-tax-deduction-continued",
    "title": "Debt and Taxes",
    "section": "The Interest Tax Deduction, continued",
    "text": "The Interest Tax Deduction, continued\n\nWe saw that firms can increase value in the presence of taxes through levering up. By how much the firm value increases? You can see that the value of the\n\n\n\\[\n\\small \\underbrace{980}_{\\text{Int. Expenses w/o Leverage}} - \\underbrace{840}_{\\text{Int. Expenses w/ Leverage}} = \\underbrace{140}_{\\text{Interest Tax-Shield}}\n\\]\n\nThe Interest Tax-Shield is the reduction in taxes achieved due to the higher leverage. Assuming a tax-rate of \\(\\small 35\\%\\), we can calculate it as:\n\n\n\n\\[\n\\small \\text{Tax-Shield}= \\tau \\times \\text{Interest Payments} \\rightarrow 35\\% \\times 400 = 140\n\\]\n\nIn words, because interest expenses reduce the amount of taxable income, the firm pay less taxes!"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#valuing-the-interest-tax-shield",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#valuing-the-interest-tax-shield",
    "title": "Debt and Taxes",
    "section": "Valuing the Interest Tax Shield",
    "text": "Valuing the Interest Tax Shield\n\nWhen a firm uses debt, the interest tax shield provides a corporate tax benefit each and every year when interest payments are computed:\n\nIt is if the firm was receiving an extra cash flow each year, and this will be used to pay down investors (debtholders + shareholders)\nThis benefit is then computed as the present value of the stream of future interest tax shields the firm will receive\n\nAs a consequence, the value of the firm, when calculated on the basis of the discounted future cash-flows, will increase due to leverage! Formally, we can reframe our Modigliani-Miller Proposition I in the presence of with taxes: the total value of the levered firm exceeds the value of the firm without leverage due to the present value of the tax savings from debt:\n\n\n\\[\n\\small \\underbrace{V^L}_{\\text{Value of Levered Firm}} = \\underbrace{V^U}_{\\text{Value of Unlevered Firm (all-equity)}} + \\underbrace{PV(ITS)}_{\\text{Value of Tax-Shields}}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#levered-and-unlevered-value",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#levered-and-unlevered-value",
    "title": "Debt and Taxes",
    "section": "Levered and Unlevered Value",
    "text": "Levered and Unlevered Value\n\n\n\n\n\n\\(\\rightarrow\\) By increasing the cash flows paid to debt holders through interest payments, a firm reduces the amount paid in taxes!"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#valuing-the-interest-tax-shield-practice",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#valuing-the-interest-tax-shield-practice",
    "title": "Debt and Taxes",
    "section": "Valuing the Interest Tax Shield, practice",
    "text": "Valuing the Interest Tax Shield, practice\nSuppose ALCO plans to pay 60 million in interest each year for the next eight years, and then repay the principal of 1 billion in year 8. These payments are risk free, and ALCO’s marginal tax rate will remain 39% throughout this period. If the risk-free interest rate is 6%, by how much does the interest tax shield increase the value of ALCO?\n\n\\(\\rightarrow\\) Solution: The annual interest tax shield that is accrued to the firm in from years 1 to 8 is:\n\n\n\\[\n\\small 1\\text{ billion}\\times 6\\% \\times 39\\% = 23.4\\text{ million}\n\\]\n\nTherefore, we can calculate the present value of the tax-shields as:\n\n\n\n\\[\n\\small PV(ITS) = \\frac{23,4}{(1+6\\%)^1} + \\frac{23,4}{(1+6\\%)^2}+. . . + \\frac{23,4}{(1+6\\%)^8} = 145.31 \\text{ million}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#valuing-the-interest-tax-shield-in-practice",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#valuing-the-interest-tax-shield-in-practice",
    "title": "Debt and Taxes",
    "section": "Valuing the Interest Tax Shield in practice",
    "text": "Valuing the Interest Tax Shield in practice\n\nWhen analyzing levered firms, one may think about several debt dynamics that can occurs. Typically, the level of future interest payments is uncertain due to factors such as:\n\nChanges in the marginal tax rate;\nAmount of debt outstanding;\nInterest rate on the actual debt;\nFirm’s risk, among others\n\nQuestion: how can we value the interest tax-shield when taking these nuances in consideration? For simplicity, we’ll start by considering the special case in which the above variables are kept constant. This is reasonable because:\n\nMany corporations have policies for fixed amounts of debt\nAs old bonds and loans mature, new borrowing takes place\nFinally, debt can be assumed as permanent, because it is fixed through time"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#case-i-valuing-the-interest-tax-shield---permanent-debt",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#case-i-valuing-the-interest-tax-shield---permanent-debt",
    "title": "Debt and Taxes",
    "section": "Case I: Valuing the Interest Tax Shield - Permanent Debt",
    "text": "Case I: Valuing the Interest Tax Shield - Permanent Debt\n\nSuppose a firm borrows a given level of debt \\(D\\) and keeps the it permanently on its balance-sheets. If the firm’s marginal tax rate is \\(\\tau_c\\), and if the debt is riskless with a risk-free interest rate \\(r_f\\), then the interest tax shield each year is \\(\\tau_c \\times r_f \\times D\\), and the tax shield can be valued as a perpetuity:\n\n\n\\[\n\\small PV(ITS) = \\frac{\\tau_c \\times \\text{Int. Expenses}}{r_f} = \\frac{\\tau_c \\times (r_f \\times D)}{r_f} = \\tau_c \\times D\n\\]\n\nFor example, given a \\(\\small 21\\%\\) corporate tax rate, this equation implies that for every \\(\\small \\$1\\) in new permanent debt that the firm issues, the value of the firm increases by \\(\\small \\$0.21\\)!"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#valuing-the-interest-tax-shield---dynamics",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#valuing-the-interest-tax-shield---dynamics",
    "title": "Debt and Taxes",
    "section": "Valuing the Interest Tax Shield - Dynamics",
    "text": "Valuing the Interest Tax Shield - Dynamics\n\nYou may wonder from where the increased value of the levered firm (\\(\\small V^L &gt; V^U\\)) came from\nIt is easy to see that when a firm uses debt financing, the cost of the interest it must pay is offset to some extent by the tax savings from the interest tax shield. Assuming a marginal tax-rate of \\(\\small \\tau_c = 21\\%\\) and a permanent level of debt \\(\\small D = 100,000\\), at \\(\\small10\\%\\) percent interest per year:\n\n\n\n\n\nInterest expense\n$10,000\n\n\nTax savings\n- $2,100\n\n\nAfter-tax cost of debt\n$7,900\n\n\n\n\nTherefore, the effective (or after-tax cost of debt) is:\n\n\n\n\\[\n\\small \\underbrace{r_d}_{\\text{Cost of Debt}} \\times \\underbrace{(1-\\tau_c)}_{\\text{Tax-Shield Factor}}\\rightarrow 10\\%\\times(1-21\\%)=7.9\\%\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#valuing-the-interest-tax-shield---dynamics-continued",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#valuing-the-interest-tax-shield---dynamics-continued",
    "title": "Debt and Taxes",
    "section": "Valuing the Interest Tax Shield - Dynamics (continued)",
    "text": "Valuing the Interest Tax Shield - Dynamics (continued)\n\nTherefore, we can write the After-tax WACC:\n\n\n\\[\\small r_{WACC} = \\frac{E}{E+D} \\times r_e + \\frac{D}{E+D} \\times r_d \\times (1-\\tau_c)\\]\n\nMore specifically, you can see this in terms of:\n\n\n\n\\[\n\\small\nr_{WACC} = \\underbrace{\\frac{E}{E+D} \\times r_e + \\frac{D}{E+D} \\times r_d}_{\\text{pre-tax WACC}} \\underbrace{-\\frac{D}{E+D} \\times r_d \\times \\tau_c}_{\\text{Reduction due to Interest Tax-Shield}}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#valuing-the-interest-tax-shield---dynamics-continued-1",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#valuing-the-interest-tax-shield---dynamics-continued-1",
    "title": "Debt and Taxes",
    "section": "Valuing the Interest Tax Shield - Dynamics (continued)",
    "text": "Valuing the Interest Tax Shield - Dynamics (continued)"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#case-ii-assuming-a-target-debt-equity-ratio",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#case-ii-assuming-a-target-debt-equity-ratio",
    "title": "Debt and Taxes",
    "section": "Case II: assuming a Target Debt-Equity Ratio",
    "text": "Case II: assuming a Target Debt-Equity Ratio\n\nSo far, the value of the tax shield was found assuming a constant level of debt\n\nInstead of assuming a permanent, or fixed value of debt, we can also assume that a firm maintains a constant debt-equity ratio\nIn other words, although the absolute debt levels change with the size of the firm, the proportions of debt and equity are constant!\n\nWhen a firm adjusts its leverage to maintain a target debt-equity ratio, we can compute its value with leverage, \\(\\small V^L\\), by discounting its free cash flow using the after-tax WACC. The value of the interest tax shield can be found by comparing:\n\nThe value of the levered firm, \\(\\small V^L\\)\nAnd the value of the unlevered firm, \\(\\small V^U\\)\n\n\n\n\\[\n\\small \\underbrace{V^L}_{\\text{Value of Levered Firm}} = \\underbrace{V^U}_{\\text{Value of Unlevered Firm (all-equity)}} + \\underbrace{PV(ITS)}_{\\text{Value of Tax-Shields}}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#case-ii-assuming-a-target-debt-equity-ratio-practice",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#case-ii-assuming-a-target-debt-equity-ratio-practice",
    "title": "Debt and Taxes",
    "section": "Case II: assuming a Target Debt-Equity Ratio (practice)",
    "text": "Case II: assuming a Target Debt-Equity Ratio (practice)\nHarris Solutions expects to have free cash flow in the coming year of \\(\\small \\$1.75\\) million, and its free cash flow is expected to grow at a rate of \\(\\small 3.5\\%\\) per year thereafter. Harris Solutions has an equity cost of capital of \\(\\small 12\\%\\) and a debt cost of capital of \\(\\small7\\%\\), and it pays a corporate tax rate of \\(\\small 40\\%\\). If Harris Solutions maintains a debt-equity ratio of \\(\\small2.5\\), what is the value of its interest tax shield?\n\n\\(\\rightarrow\\) Solution: First, compute pre-tax WACC and \\(\\small V^U\\):\n\n\n\\[\n\\small \\text{WACC}_{\\text{Pre-Tax}}=\\small \\frac{E}{E+D} \\times r_e + \\frac{D}{E+D} \\times r_d  =  \\frac{1}{1+2.5} \\times 12\\% + \\frac{2.5}{1+2.5} \\times 7\\% = 8.43\\%\n\\]\n\nTherefore, the unlevered value, or \\(\\small V^U\\), is:\n\n\n\n\\[\n\\small V^U = \\frac{1.75}{8.43\\% - 3.5\\%} = 35.50 \\text{ million}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#case-ii-assuming-a-target-debt-equity-ratio-practice-1",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#case-ii-assuming-a-target-debt-equity-ratio-practice-1",
    "title": "Debt and Taxes",
    "section": "Case II: assuming a Target Debt-Equity Ratio (practice)",
    "text": "Case II: assuming a Target Debt-Equity Ratio (practice)\n\nWe know the value of the unlevered firm. Lnowing that the firm maintains a constant debt-to-equity ratio, the effective cost of debt will be lower due to the tax-shield. We know that the value of the levered firm, \\(V^L\\), can be found using the after-tax WACC:\n\n\n\\[\n\\small \\text{WACC}_{\\text{After-Tax}}=\\frac{1}{1+2.5} \\times 12\\% + \\frac{2.5}{1+2.5} \\times 7\\% \\times (1-40\\%) = 6.43\\%\n\\]\n\n\n\\[\n\\small V^L = \\frac{1.75}{6.43\\% - 3.5\\%} = 59.73 \\text{ million}\n\\]\n\nAs a consequence, the value of the interest tax shield must be:\n\n\n\n\\[\n\\small V^L - V^U = 59.73 - 35.50 = 24.23 \\text{ million}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies",
    "title": "Debt and Taxes",
    "section": "Recapitalization strategies",
    "text": "Recapitalization strategies\n\nWhen a firm makes a significant change to its capital structure, the transaction is called a recapitalization (or simply a “recap”)\nFor example, in a leveraged recapitalization, a firm issues a large amount of debt and uses the proceeds to pay a special dividend or to repurchase shares\n\nIn general, these transactions can reduce firm’s tax payments\nBut do these transactions ultimately benefit shareholders?\n\nAs we’ll see in the next example, the answer is yes: when we alter the amount of taxes due, the current shareholders of the firm benefit from this change!"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies---example",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies---example",
    "title": "Debt and Taxes",
    "section": "Recapitalization strategies - Example",
    "text": "Recapitalization strategies - Example\nAssume that Midco Industries wants to boost its stock price. The company currently has \\(\\small 20\\) million shares outstanding with a market price of \\(\\small \\$15\\) per share and no debt. Midco has had consistently stable earnings and pays a \\(\\small 21\\%\\) tax rate. Management plans to borrow \\(\\small \\$100\\) million on a permanent basis, and they will use the borrowed funds to repurchase outstanding shares.\n\n\nTheir expectation is that the tax savings from this transaction will boost Midco’s stock price and benefit shareholders\nQuestion: is this this expectation realistic?"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies---example-continued",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies---example-continued",
    "title": "Debt and Taxes",
    "section": "Recapitalization strategies - Example (continued)",
    "text": "Recapitalization strategies - Example (continued)\n\nFirst, we can calculate the value of Midco Industries without leverage:\n\n\n\\[\n\\small V^U = 20,000,000 \\times 15 = 300 \\text{ million}\n\\]\n\nWe know that the present value of the tax-shiedls (after recapitalization) is:\n\n\n\n\\[\n\\small \\tau_c \\times D = 21\\% \\times 100 \\text{ million} = 21 \\text{ million}\n\\]\n\nTherefore, the total value of the levered firm, \\(\\small V^L\\) is:\n\n\n\n\\[\n\\small V^L=V^U+PV(ITS)\\rightarrow 300+21= 321 \\text{ million}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies---example-continued-1",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies---example-continued-1",
    "title": "Debt and Taxes",
    "section": "Recapitalization strategies - Example (continued)",
    "text": "Recapitalization strategies - Example (continued)\n\nBecause the value of the debt is \\(\\small \\$100\\) million, the value of the equity is:\n\n\n\\[\n\\small 321 - 100 = 221 \\text{ million}\n\\]\n\nAlthough the value of the shares outstanding drops from to \\(\\small \\$300\\) to \\(\\small\\$ 221\\) million, shareholders will also receive \\(\\small \\$100\\) million that Midco Industries will pay out through the share repurchases\n\nIn total, they will receive the full \\(\\small \\$321\\) million, a gain of \\(\\small \\$21\\) million over the value of their shares without leverage!\nThat is, the firm has the incentive to make such recapitalization"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies---example-continued-2",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies---example-continued-2",
    "title": "Debt and Taxes",
    "section": "Recapitalization strategies - Example (continued)",
    "text": "Recapitalization strategies - Example (continued)\n\nAssume Midco Industries repurchases its shares at the current price of \\(\\small \\$15\\) per share. The firm will repurchase \\(\\small 100/15\\approx 6.67\\) million shares. Therefore, the remaining number of shares is:\n\n\n\\[\n\\small 20- 6.67= 13.33 \\text{ million shares outstanding}\n\\]\n\nThe total value of equity under leverage is \\(\\small \\$221\\) million. Therefore, the new share price is:\n\n\n\n\\[\n\\small \\frac{221}{13.33} = 16.575\n\\]\n\nAll in all, the total gain to shareholders 21 million, which is exactly the interest tax-shield:\n\n\n\n\\[\n\\small (\\underbrace{16.575}_{\\text{New Price}} - \\underbrace{15}_{\\text{Prev. Price}}) = \\underbrace{1.575}_{\\text{Capital Gain}} \\times \\underbrace{13.33}_{\\text{Million Shares}} = 21 \\text{ million}\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies---example-continued-3",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies---example-continued-3",
    "title": "Debt and Taxes",
    "section": "Recapitalization strategies - Example (continued)",
    "text": "Recapitalization strategies - Example (continued)\n\nIn the previous case, the shareholders who remain after the recap receive the benefit of the tax-shield\nHowever, you may have noticed something odd in the previous calculations…\n\nWe assumed that Midco Industries was able to repurchase the shares at the initial price of \\(\\small \\$15\\) per share, and then demonstrated that the shares would be worth $16.575 after the transaction\nBut if the shares are worth \\(\\small \\$16.575\\) after the repurchase, why would shareholders tender their shares to Midco at \\(\\small \\$15\\) per share?"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies---example-continued-4",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies---example-continued-4",
    "title": "Debt and Taxes",
    "section": "Recapitalization strategies - Example (continued)",
    "text": "Recapitalization strategies - Example (continued)\n\nIn theory, this represents an Arbitrage Opportunity: if investors could buy shares for \\(\\small \\$15\\) immediately before the repurchase and sell these shares immediately after at a higher price, this would represent a gain without any risks!\nIn practice, the value of the Midco Industries equity will rise immediately, from \\(\\small \\$300\\) million to \\(\\small \\$321\\) million, right after the repurchase announcement. In other words, the stock price will rise from \\(\\small \\$15 \\rightarrow \\$16.05\\) immediately!\n\n\nWith \\(\\small 20\\) million shares outstanding, the share price will rise to: \\(\\small \\$321/20=\\$16.05\\)\nIn other words, Midco Industries must offer at least this price to repurchase the shares, leading to a profit of \\(\\small \\$16.05-\\$15=\\$1.05\\) to shareholders who sell at this price\nNote that all shareholders benefit from this policy: both shareholders who tender their shares and the shareholders who hold their shares both are better-off!"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies---example-continued-5",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#recapitalization-strategies---example-continued-5",
    "title": "Debt and Taxes",
    "section": "Recapitalization strategies - Example (continued)",
    "text": "Recapitalization strategies - Example (continued)\n\nWith a repurchase price of \\(\\small \\$16.05\\), both shareholders who tender their shares and the ones who hold their shares both gain \\(\\small \\$16.05 − \\$15 = \\$1.05\\) per share as a result of the transaction\nThe benefit of the interest tax shield goes to all \\(\\small 20\\) million shares outstanding for a total benefit of:\n\n\n\\[\n\\small \\underbrace{1.05}_{\\text{Gain per share}}\\times \\underbrace{20}_{\\text{Shares Outstanding}} = \\underbrace{21 \\text{ million}}_{\\text{Interest Tax-Shield}}\n\\]\n\n\n\\(\\rightarrow\\) When securities are fairly priced, the original shareholders of a firm capture the full benefit of the interest tax shield from an increase in leverage!\n\\(\\rightarrow\\) See (Berk and DeMarzo 2023), page 566, for a more comprehensible simulation for different values of tender prices"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#practical-exercise",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#practical-exercise",
    "title": "Debt and Taxes",
    "section": "Practical Exercise",
    "text": "Practical Exercise\nAssume that a firm maintains a debt-equity ratio of \\(\\small0.85\\), and has an equity cost of capital of \\(\\small12\\%\\), and a debt cost of capital of \\(\\small7\\%\\). The applicable corporate tax rate is \\(\\small25\\%\\), and its market capitalization is \\(\\small\\$220\\) million. If the firms’ free cash flow is expected to be \\(\\small\\$10\\) million in one year, what constant expected future growth rate is consistent with the firm’s current market value?\n\n\\(\\rightarrow\\) Solution: first, we calculate the WACC as:\n\n\n\\[\n\\small \\text{WACC}= \\dfrac{1}{1+0.85}\\times 12\\% + \\dfrac{0.85}{1+0.85}\\times 7\\% \\times (1-25\\%)=8.90\\%\n\\]\nNote that \\(\\small V^L=E+D\\). If the market capitalization is \\(\\small 220\\), then \\(\\small D = E\\times 0.85=187\\). Therefore, the firm value can be estimated in terms of a growing perpetuity:\n\n\n\\[\n\\small V^L=\\dfrac{FC}{r-g}= (220+187)=\\dfrac{10}{8.90\\%-g}\\rightarrow g \\approx 6.44\\%\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#practical-exercise-1",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#practical-exercise-1",
    "title": "Debt and Taxes",
    "section": "Practical Exercise",
    "text": "Practical Exercise\nBased on the previous exercise, estimate the value of firm’s interest tax shield.\n\n\\(\\rightarrow\\) Solution: first, we calculate the pre-tax WACC (or \\(\\small r_U\\)) as:\n\n\n\\[\n\\small \\text{pre-tax WACC}= \\dfrac{1}{1+0.85}\\times 12\\% + \\dfrac{0.85}{1+0.85}\\times 7\\%=9.70\\%\n\\]\nRemember that you can find the value of unlevered equity (\\(\\small V^U\\)) by discounting using the unlevered cost of capital (or pre-tax WACC):\n\n\n\\[\n\\small V^U=\\dfrac{10}{9,7\\%-6.44\\%}= 307\n\\] Therefore, the present value of the interest tax shield is simply the difference between the levered and unlevered equity:\n\n\n\\[\n\\small PV(ITS)=V^L-V^U\\rightarrow 407-307=100\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#accounting-for-personal-taxes",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#accounting-for-personal-taxes",
    "title": "Debt and Taxes",
    "section": "Accounting for Personal Taxes",
    "text": "Accounting for Personal Taxes\n\nSo far, we have looked to the benefits of leverage in the presence of corporate taxes. Although it is correct to say that the firm has a higher cash flow due to tax-shield savings, it is not straightforward to say that shareholders are fully benefiting from such increase\nWhat happens to our conclusions on debt benefits when we account for the effect of personal taxes?\n\nThe rate that corporations paid as tax is usually different than that investors pay\nAdditionally, the cash flows to investors are typically taxed twice. Once at the corporate level and then investors are taxed again when they receive their interest or divided payment\n\n\n\n\\(\\rightarrow\\) Since personal taxes have the potential to offset some of the corporate tax benefits of leverage, in order to determine the true tax benefit of leverage, we need to evaluate the combined effect of both corporate and personal taxes!"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#accounting-for-personal-taxes-continued",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#accounting-for-personal-taxes-continued",
    "title": "Debt and Taxes",
    "section": "Accounting for Personal Taxes, continued",
    "text": "Accounting for Personal Taxes, continued\n\nIn the United States and many other countries, interest income has historically been taxed more heavily than capital gains from equity1\nTo determine the true tax benefit of leverage, we need to evaluate the combined effect of both corporate and personal taxes. In order to see that, consider a firm with \\(\\small\\$1\\) of earnings before interest and taxes (EBIT). The firm has two options:\n\nThe firm can either pay this \\(\\small\\$1\\) to debt holders as interest\nAlternatively, it can be used pay equity holders directly - either directly, with a dividend, or indirectly, by retaining earnings\n\nDepending on the type of investor, tax rates are different:\n\nFor a debtholder, the amount paid net of taxes is \\(\\small(1-\\tau_i)\\)\nFor an equityholder, not only the firm pays \\(\\small \\tau_c\\) on its taxable income, but individuals are taxed by \\(\\small \\tau_e\\) at a personal level: \\(\\small (1-\\tau_c)\\times(1-\\tau_e)\\)\n\n\nNote that, in Brazil, dividends are not taxed."
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#comparing-after-tax-cashflows",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#comparing-after-tax-cashflows",
    "title": "Debt and Taxes",
    "section": "Comparing after-tax cashflows",
    "text": "Comparing after-tax cashflows\n\nWe can summarize the previous example using U.S. tax rates as of 2018:\n\n\n\n\n\n\n\n\n\n\nPayment Due\nAfter-Tax Cash Flows\nUsing Current Tax Rates\n\n\n\n\nTo debt holders\n\\(\\small(1 −\\tau_i)\\)\n( 1 − 0.37 ) = 0.63\n\n\nTo equity holders\n\\(\\small(1 − \\tau_c)\\times(1-\\tau_e)\\)\n(1 − 0.21)( 1 − 0.20 ) = 0.632\n\n\n\n\nBased on this, we can write:\n\n\n\n\\[\n\\small (1-\\tau^*) \\times (1-\\tau_i) = (1-\\tau_c)\\times(1-\\tau_e)\n\\]\n\nWe can interpret \\(\\small \\tau^*\\) as the effective tax advantage of debt: if the corporation paid \\(\\small (1-\\tau^*)\\) in interest, debt holders would receive the same amount after taxes as equity holders would receive if the firm paid \\(\\small \\$1\\) in profits to equity holders"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#accounting-for-personal-taxes-continued-1",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#accounting-for-personal-taxes-continued-1",
    "title": "Debt and Taxes",
    "section": "Accounting for Personal Taxes, continued",
    "text": "Accounting for Personal Taxes, continued"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#tax-advantages-of-debt",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#tax-advantages-of-debt",
    "title": "Debt and Taxes",
    "section": "Tax advantages of debt",
    "text": "Tax advantages of debt\n\nUsing this structure, we can calculate the Effective Advantage of Debt as follows:\n\n\n\\[\n\\small \\tau^* = 1-\\frac{(1-\\tau_c)\\times(1-\\tau_e)}{(1-\\tau_i)}\n\\]\n\nFinally, notice that if interest payments and equity payment tax burdens are equivalent, debt policy is irrelevant:\n\n\n\n\\[\n\\small(1-\\tau_e) \\times (1-\\tau_c) = (1-\\tau_i)\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#accounting-for-personal-taxes-practice",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#accounting-for-personal-taxes-practice",
    "title": "Debt and Taxes",
    "section": "Accounting for Personal Taxes, practice",
    "text": "Accounting for Personal Taxes, practice\nAssume that the corporate tax rate (\\(\\small \\tau_c\\)) is \\(\\small 34\\%\\), the personal tax rate on equity (\\(\\small \\tau_e\\)) is \\(\\small 28\\%\\), and the personal tax rate on debt (\\(\\small \\tau_i\\)) is also \\(\\small 28\\%\\). What is the tax advantage of debt for a firm?\n\\(\\rightarrow\\) Solution: using the formula, we have that:\n\\[\n\\small \\tau^* = 1-\\frac{(1-\\tau_c)\\times(1-\\tau_e)}{(1-\\tau_i)}\\rightarrow 1-\\dfrac{(1-34\\%)\\times(1-28\\%)}{(1-28\\%)}=34\\%\n\\]\n\nIn other words, the tax advantage of debt is the corporate tax rate if there is no difference between the tax rate on interest payments or equity\nIf, for example, equity income is taxed less heavily than interest income, then \\(\\small \\tau^\\star\\), the tax advantage of debt, will be lower than the corporate tax rate"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#accounting-for-personal-taxes-practice-1",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#accounting-for-personal-taxes-practice-1",
    "title": "Debt and Taxes",
    "section": "Accounting for Personal Taxes, practice",
    "text": "Accounting for Personal Taxes, practice\n\nConsider the stylized Brazilian case and compute the effective tax advantage of debt.\n\nPersonal tax rate: \\(\\small \\tau_i = 27.5%\\)\nEquity income tax rate: \\(\\small \\tau_e= 15\\%\\)\nCorporate tax rate: \\(\\small \\tau_c = 34\\%\\)\n\nThe tax advantage of debt in a Brazilian setting is:\n\n\n\\[\n\\small \\tau^* = 1-\\frac{(1-34\\%)\\times(1-15\\%)}{(1-27.5\\%)} = 22.6\\%\n\\]"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#practical-implications-personal-taxes",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#practical-implications-personal-taxes",
    "title": "Debt and Taxes",
    "section": "Practical implications Personal Taxes",
    "text": "Practical implications Personal Taxes\n\nThere is a rationable to argue that that tax advantages of debt differ across investors:\n\nTax rates vary for individual investors, and many investors are in lower tax brackets\nHolding periods might also have an effect on personal taxes\n\nGiven the wide range of tax preferences and brackets across investors, it is difficult to know the true value of \\(\\tau^*\\) for a firm\n\nUsing the top personal tax rates likely understates \\(\\tau^*\\)\n\nIgnoring personal taxes likely overstates \\(\\tau^*\\)"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#personal-taxes-and-firm-value",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#personal-taxes-and-firm-value",
    "title": "Debt and Taxes",
    "section": "Personal Taxes and Firm Value",
    "text": "Personal Taxes and Firm Value\n\nWe now can write:\n\n\n\\[V^L = V^U + \\tau^*  \\times D\\]\n\nWe still compute the WACC using the corporate tax rate \\(\\tau_c\\).\n\nWith personal taxes the firm’s equity and debt costs of capital will adjust to compensate investors for their respective tax burdens\nThe net result is that a personal tax disadvantage for debt causes the WACC to decline more slowly with leverage than it otherwise would."
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#estimating-the-interest-tax-shield-with-personal-taxes",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#estimating-the-interest-tax-shield-with-personal-taxes",
    "title": "Debt and Taxes",
    "section": "Estimating the Interest Tax Shield with Personal Taxes",
    "text": "Estimating the Interest Tax Shield with Personal Taxes\nMoreLev Inc. currently has a market cap of \\(\\small\\$500\\) million with \\(\\small40\\) million shares outstanding. It expects to pay a \\(\\small 21\\%\\) corporate tax rate. It estimates that its marginal corporate bondholder pays a \\(\\small20\\%\\) tax rate on interest payments, whereas its marginal equity holder pays on average a \\(\\small10\\%\\) tax rate on income from dividends and capital gains. MoreLev plans to add permanent debt. Based on this information, estimate the firm’s share price after a \\(\\small\\$220\\) million leveraged recap.\n\n\\(\\rightarrow\\) Solution: using the formula for the tax advantage of debt, we have that:\n\n\n\\[\n\\small \\tau^* = 1-\\frac{(1-21\\%)\\times(1-10\\%)}{(1-20\\%)} = 11.1\\%\n\\]\n\nTherefore, having debt in the firm’s capital structure increases the firm’s value to:\n\n\n\n\\[\n\\small \\tau^\\star\\times D \\rightarrow 11.1\\%\\times 220 = 24.4 \\text{ million}\n\\]\n\nShare prices rise by \\(\\small \\$24.4/40\\approx\\$0.61\\), and the new share price is \\(\\small \\$12.5+\\$0.61 = \\$13.11\\)"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#do-firms-prefer-debt-equity-and-debt-issuances",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#do-firms-prefer-debt-equity-and-debt-issuances",
    "title": "Debt and Taxes",
    "section": "Do firms prefer Debt? Equity and Debt issuances",
    "text": "Do firms prefer Debt? Equity and Debt issuances"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#do-firms-prefer-debt-debt-to-value-ratios",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#do-firms-prefer-debt-debt-to-value-ratios",
    "title": "Debt and Taxes",
    "section": "Do firms prefer Debt? Debt-to-Value ratios",
    "text": "Do firms prefer Debt? Debt-to-Value ratios"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#do-firms-prefer-debt-the-cross-section-of-debt-ratios",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#do-firms-prefer-debt-the-cross-section-of-debt-ratios",
    "title": "Debt and Taxes",
    "section": "Do firms prefer Debt? The Cross-section of Debt Ratios",
    "text": "Do firms prefer Debt? The Cross-section of Debt Ratios\n\n\n\n\n\nNote: full picture in (Berk and DeMarzo 2023)"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#optimal-capital-structure-with-taxes",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#optimal-capital-structure-with-taxes",
    "title": "Debt and Taxes",
    "section": "Optimal Capital Structure with Taxes",
    "text": "Optimal Capital Structure with Taxes\n\nThe evidence from the U.S. market outlined in the previous three slides shows us that:\n\nFor the average firm, the result is that debt as a fraction of firm value has varied in a range from 30% to 45%\nThe use of debt varies greatly by industry\nFirms in growth industries like biotechnology or high technology carry very little debt, while airlines, automakers, utilities, and financial firms have high leverage ratios\nFinally, many firms hold huge amounts of cash, actually making net debt negative\n\n\n\nQuestion: considering all the tax benefits of debt, why is that firms do not use more debt?\n\n\n\\(\\rightarrow\\) A potential explanation (among other hypotheses) is that there are limits to the tax benefit of debt that makes firm to optimally set a given debt-to-value ratio"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#limits-to-tax-benefits",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#limits-to-tax-benefits",
    "title": "Debt and Taxes",
    "section": "Limits to tax benefits",
    "text": "Limits to tax benefits\n\nWhen it comes to taxable earnings, there may be limits to how much a firm can deduct interest expenses on its taxable income - in our case, EBIT\n\nTo receive the full tax benefits of leverage, a firm need not use 100% debt financing, but the firm does need to have taxable earnings\nThis constraint may limit the amount of debt needed as a tax shield\n\nFor example, in the United States, there is a limit of \\(\\small 30\\%\\) of the EBIT for how much can be deducted in taxable income due to interest payments1\nConsider a case where EBIT is \\(\\small \\$1,000\\) and the corporate tax rate is \\(\\small 21\\%\\). In a situation like this, a firm may limit its leverage to achieve the highest interest shield possible from a tax perspective - see Table in the next slide\n\nInterest deductions in excess of the 30% limit can be carried forward and used the next time the firm does not exceed the limit. This delay reduces the present value of the tax benefit (and it may ultimately be lost if the firm’s interest coverage remains too low)"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#limits-to-tax-benefits---example",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#limits-to-tax-benefits---example",
    "title": "Debt and Taxes",
    "section": "Limits to tax benefits - example",
    "text": "Limits to tax benefits - example\n\n\n\n\n\n\n\n\n\n\nNo Leverage\nModerate Leverage\nExcess Leverage\n\n\n\n\nEBIT\n$1,000\n$1,000\n$1,000\n\n\nInterest Expenses\n$0\n$300\n$500\n\n\n30% Limit\n$300\n$300\n$300\n\n\nInterest Deduction\n$0\n$300\n$300\n\n\nTaxable Income\n$1,000\n$700\n$700\n\n\nTaxes (21%)\n$210\n$147\n$147\n\n\nNet Income\n$790\n$553\n$353\n\n\nTax Shield\n$0\n$63\n$63\n\n\n\n\\(\\rightarrow\\) The optimal level of leverage from a tax saving perspective is the level such that interest just equals the income limit"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#growth-and-debt",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#growth-and-debt",
    "title": "Debt and Taxes",
    "section": "Growth and Debt",
    "text": "Growth and Debt\n\nHow firms could optimally set their leverage from a tax savings perspective? A way to think about it is to relate to its future EBIT (taxable earnings):\n\nGrowing firms often have no taxable earnings and therefore do not use much debt\nAt the optimal level of leverage, the firm shields all of its taxable income, and it does not have any tax-disadvantaged excess interest\n\n\nHowever, it is unlikely that a firm can predict its future EBIT (and the optimal level of debt) precisely\nIf there is uncertainty regarding EBIT, then there is a risk that interest will exceed EBIT\nAs a result, the tax savings for high levels of interest falls, possibly reducing the optimal level of the interest payment\n\n\n\n\\(\\rightarrow\\) In general, as a firm’s interest expense approaches its expected taxable earnings, the marginal tax advantage of debt declines, limiting the amount of debt the firm should use"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#wrapping-up-the-low-leverage-puzzle",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#wrapping-up-the-low-leverage-puzzle",
    "title": "Debt and Taxes",
    "section": "Wrapping-up: the Low leverage puzzle",
    "text": "Wrapping-up: the Low leverage puzzle\n\n\n\n\n\n\\(\\rightarrow\\) It seems that firms are under-leveraged consistently. Therefore, there must be more to this capital structure story"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#practice",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#practice",
    "title": "Debt and Taxes",
    "section": "Practice",
    "text": "Practice\n\nTake a look at an example of a Debenture Prospectus for Raízen S.A - access here\nRead more about the dispute between BTG Pactual and Americanas S.A on an accelerated repayment agreement close to when news about the firm’s accounting inconsistencies became public - access here\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPractice using the following links:\n\nMultiple-choice Questions"
  },
  {
    "objectID": "fin-strat/Lecture 8 - Debt and Taxes/index.html#references",
    "href": "fin-strat/Lecture 8 - Debt and Taxes/index.html#references",
    "title": "Debt and Taxes",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Global Edition / English Textbooks. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nBrealey, R. A., S. C. Myers, and F. Allen. 2020. Principles of Corporate Finance. The Irwin/McGraw-Hill Series in Finance, Insurance, and Real Estate. McGraw-Hill Education. https://books.google.com.br/books?id=nsrHuwEACAAJ."
  },
  {
    "objectID": "fin-strat.html#about-the-course",
    "href": "fin-strat.html#about-the-course",
    "title": "Financial Strategy",
    "section": "About the course",
    "text": "About the course\nFamiliarize students with concepts and techniques for analyzing and making strategic investment and financing decisions in terms of risk \\(\\times\\) return.\nAt the end of the semester, the student should be able to master the concepts of risk analysis and measurement applications of cost and capital structure, earnings policy and determination of the company’s value.\n\n\n\n\n\n\nFor students\n\n\n\nBelow you find the persistent links to all lectures of the course. As they are continuously updated with fixes and new implementations, you might expect some changes from time to time in the contents of each file.\nThese slides leverage Quarto, an open-source scientific and technical publishing system. They contain both static and dynamic content that will be displayed in your internet browser. Some useful tips for using these slides:\n\nHit F for full-screen mode\nIf you are interested in getting a .pdf version of the slides, hit E to switch to print mode and then Ctrl + P"
  },
  {
    "objectID": "quant-fin/Introduction/index.html#welcome-to-the-course",
    "href": "quant-fin/Introduction/index.html#welcome-to-the-course",
    "title": "Introduction and Course Overview",
    "section": "Welcome to the Course",
    "text": "Welcome to the Course\n\n\nOverview and Course Organization\nGrading and Evaluations\nNavigating through the syllabus\nHow you can get the best of this course\nOverall Q&A"
  },
  {
    "objectID": "quant-fin/Introduction/index.html#overview",
    "href": "quant-fin/Introduction/index.html#overview",
    "title": "Introduction and Course Overview",
    "section": "Overview",
    "text": "Overview\n\nWith the recent advances in technical resources and the vast availability of financial information, finance practitioners are required to generate reproducible and scalable analysis in a timely fashion to guide decision-making:\n\nHow can I continuously optimize the risk-return trade-off of my portfolio over time?\nWhat is the sensitivity of my investment decisions to changes in growth rates, discount rates, and expected margins?\nHow can I assess the ability of a given trading strategy to outperform the market over time?\n\n\n\n\\(\\rightarrow\\) To that point, departing from the general tools, such as Excel, to more advanced tools, such as  and , is an imperative change!\n\nThe goal of this course: translate theoretical concepts learned on the core finance courses to practical applications that can guide decision making in real-world financial markets"
  },
  {
    "objectID": "quant-fin/Introduction/index.html#course-structure",
    "href": "quant-fin/Introduction/index.html#course-structure",
    "title": "Introduction and Course Overview",
    "section": "Course Structure",
    "text": "Course Structure\n\nThis is a hands-on, practical course on Quantitative Finance with applications using  and , two of the most widely used open-source software for data analysis. It will be structured in topics that are of interest to Finance practitioners, aiming to include, but not limited to:\n\nCollecting and organizing financial data\nCAPM, Fama-French, and multi-factor models of risk\nEquily Valuation, sensitivity analysis, and Simulation\nPortfolio optimization and strategy backtesting\nEvent Analysis\nSentiment Analysis\n\nStudents are also expected to interact with leading industry practitioners focused on financial applications using open-source languages, aiming to discover more about the possibilities of applying the skills learned in this course in the financial industry"
  },
  {
    "objectID": "quant-fin/Introduction/index.html#bibliography",
    "href": "quant-fin/Introduction/index.html#bibliography",
    "title": "Introduction and Course Overview",
    "section": "Bibliography",
    "text": "Bibliography\n\n\nCoursework: we will mostly follow Tidy Finance (Scheuch, Voigt, and Weiss 2023), as our text-book. Other relevant reading materials include:\n\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\nMastering Shiny (Wickham 2021)\n\nSupplementary Reading:\n\nCorporate Finance (Berk and DeMarzo 2023) - a companion for finance-related topics\nOther optional contents, such as Harvard Business School (HBS) Cases\n\n\n\n\n\n\n\n\n\nImportant\n\n\nMost of the references listed in this bibliography have an open-source version that is hosted online, where you can copy-paste code chunks directly into your  session. All contents with restricted access will be provided upfront."
  },
  {
    "objectID": "quant-fin/Introduction/index.html#grading-and-evaluations",
    "href": "quant-fin/Introduction/index.html#grading-and-evaluations",
    "title": "Introduction and Course Overview",
    "section": "Grading and Evaluations",
    "text": "Grading and Evaluations\n\nGrading will be composed of the following activities:\n\nHandout Data Cases (40%)\nCapstone Project (40%)\nProject Showcase (15%)\nIn-class Participation (5%)\n\n\n\n\nYou can find the details of any of these activities in the official syllabus (available on eClass®)\nIn case of any questions, feel free to reach out to lucas.macoris@fgv.br\n\n\n\n\n\n\n\n\n\nOffice-hours\n\n\nI also host office-hours (by appointment) on Thursdays, 5PM-6PM. In these sessions, I’ll be more than happy to help you with anything you need from this course. Use the Office-hour Appointments link at the bottom of this slide to schedule an appointment (or click here)."
  },
  {
    "objectID": "quant-fin/Introduction/index.html#getting-the-best-of-this-course",
    "href": "quant-fin/Introduction/index.html#getting-the-best-of-this-course",
    "title": "Introduction and Course Overview",
    "section": "Getting the best of this course",
    "text": "Getting the best of this course"
  },
  {
    "objectID": "quant-fin/Introduction/index.html#getting-the-best-of-this-course-continued",
    "href": "quant-fin/Introduction/index.html#getting-the-best-of-this-course-continued",
    "title": "Introduction and Course Overview",
    "section": "Getting the best of this course, continued",
    "text": "Getting the best of this course, continued\n\nTech setup: in the official page of your course, you will find instructions on how to properly set up your computer in terms of downloading all necessary softwares, packages, and customizing your  session1\nCode Replication: right after we are done with a given topic, try to replicate the in-class handouts on your end and check if you are able to yield the same outputs\nShowcase: programming, data science, analytics, machine learning, and so on…these terms are on the hype of today’s job market - although few people really know how to make meaningful impact with it. Use this course as an opportunity to differentiate and showcase the skills you’ve learned and stand out to potential employers2\n\nWhile we will be using FGV-EAESP’s facilities, where you will have immediate access to all the necessary configurations, ensure to setup your personal computer so as to ensure that you can replicate the contents from the lectures.See Artificial Intelligence is losing hype (The Economist)"
  },
  {
    "objectID": "quant-fin/Introduction/index.html#on-the-usage-of-gen-ai",
    "href": "quant-fin/Introduction/index.html#on-the-usage-of-gen-ai",
    "title": "Introduction and Course Overview",
    "section": "On the usage of gen-AI",
    "text": "On the usage of gen-AI\n\n\n\n\n\n\nOn the usage of ChatGPT and other gen-AI tools\n\n\nGenerative Artificial Intelligent (gen-AI) adoption is quickly spreading through corporate life and universities. At this point, it is worth the question…am I allowed to use gen-AI tools in this course?\n\n\n\n\nThe answer is yes! Not only you are allowed, but also encouraged to do so:\n\nUse gen-AI tools to proof-read your work, get insights, and troubleshoot errors\nLearn to be skeptical around the solutions you have been provided with\nEvaluations will be based on how you can interpret, understand, and showcase your solution to the broader audience!\n\n\n\n\\(\\rightarrow\\) See AI-powered coding pulls in almost $1bn of funding to claim ‘killer app’ status (Financial Times)"
  },
  {
    "objectID": "quant-fin/Introduction/index.html#how-to-use-these-slides",
    "href": "quant-fin/Introduction/index.html#how-to-use-these-slides",
    "title": "Introduction and Course Overview",
    "section": "How to use these slides",
    "text": "How to use these slides\n\nThese slides leverage Quarto, an open-source scientific and technical publishing system from Posit (formerly RStudio):\n\nCreate dynamic content with , , , among other programming languages\nPublish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, Word, ePub, and more\nWrite beatufil, clean technical documents using markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more\n\nFor our course, we’ll use the following notation:\n\n\nLinks will be colored in blue\nInline equations and variables will be rendered in gray\nCode chunks will be provided along with outputs (R)"
  },
  {
    "objectID": "quant-fin/Introduction/index.html#an-example-of-a-code-chunk",
    "href": "quant-fin/Introduction/index.html#an-example-of-a-code-chunk",
    "title": "Introduction and Course Overview",
    "section": "An example of a code chunk",
    "text": "An example of a code chunk\n\nResultR\n\n\n\n\n\n\n\n\nNote\n\n\nIn the R panel, hit Show the Code to display the code inside the tabset. Hit the  button at the top-right to copy it to your session.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\ndat &lt;- data.frame(cond = rep(c(\"A\", \"B\"), each=10),\n                  xvar = 1:20 + rnorm(20,sd=3),\n                  yvar = 1:20 + rnorm(20,sd=3))\n\nggplot(dat, aes(x=xvar, y=yvar)) +\n  geom_point(shape=1) + \n  geom_smooth()"
  },
  {
    "objectID": "quant-fin/Introduction/index.html#interactive-learning",
    "href": "quant-fin/Introduction/index.html#interactive-learning",
    "title": "Introduction and Course Overview",
    "section": "Interactive Learning",
    "text": "Interactive Learning\n\n\n\nListing 1: We will extensively leverage interactive learning whenever possible. In selected sections, called Listings, you will be prompted with an interactive R console that you can use to run existing and new code to a “virtual” session. Try changing the ticker to NVDA and check if anything has changed."
  },
  {
    "objectID": "quant-fin/Introduction/index.html#interactive-learning-continued",
    "href": "quant-fin/Introduction/index.html#interactive-learning-continued",
    "title": "Introduction and Course Overview",
    "section": "Interactive Learning, continued",
    "text": "Interactive Learning, continued\n\nExerciseHintsSolution\n\n\n\n\n\nListing 2: You can use the Hints and Solution buttons to interact with the prompt. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution. For example, complete the code to read Microsoft price (MSFT.csv) data and select the latest 10 OHLC (Open, High, Low, Close) information. The dataset is arranged in descendant format (latest price information is at the bottom of the table).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can access the names of the columns using names(Data) to find the column that refers to closing prices:\nnames(Data)\nYou can also select specific columns by using the select() function:\nselect(Data,c(column1,column2,...,column_n))`\n\n\n\n\nFirst, use the names() function to retrieve the names of the columns available in the dataset.\nAfter that, use the tail() function to retrieve only the latest 10 observations.\n\n\n\n#Set the ticker\nData=read.csv(glue('Assets/MSFT.csv'))\nData=select(Data,c(Timestamp,Open,High,Low,Close))\nData=tail(Data,10)\nData\n\n#Set the ticker\nData=read.csv(glue('Assets/MSFT.csv'))\nData=select(Data,c(Timestamp,Open,High,Low,Close))\nData=tail(Data,10)\nData"
  },
  {
    "objectID": "quant-fin/Introduction/index.html#why-r",
    "href": "quant-fin/Introduction/index.html#why-r",
    "title": "Introduction and Course Overview",
    "section": "Why R?",
    "text": "Why R?\n\nWe will be using  throughout the course. Although you should feel that this course is language-agnostic, R is among one of the best choices for the task:\n\n\n\nFree and open-source\nDiverse and active community working on a broad range of tools\nActively maintained packages for various purposes - e.g., data manipulation, visualization, modeling, etc)\nPowerful tools for communication, such as Quarto and Shiny\nSmooth integration with other programming languages, e.g., SQL, Python, C, C++, Fortran\nRStudio is one of the best development environments for interactive data analysis\nTop-notch tools and packages for handling financial data and analysis1\n\n\nFor a comprehensive list of R packages designed for finance, please refer to the R Task View: Empirical Finance index - access here."
  },
  {
    "objectID": "quant-fin/Introduction/index.html#setting-up-your-environment",
    "href": "quant-fin/Introduction/index.html#setting-up-your-environment",
    "title": "Introduction and Course Overview",
    "section": "Setting up your environment",
    "text": "Setting up your environment\n\nAs we get started, there are a couple of things you should remember:\n\n works with libraries, which consists of a bundle of functions, methods, data and other components that can be loaded in your session (i.e, as you open RStudio or any other IDE of your preference)\nTo load a library, you call library(x), where x refers to the package name\n\n\nIf x is already installed in your computer, you are good to go\nIf, on the other hand, x is not installed, you need to call install.packages('x') before you attempt to load it\n\n\ninstall.packages() needs to be called once; library() needs to be called at the beginning of each session!"
  },
  {
    "objectID": "quant-fin/Introduction/index.html#setting-up-your-environment-continued",
    "href": "quant-fin/Introduction/index.html#setting-up-your-environment-continued",
    "title": "Introduction and Course Overview",
    "section": "Setting up your environment, continued",
    "text": "Setting up your environment, continued\n\nTo make things easier, ensure to install these packages in your computer and load it at the beginning of every session - I’ll make sure to update this list whenever needed throughout the sessions in the course’s official webpage\n\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\",\"glue\",\"scales\",\"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\nlapply(packages, library, character.only = TRUE)\n\n#Note that you could simply do it iteratively:\n\n#Install if not already available\n  #install.packages('tidyverse')\n  #install.packages('tidyquant')\n  #install.packages('tidymodels')\n  #install.packages('xts')\n  #install.packages('glue')\n  #install.packages('scales')\n  #install.packages('ggthemes')\n\n#Load\n  #library(tidyverse)\n  #library(tidyquant)\n  #library(tidymodels)\n  #library(xts)\n  #library(glue)\n  #library(scales)"
  },
  {
    "objectID": "quant-fin/Introduction/index.html#other-things-you-should-consider-quarto",
    "href": "quant-fin/Introduction/index.html#other-things-you-should-consider-quarto",
    "title": "Introduction and Course Overview",
    "section": "Other things you should consider: Quarto",
    "text": "Other things you should consider: Quarto\n\nIn this course, you’ll be assigned with three data cases, where you’ll need to manipulate code and write your insights altogether. You may have heard of Jupyter Notebooks before as a way to do it. I want to encourage you to give Quarto a try"
  },
  {
    "objectID": "quant-fin/Introduction/index.html#other-things-you-should-consider-quarto-continued",
    "href": "quant-fin/Introduction/index.html#other-things-you-should-consider-quarto-continued",
    "title": "Introduction and Course Overview",
    "section": "Other things you should consider: Quarto, continued",
    "text": "Other things you should consider: Quarto, continued\n\nTo install Quarto, follow this link and choose your Operating System. RStudio will automatically locate it and make it as an option:\n\n\n\n\n\n\nKey Highlights:\n\nMulti-language support (Python, R, Julia, JavaScript) and seamless integration with GitHub\nAdvanced document formatting and output options: you can choose pdf, html, docx, or even a reveal.js presentation (like the one you’re reading right now)"
  },
  {
    "objectID": "quant-fin/Introduction/index.html#getting-started",
    "href": "quant-fin/Introduction/index.html#getting-started",
    "title": "Introduction and Course Overview",
    "section": "Getting started",
    "text": "Getting started\n\nTo get started, you’ll need to:\n\n\n\nInstall R using this link\nInstall RStudio using this link\nInstall Quarto using this link\n\n\n\n\nIf you’re new to R or need a refresher on the basics, please refer to Projects I and II of the Hands-On Programming with R (Grolemund 2014)\n\n\n\n\n\n\n\n\n\nTech-setup\n\n\nIn the official webpage of this course, I have outlined all necessary steps to get started using R, as well as some useful tips for those that want to get up to speed on the course’s requirements - please follow this link and carefully read the instructions."
  },
  {
    "objectID": "quant-fin/Introduction/index.html#course-pre-assessment",
    "href": "quant-fin/Introduction/index.html#course-pre-assessment",
    "title": "Introduction and Course Overview",
    "section": "Course pre-assessment",
    "text": "Course pre-assessment\n\nFill out the form below, share your thoughts, and help me tailor the course to meet your needs and track your progress 🙂"
  },
  {
    "objectID": "quant-fin/Introduction/index.html#references",
    "href": "quant-fin/Introduction/index.html#references",
    "title": "Introduction and Course Overview",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with R. Sebastopol, CA: O’Reilly Media.\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley. 2021. Mastering Shiny. O’Reilly Media. https://mastering-shiny.org/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#outline",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#outline",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\nTidy Finance (Scheuch, Voigt, and Weiss 2023)\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\n\n\n\n\n\n\n\n\nCoding Replications\n\n\nFor coding replications, whenever applicable, please follow this page or hover on the specific slides with containing coding chunks.\n\nEnsure that you have your  session properly set-up according to the instructions outlined in the course webpage\nAlong with the slides, this lecture will also contain a replication file, in .qmd format, containing a thorough discussion for all examples that have been showcased. This file, that will be posted on eClass®, can be downloaded and replicated on your side. To do that, download the file, open it up in RStudio, and render the Quarto document using the Render button (shortcut: Ctrl+Shift+K).\nAt the end of this lecture, you will be prompted with a hands-on exercise to test your skills using the tools you’ve learned as you made your way through the slides. A suggested solution will be provided in the replication file."
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#organizing-financial-data",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#organizing-financial-data",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Organizing Financial Data",
    "text": "Organizing Financial Data\n\n\n\n\n\n\nA note on Tidy Data\n\n\n“Tidy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).”\n\n\n\n\nStock price information is an example of raw data that can be easily pulled from providers such as Yahoo! Finance. However, it is not often structured in a tidy and convenient way\n\nIn this lecture, we will be working with daily stock price data from the Magnificent Seven (AAPL, GOOG, MSFT, NVDA, TSLA, AMZN, and META)\nI have already downloaded the data for you using the tidyquant package, which allows us to pull stock price data from multiple securities in a convenient format. You can hit the Download button to get a grasp on how the data looks like\n\n\n\n\n Download"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-basic-dplyr-verbs-recap",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-basic-dplyr-verbs-recap",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The basic dplyr verbs: recap",
    "text": "The basic dplyr verbs: recap\n\n\ndplyr is a grammar of data manipulation, contained in the tidyverse, providing a consistent set of verbs that help you solve the most common data manipulation challenges:\n\n\n\n\n\n\nmutate() adds new variables that are functions of existing variables\nselect() picks variables based on their names\nfilter() picks cases based on their values\nsummarise() reduces multiple values down to a single summary\narrange() changes the ordering of the rows"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-mutate-function",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-mutate-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The mutate() function",
    "text": "The mutate() function\n\n\n\n\n\n\nDefinition\n\n\nThe mutate() function adds new variables that are functions of existing variables:\n\nmutate(.data, #The object you are performing the calculations \n       new_variable_1 = var1 * 2, #Can use basic operations...\n       new_variable_2 = median(var2), #Or predefined functions)\n       variable_3 = as.character(var3) #And can be used to modify existing variables)\n       ) \n\n\n\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nIt sequentially creates the columns you asked for and place them to the right of your data.frame (or tibble)\nYou can use any function, predefined or custom, and apply it to mutate()\nIt can also modify any columns you want (if the name is the same as an existing column)"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-mutate-function-practice",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-mutate-function-practice",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The mutate() function, practice",
    "text": "The mutate() function, practice\n\nExerciseSolution\n\n\n\n\n\nListing 1: Use columns high and low and create a new column, mid, defined as the average between daily high and low prices. The M7 dataset has been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, use the mutate() function to create the mid column.\nAfter that, define mid as (high+low)/2 to calculate the average between the two values.\n\n\n\n#Apply function to the data\nM7=mutate(M7, mid= (high+low)/2)\n\n#Show the first 10 observations\nhead(M7)\n\n#Apply function to the data\nM7=mutate(M7, mid= (high+low)/2)\n\n#Show the first 10 observations\nhead(M7)"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-select-function",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-select-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The select() function",
    "text": "The select() function\n\n\n\n\n\n\nDefinition\n\n\nThe select() function select (and optionally rename) variables in a data frame, using a concise mini-language that makes it easy to refer to variables based on their name (e.g. a:f selects all columns from a on the left to f on the right) or type (e.g. where(is.numeric) selects all numeric columns)\n\nselect(.data, #The object which you are performing the operations \n       variable_3, #Can reorder columns\n       variable_1, \n       variable_2:variable_4, #Matches position patterns \n       where(is.numeric) #Can select all columns that match a given pattern\n       ) \n\n\n\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd select only the columns you’ve asked for\nYou can also use select(.data,-variable) to remove a variable\nIt keeps the structure of the data.frame intact - no rows are affected"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-select-function-continued",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-select-function-continued",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The select() function, continued",
    "text": "The select() function, continued\n\nThe select() function also comes with a handy companion of selectors, which are functions that help you cherry pick columns in a concise way, rather than hardcoding them altogether:\n\n: for selecting a range of consecutive variables.\nstarts_with() starts with a string\nends_with() ends with a string\ncontains() contains a string\nmatches()matches a regular expression.\nwhere()a function to all variables and selects those for which the function returns TRUE"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-select-function-practice",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-select-function-practice",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The select() function, practice",
    "text": "The select() function, practice\n\nExerciseSolution\n\n\n\n\n\nListing 2: Select only the symbol, date, volume, and adjusted, in that order. The M7 dataset has been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the select() function to select only a subset of the available columns:\n\n\n\n#Apply function to the data\nM7=select(M7,symbol,date,volume,adjusted)\n\n#Show the first 10 observations\nhead(M7)\n\n#Apply function to the data\nM7=select(M7,symbol,date,volume,adjusted)\n\n#Show the first 10 observations\nhead(M7)"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-filter-function",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-filter-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The filter() function",
    "text": "The filter() function\n\n\n\n\n\n\nDefinition\n\n\nThe filter() function is used to subset a data frame, retaining all rows that satisfy your conditions. To be retained, the row must produce a value of TRUE for all conditions.\n\nfilter(.data, #The object which you are performing the operations\n       variable_1 &gt;10, #Simple arithmetic operators\n       variable_2 %in% c('AAPL','MSFT','FORD'), #Pattern search\n       !(variable_3 %in% c('Boston','Mass','Silicon Valley')), #Negate pattern search\n       variable_4 &gt;=10 & variable_3&lt;= 4 | is.na(variable_4) #IF and OR conditions\n       ) \n\n\n\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd filter the rows based on the conditions outlined\nYou can use any function, predefined or custom, and apply it to filter()\nIt returns a subset of the whole object, keeping the columns and the data structure intact"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-filter-function-practice",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-filter-function-practice",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The filter() function, practice",
    "text": "The filter() function, practice\n\nExerciseSolution\n\n\n\n\n\nListing 3: Filter for observations that occurred in 2025, only. You can use the year() function with the date variable to retrieve the year. The M7 dataset has been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the filter() together with year(date):\n\n\n\n#Apply function to the data\nM7=filter(M7,year(date)==2025)\n\n#Show the first 10 observations\nhead(M7)\n\n#Apply function to the data\nM7=filter(M7,year(date)==2025)\n\n#Show the first 10 observations\nhead(M7)"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-arrange-function",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-arrange-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The arrange() function",
    "text": "The arrange() function\n\n\n\n\n\n\nDefinition\n\n\nThe arrange() function reorders the rows of a data frame by the values of selected columns:\n\n#Some Options, always in the following format: the object you are rearranging + the reordering scheme\narrange(.data, variable1) #Ascending by variable_1\narrange(.data, variable1, variable_2) #Ascending by variable_1 and then variable_2\narrange(.data, variable2, variable_1) #Ascending by variable_2 and then variable 1\narrange(.data, variable1, desc(variable_2)) #Ascending by variable_1, and then descending by variable_2\n\n\n\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd reorders the rows of your data.frame (or tibble)\nThis can be useful for visualization, but also for applying position-dependent functions, like lag(), lead(), head(), and tail()"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-arrange-function-practice",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-arrange-function-practice",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The arrange() function, practice",
    "text": "The arrange() function, practice\n\nExerciseSolution\n\n\n\n\n\nListing 4: Arrange the dataset by descending date (newest to oldest) and symbol. The M7 dataset has been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the arrange() together with desc(date) and symbol:\n\n\n\n#Apply function to the data\nM7=arrange(M7,desc(date),symbol)\n\n#Show the first 10 observations\nhead(M7)\n\n#Apply function to the data\nM7=arrange(M7,desc(date),symbol)\n\n#Show the first 10 observations\nhead(M7)"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-summarize-function",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-summarize-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The summarize() function",
    "text": "The summarize() function\n\n\n\n\n\n\nDefinition\n\n\nThe summarise() - or summarize() - function creates a new data frame. It returns one row for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarising all observations in the input. It will contain one column for each grouping variable and one column for each of the summary statistics that you have specified.\n\nsummarize(.data, #The object which you are performing the operations \n       new_variable_1 = mean(var1,na.rm=TRUE), #Average of var1, removing NA values\n       new_variable_2 = median(var2,na.rm=TRUE), #Median of var1, removing, NA values\n       new_variable_3 = n_distinct(var2) #Number of unique values of var2\n       ) \n\n\n\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd reshapes the data.frame (or tibble) by the aggregation functions\nAs the name suggests, it is used to summarize a table"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-summarize-function-practice",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-summarize-function-practice",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The summarize() function, practice",
    "text": "The summarize() function, practice\n\nExerciseSolution\n\n\n\n\n\nListing 5: Summarize the dataset by creating an average column, defined as the average adjusted prices. You can use the mean() function to get the average. Use the option na.rm=TRUE inside the mean function to make sure that NA values are disregarded. The M7 dataset has been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the summarize() together with mean():\n\n\n\n#Apply function to the data\nSummary=summarize(M7,average=mean(adjusted,na.rm=TRUE))\n\n#Show the first observations\nhead(Summary)\n\n#Apply function to the data\nSummary=summarize(M7,average=mean(adjusted,na.rm=TRUE))\n\n#Show the first observations\nhead(Summary)"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#slice-and-dice-through-group_by",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#slice-and-dice-through-group_by",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Slice and dice through group_by()",
    "text": "Slice and dice through group_by()\n\nYou saw how the tidyverse verbs helps us getting ahead of the game when it comes to data operations. In most cases, however, you may need to add an extra layer of complexity: perform operations groupwise:\n\nGet the average returns by each stock\nFilter for the 10 highest prices for each year\nCalculate the median return for each industry\n\nFor cases like this, we need to find a convenient way of repeating the same operation across subsets of our data\nIt goes without saying that there should be a function in the tidyverse that makes this operation straightforward: fortunately, you can use group_by() together with all previous dplyr verbs!"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#slice-and-dice-through-group_by-continued",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#slice-and-dice-through-group_by-continued",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Slice and dice through group_by(), continued",
    "text": "Slice and dice through group_by(), continued\n\n\n\n\n\n\nDefinition\n\n\nThe group_by() function takes an existing table and converts it into a grouped table where operations are performed “by group”. Using ungroup() removes grouping.\n\nData=group_by(Data,v1,v2,v3)\nData=summarize(avg=mean(x,na.rm=TRUE))\n\n\n\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd creates the avg variable taking the average of x within each tuple defined by the grouping variables (in this case, v1,v2, and v3 )\nIt returns a grouped dataframe, with the results of avg displayed for each unique combination of v1,v2, and v3"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#revisiting-average-prices",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#revisiting-average-prices",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Revisiting average prices",
    "text": "Revisiting average prices\n\nLet’s try the latest summarize() call again, but now grouping the data by symbol first"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#combining-multiple-operations",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#combining-multiple-operations",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Combining multiple operations",
    "text": "Combining multiple operations\n\nIn the previous exercises, you have used the main dplyr verbs to create, select, arrange, filter, and summarize data, one by one. In practical applications, however it is likely that you need more than one of these functionalities at the same time\nIt is tempting to do it piecewise:\n\n\n#Start with the data\nData = read.csv('Data.csv')\n#Mutate\nData = mutate(Data, new_var_1=var_1*10)\n#Select\nData = select(Data, var_1,var_2,new_var_1,where(is.numeric))\n#Filter\nData = filter(Data, new_var_1&gt;5)\n#Arrange\nData = arrange(Data, new_var_1,desc(var2))\n#Summarize\nData = summarize(Data, new_var=mean(new_var_1,na.rm=TRUE))\n\n\nAlthough organized, it is wildly inefficient: you are sequentially (re)creating the same object all over again - not to mention that you called Data \\(10\\) times!"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#pipe-your-way-through-the-code",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#pipe-your-way-through-the-code",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Pipe your way through the code %>%",
    "text": "Pipe your way through the code %&gt;%\n\nThe dplyr verbs, in isolation, are a great tool for data analysts, but what really makes them to shine is what glues them together: I introduce you the pipe (%&gt;% or |&gt;)"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#pipe-your-way-through-the-code-continued",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#pipe-your-way-through-the-code-continued",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Pipe your way through the code %>%, continued",
    "text": "Pipe your way through the code %&gt;%, continued\n\nRené Magritte was right when he claimed that, in The Treachery of Images (La Trahison des images), there was not a pipe\nIn fact, the pipe that is relevant for us, R users, was only introduced recently, in the magrittr package, which makes clear allusion to the artist\nThe pipe operator (%&gt;% or |&gt;) helps you chain operations sequentially, in such a way that the output of one operation serves as the input of the subsequent one!\n\nInstead of repeating the .data input multiple times, you chain the operations using the pipe operator\nIt tremendously improve code readability and minimizes spelling errors (now you only need to type in the .data argument once!)\nFinally, it makes your code much more efficient: you don’t need to allocate memory to (re)create the same object in each step"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#pipe-your-way-through-the-code-continued-1",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#pipe-your-way-through-the-code-continued-1",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Pipe your way through the code %>%, continued",
    "text": "Pipe your way through the code %&gt;%, continued\n\n#Instead of \nData = read.csv('Data.csv') #Start with the data\nData = mutate(Data, new_var_1=var_1*10)#Mutate\nData = select(Data, var_1,var_2,new_var_1,where(is.numeric))#Select\nData = filter(Data, new_var_1&gt;5)#Filter\nData = arrange(Data, new_var_1,desc(var2))#Arrange\nData = summarize(Data, new_var=mean(new_var_1,na.rm=TRUE))#Summarize\n\n#Do\nData = read.csv('Data.csv')%&gt;% #Start with the data\n        mutate(new_var_1=var_1*10)%&gt;% #Mutate\n        select(var_1,var_2,new_var_1,where(is.numeric))%&gt;% #Select\n        filter(new_var_1&gt;5)%&gt;% #Filter\n        arrange(new_var_1,desc(var2))%&gt;% #Arrange\n        summarize(new_var=mean(new_var_1,na.rm=TRUE))#Summarize\n\n\nThe pipe operator lets you pass the object on its left-hand side to the first argument of the function on the right-hand side\nAnother nice feature in R is lazy evaluation: function arguments are only evaluated if (and when) they are accessed. This allows us to refer to variables that will only be created within the pipe without breaking the code!"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#practical-exercise",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#practical-exercise",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Practical Exercise",
    "text": "Practical Exercise\n\nOn January \\(25^{th}\\), chinese startup DeepSeek disrupted the tech stock market as investors reassessed the likely future investment in Artificial Intelligence hardware\n\n\n\n\n\n\n\\(\\rightarrow\\) Read: Tech stocks slump as China’s DeepSeek stokes fears over AI spending (Financial Times)"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#hands-on-exercise",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#hands-on-exercise",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\nAs part of your work as a buy-side analyst, you were asked to analyze how the Magnificent 7 performed after the DeepSeek\nFollow the instructions and answer to the following question: which stock suffered the most during January 2025?\n\n\nTo answer this question, you will be using all dplyr verbs you’ve practiced so far\nFurthermore, you will be also using some common base R and ther dplyr functions, like lag(), prod(), as.Date() and drop_na()\n\n\nThe expected result is a data.frame object that shows, for each symbol, the monthly return on January, 2025, ordered from lowest-to-highest\n\n\\(\\rightarrow\\) Suggested solution will be provided in the replication file for this lecture."
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#hands-on-exercise-continued",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#hands-on-exercise-continued",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Hands-On Exercise, continued",
    "text": "Hands-On Exercise, continued\n\n\n\n\n\n\nInstructions\n\n\nThe data, stored in M7.csv, can be loaded using read.csv('M7.csv'). You can download it using the link shown in Slide 4.\n\nSelect only the symbol, date, and adjusted columns, and arrange the dataset from oldest to newest\nMutate your date variable, making sure to read it as a Date object using as.Date()\nCreate a Year variable and filter only on observations happening in 2025. You can use the year() function to retrieve the year of a given Date column.\nGroup data by symbol\nCreate, for each different symbol, a Return variable that is defined as \\(P_{t+1}/P_{t}\\), where \\(t\\) refers to a date. You can use the lag() function for this\nYou will see that lag produces an NA whenever you try to lag the first observation. To make sure your data does not contain any NA, call drop_na()\nCreate, for each different symbol, a Cum_Return variable that is defined as the cumulative return. Compounded returns over time can be written as \\(\\small \\prod(1+R_t)=(1+R_1)\\times(1+R_2)\\times...\\times(1+R_t)\\). For this, you can use the prod() function.\nPick the latest observation from each symbol and arrange the table from lowest-to-highest return. The function slice_tail(n=x) retrieves the bottom x observations, whereas slice_head(n=y) retrieves the top y."
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#helpful-tips-while-using-r-and-quarto",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#helpful-tips-while-using-r-and-quarto",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Helpful tips while using R and Quarto",
    "text": "Helpful tips while using R and Quarto\n\nAs you pave your way through the coding exercises, there are a couple of best practices that will make your life easier when dealing with data in an R session:\n\nWhenever you are loading data, make sure to refer to the correct path where the file is located. You can use the function getwd() without any arguments to retrieve the current path, and setwd('C:/Users/you/newpath/') to set up a new working directory\nThe easiest way to make this logic redundant is to place the .R (or .qmd) script in the same folder as the data file (in our case, M7.csv). When you open the script, it will point to its own directory as the working directory - which will coincide with the data directory\n\n\n\ngetwd() #Gets the current directory. For Windows users, this is generally defaulted to C:/Users/USER/Documents\nsetwd('C:/Users/Lucas/Desktop') #changing this to desktop\nlist.files() #You can confirm if your data is listed in this directory\nread.csv('M7.csv') #It works without entering the full path since M7 is in the current path"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#helpful-tips-while-using-r-and-quarto-continued",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#helpful-tips-while-using-r-and-quarto-continued",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Helpful tips while using R and Quarto, continued",
    "text": "Helpful tips while using R and Quarto, continued\n\nRStudio allows you to interact with your session piece wise - this means that you don’t need to run your code chunks all at once\n\nTo run only a specific portion of your code, select the specific code lines you want to run and hit Ctrl + Enter\nAs an effect, R will run only the selected lines directly into your session, and the output from these lines will be prompted into your session\n\nIf you are working on a Quarto notebook, you can use the same strategy to run specific lines directly into your session, or you can hit Ctrl+Shift+K to render the output altogether\n\n\nTry this copy-pasting the code from the next slide in your R Session - we’ll dive into the details later on, but note how the output printed in your session changes as you increase the selected lines"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#references",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#references",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#outline",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#outline",
    "title": "Data Visualization",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\nTidy Finance (Scheuch, Voigt, and Weiss 2023)\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\nThe Grammar of Graphics (Wilkinson 2005)\n\n\n\n\n\n\n\n\nCoding Replications\n\n\nFor coding replications, whenever applicable, please follow this page or hover on the specific slides with containing coding chunks.\n\nEnsure that you have your  session properly set-up according to the instructions outlined in the course webpage\nAlong with the slides, this lecture will also contain a replication file, in .qmd format, containing a thorough discussion for all examples that have been showcased. This file, that will be posted on eClass®, can be downloaded and replicated on your side. To do that, download the file, open it up in RStudio, and render the Quarto document using the Render button (shortcut: Ctrl+Shift+K).\nAt the end of this lecture, you will be prompted with a hands-on exercise to test your skills using the tools you’ve learned as you made your way through the slides. A suggested solution will be provided in the replication file."
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#visualizing-financial-data",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#visualizing-financial-data",
    "title": "Data Visualization",
    "section": "Visualizing Financial Data",
    "text": "Visualizing Financial Data\n\nPart of your work as a Financial Analyst is to convey the information to a broader public:\n\nOften times, Fund managers do not use R, Python, and won’t be proof-reading your .xlsx file\nInvestors, Journalists, or the general audience all alike may not have the set of skills to ingest and analyze data like you do\n\nIt is imperative to think about how you should structure your message to effectively communicate it to your audience’s needs\n\n\n\n\n\n\n\nIntroducing: the Grammar of Graphics\n\n\nThe Grammar of Graphics sets up the foundations that underlie the production of all types of charts, ranging from pie charts, bar charts, scatterplots, and many more. To that matter, the Grammar of Graphics presents a unique foundation for producing charts from quantitative information that are widely used in scientific journals, newspapers, statistical packages, and data visualization systems.\n\n\n\n\nHow can you implement these foundations to showcase your data analysis? Luckily, there is a way to do that in R: I introduce you to the wonderful world of ggplot2"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#introducing-ggplot2",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#introducing-ggplot2",
    "title": "Data Visualization",
    "section": "Introducing ggplot2",
    "text": "Introducing ggplot2\n\nggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics\n\n\n\n\n\n\nYou start with a call to ggplot(), supplying the data and a aesthetic mapping (aes), like x and y axis, groupings, etc\nAfter that, you choose the geometry (geom), the shape of the visual elements contained in the visualization\nFinally, you add layers on top on the geometry (titles, annotations, etc) and customize your theme (font size, background color, etc)\n\n\n\nKey Highlights\n\nIt is, by and large, the richest and most widely used plotting ecosystem in the  language\nggplot2 has a rich ecosystem of extensions - ranging from annotations and interactive visualizations to specialized genomics - click here a community maintained list"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#ggplot2-foundations",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#ggplot2-foundations",
    "title": "Data Visualization",
    "section": "ggplot2 foundations",
    "text": "ggplot2 foundations\n\nWe will illustrate the use of ggplot2 to replicate the Grammar of Graphics foundations using the FANG dataset, which is loaded together with your slides - if you prefer to do it direclty in R, hit the download button and load it using read_delim('FANG.txt')\nTo get ggplot2 in your session, either load tidyverse altogether of directly load the library:\n\n\n#Load the tidyquant package\nlibrary(tidyquant)\n\n#Option 1: load the tidyverse, which includes ggplot2\nlibrary(tidyverse)\n\n#Option 2: load ggplot2 directly\nlibrary(ggplot2)\n\n\nIn what follows, we will be working with a step-by-step example of how to use ggplot2 for data visualizations\n\n\n\n Download Raw data"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#step-1-the-data",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#step-1-the-data",
    "title": "Data Visualization",
    "section": "Step 1: the data",
    "text": "Step 1: the data\n\nWe will be using the FANG dataset, which contains basic stock information from popular U.S. techonology firms: Facebook (Meta), Amazon, Netflix, and Google (Alphabet)\nThe first step in using ggplot2 is to call your data dataframe and supply the aesthetic mapping, which we’ll refer to as aes\n\n\nggplot(data=your_data, aes(x= variable_1, y=variable_2, ...))\n\n\nThe data argument refers to the dataset used\nThe aes argument contains all the aesthetic mappings that will be used\n\n\nTogether, these constitute the backbone of your visualization: they tell ggplot2 what the raw information to be used and where it should be mapped!"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#step-1-the-data-practice",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#step-1-the-data-practice",
    "title": "Data Visualization",
    "section": "Step 1: the data, practice",
    "text": "Step 1: the data, practice\n\nExerciseSolution\n\n\n\n\n\nListing 1: Use the newly created META dataset and call ggplot, mapping the date variable in the x axis, adjusted variable in the y axis, and symbol in the group aesthetic. The FANG dataset and ggplot2 have been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the ggplot() function together with aes(x, y, group):\n\n\n#Let's use Apple (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol))\n#Let's use Apple (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol))"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#step-2-adding-your-geom",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#step-2-adding-your-geom",
    "title": "Data Visualization",
    "section": "Step 2: adding your geom",
    "text": "Step 2: adding your geom\n\nYou probably thought you did something wrong when you saw an empty chart with the named axis, right? However, I can assure: you did great!\n\nIt is all about the philosophy embedded in the Grammar of Graphics: you first provide the data and the aes(thetic) mapping to your data\nNow, ggplot knows exactly which information to select and where to place it. However, it is still agnostic about how to display it\n\nWe will now add a geometry layer - in short, a geom:\n\nYou can add layers on top of ggplot object addition symbol (+)\nThere are many types of potential geometries, to name a few: geom_point(), geom_col(), geom_line() - access here for a complete list\n\n\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted)) +\n#Map a given geometry\ngeom_{yourgeomhere}()"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#step-2-adding-your-geom-practice",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#step-2-adding-your-geom-practice",
    "title": "Data Visualization",
    "section": "Step 2: adding your geom, practice",
    "text": "Step 2: adding your geom, practice\n\nExerciseSolution\n\n\n\n\n\nListing 2: In your ggplot object, try out the following geoms: geom_point(), geom_col(), and geom_line(). Which one do you think is the best for the task? The FANG dataset and ggplot2 have been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn general, using geom_line() suits the best for time series\n\n\n#Let's use Meta (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()\n#Let's use Meta (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#step-3-be-creative-with-additional-layers",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#step-3-be-creative-with-additional-layers",
    "title": "Data Visualization",
    "section": "Step 3: be creative with additional layers",
    "text": "Step 3: be creative with additional layers\n\nYour main chart is now all set:\n\nIt contains the data and the necessary aes(thetic) mappings to the chart;\nIt also contains a shape, or geom(metry), that was selected to display the data\n\nThe philosophy behind the Grammar of Graphics is now to add layers of information on top of the base chart using the + operator, like before\nWe will proceed by including several layers of information that will either add or modify the behavior of the chart, making it more appealing to our audience:\n\nAdding trend-lines using geom_smooth()\nAdding annotations and labels using annotation and labs\nModifying the behavior of the scales using scale_y and scale_x\n\nTry to sequentially add these layers and re-run the code to see how it reflects on the output!"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#step-3-be-creative-with-additional-layers-1",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#step-3-be-creative-with-additional-layers-1",
    "title": "Data Visualization",
    "section": "Step 3: be creative with additional layers",
    "text": "Step 3: be creative with additional layers\n\nExerciseSolution\n\n\n\n\n\nListing 3: In your previously created ggplot object, add a smoothed trend of adjusted prices using the geom_smooth(method='loess') geometry and adjust the labels of your axis, chart title, and subtitle. You can pass additional layers using the + operator. For changing the labels, you can use the labs(x='Your X Label',y='Your Y Label', title='Your Title', subtitle='Your subtitle') syntax. The x-axis should be called “Date,” y-axis should be called “Adjusted Prices”, the title should be called “META Prices Over Time”, and the subtitle should be called “Source: Yahoo! Finance”. The FANG dataset and ggplot2 have been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can call the geom_smooth() along with method='loess' to have a smoothed trend added on top of your chart, and customize your labels by calling the labs() argument. You can chain these operations on top of your chart using the + sign.\n\n\n#Let's use Meta (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()+\n#Adding a trend\ngeom_smooth(method='loess')+\n#Adding Annotations\nlabs(title='META adjusted prices',\n     subtitle = 'Source: Yahoo! Finance',\n     x = 'Year',\n     y = 'Adjusted Prices')\n#Let's use Meta (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()+\n#Adding a trend\ngeom_smooth(method='loess')+\n#Adding Annotations\nlabs(title='META adjusted prices',\n     subtitle = 'Source: Yahoo! Finance',\n     x = 'Year',\n     y = 'Adjusted Prices')"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#adding-multiple-data-points",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#adding-multiple-data-points",
    "title": "Data Visualization",
    "section": "Adding multiple data points",
    "text": "Adding multiple data points\nQuestion: what if we wanted to add more data?\n\n\nIn our first example, we set filter(symbol)=='META' to select only information from Meta to your chart\nHowever, one might be interested in understanding how did Meta perform relative to its FANG peers\n\n\n\n\nIt is easy to do it with ggplot:\n\nBecause you have set group=symbol, ggplot already knows that it needs to group by each different string contained in the ticker column\nIn such a way, all you need to do is to add a new aes mapping, colour=symbol, so that ggplot knows that each symbol needs to have a different color!\n\nIn what follows, we will be charting all four FANG stocks in the same chart, adjusting the layers to try keeping aesthetics as good as possible"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#adding-multiple-data-points-practice",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#adding-multiple-data-points-practice",
    "title": "Data Visualization",
    "section": "Adding multiple data points, practice",
    "text": "Adding multiple data points, practice"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#facet-it-until-you-make-it",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#facet-it-until-you-make-it",
    "title": "Data Visualization",
    "section": "Facet it until you make it",
    "text": "Facet it until you make it\n\nWe have included all FANG stocks into the same chart. Easy peasy, lemon squeezy!\nAs far as we could go on adjusting the layers, it seems that the chart conveys too much information:\n\nBecause of the different scales, you can hardly tell the different between AMZN AND GOOG during 2015-2018\nFurthermore, trend lines are, in some cases, effectively hiding the data undernearth\n\nAlthough you could easily remove the trend lines, ggplot2 also comes with a variety of alternatives when it comes to charting multiple data that may come in handy:\n\nYou can facet your chart using facet_wrap, controlling the axis as well as the number of rows and columns\nYou can grid your chart, making the comparison easier with fixed axes"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#option-1-using-facet_wrap",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#option-1-using-facet_wrap",
    "title": "Data Visualization",
    "section": "Option 1: using facet_wrap()",
    "text": "Option 1: using facet_wrap()"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#option-2-using-facet_grid",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#option-2-using-facet_grid",
    "title": "Data Visualization",
    "section": "Option 2: using facet_grid()",
    "text": "Option 2: using facet_grid()"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#adding-themes-youre-in-full-control-of-your-message",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#adding-themes-youre-in-full-control-of-your-message",
    "title": "Data Visualization",
    "section": "Adding themes: you’re in full control of your message!",
    "text": "Adding themes: you’re in full control of your message!\n\nBy now, you are already looking like a data manipulation wizard in your firm:\n\nYou have created a fully automated data ingestion process using tq_get() to get live FANG prices.\nSet up ggplot to automatically update the chart;\nFinally, you have adjusted all aesthetics to make it more much more professional\n\nPlus: you haven’t even opened Excel! Well, what’s next?\n\n\n\nA lot of the ggplot adoption throughout the R usiverse relates to themes: complete configurations which control all non-data display\n\nThere are a lot of available themes that you can pass to your ggplot, like theme_minimal(), theme_bw()\nAlternatively, you can pass theme() if you just need to tweak the display of an existing theme"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#adding-themes-to-your-chart",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#adding-themes-to-your-chart",
    "title": "Data Visualization",
    "section": "Adding themes() to your chart",
    "text": "Adding themes() to your chart"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#the-r-community-is-on-your-side",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#the-r-community-is-on-your-side",
    "title": "Data Visualization",
    "section": "The R community is on your side!",
    "text": "The R community is on your side!\n\nThere are endless customizations that you could think of that could be applied to a theme\nIn special, the package ggthemes provides extra themes, geoms, and scales for ggplot2 that replicate the look of famous aesthetics that you have often looked and said: “how could I replicate that?”\nTo get access to these additional graphical resources in your R session, install and load the package using:\n\n\ninstall.packages('ggthemes') #Install if not available\nlibrary(ggthemes) #Load\n\nyour_previous_ggplot_object + theme_{insertyourtheme}\n\n\nTo check all available themes, check the ggthemes library here website"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#endless-possibilities-for-theme-customization",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#endless-possibilities-for-theme-customization",
    "title": "Data Visualization",
    "section": "Endless possibilities for theme customization",
    "text": "Endless possibilities for theme customization\n\nWSJThe EconomistExcelFiveThirtyEightGoogle Docs"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#further-theme-customization",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#further-theme-customization",
    "title": "Data Visualization",
    "section": "Further theme customization",
    "text": "Further theme customization\n\nEven with customized themes, you might still want to do your own customizations\nIt is easy to access each and every component of the chart by adding theme (using the + operator):\n\n\ninstall.packages('ggthemes') #Install if not available\nlibrary(ggthemes) #Load\n\nyour_previous_ggplot_object +\n  theme_{insertyourtheme}+\n  theme(component_1 = configuration_1,\n        component_2 = configuration_2,\n        ...\n        component_n = configuration_n)\n\n\nIn what follows, we will be using the theme() function to adjust some aspects of our chart, such as font size, angle, and text width, to make it look more professional"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#doing-custom-theme-adjustments-to-the-chart",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#doing-custom-theme-adjustments-to-the-chart",
    "title": "Data Visualization",
    "section": "Doing custom theme() adjustments to the chart",
    "text": "Doing custom theme() adjustments to the chart"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#integrating-tidyquant",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#integrating-tidyquant",
    "title": "Data Visualization",
    "section": "Integrating tidyquant",
    "text": "Integrating tidyquant\n\nLike in our previous lecture, tidyquant added very important functionalities for those who work in finance to easily manage financial time series using the well-established foundations of the tidyverse\nWhen it comes to data visualization, tidyquant also provides a handful of integrations that can be inserted into your ggplot call:\n\nPossibility of using geom_barchart and geom_candlestick\nMoving average visualizations and Bollinger Bands available using geom_ma and geom_bbands\nA new theme, theme_tq, available\n\n\n\\(\\rightarrow\\) For a thorough discussion, see a detailed discussion on tidyquant’s charting capabilities here"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#integrating-tidyquant-continued",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#integrating-tidyquant-continued",
    "title": "Data Visualization",
    "section": "Integrating tidyquant, continued",
    "text": "Integrating tidyquant, continued\n\nCodeOutput\n\n\n\n#Set up start and end dates\nend=Sys.Date()\nstart=end-weeks(5)\n\nFANG%&gt;%\n  #Make sure that date is read as a Date object\n  mutate(date=as.Date(date))%&gt;%\n  #Filter\n  filter(date &gt;= start, date&lt;=end)%&gt;%\n  #Basic layer - aesthetic mapping including fill\n  ggplot(aes(x=date,y=close,group=symbol))+\n  #Charting data - you could use geom_line(), geom_col(), geom_point(), and others\n  geom_candlestick(aes(open = open, high = high, low = low, close = close))+\n  geom_ma(ma_fun = SMA, n = 5, color = \"black\", size = 0.25)+\n  #Facetting\n  facet_wrap(symbol~.,scales='free_y')+\n  #DeepSeek date\n  geom_vline(xintercept=as.Date('2025-01-24'),linetype='dashed')+\n  #Annotations\n  labs(title='FANG adjusted prices before/after DeepSeek announcement',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Date',\n       y = 'Adjusted Prices')+\n  #Scales\n  scale_x_date(date_breaks = '3 days') +\n  scale_y_continuous(labels = dollar) +\n  #Custom 'The Economist' theme\n  theme_economist()+\n  #Adding further customizations\n  theme(legend.position='none',\n        axis.title.y = element_text(vjust=+4,face='bold'),\n        axis.title.x = element_text(vjust=-3,face='bold'),\n        plot.subtitle = element_text(size=8,vjust=-2,hjust=0,margin = margin(b=15)),\n        axis.text.y = element_text(size=8),\n        axis.text.x = element_text(angle=90,size=8))"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#wrapping-up",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#wrapping-up",
    "title": "Data Visualization",
    "section": "Wrapping-up",
    "text": "Wrapping-up\n\nAll in all, ggplot2 is a powerful companion on data visualization for R users\ntidyquant helps you bridge the gap between the lack of powerful libraries for financial data visualization and ggplot2’s capabilities\nTopics not covered in this lecture, but definitely worth the time:\n\nAdd interactivity to your ggplot objects with ggplotly package - access here\nOther interactive charting libraries: take a look at highcharter, a wrapper for a famous JavaScript library - access here\nIf you wish to know more about the usage of ggplot2, nurture your creativity by looking at some community maintained list of examples here"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#alternatives-to-ggplot2",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#alternatives-to-ggplot2",
    "title": "Data Visualization",
    "section": "Alternatives to ggplot2",
    "text": "Alternatives to ggplot2\n\nggplot2 is, by and large, the richest and most widely used plotting ecosystem in the  language\nHowever, there are also other interesting options, especially when it comes to interactive data visualization\n\nThe plotly ecosystem provides interactive charts for R, Python, Julia, Java, among others - you can install the R package using install.packages('plotly')\nThe Highcharts is another option whenever there is a need for interactive data visualization - you can install the R package using install.packages('highcharter')\n\nIn special, the highcharter package works seamlessly with time series data, especially those retrieved by the tidyquant’s tq_get() function"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#using-the-highcharter-package",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#using-the-highcharter-package",
    "title": "Data Visualization",
    "section": "Using the highcharter package",
    "text": "Using the highcharter package\n\nCodeSolution\n\n\n\n#Install the highcharter package (if not installed yet)\n#install.packages('highcharter')\n\n#Load the highcharter package (if not loaded yet)\nlibrary(highcharter)\n\n#Select the Google Stock with OHLC information and transform to an xts object\nGOOG=tq_get('GOOG')%&gt;%select(-symbol)%&gt;%as.xts()\n\n  #Initialize an empty highchart\n  highchart(type='stock')%&gt;%\n  #Add the Google Series\n  hc_add_series(GOOG,name='Google')%&gt;%\n  #Add title and subtitle\n  hc_title(text='A Dynamic Visualization of Google Stock Prices Over Time')%&gt;%\n  hc_subtitle(text='Source: Yahoo! Finance')%&gt;%\n  #Customize the tooltip\n  hc_tooltip(valueDecimals=2,valuePrefix='$')%&gt;%\n  #Convert it to a 'The Economist' theme\n  hc_add_theme(hc_theme_economist())"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#i-hope-you-are-excited-to-whats-next",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#i-hope-you-are-excited-to-whats-next",
    "title": "Data Visualization",
    "section": "I hope you are excited to what’s next!",
    "text": "I hope you are excited to what’s next!"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#hands-on-exercise",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#hands-on-exercise",
    "title": "Data Visualization",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\nIn late January 2021, Reddit traders took on the short-sellers by forcing them to liquidate their short positions using GameStop stocks. This coordinated behavior had significant repercussions for various investment funds, such as Melvin Capital - see here and here\n\n\n\n\n\n\n\nExercise\n\n\n\nUse tq_get() to load information for GameStop (ticker: GME) and store it in a data.frame. Using the arguments from and to from tq_get(), filter for observations between occurring in between December 2020 (beginning of) and March 2021 (end of)\nUse ggplot(aes(x=date,group=symbol)), along with geom_candlestick() and its appropriate arguments, to chart the historical OHLC prices\nCreate a vertical line annotation using geom_vline, setting the xintercept argument to the date of the Reddit frenzy (as.Date('2021-01-25'))\nUse the theme from The Economist calling theme_economist(). Make sure to have the ggthemes package installed and loaded\nFinally, call theme() and labs() to adjust the aesthetics of your theme and labels as you think it would best convey your message. For example, you can use the scales package to format the appearance of your x and y labels (for example, displaying a dollar sign in front of adjusted prices)"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#hands-on-exercise-solutions",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#hands-on-exercise-solutions",
    "title": "Data Visualization",
    "section": "Hands-On Exercise, solutions",
    "text": "Hands-On Exercise, solutions\n\nCodeOutput\n\n\n\n#Libraries\nlibrary(tidyquant)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(scales)\n\n#Setting start/end dates + reddit date\nstart='2020-12-01'\nend='2021-03-31'\nreddit_date=as.Date('2021-01-25')\n\n#Get the data\ntq_get('GME',from=start,to=end)%&gt;%\n  #Mapping\n  ggplot(aes(x=date,group=symbol))+\n  #Geom\n  geom_candlestick(aes(open = open, high = high, low = low, close = close))+\n  #Labels\n  labs(x='',\n       y='Adjusted Prices',\n       title='GameStop (ticker: GME) prices during the reddit (Wall St. Bets) frenzy',\n       subtitle='Source: Yahoo! Finance')+\n  #Annotation\n  geom_vline(xintercept=reddit_date,linetype='dashed')+\n  annotate(geom='text',x=reddit_date-5,y=75,label='Reddit Frenzy Starts',angle=90)+\n  #Scales\n  scale_x_date(date_breaks = '2 weeks') +\n  scale_y_continuous(labels = dollar) +\n  #Custom 'The Economist' theme\n  theme_economist()+\n  #Adding further customizations\n  theme(legend.position='none',\n        axis.title.y = element_text(vjust=+4,face='bold'),\n        axis.title.x = element_text(vjust=-3,face='bold'),\n        plot.title = element_text(size=10),\n        plot.subtitle = element_text(size=8,vjust=-2,hjust=0,margin = margin(b=15)),\n        axis.text.y = element_text(size=8),\n        axis.text.x = element_text(angle=45,size=8,vjust=0.75))"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#references",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#references",
    "title": "Data Visualization",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed. Statistics and Computing. New York, NY: Springer."
  },
  {
    "objectID": "quant-fin-replications/L1/L1-Replication.html",
    "href": "quant-fin-replications/L1/L1-Replication.html",
    "title": "Bridging Data with Programming - Replication",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nThis is our first lecture, and I do not expect you to fully understand the underlying code. The goal is to showcase the most important R packages, how they work, and how you can make sure to have them available in our session. As we progress through the lectures, we will have a deep-dive on some of the most important aspects of these packages, and you will have some hands-on exercises to practice your coding skills."
  },
  {
    "objectID": "quant-fin-replications/L1/L1-Replication.html#about-this-document",
    "href": "quant-fin-replications/L1/L1-Replication.html#about-this-document",
    "title": "Bridging Data with Programming - Replication",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nThis is our first lecture, and I do not expect you to fully understand the underlying code. The goal is to showcase the most important R packages, how they work, and how you can make sure to have them available in our session. As we progress through the lectures, we will have a deep-dive on some of the most important aspects of these packages, and you will have some hands-on exercises to practice your coding skills."
  },
  {
    "objectID": "quant-fin-replications/L1/L1-Replication.html#introduction-to-install.packages-and-library-in-r",
    "href": "quant-fin-replications/L1/L1-Replication.html#introduction-to-install.packages-and-library-in-r",
    "title": "Bridging Data with Programming - Replication",
    "section": "1. Introduction to install.packages() and library() in R",
    "text": "1. Introduction to install.packages() and library() in R\nWhen you’re starting with R, you’ll frequently need to install and load packages to access additional functions beyond the base R functionalities. Two essential functions for this are:\n\ninstall.packages() – Downloads and installs a package from CRAN (Comprehensive R Archive Network).\nlibrary() – Loads an installed package so that you can use its functions\n\nFor installing new packages, the syntax is:\n\ninstall.packages(\"package_name\")\n\nNote that:\n\nYou only need to install a package once (unless you update or reinstall R).\nPackages are stored in a library (a folder on your computer).\nIf a package is not available, check your internet connection or make sure CRAN is accessible.\n\nOnce a package is installed, you need to load it every time you start a new R session. The function library() makes the functions from the referred package available for use. For example, after installing ggplot2 using install.packages(\"ggplot2\"), load it by calling:\n\nlibrary(ggplot2)\n\nIf you forget to load a package and try to use its functions, R will throw an error:\n\nError in ggplot(): could not find function \"ggplot\"\n\nIn this course, we will be mostly using a handful of packages for data collection, data wrangling, and visualization. Namely, we will be mostly working with the following packages:\n\ntidyverse: for data manipulation and visualization, including packages such as dplyr,gpplot2, and tidyr\ntidyquant: for retrieving and working with financial data series\ntidymodels: a set of routines for running statistical models\nglue: a simple and efficient way to interpolate strings\nscales: provides functions that are particularly useful for visualization, helping scaling axes, formatting labels, and applying transformations like logarithms or percentages.\nggthemes: a set of powerful themes for data visualization\n\nFor installing these packages, you could simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)\n\nAlternatively, the code below searches for a given set of packages in our computer, installing only the packages that are not found. After installing all missing packages (if any) it loads all packages together:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\", \"glue\",\"scales\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nMake sure to install all packages and load them, either by calling library() individually, or running the aforementioned code.\n\n\n\n\n\n\nNote\n\n\n\nFor the remainder of the sessions, you will use financial data from multiple sources, either that being from a local .csv file or a pull from Yahoo! Finance. Whenever you are working with local files, it is always important to make sure that your R is able to locate it. To check the working directory of your session, simply type getwd(), and it will prompt your current directory. If you want to change your directory, simply type setwd('C:/path/to/your/folder') with the specific path to your desired folder. To make sure that you switched directories, you can type getwd() to confirm the new directory.\nMost of the issues regarding not being able to load a specific file, like .csv and .xlsx spreadsheets can be easily solved by placing your R file (either a plain script, like myscript.R, or a quarto document, myquartodoc.qmd) in the same folder as of your data. When you open your R script or Quarto document, it will automatically set that folder (which coincides with the data folder) as the working directory. To confirm which files are available to you, you can simply type list.files() to get the list of all files that R can find in the working directory.\nIf you prefer, whenever you are calling a function that requires a path to your computer, you can always provide the full path of the file: for example, using \"C:/Users/Lucas/Documents/GitHub/Folder/test.csv' would find the test.csv even if Folder is not your working directory."
  },
  {
    "objectID": "quant-fin-replications/L1/L1-Replication.html#using-dplyr-the-data-manipulation-package-in-the-tidyverse",
    "href": "quant-fin-replications/L1/L1-Replication.html#using-dplyr-the-data-manipulation-package-in-the-tidyverse",
    "title": "Bridging Data with Programming - Replication",
    "section": "2. Using dplyr, the data manipulation package in the tidyverse",
    "text": "2. Using dplyr, the data manipulation package in the tidyverse\nThe dplyr package is one of the core packages in the tidyverse and is designed for efficient and readable data manipulation. It provides a set of functions (also called “verbs”) that make working with data frames (or tibbles) intuitive and expressive. Key Features:\n\nFilter rows: filter()\nSelect columns: select()\nMutate (create new columns): mutate()\nSummarize data: summarize()\nGroup operations: group_by()\nJoin tables: left_join(), right_join(), inner_join(), full_join()\n\nFor this section, you will be using a .csv file that contains Nvidia (ticker: NVDA) prices collected directly from Yahoo! Finance. You can download the data using directly on eClass® and place it in the same folder of your R/Quarto report. In my case, I have created a folder, called Assets, inside my working directory.\n\nread.csv('Assets/NVDA.csv')%&gt;%\n  select(Timestamp,Adjusted)%&gt;%\n  mutate(Date=as.Date(Timestamp),\n         Year=year(Date))%&gt;%\n  filter(Year!='2025')%&gt;%\n  arrange(Date)%&gt;%\n  mutate(Return = Adjusted/lag(Adjusted,1)-1)%&gt;%\n  group_by(Year)%&gt;%\n  summarize(\n    `Average Daily Return` = percent(mean(Return,na.rm=TRUE),accuracy = 0.01),\n    `Annualized Return`= percent(prod(1+Return,na.rm=TRUE)-1,accuracy = 0.01))\n\n# A tibble: 18 × 3\n    Year `Average Daily Return` `Annualized Return`\n   &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt;              \n 1  2007 0.18%                  41.44%             \n 2  2008 -0.42%                 -76.28%            \n 3  2009 0.40%                  131.47%            \n 4  2010 -0.04%                 -17.56%            \n 5  2011 0.02%                  -10.00%            \n 6  2012 -0.03%                 -10.97%            \n 7  2013 0.12%                  33.52%             \n 8  2014 0.11%                  27.40%             \n 9  2015 0.23%                  67.12%             \n10  2016 0.51%                  226.96%            \n11  2017 0.27%                  81.99%             \n12  2018 -0.10%                 -30.82%            \n13  2019 0.26%                  76.94%             \n14  2020 0.38%                  122.30%            \n15  2021 0.36%                  125.48%            \n16  2022 -0.20%                 -50.26%            \n17  2023 0.53%                  239.02%            \n18  2024 0.45%                  171.25%            \n\n\nLet’s break this code down. First, the function read.csv() reads an NVDA.csv file inside the Assets subfolder. This function returns a data.frame object that is an input to the rest of the code. All other functions are from dplyr, and are intended to facilitate data wrangling. We’ll cover all these functions in our upcoming lecture."
  },
  {
    "objectID": "quant-fin-replications/L1/L1-Replication.html#using-ggplot2-for-data-visualization",
    "href": "quant-fin-replications/L1/L1-Replication.html#using-ggplot2-for-data-visualization",
    "title": "Bridging Data with Programming - Replication",
    "section": "3. Using ggplot2 for data visualization",
    "text": "3. Using ggplot2 for data visualization\nThe ggplot2 package is the most powerful and widely used data visualization package in R. It is part of the tidyverse and follows the Grammar of Graphics, a systematic approach to creating complex graphics by layering components.\nUnlike base R plotting functions, like plot(), ggplot2 provides:\n\nHighly customizable plots. Its modular approach makes it easy to modify and extend plots.\nElegant default themes. Default themes are aesthetically pleasing and can be customized.\nLayered structure for complex graphics. Users can combine multiple layers, change colors, themes, scales, and annotations effortlessly.\nSeamless integration with the tidyverse. It integrates seamlessly with dplyr, tidyr and tidyquant, allowing smooth data manipulation and visualization.\n\nThe code below shows how you can use ggplot2 to load a file. It reads a .csv file containing Nvidia stock prices, processes the data, and visualizes the stock price trends for 2023 and 2024 using ggplot2. Note that I have not called library(tidyverse) again since my session already had it loaded for the previous code chunks!\n\nread.csv('Assets/NVDA.csv')%&gt;%\n  select(Timestamp,Adjusted)%&gt;%\n  mutate(Date=as.Date(Timestamp))%&gt;%\n  arrange(Date)%&gt;%\n  filter(year(Date) %in% c(2023,2024))%&gt;%\n  ggplot(aes(x=Date,y=Adjusted))+\n  geom_line()+\n  theme_light()+\n  labs(title = 'Nvidia stock prices between 2023 and 2024',\n       subtitle = 'Source: Yahoo! Finance',\n       x='',\n       y='Adjusted Close (in $)')+\n  scale_y_continuous(labels = dollar)"
  },
  {
    "objectID": "quant-fin-replications/L1/L1-Replication.html#using-tidyr-to-reshape-and-simplify-data",
    "href": "quant-fin-replications/L1/L1-Replication.html#using-tidyr-to-reshape-and-simplify-data",
    "title": "Bridging Data with Programming - Replication",
    "section": "4. Using tidyr to reshape and simplify data",
    "text": "4. Using tidyr to reshape and simplify data\nThe tidyr package in R is designed for data cleaning and reshaping, making datasets “tidy” for easier analysis. It provides functions to pivot, separate, unite, and fill missing data efficiently. For this section, you will be using a .csv file that contains information from multiple tickers collected directly (Multiple_Assets.csv). You can download the data using directly on eClass® and place it in the same folder of your R/Quarto report. In my case, I have created a folder, called Assets, inside my working directory.\nOpening this file promptly shows that the .csv file is in a very messy format, with different columns that refer to the same variable, like closing prices. Using tidyr facilitates the transitions towards a tidy format that will be key for data manipulation:\n\nread.csv('Assets/Multiple_Assets.csv')%&gt;%\n  pivot_longer(cols=matches('Open|High|Low|Close|Volume|Adjusted'),\n               names_to = c('Asset','Metric'),\n               names_sep = '\\\\.',\n               values_to = 'Value')%&gt;%\n  mutate(Date=as.Date(Timestamp),\n         Year=year(Date))%&gt;%\n  group_by(Asset,Metric,Year)%&gt;%\n  summarize(Value=mean(Value,na.rm=TRUE))%&gt;%\n  pivot_wider(names_from = c('Metric'),values_from = 'Value')\n\n# A tibble: 57 × 8\n# Groups:   Asset [3]\n   Asset  Year Adjusted Close  High   Low  Open      Volume\n   &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1 AAPL   2007     3.86  4.58  4.65  4.51  4.59  984047751.\n 2 AAPL   2008     4.28  5.07  5.18  4.96  5.08 1130360498.\n 3 AAPL   2009     4.42  5.24  5.30  5.18  5.24  568467011.\n 4 AAPL   2010     7.83  9.28  9.37  9.17  9.28  599305267.\n 5 AAPL   2011    11.0  13.0  13.1  12.9  13.0   492298967.\n 6 AAPL   2012    17.4  20.6  20.8  20.4  20.6   527856818.\n 7 AAPL   2013    14.6  16.9  17.1  16.7  16.9   406434800 \n 8 AAPL   2014    20.4  23.1  23.3  22.9  23.1   252610922.\n 9 AAPL   2015    27.0  30.0  30.3  29.7  30.0   207397617.\n10 AAPL   2016    24.0  26.2  26.4  25.9  26.1   153690124.\n# ℹ 47 more rows"
  },
  {
    "objectID": "quant-fin-replications/L1/L1-Replication.html#collecting-and-exporting-data",
    "href": "quant-fin-replications/L1/L1-Replication.html#collecting-and-exporting-data",
    "title": "Bridging Data with Programming - Replication",
    "section": "5. Collecting and exporting data",
    "text": "5. Collecting and exporting data\nEverybody who has experience working with data is also familiar with storing and reading data in formats like .csv, .xls, .xlsx or other delimited value storage. However, if your goal is to replicate a common task at a predefined time interval, like charting weekly stock prices for a selected bundle of stocks every end-of-week, it might be overwhelming to manually perform these tasks every week. Our slides covered a handful of data sources that are widely used among finance practictioners, such as stock-level data, macroeconomic data, among others.\nWhen it comes to stock-level data, the tidyquant is a powerful package in R that simplifies financial data retrieval and analysis. One of its key features is the ability to fetch stock market data from Yahoo! Finance using the tq_get() function.\nTo get historical stock prices from Yahoo! Finance, use the tq_get() function:\n\n# Fetch historical stock prices for FANG (Facebook, Amazon, Netflix, Google) stocks\nFANG_data &lt;- tq_get(c(\"META\",\"AMZN\",\"NFLX\",\"GOOG\"), from = \"2020-01-01\", to = \"2024-01-01\")"
  },
  {
    "objectID": "quant-fin-replications/L1/L1-Replication.html#using-write.csv-to-export-a-file",
    "href": "quant-fin-replications/L1/L1-Replication.html#using-write.csv-to-export-a-file",
    "title": "Bridging Data with Programming - Replication",
    "section": "6. Using write.csv() to export a file",
    "text": "6. Using write.csv() to export a file\nThe write.csv() function in R allows users to export data frames to .csv (Comma-Separated Values) files. These files are commonly used for storing tabular data and can be opened in Excel, Google Sheets, or other data analysis tools. It works by defining the following arguments:\n\nwrite.csv(x,file)\n\n\nx: The data frame to export.\nfile: The file path where the CSV will be saved\n\nIt is now easy to use tq_get() in conjunction with write.csv() to retrieve data from Yahoo! Finance and export it to a .csv file:\n\nwrite.csv(FANG_data,'FANG_prices.csv')\n\nThe code above assumes that you have created the FANG_data object in your R session."
  },
  {
    "objectID": "quant-fin-replications/L1/L1-Replication.html#try-doing-some-edits-on-your-own",
    "href": "quant-fin-replications/L1/L1-Replication.html#try-doing-some-edits-on-your-own",
    "title": "Bridging Data with Programming - Replication",
    "section": "Try doing some edits on your own!",
    "text": "Try doing some edits on your own!\nThe code below downloads data for all stocks contained in the assets vector using the tq_get() function and the arguments from and to, and exports a .csv file with your newly retrieved data. Try changing this code to download data for a list of stocks of your choice and a specific timeframe. Make sure to write the date ranges in YYYY-MM-DD format.\n\n#Define the list of assets\nassets = c('MMM','GOOGL','NFLX','WEGE3.SA')\n\n#Define the time ranges\nstart=\"2020-01-01\"\nend=Sys.Date() #Today\n\n# Fetch historical stock prices for selected assets\nfinancial_data &lt;- tq_get(assets, from = start, to=end)\n\n# Export it\nwrite.csv(financial_data,'my_first_export.csv')"
  },
  {
    "objectID": "quant-fin-replications/L3/L3-Replication.html",
    "href": "quant-fin-replications/L3/L3-Replication.html",
    "title": "Manipulating Time Series",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nIn this lecture, we will be working with daily stock price data from several stocks included in the S&P 500 index. Instead of loading the data from a .csv file, we will pull the data directly from R using the tidyquant package. Before you start, make sure to follow the instructions from our previous replication to set up your working directory correctly."
  },
  {
    "objectID": "quant-fin-replications/L3/L3-Replication.html#about-this-document",
    "href": "quant-fin-replications/L3/L3-Replication.html#about-this-document",
    "title": "Manipulating Time Series",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nIn this lecture, we will be working with daily stock price data from several stocks included in the S&P 500 index. Instead of loading the data from a .csv file, we will pull the data directly from R using the tidyquant package. Before you start, make sure to follow the instructions from our previous replication to set up your working directory correctly."
  },
  {
    "objectID": "quant-fin-replications/L3/L3-Replication.html#loading-packages",
    "href": "quant-fin-replications/L3/L3-Replication.html#loading-packages",
    "title": "Manipulating Time Series",
    "section": "Loading packages",
    "text": "Loading packages\nAs we get started, we will be loading all packages referred in our official website.\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\", \"glue\",\"scales\",\"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nNote that you could easily get around this by installing and loading all necessary packages using a more simple syntax:\n\n#Install if not already available - I have commented these lines so that R does not attempt to install it everytime\n  #install.packages('tidyverse')\n  #install.packages('tidyquant')\n  #install.packages('glue')\n  #install.packages('scales')\n  #install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)"
  },
  {
    "objectID": "quant-fin-replications/L3/L3-Replication.html#working-with-time-series",
    "href": "quant-fin-replications/L3/L3-Replication.html#working-with-time-series",
    "title": "Manipulating Time Series",
    "section": "Working with Time Series",
    "text": "Working with Time Series\nIn the previous lectures, you worked your way through the exercises by using the amazing dplyr functionalities on data.frames. In some cases, however, you had to do some workarounds with drop_na(), slice_tail() and lag() simply because you were manipulating time series data.\nThe xts, which stantds for eXtensible Time Series is an R package that is is widely used for handling and manipulating time series data. It extends the functionality of the zoo package by providing a structured framework for managing time-indexed data efficiently. Such package provides a matrix-like structure where each row is indexed by a date-time value, allowing for efficient subsetting, merging, and manipulation of time series data. It is especially useful in financial applications where time-stamped data is common.\n\nlibrary(xts) # Note that this is loaded together with the tidyquant package\n\n# Create a vector of random values\ndata_values &lt;- rnorm(10)\n\n# Create a sequence of dates\ndates &lt;- as.Date(\"2024-01-01\") + 0:9\n\n# Convert to xts\nxts_data &lt;- xts(data_values, order.by = dates)\n\n# Print the xts object\nprint(xts_data)\n\n                   [,1]\n2024-01-01  0.187052628\n2024-01-02 -2.118986610\n2024-01-03 -0.189449810\n2024-01-04  0.766324475\n2024-01-05  0.009011437\n2024-01-06 -1.101096847\n2024-01-07  0.258730380\n2024-01-08 -0.471201630\n2024-01-09  1.065146769\n2024-01-10 -0.281211504\n\n\nThe output shows an interesting feature of an xts format in R:\n\nThe first column contains the values\nThe row names are timestamps\nThe xts object retains an efficient internal structure"
  },
  {
    "objectID": "quant-fin-replications/L3/L3-Replication.html#core-features-of-xts",
    "href": "quant-fin-replications/L3/L3-Replication.html#core-features-of-xts",
    "title": "Manipulating Time Series",
    "section": "Core features of xts",
    "text": "Core features of xts\n\nTime-Based Indexing & Subsetting: you can subset an xts object using time-based indexing in a variety of ways. If you were to do this in a data.frame, R wouldn’t be able to retrieve the data, as a data.frame does not carry any time series properties that are needed for the job:\n\n\n# Subset a specific date\nxts_data[\"2024-01-03\"]\n\n                 [,1]\n2024-01-03 -0.1894498\n\n# Subset a range\nxts_data[\"2024-01-03/2024-01-06\"]\n\n                   [,1]\n2024-01-03 -0.189449810\n2024-01-04  0.766324475\n2024-01-05  0.009011437\n2024-01-06 -1.101096847\n\n# Subset a custom Y-M-D definition\nxts_data[\"2024\"]       # Returns all data from 2024\n\n                   [,1]\n2024-01-01  0.187052628\n2024-01-02 -2.118986610\n2024-01-03 -0.189449810\n2024-01-04  0.766324475\n2024-01-05  0.009011437\n2024-01-06 -1.101096847\n2024-01-07  0.258730380\n2024-01-08 -0.471201630\n2024-01-09  1.065146769\n2024-01-10 -0.281211504\n\nxts_data[\"2024-01\"]    # Returns all data from January 2024\n\n                   [,1]\n2024-01-01  0.187052628\n2024-01-02 -2.118986610\n2024-01-03 -0.189449810\n2024-01-04  0.766324475\n2024-01-05  0.009011437\n2024-01-06 -1.101096847\n2024-01-07  0.258730380\n2024-01-08 -0.471201630\n2024-01-09  1.065146769\n2024-01-10 -0.281211504\n\n\n\nMerging and Combining Time Series: you can merge two xts objects using by the timestamp component that is embedded in the xts structure:\n\n\ndata1 &lt;- xts(rnorm(5), order.by = as.Date(\"2024-01-01\") + 0:4)\ndata2 &lt;- xts(rnorm(5), order.by = as.Date(\"2024-01-01\") + 2:6)\n\nmerged_data &lt;- merge(data1, data2, join = \"outer\")\nprint(merged_data)\n\n                data1       data2\n2024-01-01 -0.3188591          NA\n2024-01-02  1.3669653          NA\n2024-01-03 -0.5614160 -0.03716457\n2024-01-04  2.3792035 -0.26900819\n2024-01-05 -0.7954646 -1.21316667\n2024-01-06         NA  0.04639750\n2024-01-07         NA -1.28359390\n\n\n\njoin = \"outer\" ensures all time points are included\nIf a time point is missing in one dataset, it is filled with NA\nMerging and Combining Time Series: using functions that retrieve leads and lags of a given variable are a key component of xts objects. Due to its timestamp component, one can shift variables backwards (using the lag() function) as well as forward (using the lag()) function:\n\n\nlagged_data &lt;- lag(data1)  # Shift values by 1 day\nprint(lagged_data)\n\n                 [,1]\n2024-01-01         NA\n2024-01-02 -0.3188591\n2024-01-03  1.3669653\n2024-01-04 -0.5614160\n2024-01-05  2.3792035\n\nmerged_data &lt;- merge(lagged_data, data1, join = \"outer\")\nprint(merged_data)\n\n           lagged_data      data1\n2024-01-01          NA -0.3188591\n2024-01-02  -0.3188591  1.3669653\n2024-01-03   1.3669653 -0.5614160\n2024-01-04  -0.5614160  2.3792035\n2024-01-05   2.3792035 -0.7954646\n\n\nAll in all, when it comes to working with financial data, xts objects are particularly useful for:\n\nHandling stock price data from tidyquant\nCalculating log returns for portfolio management\nAligning time series data (e.g., joining different financial datasets)\nAggregating financial data (e.g., monthly returns)\nSubsetting data by years/months/days\nCalculating rolling functions (e.g, yearly averages)\nAggregating data at different intervals (e.g, convert daily to weekly prices)\n\nUnfortunately, there is an issue: the tidyverse is not fully designed to work with time series classes, such as xts. Since xts is optimized for time series operations, some functions that would work well when managing time series are not easily translated using the packages from the tidyverse:\n\ntidyverse: Designed for tabular (data frame-like) structures, emphasizing “tidy” data (each row is an observation, each column is a variable)\nxts: Designed for time series, optimizing time-based indexing and calculations but less compatible with tidyverse workflows"
  },
  {
    "objectID": "quant-fin-replications/L3/L3-Replication.html#introducing-the-tidyquant-package",
    "href": "quant-fin-replications/L3/L3-Replication.html#introducing-the-tidyquant-package",
    "title": "Manipulating Time Series",
    "section": "Introducing the tidyquant package",
    "text": "Introducing the tidyquant package\nThe tidyquant package (see official documentation here) helps integrate both paradigms, making financial analysis more intuitive in R. It integrates several financial packages, like zoo, xts, quantmod, TTR, and PerformanceAnalytics, with the tidy data infrastructure of the tidyverse, allowing for seamless interaction between tidyverse data manipulation and financial functions.\nThere are mainly three functions in the tidyquant package that we will be using throughout this lecture: tq_get(), tq_mutate(), and tq_transmute()."
  },
  {
    "objectID": "quant-fin-replications/L3/L3-Replication.html#the-tq_get-function",
    "href": "quant-fin-replications/L3/L3-Replication.html#the-tq_get-function",
    "title": "Manipulating Time Series",
    "section": "The tq_get() function",
    "text": "The tq_get() function\n\n\n\n\n\n\nDefinition\n\n\n\nThe tq_transmute() returns only newly created columns and is typically used when periodicity changes. Its syntax is the following:\nThe tq_get() function is a powerful tool for retrieving financial data in a tidy format. It provides an easy way to import stock prices, economic data, and key financial metrics from multiple sources, including Yahoo Finance, FRED, Alpha Vantage, and Quandl.\n\ntq_get(x, get = \"stock.prices\", from = NULL, to = NULL, ...)\n\n\nx: a character string or vector specifying the stock symbol(s) or identifier(s)\nget: the type of data to retrieve. Defaults to \"stock.prices\"\nfrom: start date, in YYYY-MM-DD format. Defaults to NULL (gets all available data)\nto: end date, in YYYY-MM-DD format. Defaults to NULL (gets all available data)\n...: additional arguments specific to the data source\n\n\n\nOne of the most common uses of tq_get() is fetching stock price data. Say, for example, that you want to fetch data from Apple between January and February for 2024. It is easy to use tq_get() to retrieve such information:\n\n# Assuming you have the tidyquant loaded in your session\n\n# Fetch Apple (AAPL) stock data from 2024-01-01 to 2024-02-01\naapl_data &lt;- tq_get(\"AAPL\", from = \"2024-01-01\", to = \"2024-02-01\")\n\n# Print first few rows\nhead(aapl_data)\n\n# A tibble: 6 × 8\n  symbol date        open  high   low close   volume adjusted\n  &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 AAPL   2024-01-02  187.  188.  184.  186. 82488700     185.\n2 AAPL   2024-01-03  184.  186.  183.  184. 58414500     183.\n3 AAPL   2024-01-04  182.  183.  181.  182. 71983600     181.\n4 AAPL   2024-01-05  182.  183.  180.  181. 62303300     180.\n5 AAPL   2024-01-08  182.  186.  182.  186. 59144500     184.\n6 AAPL   2024-01-09  184.  185.  183.  185. 42841800     184.\n\n\n\nThe result is a tidy tibble, unlike quantmod’s xts format."
  },
  {
    "objectID": "quant-fin-replications/L3/L3-Replication.html#the-tq_mutate-function",
    "href": "quant-fin-replications/L3/L3-Replication.html#the-tq_mutate-function",
    "title": "Manipulating Time Series",
    "section": "The tq_mutate() function",
    "text": "The tq_mutate() function\n\n\n\n\n\n\nDefinition\n\n\n\nThe tq_mutate() function adds adds new variables to an existing tibble:\n\ntq_mutate(.data, #The object you are performing the calculations \n       selected_variables, #The columns to send to the mutation function\n       mutate_fun, #The mutation function from either the xts, quantmod, or TTR package.\n       col_rename #A string or character vector containing names that can be used to quickly rename columns\n       ) \n\n\n\n\nThe main advantage is the results are returned as a tibble and the function can be used with the tidyverse\nIt is used when you expected additional columns to be added to the resulting data frame\nYou can use several time series related functions from other R packages - call tq_mutate_fun_options() to see the list of available options\nAll in all, it is similar in spirit to mutate()"
  },
  {
    "objectID": "quant-fin-replications/L3/L3-Replication.html#the-tq_transmute-function",
    "href": "quant-fin-replications/L3/L3-Replication.html#the-tq_transmute-function",
    "title": "Manipulating Time Series",
    "section": "The tq_transmute() function",
    "text": "The tq_transmute() function\n\n\n\n\n\n\nDefinition\n\n\n\nThe tq_transmute() returns only newly created columns and is typically used when periodicity changes. Its syntax is the following:\n\ntq_mutate(.data, #The object you are performing the calculations \n       selected_variables, #The columns to send to the mutation function\n       mutate_fun, #The mutation function from either the xts, quantmod, or TTR package.\n       col_rename #A string or character vector containing names that can be used to quickly rename columns\n       )\n\n\n\n\ntq_transmute() works exactly like tq_mutate() except it only returns the newly created columns\nThis is helpful when changing periodicity where the new columns would not have the same number of rows as the original tibble\nAll in all, it is similar in spirit to summarize()"
  },
  {
    "objectID": "quant-fin-replications/L3/L3-Replication.html#working-with-time-series-objects",
    "href": "quant-fin-replications/L3/L3-Replication.html#working-with-time-series-objects",
    "title": "Manipulating Time Series",
    "section": "Working with time series objects",
    "text": "Working with time series objects\nAn immediate useful example of using a time series specific functionality with a tidyverse logic relates to filtering. Sometimes, we may be interested in getting only a subset of the data (for example, only GOOG information). Furthermore, we may be interested in subsetting only a specific time frame for our analysis\nIt is relatively straightforward to do it with tidyquant:\n\nUse filter() to select only rows where symbol=='GOOG'\nIn the same call, filter for date&gt;= min_date and date&lt;=max_date\n\n\n#Assuming you have the tidyverse and the tidyquant packages loadded\n\n#Set up the list of assets\nassets=c('AMZN','GOOG','META','GME')\n\n#Filter out\nassets%&gt;%\n  tq_get()%&gt;%\n  filter(symbol=='GOOG',\n         date&gt;='2020-01-01',\n         date&lt;='2024-12-31')\n\n# A tibble: 1,258 × 8\n   symbol date        open  high   low close   volume adjusted\n   &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 GOOG   2020-01-02  67.1  68.4  67.1  68.4 28132000     68.1\n 2 GOOG   2020-01-03  67.4  68.6  67.3  68.0 23728000     67.8\n 3 GOOG   2020-01-06  67.5  69.8  67.5  69.7 34646000     69.5\n 4 GOOG   2020-01-07  69.9  70.1  69.5  69.7 30054000     69.4\n 5 GOOG   2020-01-08  69.6  70.6  69.5  70.2 30560000     70.0\n 6 GOOG   2020-01-09  71.0  71.4  70.5  71.0 30018000     70.7\n 7 GOOG   2020-01-10  71.4  71.7  70.9  71.5 36414000     71.2\n 8 GOOG   2020-01-13  71.8  72.0  71.3  72.0 33046000     71.7\n 9 GOOG   2020-01-14  72.0  72.1  71.4  71.5 31178000     71.3\n10 GOOG   2020-01-15  71.5  72.1  71.5  72.0 25654000     71.7\n# ℹ 1,248 more rows\n\n\nAnother example of using a time series specific functionality is working with leads and lags: sometimes, we need to shift our variables by a specific interval, like getting the previous day’s price. Say, for example, that you want to understand how S&P returns levels relate to NFLX returns one-week ahead. It is relatively straightforward to do it with tidyquant:\n\nDownload S&P 500 and NFLX data using the tq_get() function\nUse tq_transmute() to compute the weekly returns for each security based on daily data\nUse tq_mutate() to generate a lagged series of S&P 500 returns\n\n\n#Assuming you have the tidyverse and the tidyquant packages loadded\n\n#Netflix Data\nNFLX=tq_get('NFLX')%&gt;%\n  #Select only the necessary columns\n  select(date,symbol,adjusted)%&gt;%\n  #Apply the weeklyReturn function and call the new column 'NFLX'\n  tq_transmute(mutate_fun = weeklyReturn,\n               col_rename = 'NFLX')\n\n#S&P Data\nSP500=tq_get('^GSPC')%&gt;%\n  #Select only the necessary columns\n  select(date,symbol,adjusted)%&gt;%\n  #Apply the weeklyReturn function and call the new column 'SP500'\n  tq_transmute(mutate_fun = weeklyReturn,\n               col_rename = 'SP500')%&gt;%\n  #Apply the lag function for n=1 week and call the new column 'SP500'\n  tq_transmute(mutate_fun = lag.xts,\n            n=1,\n            col_rename = 'SP500')%&gt;%\n  #Drop all rows with NA information (row 1, in this case)\n  drop_na()\n\n#Merge Data \ninner_join(NFLX,SP500)\n\nJoining with `by = join_by(date)`\n\n\n# A tibble: 528 × 3\n   date           NFLX    SP500\n   &lt;date&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 2015-01-09 -0.0563   0      \n 2 2015-01-16  0.0244  -0.00651\n 3 2015-01-23  0.297   -0.0124 \n 4 2015-01-30  0.00992  0.0160 \n 5 2015-02-06  0.00579 -0.0277 \n 6 2015-02-13  0.0489   0.0303 \n 7 2015-02-20  0.0260   0.0202 \n 8 2015-02-27 -0.00688  0.00635\n 9 2015-03-06 -0.0438  -0.00275\n10 2015-03-13 -0.0346  -0.0158 \n# ℹ 518 more rows"
  },
  {
    "objectID": "quant-fin-replications/L3/L3-Replication.html#rolling-functions",
    "href": "quant-fin-replications/L3/L3-Replication.html#rolling-functions",
    "title": "Manipulating Time Series",
    "section": "Rolling functions",
    "text": "Rolling functions\nFinance practitioners are often asked to perform analysis on a rolling basis: we may want to calculate a given signal on day \\(t\\) based on past \\(x\\) periods of information. Say, for example, that you want to calculate a simple and exponential moving average of adjusted prices from 5 days back for a given stock. It is relatively straightforward to do it with tidyquant:\n\nDownload stock data using the tq_get() function\nUse tq_mutate() twice along with the SMA() and EMA() functions setting n=5\n\n\n#Assuming you have the tidyverse and the tidyquant packages loadded\n\n#Set up the list of assets\nassets=c('AMZN')\n\nassets%&gt;%\n  tq_get()%&gt;%\n  select(date,symbol,adjusted)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_mutate(adjusted, mutate_fun = SMA, n = 5)%&gt;%\n  tq_mutate(adjusted, mutate_fun = EMA, n = 5)\n\n# A tibble: 2,546 × 5\n# Groups:   symbol [1]\n   symbol date       adjusted   SMA   EMA\n   &lt;chr&gt;  &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 AMZN   2015-01-02     15.4  NA    NA  \n 2 AMZN   2015-01-05     15.1  NA    NA  \n 3 AMZN   2015-01-06     14.8  NA    NA  \n 4 AMZN   2015-01-07     14.9  NA    NA  \n 5 AMZN   2015-01-08     15.0  15.0  15.0\n 6 AMZN   2015-01-09     14.8  14.9  15.0\n 7 AMZN   2015-01-12     14.6  14.8  14.8\n 8 AMZN   2015-01-13     14.7  14.8  14.8\n 9 AMZN   2015-01-14     14.7  14.8  14.8\n10 AMZN   2015-01-15     14.3  14.6  14.6\n# ℹ 2,536 more rows\n\n\nLastly, financial analysts often cover a collection of securities on a rolling basis. For example, a buy-side analyst will monitor stocks from a given industry so as to understand which ones are overvalued, and which ones are undervalued. Say, for example, that you want to focus on a subset of 4 stocks, and you need to compare the cumulative return up to the latest closing price.\nIt is easy to integrate the tidyquant functions along with the group_by() function you’ve learned when working with dplyr:\n\nGet the information using tq_get()\nGroup the data by symbol\nApply the tq_mutate and tq_transmute functions to pass time series functions to the data - in this case, the dailyReturn() and the Return.cumulative() function\n\n\n#Assuming you have the tidyverse and the tidyquant packages loadded\n\n#Set up the list of assets\nassets=c('AMZN','GOOG','META','GME')\n\nassets%&gt;%\n  tq_get()%&gt;%\n  select(date,symbol,adjusted)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_mutate(adjusted, mutate_fun = dailyReturn,col_rename = 'daily_return')%&gt;%\n  tq_transmute(daily_return,mutate_fun = Return.cumulative)%&gt;%\n  mutate(across(where(is.numeric),percent,big.mark='.'))%&gt;%\n  setNames(c('Ticker','Cumulative Return up-to-date'))\n\nWarning: There were 5 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(where(is.numeric), percent, big.mark = \".\")`.\nℹ In group 1: `symbol = \"AMZN\"`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\nℹ Run `dplyr::last_dplyr_warnings()` to see the 4 remaining warnings.\n\n\n# A tibble: 4 × 2\n# Groups:   Ticker [4]\n  Ticker `Cumulative Return up-to-date`\n  &lt;chr&gt;  &lt;chr&gt;                         \n1 AMZN   1.382%                        \n2 GOOG   617%                          \n3 META   843%                          \n4 GME    329%"
  },
  {
    "objectID": "quant-fin-replications/L3/L3-Replication.html#hands-on-exercise",
    "href": "quant-fin-replications/L3/L3-Replication.html#hands-on-exercise",
    "title": "Manipulating Time Series",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\nYour manager (who did not lift any weights past the last 5 years) wanted to replicate the returns of the Deadlift ETF from 2020 to 2024. You job is to create a simple table of yearly returns comparing the Deadlift ETF vis-a-vis the S&P 500 Index. Follow the instructions and answer to the following questions:\n\nWhen looking at the yearly results from both the Deadlift ETF and S&P 500, which one did perform better?\nWhat are the potential explanations for the result you have found?\n\nTo answer to these questions, you will be using the a combination of dplyr and tidyquant functions you have learned so far. The expected result is a data.frame object that shows both the Deadlift ETF as well as the S&P 500 returns (columns) on a yearly basis (rows).\n\n\n\n\n\n\nInstructions\n\n\n\nBefore you start, make sure to have the tidyverse and the tidyquant packages loaded in your session. Following the instructions from the previous lectures, you can either make a direct call to each package, library(tidyverse) and library(tidyquant), or copy-paste the script from the course’s official website.\n\nUse tq_get() to load information from the S&P Index and the Deadlift ETF constituents in two separate objects. You can use the code ^GSPC to retrieve information for the index, and you can pass a vector c('ticker1','ticker2',...,'ticker_n') to get information on the Deadlift ETF constituents\nFilter for observations starting between 2020 (beginning of) and 2024 (end of) using the from and to arguments of the tq_get() function\nGroup the Deadlift ETF data by symbol using the group_by() function\nFor both data sets, create a yearly_ret variable that calculates the yearly return of a given security. You can use the tq_transmute() function, passing the yearlyReturn() function along the chain\nFor the Deadlift data set, regroup the data by date and calculate the Deadlift returns using a mutate() function (Hint: it is an equally weighted portfolio)\nMerge both datasets using inner_join()"
  },
  {
    "objectID": "quant-fin-replications/L3/L3-Replication.html#solution-walkthrough",
    "href": "quant-fin-replications/L3/L3-Replication.html#solution-walkthrough",
    "title": "Manipulating Time Series",
    "section": "Solution walkthrough",
    "text": "Solution walkthrough\n\n#Assuming you have the tidyverse and the tidyquant packages loadded\n\n# Set up the list of assets\ndeadlift=c('META','AMZN','GS','UBER','MSFT','AAPL','BLK','NVDA')\n\n#Set up the starting date\nstart='2020-01-01'\nend='2024-12-31'\n\n#Step 1: Read the Deadlift data using tidyquant\nDeadlift_Performance=deadlift%&gt;%\n  tq_get(from=start,to=end)%&gt;%\n  #Select only the columns of interest\n  select(symbol,date,adjusted)%&gt;%\n  #Group by symbol and date\n  group_by(symbol)%&gt;%\n  #Use tq_transmute to aggregate and calculate weekly returns\n  tq_transmute(selected=adjusted,\n               mutate_fun=yearlyReturn,\n               col_rename = 'Deadlift')%&gt;%\n  #Group by date\n  group_by(date)%&gt;%\n  #Summarize average return (since it is an equally-weighted portfolio)\n  summarize(Deadlift=mean(Deadlift,na.rm=TRUE))\n\n#Step 2: Read the S&P 500 data using tidyquant\nSP500_Performance=tq_get('^GSPC',from=start,to=end)%&gt;%\n  #Select only the columns of interest\n  select(symbol,date,adjusted)%&gt;%\n  #Group by symbol and date\n  group_by(symbol)%&gt;%\n  #Use tq_transmute to aggregate and calculate weekly returns\n  tq_transmute(selected=adjusted,\n               mutate_fun=yearlyReturn,\n               col_rename = 'SP500')%&gt;%\n  ungroup()%&gt;%\n  select(-symbol)\n    \n#Merge\nSP500_Performance%&gt;%\n  inner_join(Deadlift_Performance)%&gt;%\n  mutate(across(where(is.numeric),percent))%&gt;%\n  mutate(date=year(date))%&gt;%\n  setNames(c('Year','S&P 500','DeadLift ETF'))\n\nJoining with `by = join_by(date)`\n\n\n# A tibble: 5 × 3\n   Year `S&P 500` `DeadLift ETF`\n  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;         \n1  2020 15.29%    57.9%         \n2  2021 26.89%    37.2%         \n3  2022 -19.44%   -36.0%        \n4  2023 24.23%    100.5%        \n5  2024 23.84%    52.1%         \n\n\nThis solution uses tidyquant and tidyverse to analyze the yearly returns of a custom portfolio (i.e., the “Deadlift ETF”) consisting of eight stocks and compares it with the S&P 500.\n\nDefine Assets and Time Range. first, we dedfine a custom portfolio (deadlift) containing eight stocks, and set the start and end dates for data collection.\nFetch & Process the Deadlift Portfolio Returns. Starting with the Deadlift ETF, we first fetch historical stock prices using tq_get() for each asset contained in the ETF. After that, we only keep the relevant columns - namely, symbol, date, and adjusted. Using group_by() to group by symbol, we use the tq_transmute() function to apply the yearlyReturns function to the adjusted column, renaming it as Deadlift.\nCalculating portfolio returns. Since this ETF is an equally-weighted portfolio, and using the fact that a portfolio return is a weighted average of the individual securities, you can safely use the mean() function to calculate the return of such portfolio. To make sure that the calculation is performed for each year, use group_by() again, grouping by date."
  },
  {
    "objectID": "quant-fin-replications/L3/L3-Replication.html#try-doing-some-edits-on-your-own",
    "href": "quant-fin-replications/L3/L3-Replication.html#try-doing-some-edits-on-your-own",
    "title": "Manipulating Time Series",
    "section": "Try doing some edits on your own!",
    "text": "Try doing some edits on your own!\nTry thinking about changes you could do to either improve code readability of the analysis. A couple of edits that can be made include, but are not limited, to:\n\nAdding more time periods to the analysis\nContrasting the DeadLift ETF with the S&P 500 in terms of variance and Sharpe Ratio\nDoing the comparison using value-weighted returns (i.e, weighting the securities inside the portfolio according to its market capitalization) or inverse volatility (i.e, riskier assets have lower weights)\n\nPlay around with these concepts to get familiar with all the data manipulation tools that come with the tidyquant package!"
  },
  {
    "objectID": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#solution-5",
    "href": "quant-fin/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#solution-5",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Solution",
    "text": "Solution\n\nCodeOutput\n\n\n\n#Read the Data\nM7%&gt;%\n#Select only the columns of interest\nselect(symbol,date,adjusted)%&gt;%\n#Make sure date is read as a Date object\nmutate(date=as.Date(date))%&gt;%\n#Filter for observations happening in 2025\nfilter(year(date)==2025)%&gt;%\n#Arrange from chronological order\narrange(date)%&gt;%\n#Group by Symbol to perform the calculations\ngroup_by(symbol)%&gt;%\n#Create the return\nmutate(Return = adjusted/lag(adjusted,default = NA))%&gt;%\n#Remove NAs before doing the cumulative product\ndrop_na()%&gt;%\nmutate(Cum_Return = cumprod(Return)-1)%&gt;%\n#Select the latest observation from each symbol\nslice_tail(n=1)%&gt;%\n#Select symbol, date, and cumulative return\nselect(symbol,date,Cum_Return)%&gt;%\n#Arrange from lowest-to-highest\narrange(Cum_Return)\n\n\n\n\n\n# A tibble: 7 × 3\n# Groups:   symbol [7]\n  symbol date       Cum_Return\n  &lt;chr&gt;  &lt;date&gt;          &lt;dbl&gt;\n1 NVDA   2025-01-29    -0.106 \n2 AAPL   2025-01-29    -0.0184\n3 TSLA   2025-01-29     0.0259\n4 GOOG   2025-01-29     0.0344\n5 MSFT   2025-01-29     0.0567\n6 AMZN   2025-01-29     0.0765\n7 META   2025-01-29     0.129"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#more-annotations",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#more-annotations",
    "title": "Data Visualization",
    "section": "More annotations",
    "text": "More annotations\n\nApart from simply changing the labels of your axis, titles and subtitles, you can also use ggplot2 to customize the appearance of your axis:\n\nThe family of functions scale_x_{} apply a given structure to the x-axis - e.g, scale_x_date(),scale_x_continuous()\nThe family of functions scale_y_{} apply a given structure to the y-axis - e.g, scale_y_continuous() etc\n\nWith that, you can, for example:\n\nForce the x-axis to be formatted as a date, adjusting how it is being displayed\nForce the y-axis to be formatted in terms of dollar amounts\n\nIn this way, you can impose meaningful structures in your chart depending on the type of data you are considering in your mapping to x and y axis!"
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#more-annotations-continued",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#more-annotations-continued",
    "title": "Data Visualization",
    "section": "More annotations, continued",
    "text": "More annotations, continued\n\nFor example, the code snippet below formats the x-axis to show breaks at the year level, and formats the y-axis in such a way that it goes from \\(\\small\\$0\\) to \\(\\small\\$1,000\\) by increments of \\(\\small\\$50\\)\n\n\n  #Your previous ggplot call up to now\n  {your_previous_ggplot} +\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar, breaks = seq(from=0,to=1000,by=50))\n\n\nClick here to see comprehensive list of all customizations that can be done across both x-axis and y-axis for continuous scales (scale_x_continuous() and scale_y_continuous())\nClick here to see comprehensive list of all customizations that can be done across both x-axis and y-axis for date scales (scale_x_date() and scale_y_date())\n\n\n\n\n\n\n\nFormatting scales\n\n\nTo properly format the appearance of your axis, make sure to have the scales package properly installed and loaded. You can do so by calling install.packages('scales') and library(scales)."
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html#step-4-customize-your-axis",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html#step-4-customize-your-axis",
    "title": "Data Visualization",
    "section": "Step 4: customize your axis",
    "text": "Step 4: customize your axis\n\nExerciseSolution\n\n\n\n\n\nListing 4: Using your previously created ggplot object, customize the appearance of the x-axis and y-axis in the following way: the x-axis shoudl be formatted as a date using an appropriate function that shows each year as a breakpoint, whereas the y-axis should be formatted in dollar terms, ranging from zero to one thousand dollars, by increments of 50, using an appropriate function. You can pass additional layers using the + operator. The FANG dataset and ggplot2 have been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse scale_x_date() with the appropriate arguments to format the x-axis, doing the same thing for the y-axis using scale_y_continuous():\n\n\n#Let's use Meta (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()+\n#Adding a trend\ngeom_smooth(method='loess')+\n#Adding Annotations\nlabs(title='META adjusted prices',\n     subtitle = 'Source: Yahoo! Finance',\n     x = 'Year',\n     y = 'Adjusted Prices')+\n#Changing the behavior of scales\nscale_x_date(date_breaks = '1 year',labels = year) +\nscale_y_continuous(labels = dollar, breaks = seq(from=0,to=1000,by=50))\n#Let's use Meta (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()+\n#Adding a trend\ngeom_smooth(method='loess')+\n#Adding Annotations\nlabs(title='META adjusted prices',\n     subtitle = 'Source: Yahoo! Finance',\n     x = 'Year',\n     y = 'Adjusted Prices')+\n#Changing the behavior of scales\nscale_x_date(date_breaks = '1 year',labels = year) +\nscale_y_continuous(labels = dollar, breaks = seq(from=0,to=1000,by=50))"
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html",
    "href": "quant-fin-replications/L4/L4-Replication.html",
    "title": "Data Visualization",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nIn this lecture, we will be using the FANG dataset, which contains basic stock information from popular U.S. techonology firms: Facebook (Meta), Amazon, Netflix, and Google (Alphabet). Instead of loading the data from a .csv file, we will be loading data from a .txt file using the read_delim() function from readr, a package that is included in the tidyverse. Before you start, make sure to follow the instructions from our previous replication to set up your working directory correctly."
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#about-this-document",
    "href": "quant-fin-replications/L4/L4-Replication.html#about-this-document",
    "title": "Data Visualization",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nIn this lecture, we will be using the FANG dataset, which contains basic stock information from popular U.S. techonology firms: Facebook (Meta), Amazon, Netflix, and Google (Alphabet). Instead of loading the data from a .csv file, we will be loading data from a .txt file using the read_delim() function from readr, a package that is included in the tidyverse. Before you start, make sure to follow the instructions from our previous replication to set up your working directory correctly."
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#loading-packages",
    "href": "quant-fin-replications/L4/L4-Replication.html#loading-packages",
    "title": "Data Visualization",
    "section": "Loading packages",
    "text": "Loading packages\nAs we get started, we will be loading all packages referred in our official website.\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"glue\",\"scales\", \"ggthemes\",\"highcharter\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\nlapply(packages, library, character.only = TRUE)\n\nNote that you could easily get around this by installing and loading all necessary packages using a more simple syntax:\n\n#Install if not already available - I have commented these lines so that R does not attempt to install it everytime\n  #install.packages('tidyverse')\n  #install.packages('tidyquant')\n  #install.packages('glue')\n  #install.packages('scales')\n  #install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)\n  library(highcharter)"
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#data-visualization-in-r",
    "href": "quant-fin-replications/L4/L4-Replication.html#data-visualization-in-r",
    "title": "Data Visualization",
    "section": "Data Visualization in R",
    "text": "Data Visualization in R\nThe ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. The Grammar of Graphics, developed by Leland Wilkinson, is a structured approach to visualization where:\n\nData is mapped to aesthetic attributes (e.g., color, shape, size)\nA geometric object (geom) represents data visually (e.g., points, lines, bars)\nStatistical transformations (stats) summarize data\nScales control how data is mapped to visual properties\nFacets split data into panels for comparison\n\nKey Highlights\n\nIt is, by and large, the richest and most widely used plotting ecosystem in the  language\nggplot2 has a rich ecosystem of extensions - ranging from annotations and interactive visualizations to specialized genomics - click here a community maintained list"
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#ggplot2-foundations",
    "href": "quant-fin-replications/L4/L4-Replication.html#ggplot2-foundations",
    "title": "Data Visualization",
    "section": "ggplot2 foundations",
    "text": "ggplot2 foundations\nWe will illustrate the use of ggplot2 to replicate the Grammar of Graphics foundations using the FANG dataset. To load it into your R session, hit the download button and load it using read_delim('FANG.txt') or download the FANG.txt file directly on eClass®. To get ggplot2 in your session, either load tidyverse altogether of directly load the library:\n\n#Load the tidyquant package\nlibrary(tidyquant)\n\n#Option 1: load the tidyverse, which includes ggplot2\nlibrary(tidyverse)\n\n#Option 2: load ggplot2 directly\nlibrary(ggplot2)\n\n\nFANG=read_delim('FANG.txt')\n\n\n\n Download Raw data"
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#hands-on-exercise",
    "href": "quant-fin-replications/L4/L4-Replication.html#hands-on-exercise",
    "title": "Data Visualization",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\n\nIn late January 2021, Reddit traders took on the short-sellers by forcing them to liquidate their short positions using GameStop stocks. This coordinated behavior had significant repercussions for various investment funds, such as Melvin Capital - see here and here\n\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse tq_get() to load information for GameStop (ticker: GME) and store it in a data.frame. Using the arguments from and to from tq_get(), filter for observations between occurring in between December 2020 (beginning of) and March 2021 (end of)\nUse ggplot(aes(x=date,group=symbol)), along with geom_candlestick() and its appropriate arguments, to chart the historical OHLC prices\nCreate a vertical line annotation using geom_vline, setting the xintercept argument to the date of the Reddit frenzy (as.Date('2021-01-25'))\nUse the theme from The Economist calling theme_economist(). Make sure to have the ggthemes package installed and loaded\nFinally, call theme() and labs() to adjust the aesthetics of your theme and labels as you think it would best convey your message. For example, you can use the scales package to format the appearance of your x and y labels (for example, displaying a dollar sign in front of adjusted prices)"
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#solution-walkthrough",
    "href": "quant-fin-replications/L4/L4-Replication.html#solution-walkthrough",
    "title": "Data Visualization",
    "section": "Solution walkthrough",
    "text": "Solution walkthrough\n\n#Libraries\nlibrary(tidyquant)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(scales)\n\n#Setting start/end dates + reddit date\nstart='2020-12-01'\nend='2021-03-31'\nreddit_date=as.Date('2021-01-25')\n\n#Get the data\ntq_get('GME',from=start,to=end)%&gt;%\n  #Mapping\n  ggplot(aes(x=date,group=symbol))+\n  #Geom\n  geom_candlestick(aes(open = open, high = high, low = low, close = close))+\n  #Labels\n  labs(x='',\n       y='Adjusted Prices',\n       title='GameStop (ticker: GME) prices during the reddit (Wall St. Bets) frenzy',\n       subtitle='Source: Yahoo! Finance')+\n  #Annotation\n  geom_vline(xintercept=reddit_date,linetype='dashed')+\n  annotate(geom='text',x=reddit_date-5,y=75,label='Reddit Frenzy Starts',angle=90)+\n  #Scales\n  scale_x_date(date_breaks = '2 weeks') +\n  scale_y_continuous(labels = dollar) +\n  #Custom 'The Economist' theme\n  theme_economist()+\n  #Adding further customizations\n  theme(legend.position='none',\n        axis.title.y = element_text(vjust=+4,face='bold'),\n        axis.title.x = element_text(vjust=-3,face='bold'),\n        plot.title = element_text(size=10),\n        plot.subtitle = element_text(size=8,vjust=-2,hjust=0,margin = margin(b=15)),\n        axis.text.y = element_text(size=8),\n        axis.text.x = element_text(angle=45,size=8,vjust=0.75))\n\n\n\n\n\n\n\n\nThis code visualizes GameStop (GME) stock prices during the Reddit (Wall Street Bets) frenzy in early 2021 using a candlestick chart. It retrieves stock data from Yahoo! Finance, applies ggplot2 for visualization, and customizes the plot using ggthemes.\n\nDefine Date Ranges. These dates specify the period over which GME stock data will be retrieved. More specifically, start and end are used to filter the tq_get() function, whereas reddit_date marks the key event when WallStreetBets (WSB) discussions fueled the GME rally, and will be used in the ggplot call to annotate the exact period where the frenzy happened.\nRetrieve Stock Data. The tq_get() fetches stock price data for GameStop (GME) from Yahoo! Finance. It returns a data frame with the following columns: date, open, high, low, close, adjusted, volume.\nCreate Candlestick Chart. While the ggplot(aes(x = date, group = symbol)) creates a basic ggplot chart, the geom_candlestick(aes(open = open, high = high, low = low, close = close)) maps the specific variables onto the OHLC information.\nAdd Labels. Using the labs() function, it is possible to customize several aspects of the chart, such as the x and y labels, as well as the title and subtitle.\nAnnotate the Reddit Frenzy date. The geom_vline() adds a dashed vertical line at reddit_date (Jan 25, 2021) to highlight the Reddit-driven rally. More specifically, the function places a text label “Reddit Frenzy Starts” near the line, x = reddit_date - 5 shifts text 5 days to the left for better visibility, y = 75 positions it at $75, and angle = 90 rotates the text vertically.\nCustomize Axes. Thescale_x_date(date_breaks = '2 weeks') ensures the x-axis shows date breaks every 2 weeks, while the scale_y_continuous(labels = dollar) formats y-axis values as dollar amounts.\nApply and customize predefined themes. The theme_economist() function applies a professional, clean theme from ggthemes, inspired by The Economist famous financial charts. On top of that, the theme() function edits various aspects of the predefined theme, such as bold titles, font sizes, and more, to better convey the message."
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#try-doing-some-edits-on-your-own",
    "href": "quant-fin-replications/L4/L4-Replication.html#try-doing-some-edits-on-your-own",
    "title": "Data Visualization",
    "section": "Try doing some edits on your own!",
    "text": "Try doing some edits on your own!\nTry thinking about changes you could do to either improve code readability of the analysis. A couple of edits that can be made include, but are not limited, to:\n\nAdding more time periods to the analysis\nContrasting the DeadLift ETF with the S&P 500 in terms of variance and Sharpe Ratio\nDoing the comparison using value-weighted returns (i.e, weighting the securities inside the portfolio according to its market capitalization) or inverse volatility (i.e, riskier assets have lower weights)\n\nPlay around with these concepts to get familiar with all the data manipulation tools that come with the tidyquant package!"
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#step-1-the-data",
    "href": "quant-fin-replications/L4/L4-Replication.html#step-1-the-data",
    "title": "Data Visualization",
    "section": "Step 1: the data",
    "text": "Step 1: the data\nFirst and foremost, in our call to ggplot, we need to make sure that it knows where the data is located. We will be using the FANG dataset, which contains basic stock information from popular U.S. techonology firms: Facebook (Meta), Amazon, Netflix, and Google (Alphabet). The first step in using ggplot2 is to call your data dataframe and supply the aesthetic mapping, which we’ll refer to as aes\n\nggplot(data=your_data, aes(x= variable_1, y=variable_2, ...))\n\n\nThe data argument refers to the dataset used\nThe aes argument contains all the aesthetic mappings that will be used\n\nTogether, these constitute the backbone of your visualization: they tell ggplot2 what the raw information to be used and where it should be mapped! For example, we can create another object, META, filtering for observations from FANG where symbol=='META' and chaining this the newly created dataset onto ggplot, mapping the date variable in the x axis, adjusted variable in the y axis, and symbol in the group aesthetic:\n\n#Read the data\nFANG=read_delim('FANG.txt')\n\n#Let's use Apple (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol))"
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#step-3-be-creative-with-additional-layers",
    "href": "quant-fin-replications/L4/L4-Replication.html#step-3-be-creative-with-additional-layers",
    "title": "Data Visualization",
    "section": "Step 3: be creative with additional layers",
    "text": "Step 3: be creative with additional layers\nYour main chart is now all set: it contains the data and the necessary aes(thetic) mappings to the chart, and it also contains a shape, or geom(metry), that was selected to display the data. What’s next? The philosophy behind the Grammar of Graphics is now to add layers of information on top of the base chart using the + operator, like before.\nWe will proceed by including several layers of information that will either add or modify the behavior of the chart, making it more appealing to our audience:\n\nAdding trend lines using geom_smooth()\nAdding annotations and labels using annotation and labs\nModifying the behavior of the scales using scale_y and scale_x\n\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()+\n#Adding a trend\ngeom_smooth(method='loess')+\n#Adding Annotations\nlabs(title='META adjusted prices',\n     subtitle = 'Source: Yahoo! Finance',\n     x = 'Year',\n     y = 'Adjusted Prices')"
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#more-annotations",
    "href": "quant-fin-replications/L4/L4-Replication.html#more-annotations",
    "title": "Data Visualization",
    "section": "More annotations",
    "text": "More annotations\nApart from simply changing the labels of your axis, titles and subtitles, you can also use ggplot2 to customize the appearance of your axis: 1. The family of functions scale_x_{} apply a given structure to the x-axis - e.g, scale_x_date(),scale_x_continuous() 2. The family of functions scale_y_{} apply a given structure to the y-axis - e.g, scale_y_continuous() etc\nWith that, you can, for example:\n\nForce the x-axis to be formatted as a date, adjusting how it is being displayed\nForce the y-axis to be formatted in terms of dollar amounts\n\nIn this way, you can impose meaningful structures in your chart depending on the type of data you are considering in your mapping to x and y axis! Say, for example, that you want to format the x-axis to show breaks at the year level, and the y-axis in such a way that it goes from \\(\\small\\$0\\) to \\(\\small\\$1,000\\) by increments of \\(\\small\\$50\\). You can do so by adding the following syntax to your ggplot object:\n\n  #Your previous ggplot call up to now\n  {your_previous_ggplot} +\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar, breaks = seq(from=0,to=1000,by=50))\n\n\nClick here to see comprehensive list of all customizations that can be done across both x-axis and y-axis for continuous scales (scale_x_continuous() and scale_y_continuous())\nClick here to see comprehensive list of all customizations that can be done across both x-axis and y-axis for date scales (scale_x_date() and scale_y_date())\n\n\n\n\n\n\n\nFormatting scales\n\n\n\nTo properly format the appearance of your axis, make sure to have the scales package properly installed and loaded. You can do so by calling install.packages('scales') and library(scales).\n\n\n\n#Let's use Meta (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()+\n#Adding a trend\ngeom_smooth(method='loess')+\n#Adding Annotations\nlabs(title='META adjusted prices',\n     subtitle = 'Source: Yahoo! Finance',\n     x = 'Year',\n     y = 'Adjusted Prices')+\n#Changing the behavior of scales\nscale_x_date(date_breaks = '1 year',labels = year) +\nscale_y_continuous(labels = dollar, breaks = seq(from=0,to=1000,by=50))"
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#step-2-adding-your-geom",
    "href": "quant-fin-replications/L4/L4-Replication.html#step-2-adding-your-geom",
    "title": "Data Visualization",
    "section": "Step 2: adding your geom",
    "text": "Step 2: adding your geom\nYou probably thought you did something wrong when you saw an empty chart with the named axis, right? However, I can assure: you did great! It is all about the philosophy embedded in the Grammar of Graphics: you first provide the data and the aes(thetic) mapping to your data. Now, ggplot knows exactly which information to select and where to place it. However, it is still agnostic about how to display it. We will now add a geometry layer - in short, a geom:\n\n\n\n\n\n\nAdding layers on top of a ggplot object\n\n\n\n\nYou can add layers on top of ggplot object addition symbol (+)\nThere are many types of potential geometries, to name a few: geom_point(), geom_col(), geom_line() - click here for a complete list\n\n\n\nA layer combines data, aesthetic mapping, a geom (geometric object), a stat (statistical transformation), and a position adjustment. Typically, you will create layers using a geom_{} function, overriding the default position and stat if needed. With your ggplot call, use the + operator to add a geometry layer on top of the actual empty ggplot chart - in this case, we will be using the geom_line() geometry:\n\n#Use ggplot2 to map the aesthetics to the plot and add a geom_line()\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()"
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#adding-multiple-data-points",
    "href": "quant-fin-replications/L4/L4-Replication.html#adding-multiple-data-points",
    "title": "Data Visualization",
    "section": "Adding multiple data points",
    "text": "Adding multiple data points\nWhat if we wanted to add more data? In our first example, we set filter(symbol)=='META' to select only information from Meta to your chart. However, one might be interested in understanding how did Meta perform relative to its FANG peers.m It is easy to do it with ggplot:\n\nBecause you have set group=symbol, ggplot already knows that it needs to group by each different string contained in the ticker column\nIn such a way, all you need to do is to add a new aes mapping, colour=symbol, so that ggplot knows that each symbol needs to have a different color!\n\nIn what follows, we will be charting all four FANG stocks in the same chart, adjusting the layers to try keeping aesthetics as good as possible.\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar, breaks = seq(0,1000,50))"
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#facet-it-until-you-make-it",
    "href": "quant-fin-replications/L4/L4-Replication.html#facet-it-until-you-make-it",
    "title": "Data Visualization",
    "section": "Facet it until you make it",
    "text": "Facet it until you make it\nWe have included all FANG stocks into the same chart. Easy peasy, lemon squeezy!. As far as we could go on adjusting the layers, it seems that the chart conveys too much information:\n\nBecause of the different scales, you can hardly tell the different between AMZN AND GOOG during 2015-2018\nFurthermore, trend lines are, in some cases, effectively hiding the data undernearth\n\nAlthough you could easily remove the trend lines, ggplot2 also comes with a variety of alternatives when it comes to charting multiple data that may come in handy:\n\nYou can facet your chart using facet_wrap, controlling the axis as well as the number of rows and columns\nYou can grid your chart, making the comparison easier with fixed axes\n\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting scales: free_y or fixed\n  facet_wrap(facets=symbol~.,ncol=2,nrow=2,scales='free_y')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting vertical orientation (.~symbol) and horizontal orientation (symbol~.)\n  facet_grid(rows=.~symbol,scales='fixed')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar, breaks = seq(0,1000,250))"
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#adding-themes-youre-in-full-control-of-your-message",
    "href": "quant-fin-replications/L4/L4-Replication.html#adding-themes-youre-in-full-control-of-your-message",
    "title": "Data Visualization",
    "section": "Adding themes: you’re in full control of your message!",
    "text": "Adding themes: you’re in full control of your message!\nBy now, you are already looking like a data manipulation wizard in your firm:\n\nYou have created a fully automated data ingestion process using tq_get() to get live FANG prices.\nSet up ggplot to automatically update the chart;\nFinally, you have adjusted all aesthetics to make it more much more professional\n\nA lot of the ggplot adoption throughout the R usiverse relates to themes: complete configurations which control all non-data display: first, there are a lot of available themes that you can pass to your ggplot, like theme_minimal(), theme_bw(). Alternatively, you can pass theme() if you just need to tweak the display of an existing theme.\nFor example, the code below adds theme_minimal(), a predefined theme that is loaded together with ggplot2, to further customize the appearance of the chart:\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting scales: free_y or fixed\n  facet_wrap(facets=symbol~.,ncol=2,nrow=2,scales='free_y')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar)+\n  #Using the theme_minimal() theme configuration that comes with ggplot2\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere are endless customizations that you could think of that could be applied to a theme. In special, the package ggthemes provides extra themes, geoms, and scales for ggplot2 that replicate the look of famous aesthetics that you have often looked and said: “how could I replicate that?”\nTo get access to these additional graphical resources in your R session, install and load the package using:\n\ninstall.packages('ggthemes') #Install if not available\nlibrary(ggthemes) #Load\n\nyour_previous_ggplot_object + theme_{insertyourtheme}\n\nTo check all available themes, check the ggthemes library here website. Below, you can find the same visualization using distinct themes coming from the ggthemes library:\n\nWSJThe EconomistExcelFiveThirtyEightGoogle Docs\n\n\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting scales: free_y or fixed\n  facet_wrap(facets=symbol~.,ncol=2,nrow=2,scales='free_y')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar)+\n  #Try out all available themes\n  theme_wsj()\n\n\n\n\n\n\n\n\n\n\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting scales: free_y or fixed\n  facet_wrap(facets=symbol~.,ncol=2,nrow=2,scales='free_y')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar)+\n  #Try out all available themes\n  theme_economist()\n\n\n\n\n\n\n\n\n\n\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting scales: free_y or fixed\n  facet_wrap(facets=symbol~.,ncol=2,nrow=2,scales='free_y')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar)+\n  #Try out all available themes\n  theme_excel()\n\n\n\n\n\n\n\n\n\n\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting scales: free_y or fixed\n  facet_wrap(facets=symbol~.,ncol=2,nrow=2,scales='free_y')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar)+\n  #Try out all available themes\n  theme_fivethirtyeight()\n\n\n\n\n\n\n\n\n\n\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting scales: free_y or fixed\n  facet_wrap(facets=symbol~.,ncol=2,nrow=2,scales='free_y')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar)+\n  #Try out all available themes\n  theme_gdocs()"
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#integrating-tidyquant",
    "href": "quant-fin-replications/L4/L4-Replication.html#integrating-tidyquant",
    "title": "Data Visualization",
    "section": "Integrating tidyquant",
    "text": "Integrating tidyquant\nLike in our previous lecture, the tidyquant added very important functionalities for those who work in finance to easily manage financial time series using the well-established foundations of the tidyverse. When it comes to data visualization, tidyquant also provides a handful of integrations that can be inserted into your ggplot call:\n\nPossibility of using geom_barchart and geom_candlestick\nMoving average visualizations and Bollinger Bands available using geom_ma and geom_bbands\nA new theme, theme_tq, available\n\nThe code below shows an example of how tidyquant objects can be chained on a ggplot call to generate meaningful visualization of financial time series. Say, for example, that you wanted to understand how each FANG stock behaved during the DeepSeek announcement. You could use the geom_candlestick() and geom_ma() functions with its appropriate arguments to a purely financial visualization:\n\n#Set up start and end dates\nend=Sys.Date()\nstart=end-weeks(5)\n\nFANG%&gt;%\n  #Make sure that date is read as a Date object\n  mutate(date=as.Date(date))%&gt;%\n  #Filter\n  filter(date &gt;= start, date&lt;=end)%&gt;%\n  #Basic layer - aesthetic mapping including fill\n  ggplot(aes(x=date,y=close,group=symbol))+\n  #Charting data - you could use geom_line(), geom_col(), geom_point(), and others\n  geom_candlestick(aes(open = open, high = high, low = low, close = close))+\n  geom_ma(ma_fun = SMA, n = 5, color = \"black\", size = 0.25)+\n  #Facetting\n  facet_wrap(symbol~.,scales='free_y')+\n  #DeepSeek date\n  geom_vline(xintercept=as.Date('2025-01-24'),linetype='dashed')+\n  #Annotations\n  labs(title='FANG adjusted prices before/after DeepSeek announcement',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Date',\n       y = 'Adjusted Prices')+\n  #Scales\n  scale_x_date(date_breaks = '3 days') +\n  scale_y_continuous(labels = dollar) +\n  #Custom 'The Economist' theme\n  theme_economist()+\n  #Adding further customizations\n  theme(legend.position='none',\n        axis.title.y = element_text(vjust=+4,face='bold'),\n        axis.title.x = element_text(vjust=-3,face='bold'),\n        plot.subtitle = element_text(size=8,vjust=-2,hjust=0,margin = margin(b=15)),\n        axis.text.y = element_text(size=8),\n        axis.text.x = element_text(angle=90,size=8))\n\n\n\n\n\n\n\n\nFor a thorough discussion, see a detailed discussion on tidyquant’s charting capabilities here."
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#alternatives-to-ggplot2",
    "href": "quant-fin-replications/L4/L4-Replication.html#alternatives-to-ggplot2",
    "title": "Data Visualization",
    "section": "Alternatives to ggplot2",
    "text": "Alternatives to ggplot2\nThe ggplot2 package is, by and large, the richest and most widely used plotting ecosystem in the  language. However, there are also other interesting options, especially when it comes to interactive data visualization\n\nThe plotly ecosystem provides interactive charts for R, Python, Julia, Java, among others - you can install the R package using install.packages('plotly')\nThe Highcharts is another option whenever there is a need for interactive data visualization - you can install the R package using install.packages('highcharter')\n\nIn special, the highcharter package works seamlessly with time series data, especially those retrieved by the tidyquant’s tq_get() function.\n\n#Install the highcharter package (if not installed yet)\n#install.packages('highcharter')\n\n#Load the highcharter package (if not loaded yet)\nlibrary(highcharter)\n\n#Select the Google Stock with OHLC information and transform to an xts object\nGOOG=tq_get('GOOG')%&gt;%select(-symbol)%&gt;%as.xts()\n\n  #Initialize an empty highchart\n  highchart(type='stock')%&gt;%\n  #Add the Google Series\n  hc_add_series(GOOG,name='Google')%&gt;%\n  #Add title and subtitle\n  hc_title(text='A Dynamic Visualization of Google Stock Prices Over Time')%&gt;%\n  hc_subtitle(text='Source: Yahoo! Finance')%&gt;%\n  #Customize the tooltip\n  hc_tooltip(valueDecimals=2,valuePrefix='$')%&gt;%\n  #Convert it to a 'The Economist' theme\n  hc_add_theme(hc_theme_economist())"
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#exploring-ggplot2-beyond-this-lecture",
    "href": "quant-fin-replications/L4/L4-Replication.html#exploring-ggplot2-beyond-this-lecture",
    "title": "Data Visualization",
    "section": "Exploring ggplot2 beyond this lecture",
    "text": "Exploring ggplot2 beyond this lecture\nThe ggplot2 package is an incredibly vast and flexible data visualization package. While this lecture covers the core concepts and essential functions, it is impossible to cover every aspect of ggplot2 in a single session. The package includes a wide range of geometric objects (geoms), themes, and customization options, along with an extensive ecosystem of extensions that add even more functionality.\nFor further exploration, students are encouraged to refer to the following resources:\n\nComplete List of Geoms (geometric objects): learn about all available geom functions, such as geom_violin(), geom_ribbon(), and more - click here\nThemes in ggplot2: explore built-in themes like theme_minimal(), theme_classic(), and specialized options such as theme_void() - click here\nTheme Customization: customize every visual element of a ggplot, including fonts, margins, grid lines, and legend positions - click here\nExtensions: discover additional packages that enhance ggplot2 with interactive features, advanced annotations, and more - click here\n\nBy exploring these resources, you can unlock the full potential of ggplot2 and create even more powerful and visually compelling data visualizations."
  },
  {
    "objectID": "quant-fin/Lecture 4 - Data Visualization/index.html",
    "href": "quant-fin/Lecture 4 - Data Visualization/index.html",
    "title": "Lucas S. Macoris",
    "section": "",
    "text": "This page was intentionally left blank\n#| edit: false\n#| output: false\nwebr::install(\"gradethis\", quiet = TRUE)\nlibrary(gradethis)\noptions(webr.exercise.checker = function(\n  label, user_code, solution_code, check_code, envir_result, evaluate_result,\n  envir_prep, last_value, engine, stage, ...\n) {\n  if (is.null(check_code)) {\n    # No grading code, so just skip grading\n    invisible(NULL)\n  } else if (is.null(label)) {\n    list(\n      correct = FALSE,\n      type = \"warning\",\n      message = \"All exercises must have a label.\"\n    )\n  } else if (is.null(solution_code)) {\n    list(\n      correct = FALSE,\n      type = \"warning\",\n      message = htmltools::tags$div(\n        htmltools::tags$p(\"A problem occurred grading this exercise.\"),\n        htmltools::tags$p(\n          \"No solution code was found. Note that grading exercises using the \",\n          htmltools::tags$code(\"gradethis\"),\n          \"package requires a model solution to be included in the document.\"\n        )\n      )\n    )\n  } else {\n    gradethis::gradethis_exercise_checker(\n      label = label, solution_code = solution_code, user_code = user_code,\n      check_code = check_code, envir_result = envir_result,\n      evaluate_result = evaluate_result, envir_prep = envir_prep,\n      last_value = last_value, stage = stage, engine = engine)\n  }\n})\n#| echo: false\n\n#Making data available for webR\nFANG=read_delim('Assets/FANG.txt')"
  },
  {
    "objectID": "quant-fin-replications/L4/L4-Replication.html#the-ggplot2-foundations",
    "href": "quant-fin-replications/L4/L4-Replication.html#the-ggplot2-foundations",
    "title": "Data Visualization",
    "section": "The ggplot2 foundations",
    "text": "The ggplot2 foundations\nWe will illustrate the use of ggplot2 to replicate the Grammar of Graphics foundations using the FANG dataset. To load it into your R session, hit the download button and load it using read_delim('FANG.txt') or download the FANG.txt file directly on eClass®. To get ggplot2 in your session, either load tidyverse altogether of directly load the library:\n\n#Load the tidyquant package\nlibrary(tidyquant)\n\n#Option 1: load the tidyverse, which includes ggplot2\nlibrary(tidyverse)\n\n#Option 2: load ggplot2 directly\nlibrary(ggplot2)\n\n\nFANG=read_delim('FANG.txt')\n\n\n\n Download Raw data"
  },
  {
    "objectID": "quant-fin-datacases/quarto-mock-template.html",
    "href": "quant-fin-datacases/quarto-mock-template.html",
    "title": "Data Case Prep - Exercises",
    "section": "",
    "text": "To help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. You can include your code inside the R code chunks. To run a specific code chunk, select all lines that apply and hit Ctrl+Enter (alternatively, click on the Run Current Chunk button at the top-right corner of the code chunk. Alternatively, to render the Quarto altogether, ckick on the Render button (shortcut: Ctrl+Shift+K).\nYou can copy-paste the code chunk below each time you need to manipulate data. To provide your explanation and interpretation of the results for each question, you can write outside of the R coding chunk.\n\n### Enter your code here\n\nEnter your analysis here.\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)"
  },
  {
    "objectID": "quant-fin-datacases/quarto-mock-template.html#tech-setup",
    "href": "quant-fin-datacases/quarto-mock-template.html#tech-setup",
    "title": "Data Case Prep - Exercises",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)"
  },
  {
    "objectID": "quant-fin-datacases/quarto-mock-template.html#exercise-1",
    "href": "quant-fin-datacases/quarto-mock-template.html#exercise-1",
    "title": "Data Case Prep - Exercises",
    "section": "",
    "text": "This exercise aims to gauge your skill in fetching data in R through different sources. You are asked to work on the following questions:\n\nGo to our eClass® and download the Ibovespa_Sample.txt file inside the Data Cases folder. This file contains stock price information regarding 10 publicly traded companies that are part of the Brazilian market index, Ibovespa.\nRead the file into your session using the read_delim() function, using the argument sep=';', and assign to an object called FILE_SAMPLE. Make sure you have the tidyverse package loaded.\nInspect the first 10 lines of the object you have just created using the head() function, setting n=10 as an argument.\nHow many rows and columns does your object have?\nAlternatively, fetch the exact same stock level information directly from Yahoo! Finance using the tq_get() function. Collect the data beginning in 2020 (January \\(1^{st}\\) until the end of 2024 (December \\(31^{st}\\)) using the from and to arguments of the tq_get() function. Make sure you have the tidyquant package loaded. Assign this data to a new object in your R session called YAHOO_SAMPLE.\nHow many rows and columns does this new object have?"
  },
  {
    "objectID": "quant-fin-datacases/quarto-mock-template.html#exercise-2",
    "href": "quant-fin-datacases/quarto-mock-template.html#exercise-2",
    "title": "Data Case Prep - Exercises",
    "section": "",
    "text": "This exercise aims to gauge your skill on manipulating data using the tidyverse and the tidyquant. After loading these packages, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE you created before, create a new column, Year, that is defined as the year of a given timestamp found in the date column. You can use year(date) along with a mutate function to create this new variable. Update your YAHOO_SAMPLE object to make sure you have saved this variable.\nUsing the updated YAHOO_SAMPLE object, use the filter function to filter for observations that occur between \\(2023\\) and \\(2024\\), use group_by() to group the data by symbol and Year, and use the summarize the function to create a new variable, avg_price, defined as the average adjusted prices for each year-symbol combination. Sort your dataset by year and avg_price (highest-to-lowest), storing this result in an object called 2023_2024_Summary and call it in your terminal. Which company had the highest price levels, and which one had the lowest?\nUsing the YAHOO_SAMPLE dataset, group your data by symbol and create a summary of returns up-to-date. To do that, first pipe the grouped data into the tq_mutate function to create a new variable, daily_return, using the dailyReturn function. After that, pipe the result into the tq_transmute function applying the Return.cumulative to the daily_return column, assigning it to a new variable cum_return. Arrange your result by Year and cum_return. Which stock had the best performance, and which stock had the worst?"
  },
  {
    "objectID": "quant-fin-datacases/quarto-mock-template.html#exercise-3",
    "href": "quant-fin-datacases/quarto-mock-template.html#exercise-3",
    "title": "Data Case Prep - Exercises",
    "section": "",
    "text": "This exercise aims to gauge your skill on data visualization using the ggplot2 package. After loading this package, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE object, create a ggplot object with the follow aes: the x component should refer to the date column, the y axis should refer to the adjusted column, and the group argument should refer to the symbol column.\nAdd a geometry layer to your chart using the geom_line() function and call it in your terminal.\nChange the titles of your x and y axis, as well as chart title and subtitle using the labs() function.\nFacet your data by each symbol using the facet_wrap(symbol~.) function. Set the arguments of this function in such a way that you have a 2x3 orientation (i.e, n_row=3 and n_col=2)."
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep.html",
    "href": "quant-fin-datacases/data-case-prep.html",
    "title": "Data Case Prep",
    "section": "",
    "text": "This exercise list is a preparation for students of the Practical Applications in Quantitative Finance course held at FGV-EAESP. This document serves as a refresher on key R programming concepts covered in previous lectures, reinforcing the essential skills needed for quantitative finance applications. By revisiting these foundations, you’ll ensure a solid grasp of the tools and techniques required to analyze financial data effectively. As we progress, you’ll apply these concepts to real-world finance examples, including portfolio analysis, risk assessment, and asset pricing models. Mastering these skills now will prepare you to fully engage with the practical applications we’ll explore throughout the course.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®.\n\n\n\n\n\n\nDeliverable\n\n\n\nEach student is expected to deliver his/her assignment individually, although you can freely work in groups for solving the questions. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both your code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details.\n\n\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)\n\n\n\n\n\n\n\nNote\n\n\n\nFor the remainder of the sessions, you will use financial data from multiple sources, either that being from a local file or a pull from Yahoo! Finance. Whenever you are working with local files, it is always important to make sure that your R is able to locate it. To check the working directory of your session, simply type getwd(), and it will prompt your current directory. If you want to change your directory, simply type setwd('C:/path/to/your/folder') with the specific path to your desired folder. To make sure that you switched directories, you can type getwd() to confirm the new directory.\nMost of the issues regarding not being able to load a specific file, like .csv and .xlsx spreadsheets can be easily solved by placing your R file (either a plain script, like myscript.R, or a quarto document, myquartodoc.qmd) in the same folder as of your data. When you open your R script or Quarto document, it will automatically set that folder (which coincides with the data folder) as the working directory. To confirm which files are available to you, you can simply type list.files() to get the list of all files that R can find in the working directory.\nIf you prefer, whenever you are calling a function that requires a path to your computer, you can always provide the full path of the file: for example, using \"C:/Users/Lucas/Documents/GitHub/Folder/test.csv' would find the test.csv even if Folder is not your working directory.\nWhenever you are unsure about how a specific function works, type the function in an R script and you will notice that RStudio will auto complete the function name for you. To get more information on a given function’s arguments, hit F1 to see a description at the bottom-right of your session.\n\n\n\n\n\nThis exercise aims to gauge your skill in fetching data in R through different sources. You are asked to work on the following questions:\n\nGo to our eClass® and download the Ibovespa_Sample.txt file inside the Data Cases folder. This file contains stock price information regarding 6 publicly traded companies that are part of the Brazilian market index, Ibovespa.\nRead the file into your session using the read_delim() function, and assign to an object called FILE_SAMPLE. Make sure you have the tidyverse package loaded. Depending on how your computer reads delimited data, you may need to set the argument sep to sep=';' or sep=' '.\nInspect the first 10 lines of the object you have just created using the head() function, setting n=10 as an argument.\nHow many rows and columns does your object have? You can use the function dim(yourdata) to find the exact dimensions.\nAlternatively, fetch the exact same stock level information directly from Yahoo! Finance using the tq_get() function. Collect the data beginning in 2020 (January \\(1^{st}\\) until the end of 2024 (December \\(31^{st}\\)) using the from and to arguments of the tq_get() function. Make sure you have the tidyquant package loaded. Assign this data to a new object in your R session called YAHOO_SAMPLE.\n\n\n\n\n\n\n\nHint\n\n\n\nRemember that the syntax for using tq_get() to collect stock prices is:\n\nmy_stocks &lt;- tq_get(c(\"stock1\", \"stock2\"),\n                      from = \"YYYY-MM-DD\",\n                      to   = \"YYYY-MM-DD\")\n\nFurthermore, you can use the unique(FILE_SAMPLE$symbol) to find the exact tickers contained in the file, and assign it to a new object, assets, which will then be piped onto a call to tq_get().\n\n\n\nHow many rows and columns does this new object have?\n\n\n\n\nThis exercise aims to gauge your skill on manipulating data using the tidyverse and the tidyquant. After loading these packages, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE you created before, create a new column, Year, that is defined as the year of a given timestamp found in the date column. You can use year(date) along with a mutate function to create this new variable. Update your YAHOO_SAMPLE object to make sure you have saved this variable.\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the mutate function is:\n\nmutate(.data, #The object you are performing the calculations \n       new_variable_1 = var1 * 2, #Can use basic operations...\n       new_variable_2 = median(var2), #Or predefined functions)\n       variable_3 = as.character(var3) #And can be used to modify existing variables)\n       ) \n\nFurthermore, recall that you can always use the pipe operator (%&gt;%) to chain operations along the way whenever you are using the tidyverse.\n\n\n\nUsing the updated YAHOO_SAMPLE object, use the filter function to filter for observations that occur in \\(2024\\), use group_by() to group the data by symbol, and use the summarize the function to create a new variable, avg_price, defined as the average adjusted prices for each year-symbol combination. Sort your dataset by avg_price (highest-to-lowest), storing this result in an object called 2023_2024_Summary and call it in your terminal. Which company had the highest price levels, and which one had the lowest?\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the filter function is:\n\nfilter(.data, #The object which you are performing the operations\n       variable_1 &gt;10, #Simple arithmetic operators\n       variable_2 %in% c('AAPL','MSFT','FORD'), #Pattern search\n       !(variable_3 %in% c('Boston','Mass','Silicon Valley')), #Negate pattern search\n       variable_4 &gt;=10 & variable_3&lt;= 4 | is.na(variable_4) #IF and OR conditions\n       ) \n\n\n\n\nUsing the YAHOO_SAMPLE dataset, group your data by symbol and create a summary of yearly returns. To do that, first pipe the grouped data into into the tq_transmute function applying the yearlyReturn, assigning it to a new variable,yearly_return. Arrange your result by date and yearly_return (highest-to-lowest). Which stock had the best performance in 2020, and which stock had the worst?\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the tq_transmute function is:\n\ntq_transmute(.data, #The object you are performing the calculations \n       selected_variables, #The columns to send to the mutation function\n       mutate_fun, #The mutation function from either the xts, quantmod, or TTR package.\n       col_rename #A string or character vector containing names that can be used to quickly rename columns\n       )\n\n\n\n\n\n\nThis exercise aims to gauge your skill on data visualization using the ggplot2 package. After loading this package, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE object, create a ggplot object with the follow aes: the x component should refer to the date column, the y axis should refer to the adjusted column, and the group argument should refer to the symbol column.\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for mapping your data to ggplot and adding further layers for adjusting aesthetics is:\n\nggplot(data=your_data, aes(x= variable_1, y=variable_2, group=your_group,...))+\ngeom_{yourgeom} +\nadditional_layer_1()+\nadditional_layer_2()+\nany_additional_layer_commands()\n\n\n\n\nAdd a geometry layer to your chart using the geom_line() function and call it in your terminal.\nChange the titles of your x and y axis, as well as chart title and subtitle using the labs() function.\nFacet your data by each symbol using the facet_wrap(symbol~.) function. Set the arguments of this function in such a way that you have a 2x3 orientation (i.e, nrow=2 and ncol=2).\nApply a predefined theme using the theme_minimal() function.\nSave your plot using the ggsave() function. To use it, call ggsave('myplot.jpg',width=10,height = 6) to save the last plot that has been prompted to your terminal using a 10x6 resolution (in units)."
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep.html#tech-setup",
    "href": "quant-fin-datacases/data-case-prep.html#tech-setup",
    "title": "Data Case Prep",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)\n\n\n\n\n\n\n\nNote\n\n\n\nFor the remainder of the sessions, you will use financial data from multiple sources, either that being from a local file or a pull from Yahoo! Finance. Whenever you are working with local files, it is always important to make sure that your R is able to locate it. To check the working directory of your session, simply type getwd(), and it will prompt your current directory. If you want to change your directory, simply type setwd('C:/path/to/your/folder') with the specific path to your desired folder. To make sure that you switched directories, you can type getwd() to confirm the new directory.\nMost of the issues regarding not being able to load a specific file, like .csv and .xlsx spreadsheets can be easily solved by placing your R file (either a plain script, like myscript.R, or a quarto document, myquartodoc.qmd) in the same folder as of your data. When you open your R script or Quarto document, it will automatically set that folder (which coincides with the data folder) as the working directory. To confirm which files are available to you, you can simply type list.files() to get the list of all files that R can find in the working directory.\nIf you prefer, whenever you are calling a function that requires a path to your computer, you can always provide the full path of the file: for example, using \"C:/Users/Lucas/Documents/GitHub/Folder/test.csv' would find the test.csv even if Folder is not your working directory.\nWhenever you are unsure about how a specific function works, type the function in an R script and you will notice that RStudio will auto complete the function name for you. To get more information on a given function’s arguments, hit F1 to see a description at the bottom-right of your session."
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep.html#exercise-1",
    "href": "quant-fin-datacases/data-case-prep.html#exercise-1",
    "title": "Data Case Prep",
    "section": "",
    "text": "This exercise aims to gauge your skill in fetching data in R through different sources. You are asked to work on the following questions:\n\nGo to our eClass® and download the Ibovespa_Sample.txt file inside the Data Cases folder. This file contains stock price information regarding 6 publicly traded companies that are part of the Brazilian market index, Ibovespa.\nRead the file into your session using the read_delim() function, and assign to an object called FILE_SAMPLE. Make sure you have the tidyverse package loaded. Depending on how your computer reads delimited data, you may need to set the argument sep to sep=';' or sep=' '.\nInspect the first 10 lines of the object you have just created using the head() function, setting n=10 as an argument.\nHow many rows and columns does your object have? You can use the function dim(yourdata) to find the exact dimensions.\nAlternatively, fetch the exact same stock level information directly from Yahoo! Finance using the tq_get() function. Collect the data beginning in 2020 (January \\(1^{st}\\) until the end of 2024 (December \\(31^{st}\\)) using the from and to arguments of the tq_get() function. Make sure you have the tidyquant package loaded. Assign this data to a new object in your R session called YAHOO_SAMPLE.\n\n\n\n\n\n\n\nHint\n\n\n\nRemember that the syntax for using tq_get() to collect stock prices is:\n\nmy_stocks &lt;- tq_get(c(\"stock1\", \"stock2\"),\n                      from = \"YYYY-MM-DD\",\n                      to   = \"YYYY-MM-DD\")\n\nFurthermore, you can use the unique(FILE_SAMPLE$symbol) to find the exact tickers contained in the file, and assign it to a new object, assets, which will then be piped onto a call to tq_get().\n\n\n\nHow many rows and columns does this new object have?"
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep.html#exercise-2",
    "href": "quant-fin-datacases/data-case-prep.html#exercise-2",
    "title": "Data Case Prep",
    "section": "",
    "text": "This exercise aims to gauge your skill on manipulating data using the tidyverse and the tidyquant. After loading these packages, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE you created before, create a new column, Year, that is defined as the year of a given timestamp found in the date column. You can use year(date) along with a mutate function to create this new variable. Update your YAHOO_SAMPLE object to make sure you have saved this variable.\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the mutate function is:\n\nmutate(.data, #The object you are performing the calculations \n       new_variable_1 = var1 * 2, #Can use basic operations...\n       new_variable_2 = median(var2), #Or predefined functions)\n       variable_3 = as.character(var3) #And can be used to modify existing variables)\n       ) \n\nFurthermore, recall that you can always use the pipe operator (%&gt;%) to chain operations along the way whenever you are using the tidyverse.\n\n\n\nUsing the updated YAHOO_SAMPLE object, use the filter function to filter for observations that occur in \\(2024\\), use group_by() to group the data by symbol, and use the summarize the function to create a new variable, avg_price, defined as the average adjusted prices for each year-symbol combination. Sort your dataset by avg_price (highest-to-lowest), storing this result in an object called 2023_2024_Summary and call it in your terminal. Which company had the highest price levels, and which one had the lowest?\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the filter function is:\n\nfilter(.data, #The object which you are performing the operations\n       variable_1 &gt;10, #Simple arithmetic operators\n       variable_2 %in% c('AAPL','MSFT','FORD'), #Pattern search\n       !(variable_3 %in% c('Boston','Mass','Silicon Valley')), #Negate pattern search\n       variable_4 &gt;=10 & variable_3&lt;= 4 | is.na(variable_4) #IF and OR conditions\n       ) \n\n\n\n\nUsing the YAHOO_SAMPLE dataset, group your data by symbol and create a summary of yearly returns. To do that, first pipe the grouped data into into the tq_transmute function applying the yearlyReturn, assigning it to a new variable,yearly_return. Arrange your result by date and yearly_return (highest-to-lowest). Which stock had the best performance in 2020, and which stock had the worst?\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the tq_transmute function is:\n\ntq_transmute(.data, #The object you are performing the calculations \n       selected_variables, #The columns to send to the mutation function\n       mutate_fun, #The mutation function from either the xts, quantmod, or TTR package.\n       col_rename #A string or character vector containing names that can be used to quickly rename columns\n       )"
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep.html#exercise-3",
    "href": "quant-fin-datacases/data-case-prep.html#exercise-3",
    "title": "Data Case Prep",
    "section": "",
    "text": "This exercise aims to gauge your skill on data visualization using the ggplot2 package. After loading this package, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE object, create a ggplot object with the follow aes: the x component should refer to the date column, the y axis should refer to the adjusted column, and the group argument should refer to the symbol column.\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for mapping your data to ggplot and adding further layers for adjusting aesthetics is:\n\nggplot(data=your_data, aes(x= variable_1, y=variable_2, group=your_group,...))+\ngeom_{yourgeom} +\nadditional_layer_1()+\nadditional_layer_2()+\nany_additional_layer_commands()\n\n\n\n\nAdd a geometry layer to your chart using the geom_line() function and call it in your terminal.\nChange the titles of your x and y axis, as well as chart title and subtitle using the labs() function.\nFacet your data by each symbol using the facet_wrap(symbol~.) function. Set the arguments of this function in such a way that you have a 2x3 orientation (i.e, nrow=2 and ncol=2).\nApply a predefined theme using the theme_minimal() function.\nSave your plot using the ggsave() function. To use it, call ggsave('myplot.jpg',width=10,height = 6) to save the last plot that has been prompted to your terminal using a 10x6 resolution (in units)."
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep - Solutions.html",
    "href": "quant-fin-datacases/data-case-prep - Solutions.html",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise list is a preparation for students of the Practical Applications in Quantitative Finance course held at FGV-EAESP. This document serves as a refresher on key R programming concepts covered in previous lectures, reinforcing the essential skills needed for quantitative finance applications. By revisiting these foundations, you’ll ensure a solid grasp of the tools and techniques required to analyze financial data effectively. As we progress, you’ll apply these concepts to real-world finance examples, including portfolio analysis, risk assessment, and asset pricing models. Mastering these skills now will prepare you to fully engage with the practical applications we’ll explore throughout the course.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®.\n\n\n\n\n\n\nDeliverable\n\n\n\nEach student is expected to deliver his/her assignment individually, although you can freely work in groups for solving the questions. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both your code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details.\n\n\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\n\n\n\nThis exercise aims to gauge your skill in fetching data in R through different sources. You are asked to work on the following questions:\n\nGo to our eClass® and download the Ibovespa_Sample.txt file inside the Data Cases folder. This file contains stock price information regarding 10 publicly traded companies that are part of the Brazilian market index, Ibovespa.\nRead the file into your session using the read_delim() function, and assign to an object called FILE_SAMPLE. Make sure you have the tidyverse package loaded. Depending on how your computer reads delimited data, you may need to set the argument sep to sep=';' or sep=' '.\n\n\nFILE_SAMPLE=read_delim('Ibovespa_Sample.txt',delim=' ')\n\n\nInspect the first 10 lines of the object you have just created using the head() function, setting n=10 as an argument.\n\n\nFILE_SAMPLE%&gt;%head(n=10)\n\n# A tibble: 10 × 8\n   symbol   date        open  high   low close   volume adjusted\n   &lt;chr&gt;    &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 PETR3.SA 2020-01-02  32.3  32.8  32.1  32.8  6660800     10.6\n 2 PETR3.SA 2020-01-03  33    33.2  32.0  32.0 20340400     10.4\n 3 PETR3.SA 2020-01-06  32    33.1  31.8  33.0 17549700     10.7\n 4 PETR3.SA 2020-01-07  33.0  33.0  32.4  32.6  5480400     10.6\n 5 PETR3.SA 2020-01-08  32.7  32.8  31.8  32.0 10030200     10.4\n 6 PETR3.SA 2020-01-09  32.1  32.4  31.8  32.2 15411600     10.4\n 7 PETR3.SA 2020-01-10  32.3  32.3  32.0  32.1  3867200     10.4\n 8 PETR3.SA 2020-01-13  32.2  32.3  31.9  32.1  6666500     10.4\n 9 PETR3.SA 2020-01-14  32.0  32.1  31.5  31.8  6752000     10.3\n10 PETR3.SA 2020-01-15  31.6  31.7  31.1  31.1  7113300     10.1\n\n\n\nHow many rows and columns does your object have? You can use the function dim(yourdata) to find the exact dimensions.\n\nThis object has 7464 rows and 8 columns.\n\nAlternatively, fetch the exact same stock level information directly from Yahoo! Finance using the tq_get() function. Collect the data beginning in 2020 (January \\(1^{st}\\) until the end of 2024 (December \\(31^{st}\\)) using the from and to arguments of the tq_get() function. Make sure you have the tidyquant package loaded. Assign this data to a new object in your R session called YAHOO_SAMPLE.\n\n\nassets = unique(FILE_SAMPLE$symbol)\n\nYAHOO_SAMPLE=assets%&gt;%tq_get(from='2020-01-01',to='2024-12-31')\n\n\nHow many rows and columns does this new object have?\n\nThis object has 7464 rows and 8 columns.\n\n\n\nThis exercise aims to gauge your skill on manipulating data using the tidyverse and the tidyquant. After loading these packages, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE you created before, create a new column, Year, that is defined as the year of a given timestamp found in the date column. You can use year(date) along with a mutate function to create this new variable. Update your YAHOO_SAMPLE object to make sure you have saved this variable.\n\n\nYAHOO_SAMPLE=YAHOO_SAMPLE%&gt;%\n  mutate(Year=year(date))\n\n\nUsing the updated YAHOO_SAMPLE object, use the filter function to filter for observations that occur in \\(2024\\), use group_by() to group the data by symbol, and use the summarize the function to create a new variable, avg_price, defined as the average adjusted prices for each year-symbol combination. Sort your dataset by avg_price (highest-to-lowest), storing this result in an object called 2023_2024_Summary and call it in your terminal. Which company had the highest price levels, and which one had the lowest?\n\n\nSummary_2023_2024=YAHOO_SAMPLE%&gt;%\n  filter(Year==2024)%&gt;%\n  group_by(symbol)%&gt;%\n  summarize(avg_price=mean(adjusted,na.rm=TRUE))%&gt;%\n  arrange(avg_price)\n\nSummary_2023_2024\n\n# A tibble: 6 × 2\n  symbol   avg_price\n  &lt;chr&gt;        &lt;dbl&gt;\n1 BEEF3.SA      6.48\n2 BBDC3.SA     11.8 \n3 BRFS3.SA     19.9 \n4 ITUB3.SA     28.3 \n5 PETR3.SA     36.1 \n6 WEGE3.SA     44.8 \n\n\n\nUsing the YAHOO_SAMPLE dataset, group your data by symbol and create a summary of yearly returns. To do that, first pipe the grouped data into into the tq_transmute function applying the yearlyReturn, assigning it to a new variable,yearly_return. Arrange your result by date and yearly_return (highest-to-lowest). Which stock had the best performance in 2020, and which stock had the worst?\n\n\nYAHOO_SAMPLE%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select='adjusted',\n            mutate_fun = yearlyReturn,\n            col_rename = 'yearly_return')%&gt;%\n  arrange(date,desc(yearly_return))\n\n# A tibble: 30 × 3\n# Groups:   symbol [6]\n   symbol   date       yearly_return\n   &lt;chr&gt;    &lt;date&gt;             &lt;dbl&gt;\n 1 WEGE3.SA 2020-12-30        1.17  \n 2 PETR3.SA 2020-12-30       -0.111 \n 3 ITUB3.SA 2020-12-30       -0.119 \n 4 BEEF3.SA 2020-12-30       -0.155 \n 5 BBDC3.SA 2020-12-30       -0.228 \n 6 BRFS3.SA 2020-12-30       -0.386 \n 7 PETR3.SA 2021-12-30        0.304 \n 8 BEEF3.SA 2021-12-30        0.161 \n 9 BRFS3.SA 2021-12-30        0.0218\n10 WEGE3.SA 2021-12-30       -0.119 \n# ℹ 20 more rows\n\n\nWEGE3.SA has the highest return in 2020, whereas BRFS3.SA had the worst.\n\n\n\nThis exercise aims to gauge your skill on data visualization using the ggplot2 package. After loading this package, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE object, create a ggplot object with the follow aes: the x component should refer to the date column, the y axis should refer to the adjusted column, and the group argument should refer to the symbol column.\n\n\nYAHOO_SAMPLE%&gt;%ggplot(aes(x=date,y=adjusted,group=symbol))\n\n\n\n\n\n\n\n\n\nAdd a geometry layer to your chart using the geom_line() function and call it in your terminal.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()\n\n\n\n\n\n\n\n\n\nChange the titles of your x and y axis, as well as chart title and subtitle using the labs() function.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')\n\n\n\n\n\n\n\n\n\nFacet your data by each symbol using the facet_wrap(symbol~.) function. Set the arguments of this function in such a way that you have a 2x3 orientation (i.e, nrow=2 and ncol=2).\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')+\n  facet_wrap(symbol~.,nrow=3,ncol=2)"
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep - Solutions.html#tech-setup",
    "href": "quant-fin-datacases/data-case-prep - Solutions.html#tech-setup",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))"
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep - Solutions.html#exercise-1",
    "href": "quant-fin-datacases/data-case-prep - Solutions.html#exercise-1",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise aims to gauge your skill in fetching data in R through different sources. You are asked to work on the following questions:\n\nGo to our eClass® and download the Ibovespa_Sample.txt file inside the Data Cases folder. This file contains stock price information regarding 10 publicly traded companies that are part of the Brazilian market index, Ibovespa.\nRead the file into your session using the read_delim() function, and assign to an object called FILE_SAMPLE. Make sure you have the tidyverse package loaded. Depending on how your computer reads delimited data, you may need to set the argument sep to sep=';' or sep=' '.\n\n\nFILE_SAMPLE=read_delim('Ibovespa_Sample.txt',delim=' ')\n\n\nInspect the first 10 lines of the object you have just created using the head() function, setting n=10 as an argument.\n\n\nFILE_SAMPLE%&gt;%head(n=10)\n\n# A tibble: 10 × 8\n   symbol   date        open  high   low close   volume adjusted\n   &lt;chr&gt;    &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 PETR3.SA 2020-01-02  32.3  32.8  32.1  32.8  6660800     10.6\n 2 PETR3.SA 2020-01-03  33    33.2  32.0  32.0 20340400     10.4\n 3 PETR3.SA 2020-01-06  32    33.1  31.8  33.0 17549700     10.7\n 4 PETR3.SA 2020-01-07  33.0  33.0  32.4  32.6  5480400     10.6\n 5 PETR3.SA 2020-01-08  32.7  32.8  31.8  32.0 10030200     10.4\n 6 PETR3.SA 2020-01-09  32.1  32.4  31.8  32.2 15411600     10.4\n 7 PETR3.SA 2020-01-10  32.3  32.3  32.0  32.1  3867200     10.4\n 8 PETR3.SA 2020-01-13  32.2  32.3  31.9  32.1  6666500     10.4\n 9 PETR3.SA 2020-01-14  32.0  32.1  31.5  31.8  6752000     10.3\n10 PETR3.SA 2020-01-15  31.6  31.7  31.1  31.1  7113300     10.1\n\n\n\nHow many rows and columns does your object have? You can use the function dim(yourdata) to find the exact dimensions.\n\nThis object has 7464 rows and 8 columns.\n\nAlternatively, fetch the exact same stock level information directly from Yahoo! Finance using the tq_get() function. Collect the data beginning in 2020 (January \\(1^{st}\\) until the end of 2024 (December \\(31^{st}\\)) using the from and to arguments of the tq_get() function. Make sure you have the tidyquant package loaded. Assign this data to a new object in your R session called YAHOO_SAMPLE.\n\n\nassets = unique(FILE_SAMPLE$symbol)\n\nYAHOO_SAMPLE=assets%&gt;%tq_get(from='2020-01-01',to='2024-12-31')\n\n\nHow many rows and columns does this new object have?\n\nThis object has 7464 rows and 8 columns."
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep - Solutions.html#exercise-2",
    "href": "quant-fin-datacases/data-case-prep - Solutions.html#exercise-2",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise aims to gauge your skill on manipulating data using the tidyverse and the tidyquant. After loading these packages, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE you created before, create a new column, Year, that is defined as the year of a given timestamp found in the date column. You can use year(date) along with a mutate function to create this new variable. Update your YAHOO_SAMPLE object to make sure you have saved this variable.\n\n\nYAHOO_SAMPLE=YAHOO_SAMPLE%&gt;%\n  mutate(Year=year(date))\n\n\nUsing the updated YAHOO_SAMPLE object, use the filter function to filter for observations that occur in \\(2024\\), use group_by() to group the data by symbol, and use the summarize the function to create a new variable, avg_price, defined as the average adjusted prices for each year-symbol combination. Sort your dataset by avg_price (highest-to-lowest), storing this result in an object called 2023_2024_Summary and call it in your terminal. Which company had the highest price levels, and which one had the lowest?\n\n\nSummary_2023_2024=YAHOO_SAMPLE%&gt;%\n  filter(Year==2024)%&gt;%\n  group_by(symbol)%&gt;%\n  summarize(avg_price=mean(adjusted,na.rm=TRUE))%&gt;%\n  arrange(avg_price)\n\nSummary_2023_2024\n\n# A tibble: 6 × 2\n  symbol   avg_price\n  &lt;chr&gt;        &lt;dbl&gt;\n1 BEEF3.SA      6.48\n2 BBDC3.SA     11.8 \n3 BRFS3.SA     19.9 \n4 ITUB3.SA     28.3 \n5 PETR3.SA     36.1 \n6 WEGE3.SA     44.8 \n\n\n\nUsing the YAHOO_SAMPLE dataset, group your data by symbol and create a summary of yearly returns. To do that, first pipe the grouped data into into the tq_transmute function applying the yearlyReturn, assigning it to a new variable,yearly_return. Arrange your result by date and yearly_return (highest-to-lowest). Which stock had the best performance in 2020, and which stock had the worst?\n\n\nYAHOO_SAMPLE%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select='adjusted',\n            mutate_fun = yearlyReturn,\n            col_rename = 'yearly_return')%&gt;%\n  arrange(date,desc(yearly_return))\n\n# A tibble: 30 × 3\n# Groups:   symbol [6]\n   symbol   date       yearly_return\n   &lt;chr&gt;    &lt;date&gt;             &lt;dbl&gt;\n 1 WEGE3.SA 2020-12-30        1.17  \n 2 PETR3.SA 2020-12-30       -0.111 \n 3 ITUB3.SA 2020-12-30       -0.119 \n 4 BEEF3.SA 2020-12-30       -0.155 \n 5 BBDC3.SA 2020-12-30       -0.228 \n 6 BRFS3.SA 2020-12-30       -0.386 \n 7 PETR3.SA 2021-12-30        0.304 \n 8 BEEF3.SA 2021-12-30        0.161 \n 9 BRFS3.SA 2021-12-30        0.0218\n10 WEGE3.SA 2021-12-30       -0.119 \n# ℹ 20 more rows\n\n\nWEGE3.SA has the highest return in 2020, whereas BRFS3.SA had the worst."
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep - Solutions.html#exercise-3",
    "href": "quant-fin-datacases/data-case-prep - Solutions.html#exercise-3",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise aims to gauge your skill on data visualization using the ggplot2 package. After loading this package, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE object, create a ggplot object with the follow aes: the x component should refer to the date column, the y axis should refer to the adjusted column, and the group argument should refer to the symbol column.\n\n\nYAHOO_SAMPLE%&gt;%ggplot(aes(x=date,y=adjusted,group=symbol))\n\n\n\n\n\n\n\n\n\nAdd a geometry layer to your chart using the geom_line() function and call it in your terminal.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()\n\n\n\n\n\n\n\n\n\nChange the titles of your x and y axis, as well as chart title and subtitle using the labs() function.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')\n\n\n\n\n\n\n\n\n\nFacet your data by each symbol using the facet_wrap(symbol~.) function. Set the arguments of this function in such a way that you have a 2x3 orientation (i.e, nrow=2 and ncol=2).\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')+\n  facet_wrap(symbol~.,nrow=3,ncol=2)"
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep-solutions.html",
    "href": "quant-fin-datacases/data-case-prep-solutions.html",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise list is a preparation for students of the Practical Applications in Quantitative Finance course held at FGV-EAESP. This document serves as a refresher on key R programming concepts covered in previous lectures, reinforcing the essential skills needed for quantitative finance applications. By revisiting these foundations, you’ll ensure a solid grasp of the tools and techniques required to analyze financial data effectively. As we progress, you’ll apply these concepts to real-world finance examples, including portfolio analysis, risk assessment, and asset pricing models. Mastering these skills now will prepare you to fully engage with the practical applications we’ll explore throughout the course.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®.\n\n\n\n\n\n\nDeliverable\n\n\n\nEach student is expected to deliver his/her assignment individually, although you can freely work in groups for solving the questions. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both your code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details.\n\n\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\n\n\n\nThis exercise aims to gauge your skill in fetching data in R through different sources. You are asked to work on the following questions:\n\nGo to our eClass® and download the Ibovespa_Sample.txt file inside the Data Cases folder. This file contains stock price information regarding 10 publicly traded companies that are part of the Brazilian market index, Ibovespa.\nRead the file into your session using the read_delim() function, and assign to an object called FILE_SAMPLE. Make sure you have the tidyverse package loaded. Depending on how your computer reads delimited data, you may need to set the argument sep to sep=';' or sep=' '.\n\n\nFILE_SAMPLE=read_delim('Ibovespa_Sample.txt',delim=' ')\n\n\nInspect the first 10 lines of the object you have just created using the head() function, setting n=10 as an argument.\n\n\nFILE_SAMPLE%&gt;%head(n=10)\n\n# A tibble: 10 × 8\n   symbol   date        open  high   low close   volume adjusted\n   &lt;chr&gt;    &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 PETR3.SA 2020-01-02  32.3  32.8  32.1  32.8  6660800     10.6\n 2 PETR3.SA 2020-01-03  33    33.2  32.0  32.0 20340400     10.4\n 3 PETR3.SA 2020-01-06  32    33.1  31.8  33.0 17549700     10.7\n 4 PETR3.SA 2020-01-07  33.0  33.0  32.4  32.6  5480400     10.6\n 5 PETR3.SA 2020-01-08  32.7  32.8  31.8  32.0 10030200     10.4\n 6 PETR3.SA 2020-01-09  32.1  32.4  31.8  32.2 15411600     10.4\n 7 PETR3.SA 2020-01-10  32.3  32.3  32.0  32.1  3867200     10.4\n 8 PETR3.SA 2020-01-13  32.2  32.3  31.9  32.1  6666500     10.4\n 9 PETR3.SA 2020-01-14  32.0  32.1  31.5  31.8  6752000     10.3\n10 PETR3.SA 2020-01-15  31.6  31.7  31.1  31.1  7113300     10.1\n\n\n\nHow many rows and columns does your object have? You can use the function dim(yourdata) to find the exact dimensions.\n\nThis object has 7464 rows and 8 columns.\n\nAlternatively, fetch the exact same stock level information directly from Yahoo! Finance using the tq_get() function. Collect the data beginning in 2020 (January \\(1^{st}\\) until the end of 2024 (December \\(31^{st}\\)) using the from and to arguments of the tq_get() function. Make sure you have the tidyquant package loaded. Assign this data to a new object in your R session called YAHOO_SAMPLE.\n\n\nassets = unique(FILE_SAMPLE$symbol)\n\nYAHOO_SAMPLE=assets%&gt;%tq_get(from='2020-01-01',to='2024-12-31')\n\n\nHow many rows and columns does this new object have?\n\nThis object has 7464 rows and 8 columns.\n\n\n\nThis exercise aims to gauge your skill on manipulating data using the tidyverse and the tidyquant. After loading these packages, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE you created before, create a new column, Year, that is defined as the year of a given timestamp found in the date column. You can use year(date) along with a mutate function to create this new variable. Update your YAHOO_SAMPLE object to make sure you have saved this variable.\n\n\nYAHOO_SAMPLE=YAHOO_SAMPLE%&gt;%\n  mutate(Year=year(date))\n\n\nUsing the updated YAHOO_SAMPLE object, use the filter function to filter for observations that occur in \\(2024\\), use group_by() to group the data by symbol, and use the summarize the function to create a new variable, avg_price, defined as the average adjusted prices for each year-symbol combination. Sort your dataset by avg_price (highest-to-lowest), storing this result in an object called 2023_2024_Summary and call it in your terminal. Which company had the highest price levels, and which one had the lowest?\n\n\nSummary_2023_2024=YAHOO_SAMPLE%&gt;%\n  filter(Year==2024)%&gt;%\n  group_by(symbol)%&gt;%\n  summarize(avg_price=mean(adjusted,na.rm=TRUE))%&gt;%\n  arrange(avg_price)\n\nSummary_2023_2024\n\n# A tibble: 6 × 2\n  symbol   avg_price\n  &lt;chr&gt;        &lt;dbl&gt;\n1 BEEF3.SA      6.48\n2 BBDC3.SA     11.8 \n3 BRFS3.SA     19.9 \n4 ITUB3.SA     28.0 \n5 PETR3.SA     36.1 \n6 WEGE3.SA     44.8 \n\n\n\nUsing the YAHOO_SAMPLE dataset, group your data by symbol and create a summary of yearly returns. To do that, first pipe the grouped data into into the tq_transmute function applying the yearlyReturn, assigning it to a new variable,yearly_return. Arrange your result by date and yearly_return (highest-to-lowest). Which stock had the best performance in 2020, and which stock had the worst?\n\n\nYAHOO_SAMPLE%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select='adjusted',\n            mutate_fun = yearlyReturn,\n            col_rename = 'yearly_return')%&gt;%\n  arrange(date,desc(yearly_return))\n\n# A tibble: 30 × 3\n# Groups:   symbol [6]\n   symbol   date       yearly_return\n   &lt;chr&gt;    &lt;date&gt;             &lt;dbl&gt;\n 1 WEGE3.SA 2020-12-30        1.17  \n 2 PETR3.SA 2020-12-30       -0.111 \n 3 ITUB3.SA 2020-12-30       -0.119 \n 4 BEEF3.SA 2020-12-30       -0.155 \n 5 BBDC3.SA 2020-12-30       -0.228 \n 6 BRFS3.SA 2020-12-30       -0.386 \n 7 PETR3.SA 2021-12-30        0.304 \n 8 BEEF3.SA 2021-12-30        0.161 \n 9 BRFS3.SA 2021-12-30        0.0218\n10 WEGE3.SA 2021-12-30       -0.119 \n# ℹ 20 more rows\n\n\nWEGE3.SA has the highest return in 2020, whereas BRFS3.SA had the worst.\n\n\n\nThis exercise aims to gauge your skill on data visualization using the ggplot2 package. After loading this package, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE object, create a ggplot object with the follow aes: the x component should refer to the date column, the y axis should refer to the adjusted column, and the group argument should refer to the symbol column.\n\n\nYAHOO_SAMPLE%&gt;%ggplot(aes(x=date,y=adjusted,group=symbol))\n\n\n\n\n\n\n\n\n\nAdd a geometry layer to your chart using the geom_line() function and call it in your terminal.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()\n\n\n\n\n\n\n\n\n\nChange the titles of your x and y axis, as well as chart title and subtitle using the labs() function.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')\n\n\n\n\n\n\n\n\n\nFacet your data by each symbol using the facet_wrap(symbol~.) function. Set the arguments of this function in such a way that you have a 2x3 orientation (i.e, nrow=2 and ncol=2).\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')+\n  facet_wrap(symbol~.,nrow=3,ncol=2)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nApply a predefined theme using the theme_minimal() function.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')+\n  facet_wrap(symbol~.,nrow=3,ncol=2)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSave your plot using the ggsave() function. To use it, call ggsave('myplot.jpg',width=10,height = 6) to save the last plot that has been prompted to your terminal using a 10x6 resolution (in units).\n\n\nggsave('myplot.jpg',width=10,height = 6)"
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep-solutions.html#tech-setup",
    "href": "quant-fin-datacases/data-case-prep-solutions.html#tech-setup",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))"
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep-solutions.html#exercise-1",
    "href": "quant-fin-datacases/data-case-prep-solutions.html#exercise-1",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise aims to gauge your skill in fetching data in R through different sources. You are asked to work on the following questions:\n\nGo to our eClass® and download the Ibovespa_Sample.txt file inside the Data Cases folder. This file contains stock price information regarding 10 publicly traded companies that are part of the Brazilian market index, Ibovespa.\nRead the file into your session using the read_delim() function, and assign to an object called FILE_SAMPLE. Make sure you have the tidyverse package loaded. Depending on how your computer reads delimited data, you may need to set the argument sep to sep=';' or sep=' '.\n\n\nFILE_SAMPLE=read_delim('Ibovespa_Sample.txt',delim=' ')\n\n\nInspect the first 10 lines of the object you have just created using the head() function, setting n=10 as an argument.\n\n\nFILE_SAMPLE%&gt;%head(n=10)\n\n# A tibble: 10 × 8\n   symbol   date        open  high   low close   volume adjusted\n   &lt;chr&gt;    &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 PETR3.SA 2020-01-02  32.3  32.8  32.1  32.8  6660800     10.6\n 2 PETR3.SA 2020-01-03  33    33.2  32.0  32.0 20340400     10.4\n 3 PETR3.SA 2020-01-06  32    33.1  31.8  33.0 17549700     10.7\n 4 PETR3.SA 2020-01-07  33.0  33.0  32.4  32.6  5480400     10.6\n 5 PETR3.SA 2020-01-08  32.7  32.8  31.8  32.0 10030200     10.4\n 6 PETR3.SA 2020-01-09  32.1  32.4  31.8  32.2 15411600     10.4\n 7 PETR3.SA 2020-01-10  32.3  32.3  32.0  32.1  3867200     10.4\n 8 PETR3.SA 2020-01-13  32.2  32.3  31.9  32.1  6666500     10.4\n 9 PETR3.SA 2020-01-14  32.0  32.1  31.5  31.8  6752000     10.3\n10 PETR3.SA 2020-01-15  31.6  31.7  31.1  31.1  7113300     10.1\n\n\n\nHow many rows and columns does your object have? You can use the function dim(yourdata) to find the exact dimensions.\n\nThis object has 7464 rows and 8 columns.\n\nAlternatively, fetch the exact same stock level information directly from Yahoo! Finance using the tq_get() function. Collect the data beginning in 2020 (January \\(1^{st}\\) until the end of 2024 (December \\(31^{st}\\)) using the from and to arguments of the tq_get() function. Make sure you have the tidyquant package loaded. Assign this data to a new object in your R session called YAHOO_SAMPLE.\n\n\nassets = unique(FILE_SAMPLE$symbol)\n\nYAHOO_SAMPLE=assets%&gt;%tq_get(from='2020-01-01',to='2024-12-31')\n\n\nHow many rows and columns does this new object have?\n\nThis object has 7464 rows and 8 columns."
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep-solutions.html#exercise-2",
    "href": "quant-fin-datacases/data-case-prep-solutions.html#exercise-2",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise aims to gauge your skill on manipulating data using the tidyverse and the tidyquant. After loading these packages, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE you created before, create a new column, Year, that is defined as the year of a given timestamp found in the date column. You can use year(date) along with a mutate function to create this new variable. Update your YAHOO_SAMPLE object to make sure you have saved this variable.\n\n\nYAHOO_SAMPLE=YAHOO_SAMPLE%&gt;%\n  mutate(Year=year(date))\n\n\nUsing the updated YAHOO_SAMPLE object, use the filter function to filter for observations that occur in \\(2024\\), use group_by() to group the data by symbol, and use the summarize the function to create a new variable, avg_price, defined as the average adjusted prices for each year-symbol combination. Sort your dataset by avg_price (highest-to-lowest), storing this result in an object called 2023_2024_Summary and call it in your terminal. Which company had the highest price levels, and which one had the lowest?\n\n\nSummary_2023_2024=YAHOO_SAMPLE%&gt;%\n  filter(Year==2024)%&gt;%\n  group_by(symbol)%&gt;%\n  summarize(avg_price=mean(adjusted,na.rm=TRUE))%&gt;%\n  arrange(avg_price)\n\nSummary_2023_2024\n\n# A tibble: 6 × 2\n  symbol   avg_price\n  &lt;chr&gt;        &lt;dbl&gt;\n1 BEEF3.SA      6.48\n2 BBDC3.SA     11.8 \n3 BRFS3.SA     19.9 \n4 ITUB3.SA     28.0 \n5 PETR3.SA     36.1 \n6 WEGE3.SA     44.8 \n\n\n\nUsing the YAHOO_SAMPLE dataset, group your data by symbol and create a summary of yearly returns. To do that, first pipe the grouped data into into the tq_transmute function applying the yearlyReturn, assigning it to a new variable,yearly_return. Arrange your result by date and yearly_return (highest-to-lowest). Which stock had the best performance in 2020, and which stock had the worst?\n\n\nYAHOO_SAMPLE%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select='adjusted',\n            mutate_fun = yearlyReturn,\n            col_rename = 'yearly_return')%&gt;%\n  arrange(date,desc(yearly_return))\n\n# A tibble: 30 × 3\n# Groups:   symbol [6]\n   symbol   date       yearly_return\n   &lt;chr&gt;    &lt;date&gt;             &lt;dbl&gt;\n 1 WEGE3.SA 2020-12-30        1.17  \n 2 PETR3.SA 2020-12-30       -0.111 \n 3 ITUB3.SA 2020-12-30       -0.119 \n 4 BEEF3.SA 2020-12-30       -0.155 \n 5 BBDC3.SA 2020-12-30       -0.228 \n 6 BRFS3.SA 2020-12-30       -0.386 \n 7 PETR3.SA 2021-12-30        0.304 \n 8 BEEF3.SA 2021-12-30        0.161 \n 9 BRFS3.SA 2021-12-30        0.0218\n10 WEGE3.SA 2021-12-30       -0.119 \n# ℹ 20 more rows\n\n\nWEGE3.SA has the highest return in 2020, whereas BRFS3.SA had the worst."
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep-solutions.html#exercise-3",
    "href": "quant-fin-datacases/data-case-prep-solutions.html#exercise-3",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise aims to gauge your skill on data visualization using the ggplot2 package. After loading this package, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE object, create a ggplot object with the follow aes: the x component should refer to the date column, the y axis should refer to the adjusted column, and the group argument should refer to the symbol column.\n\n\nYAHOO_SAMPLE%&gt;%ggplot(aes(x=date,y=adjusted,group=symbol))\n\n\n\n\n\n\n\n\n\nAdd a geometry layer to your chart using the geom_line() function and call it in your terminal.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()\n\n\n\n\n\n\n\n\n\nChange the titles of your x and y axis, as well as chart title and subtitle using the labs() function.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')\n\n\n\n\n\n\n\n\n\nFacet your data by each symbol using the facet_wrap(symbol~.) function. Set the arguments of this function in such a way that you have a 2x3 orientation (i.e, nrow=2 and ncol=2).\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')+\n  facet_wrap(symbol~.,nrow=3,ncol=2)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nApply a predefined theme using the theme_minimal() function.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')+\n  facet_wrap(symbol~.,nrow=3,ncol=2)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSave your plot using the ggsave() function. To use it, call ggsave('myplot.jpg',width=10,height = 6) to save the last plot that has been prompted to your terminal using a 10x6 resolution (in units).\n\n\nggsave('myplot.jpg',width=10,height = 6)"
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1.html",
    "href": "quant-fin-datacases/data-case-1/data-case-1.html",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "This Data Case is part of the Practical Applications in Quantitative Finance course, held at FGV-EAESP’s undergraduate course in business. Carefully follow the instructions contained in the data case as well as eClass® before you make your submission.\n\n\nThe pharmaceutical industry is a critical sector in financial markets, influenced by regulatory approvals, drug developments, and global health events. In this first Data Case, you will analyze stock performance for a set of 10 pharmaceutical companies over time, applying the tidyverse and tidyquant packages to extract and interpret insights from the data.\nYou are a junior analyst at Atlas Capital, a leading buyside investment firm specializing in sector-focused strategies. The firm is considering increasing its exposure to the pharmaceutical industry, given its long-term growth potential and resilience in volatile markets. In the latest investment committee meeting, your fund manager raised an important question: “How has the pharmaceutical industry performed over time? We need to identify whether now is the right time to increase our position.”\nYour team has been tasked with conducting an in-depth financial analysis of the pharmaceutical sector. The goal is to assess industry-wide trends, identify risks and opportunities, and ultimately recommend an investment stance. More specifically, your task will involve:\n\nCollecting stock price data and compute returns\nVisualizing key trends in returns and volatility\nInterpreting findings and suggest investment insights\n\nTo streamline our research, you will focus on the 10 largest publicly traded pharmaceutical companies in the U.S, analyze their performance, risks, and potential catalysts that could drive returns in the near future. As of February 2025, the 10 largest pharmaceutical companies traded in the U.S., along with their ticker symbols, are:\n\nEli Lilly and Co. (LLY): A leading pharmaceutical company known for its innovative treatments in diabetes and oncology.\nNovo Nordisk A/S (NVO): Specializing in diabetes care, Novo Nordisk has a significant presence in the U.S. market.\nJohnson & Johnson (JNJ): A diversified healthcare company with a strong pharmaceutical division.\nAbbVie Inc. (ABBV): Known for its immunology and oncology products, AbbVie is a major player in the pharmaceutical industry.\nMerck & Co., Inc. (MRK): Merck offers a wide range of prescription medicines, vaccines, and therapies.\nPfizer Inc. (PFE): A global pharmaceutical corporation recognized for its vaccines and therapeutics.\nBristol-Myers Squibb Company (BMY): Focused on oncology, cardiovascular, and immunology, Bristol-Myers Squibb is a key industry player.\nAstraZeneca PLC (AZN): A biopharmaceutical company with a strong portfolio in oncology and respiratory diseases.\nAmgen Inc. (AMGN): Specializing in biotechnology, Amgen develops therapies for serious illnesses.\nGilead Sciences, Inc. (GILD): Known for its antiviral drugs, Gilead has a significant market presence.\n\nNow, it’s up to you and your team to dive into the data, extract key insights, and present your data-driven investment thesis. Good luck—your next career milestone at Atlas Capital depends on it. 🚀\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®.\n\n\n\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\",\"ggcorrplot\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)\n\n\n\n\nUse the tq_get() function from the tidyquant package to retrieve historical adjusted closing prices for the 10 largest publicly traded pharmaceutical companies in the U.S. from Yahoo! Finance. Your dataset should cover the period from January 1, 2020, to December 31, 2024. Using the functions from the tidyverse, ensure that your data includes only the timestamp column, as well as the column that contains the daily adjusted stock price information. Store this into an object called financial_data (or something similar). Store this data set for all the subsequent analysis - make sure not to override this dataset as you move along the data case to make sure you are always referring to the raw data pull!\n\n\n\nUsing the tidyquant package, use the object you’ve just created with the tq_transmute function to compute the yearly returns for each stock over the analysis period. More specifically, pass the yearlyReturn function to adjusted column using the tq_transmute, labeling this new variable as yearly_return. Arrange your dataset by year and in descending order of yearly_return (highest-to-lowest). Store this into a new object called, for example, yearly_returns. Which stock had the highest return in 2024, and which one had the lowest? Prompt the results in your session.\n\n\n\nWith your data.frame containing the yearly returns over time for each stock, use ggplot to create a line chart of the historical cumulative returns for each stock during the study period. Which stock had the highest cumulative return up-to-date? Recall that cumulative returns can be calculated from period returns as:\n\\[\n\\text{Cumulative Return}= (1+R_1)\\times(1+R_2)\\times ... \\times(1+R_T)-1\\equiv \\prod (1+R_t)-1\n\\]\nYour chart should map date to the x-axis, the yearly return variable to the y axis, and group the results by symbol. To make sure that you are plotting a line chart, use the geom_line() function after you have mapped your data. In addition to these two layers, add any customizations that you believe that are beneficial to convey the message - see the Data Visualization\n\n\n\n\n\n\nHint\n\n\n\n\nWith the data.frame you created to store yearly returns, group by symbol, and use tq_transmute() to apply the Return.cumulative function to the data.\nNow, your resulting data.frame contains the cumulative returns for all stocks. You can adjust the column names with the setNames() function and pipe that into a ggplot call, mapping the symbol to the x-axis, the cumulative return column to the y-axis, and the geom_col() function to create a bar chart. Add as many customizations you think are worth the effort.\n\n\n\n\n\n\nAfter reviewing your initial analysis, your fund manager at Atlas Capital liked the idea of examining yearly returns to get a broader perspective on performance. However, they pointed out that pharmaceutical companies vary significantly in terms of risk exposure, so it’s crucial to account for volatility as well. To complement the analysis, use the same rationale from the previous exercise to calculate the yearly volatility for each stock. How do the risk levels differ between firms? Store your results in a new object and prompt it in your session.\n\n\n\n\n\n\nHint\n\n\n\nAs opposed to the yearlyReturn function, the tidyquant package does not have a pre-built dailyStdev function. Instead, what you can do is to use a combination of functions to get the expected result:\n\nFirst, use tq_transmute() to calculate daily returns passing the dailyReturn function\nNow, your resulting data.frame contains daily returns for all stocks. It is now in a convenient format to chain this object again, in another tq_transmute() function, applying the StdDev.annualized function and assign to a new object, like yearly_volatility. Note, however, that if you simply use StdDev.annualized, it will calculate an annualized metric for each stock for the whole period, which is not what you want.\n\nTo make sure that you have calculating the annualized standard deviation for each year, you can do a composition of apply.yearly, which applies a given function at yearly intervals, and StdDev.annualized, using the following syntax:\n\nyour_daily_return_object%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=StdDev.annualized,\n               col_rename = 'yearly_volatility')\n\nHere, tq_transmute() will apply the function defined in FUN over each interval.\n\n\n\n\n\nBuilding on your previous findings, since some companies exhibit higher returns but also greater risk, it might be a good idea to add a risk-adjusted performance metric to the analysis. The Sharpe ratio for stock \\(i\\) in period \\(t\\) measures the risk-adjusted return of an asset and is calculated as:\n\\[\n\\text{Sharpe Ratio}_{i,t}=\\dfrac{R_{i,t}-R_{f,t}}{\\sigma_{i,t}},\n\\]\nwhere \\(R_{i,t}\\) is the return of a given stock \\(i\\) in period \\(t\\), \\(R_{f,t}\\) is the risk-free return for the same period, and \\(\\sigma_{i,t}\\) is the volatility for stock \\(i\\) in period \\(t\\).\nYour task is to calculate the Sharpe Ratio for each pharmaceutical stock using yearly returns and yearly volatility. To simplify your calculations, assume a risk-free rate of \\(0\\%\\) per year (i.e, no risk-free premium). Compare the Sharpe ratios across companies. Do the highest-return stocks also have the best risk-adjusted performance? Are there any stocks that stand out as particularly efficient in generating returns relative to their risk? Are there companies that deliver strong returns but with disproportionately high volatility?\n\n\n\n\n\n\nHint\n\n\n\nThere are two ways you can use to create the Sharpe Ratio:\n\nUsing the previously created yearly_returns and yearly_volatility objects, use the left_join() function to merge them based on a common set of identifiers (in this case, date and symbol). After that, manipulate the resulting data.frame with mutate to generate the Sharpe Ratio.\nUsing tq_transmute in a very similar fashion to what you have done to calculate the yearly volatility, but now passing the the SharpeRatio.annualized function with arguments Rf=0 and scale=252.\n\nAlthough both approaches should yield similar results, potential differences might stem from rounding.\n\n\n\n\n\nWay to go! As you delve deeper into your investment analysis, your fund manager emphasizes the importance of understanding how different pharmaceutical stocks interact with one another over time. To gain insights into the relationships between these companies, your next task is to calculate the correlation of daily stock returns for the selected pharmaceutical companies for the analysis period.\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the daily returns for each stock. You can use the tq_transmute function into your dataset and apply the dailyReturn function.\nAfter that, you need to pivot your data in such a way that each column is a specific ticker with information on daily returns. You can do that by calling pivot_wider(names_from='symbol',values_from='daily_return'), assuming that your daily return variable is called daily_return.\nWith that, you’ll achieve a data frame that now has \\(11\\) columns, namely, the date and the \\(10\\) individual ticker columns with daily return information.\nTo make sure that you are calculating the correlation using a \\(10\\times10\\) matrix, use select(-date) to get rid of the date column and pipe that into cor(), which calculates the correlation across all pairs of variables within a data.frame, and outputs a correlation matrix.\n\nIf you want, you can pipe the result into ggcorplot(), a function from the ggcorplot package that provides meaningful visualizations of correlation matrices.\n\n\n\n\n\nBased on your analysis of the correlation between each stock, it seems that these pharmaceutical firms are relatively trending together. Notwithstanding, there might be gains from diversification if instead of choosing a specific firm, we decide to hold a portfolio of pharmaceutical stocks.\nInvesting in a single stock exposes an investor to company-specific (idiosyncratic) risk, such as lawsuits, failed drug trials, or regulatory changes. However, constructing a diversified portfolio of multiple stocks within the same industry can help smooth out these risks while still capturing the overall sector trends. For instance, while one pharmaceutical company may experience a stock price drop due to a failed drug trial, another might gain due to a successful FDA approval. By equally weighting multiple stocks, investors can reduce the impact of any single company’s negative performance while still benefiting from the broader industry’s growth.\nYour manager liked your idea and wanted to test it out by creating an equally-weighted portfolio of all pharmaceutical companies over time. Using the tq_transmute() function, create an object, portfolio_returns, that contains the yearly returns of a portfolio that assigns equal weights - in this case, 10% - on each stock, and compare that to the yearly returns of the S&P 500 Index. Would the fund manager be better-off by investing in the portfolio relative to the S&P500?\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the yearly returns for each stock using the tq_transmute() function as before, grouping the data by symbol and creating a new variable, yearly_return.\nKnowing that you have an equally-weighted portfolio, group your data my date and pipe the result into a summarize() function to create a new variable, portfolio_return as the average across all stocks. Assign this result to an object called portfolio_returns\nFetch S&P 500 data using a similar call to tq_get() like you did in the beginning of the exercise, but now collecting data for ^GSPC. Calculate the yearly returns and assign to a new variable, index_return. Store the result in another data.frame, index_returns.\nMerge both datasets using left_join().\n\n\n\n\n\n\nAfter analyzing the equally weighted pharmaceutical portfolio, the fund manager was impressed with the performance results. However, they remain skeptical about whether the portfolio truly provides better risk-adjusted returns compared to simply picking one of the best-performing stocks in the industry.\nAs final step, your job is to prove whether the portfolio offers superior risk-adjusted returns by computing the Sharpe Ratio for both the portfolio and its individual stocks in 2024. If the portfolio has a higher Sharpe ratio, it means that diversification helps maximize returns while controlling for risk — an essential argument when managing institutional funds.\nIn order to do that, your task is to provide a visualization of the Sharpe Ratio of the equally-weighted portfolio you’ve just created and compare that to those of the individual stocks. I have already created the portfolio results for you, so you can copy-paste that to your session:\n\nportfolio_sharpe=data.frame(symbol='Portfolio',\n                            yearly_return=0.03705213,\n                            yearly_volatility=0.1223983)\n\n\n\n\n\n\n\nHint\n\n\n\n\nCreate the portfolio_sharpe in your session using the code chunk above.\nUsing the yearly_returns object you’ve created in Exercise 2, filter by year(date)==2024 and left_join() with the yearly_volatility object you have created in Exercise 3, assigning the result to a new object\nFinally, bind portfolio_sharpe to the resulting data.frame in a rowwise manner using rbind(dataframe1,dataframe2)\nFinally, call ggplot() and adjust the aesthetics to show the relationship between risk (x-axis) and return (y-axis) for all individual stocks and the portfolio.\n\n\n\n\n\n\nNow that you have analyzed the Sharpe ratios of both individual pharmaceutical stocks and the equally weighted portfolio, take a step back and summarize your insights. Did the portfolio offer a better risk-adjusted return compared to individual stocks? If so, why? If not, what might explain the results?\nBased on your findings, what would you recommend to the fund manager? Would you suggest investing in the diversified portfolio, or do certain individual stocks offer superior risk-adjusted returns? Would you propose an alternative weighting scheme, such as a market cap-weighted portfolio, to further improve performance?\nWrite a short conclusion summarizing your key takeaways and justify your investment recommendation using data-driven insights."
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1.html#tech-setup",
    "href": "quant-fin-datacases/data-case-1/data-case-1.html#tech-setup",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\",\"ggcorrplot\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)"
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep/data-case-prep.html",
    "href": "quant-fin-datacases/data-case-prep/data-case-prep.html",
    "title": "Data Case Prep",
    "section": "",
    "text": "This exercise list is a preparation for students of the Practical Applications in Quantitative Finance course held at FGV-EAESP. This document serves as a refresher on key R programming concepts covered in previous lectures, reinforcing the essential skills needed for quantitative finance applications. By revisiting these foundations, you’ll ensure a solid grasp of the tools and techniques required to analyze financial data effectively. As we progress, you’ll apply these concepts to real-world finance examples, including portfolio analysis, risk assessment, and asset pricing models. Mastering these skills now will prepare you to fully engage with the practical applications we’ll explore throughout the course.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®.\n\n\n\n\n\n\nDeliverable\n\n\n\nEach student is expected to deliver his/her assignment individually, although you can freely work in groups for solving the questions. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both your code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details.\n\n\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)\n\n\n\n\n\n\n\nNote\n\n\n\nFor the remainder of the sessions, you will use financial data from multiple sources, either that being from a local file or a pull from Yahoo! Finance. Whenever you are working with local files, it is always important to make sure that your R is able to locate it. To check the working directory of your session, simply type getwd(), and it will prompt your current directory. If you want to change your directory, simply type setwd('C:/path/to/your/folder') with the specific path to your desired folder. To make sure that you switched directories, you can type getwd() to confirm the new directory.\nMost of the issues regarding not being able to load a specific file, like .csv and .xlsx spreadsheets can be easily solved by placing your R file (either a plain script, like myscript.R, or a quarto document, myquartodoc.qmd) in the same folder as of your data. When you open your R script or Quarto document, it will automatically set that folder (which coincides with the data folder) as the working directory. To confirm which files are available to you, you can simply type list.files() to get the list of all files that R can find in the working directory.\nIf you prefer, whenever you are calling a function that requires a path to your computer, you can always provide the full path of the file: for example, using \"C:/Users/Lucas/Documents/GitHub/Folder/test.csv' would find the test.csv even if Folder is not your working directory.\nWhenever you are unsure about how a specific function works, type the function in an R script and you will notice that RStudio will auto complete the function name for you. To get more information on a given function’s arguments, hit F1 to see a description at the bottom-right of your session.\n\n\n\n\n\nThis exercise aims to gauge your skill in fetching data in R through different sources. You are asked to work on the following questions:\n\nGo to our eClass® and download the Ibovespa_Sample.txt file inside the Data Cases folder. This file contains stock price information regarding 6 publicly traded companies that are part of the Brazilian market index, Ibovespa.\nRead the file into your session using the read_delim() function, and assign to an object called FILE_SAMPLE. Make sure you have the tidyverse package loaded. Depending on how your computer reads delimited data, you may need to set the argument sep to sep=';' or sep=' '.\nInspect the first 10 lines of the object you have just created using the head() function, setting n=10 as an argument.\nHow many rows and columns does your object have? You can use the function dim(yourdata) to find the exact dimensions.\nAlternatively, fetch the exact same stock level information directly from Yahoo! Finance using the tq_get() function. Collect the data beginning in 2020 (January \\(1^{st}\\) until the end of 2024 (December \\(31^{st}\\)) using the from and to arguments of the tq_get() function. Make sure you have the tidyquant package loaded. Assign this data to a new object in your R session called YAHOO_SAMPLE.\n\n\n\n\n\n\n\nHint\n\n\n\nRemember that the syntax for using tq_get() to collect stock prices is:\n\nmy_stocks &lt;- tq_get(c(\"stock1\", \"stock2\"),\n                      from = \"YYYY-MM-DD\",\n                      to   = \"YYYY-MM-DD\")\n\nFurthermore, you can use the unique(FILE_SAMPLE$symbol) to find the exact tickers contained in the file, and assign it to a new object, assets, which will then be piped onto a call to tq_get().\n\n\n\nHow many rows and columns does this new object have?\n\n\n\n\nThis exercise aims to gauge your skill on manipulating data using the tidyverse and the tidyquant. After loading these packages, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE you created before, create a new column, Year, that is defined as the year of a given timestamp found in the date column. You can use year(date) along with a mutate function to create this new variable. Update your YAHOO_SAMPLE object to make sure you have saved this variable.\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the mutate function is:\n\nmutate(.data, #The object you are performing the calculations \n       new_variable_1 = var1 * 2, #Can use basic operations...\n       new_variable_2 = median(var2), #Or predefined functions)\n       variable_3 = as.character(var3) #And can be used to modify existing variables)\n       ) \n\nFurthermore, recall that you can always use the pipe operator (%&gt;%) to chain operations along the way whenever you are using the tidyverse.\n\n\n\nUsing the updated YAHOO_SAMPLE object, use the filter function to filter for observations that occur in \\(2024\\), use group_by() to group the data by symbol, and use the summarize the function to create a new variable, avg_price, defined as the average adjusted prices for each year-symbol combination. Sort your dataset by avg_price (highest-to-lowest), storing this result in an object called 2023_2024_Summary and call it in your terminal. Which company had the highest price levels, and which one had the lowest?\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the filter function is:\n\nfilter(.data, #The object which you are performing the operations\n       variable_1 &gt;10, #Simple arithmetic operators\n       variable_2 %in% c('AAPL','MSFT','FORD'), #Pattern search\n       !(variable_3 %in% c('Boston','Mass','Silicon Valley')), #Negate pattern search\n       variable_4 &gt;=10 & variable_3&lt;= 4 | is.na(variable_4) #IF and OR conditions\n       ) \n\n\n\n\nUsing the YAHOO_SAMPLE dataset, group your data by symbol and create a summary of yearly returns. To do that, first pipe the grouped data into into the tq_transmute function applying the yearlyReturn, assigning it to a new variable,yearly_return. Arrange your result by date and yearly_return (highest-to-lowest). Which stock had the best performance in 2020, and which stock had the worst?\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the tq_transmute function is:\n\ntq_transmute(.data, #The object you are performing the calculations \n       selected_variables, #The columns to send to the mutation function\n       mutate_fun, #The mutation function from either the xts, quantmod, or TTR package.\n       col_rename #A string or character vector containing names that can be used to quickly rename columns\n       )\n\n\n\n\n\n\nThis exercise aims to gauge your skill on data visualization using the ggplot2 package. After loading this package, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE object, create a ggplot object with the follow aes: the x component should refer to the date column, the y axis should refer to the adjusted column, and the group argument should refer to the symbol column.\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for mapping your data to ggplot and adding further layers for adjusting aesthetics is:\n\nggplot(data=your_data, aes(x= variable_1, y=variable_2, group=your_group,...))+\ngeom_{yourgeom} +\nadditional_layer_1()+\nadditional_layer_2()+\nany_additional_layer_commands()\n\n\n\n\nAdd a geometry layer to your chart using the geom_line() function and call it in your terminal.\nChange the titles of your x and y axis, as well as chart title and subtitle using the labs() function.\nFacet your data by each symbol using the facet_wrap(symbol~.) function. Set the arguments of this function in such a way that you have a 2x3 orientation (i.e, nrow=2 and ncol=3).\nApply a predefined theme using the theme_minimal() function.\nSave your plot using the ggsave() function. To use it, call ggsave('myplot.jpg',width=10,height = 6) to save the last plot that has been prompted to your terminal using a 10x6 resolution (in units)."
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep/data-case-prep.html#tech-setup",
    "href": "quant-fin-datacases/data-case-prep/data-case-prep.html#tech-setup",
    "title": "Data Case Prep",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)\n\n\n\n\n\n\n\nNote\n\n\n\nFor the remainder of the sessions, you will use financial data from multiple sources, either that being from a local file or a pull from Yahoo! Finance. Whenever you are working with local files, it is always important to make sure that your R is able to locate it. To check the working directory of your session, simply type getwd(), and it will prompt your current directory. If you want to change your directory, simply type setwd('C:/path/to/your/folder') with the specific path to your desired folder. To make sure that you switched directories, you can type getwd() to confirm the new directory.\nMost of the issues regarding not being able to load a specific file, like .csv and .xlsx spreadsheets can be easily solved by placing your R file (either a plain script, like myscript.R, or a quarto document, myquartodoc.qmd) in the same folder as of your data. When you open your R script or Quarto document, it will automatically set that folder (which coincides with the data folder) as the working directory. To confirm which files are available to you, you can simply type list.files() to get the list of all files that R can find in the working directory.\nIf you prefer, whenever you are calling a function that requires a path to your computer, you can always provide the full path of the file: for example, using \"C:/Users/Lucas/Documents/GitHub/Folder/test.csv' would find the test.csv even if Folder is not your working directory.\nWhenever you are unsure about how a specific function works, type the function in an R script and you will notice that RStudio will auto complete the function name for you. To get more information on a given function’s arguments, hit F1 to see a description at the bottom-right of your session."
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep/data-case-prep.html#exercise-1",
    "href": "quant-fin-datacases/data-case-prep/data-case-prep.html#exercise-1",
    "title": "Data Case Prep",
    "section": "",
    "text": "This exercise aims to gauge your skill in fetching data in R through different sources. You are asked to work on the following questions:\n\nGo to our eClass® and download the Ibovespa_Sample.txt file inside the Data Cases folder. This file contains stock price information regarding 6 publicly traded companies that are part of the Brazilian market index, Ibovespa.\nRead the file into your session using the read_delim() function, and assign to an object called FILE_SAMPLE. Make sure you have the tidyverse package loaded. Depending on how your computer reads delimited data, you may need to set the argument sep to sep=';' or sep=' '.\nInspect the first 10 lines of the object you have just created using the head() function, setting n=10 as an argument.\nHow many rows and columns does your object have? You can use the function dim(yourdata) to find the exact dimensions.\nAlternatively, fetch the exact same stock level information directly from Yahoo! Finance using the tq_get() function. Collect the data beginning in 2020 (January \\(1^{st}\\) until the end of 2024 (December \\(31^{st}\\)) using the from and to arguments of the tq_get() function. Make sure you have the tidyquant package loaded. Assign this data to a new object in your R session called YAHOO_SAMPLE.\n\n\n\n\n\n\n\nHint\n\n\n\nRemember that the syntax for using tq_get() to collect stock prices is:\n\nmy_stocks &lt;- tq_get(c(\"stock1\", \"stock2\"),\n                      from = \"YYYY-MM-DD\",\n                      to   = \"YYYY-MM-DD\")\n\nFurthermore, you can use the unique(FILE_SAMPLE$symbol) to find the exact tickers contained in the file, and assign it to a new object, assets, which will then be piped onto a call to tq_get().\n\n\n\nHow many rows and columns does this new object have?"
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep/data-case-prep.html#exercise-2",
    "href": "quant-fin-datacases/data-case-prep/data-case-prep.html#exercise-2",
    "title": "Data Case Prep",
    "section": "",
    "text": "This exercise aims to gauge your skill on manipulating data using the tidyverse and the tidyquant. After loading these packages, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE you created before, create a new column, Year, that is defined as the year of a given timestamp found in the date column. You can use year(date) along with a mutate function to create this new variable. Update your YAHOO_SAMPLE object to make sure you have saved this variable.\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the mutate function is:\n\nmutate(.data, #The object you are performing the calculations \n       new_variable_1 = var1 * 2, #Can use basic operations...\n       new_variable_2 = median(var2), #Or predefined functions)\n       variable_3 = as.character(var3) #And can be used to modify existing variables)\n       ) \n\nFurthermore, recall that you can always use the pipe operator (%&gt;%) to chain operations along the way whenever you are using the tidyverse.\n\n\n\nUsing the updated YAHOO_SAMPLE object, use the filter function to filter for observations that occur in \\(2024\\), use group_by() to group the data by symbol, and use the summarize the function to create a new variable, avg_price, defined as the average adjusted prices for each year-symbol combination. Sort your dataset by avg_price (highest-to-lowest), storing this result in an object called 2023_2024_Summary and call it in your terminal. Which company had the highest price levels, and which one had the lowest?\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the filter function is:\n\nfilter(.data, #The object which you are performing the operations\n       variable_1 &gt;10, #Simple arithmetic operators\n       variable_2 %in% c('AAPL','MSFT','FORD'), #Pattern search\n       !(variable_3 %in% c('Boston','Mass','Silicon Valley')), #Negate pattern search\n       variable_4 &gt;=10 & variable_3&lt;= 4 | is.na(variable_4) #IF and OR conditions\n       ) \n\n\n\n\nUsing the YAHOO_SAMPLE dataset, group your data by symbol and create a summary of yearly returns. To do that, first pipe the grouped data into into the tq_transmute function applying the yearlyReturn, assigning it to a new variable,yearly_return. Arrange your result by date and yearly_return (highest-to-lowest). Which stock had the best performance in 2020, and which stock had the worst?\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the tq_transmute function is:\n\ntq_transmute(.data, #The object you are performing the calculations \n       selected_variables, #The columns to send to the mutation function\n       mutate_fun, #The mutation function from either the xts, quantmod, or TTR package.\n       col_rename #A string or character vector containing names that can be used to quickly rename columns\n       )"
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep/data-case-prep.html#exercise-3",
    "href": "quant-fin-datacases/data-case-prep/data-case-prep.html#exercise-3",
    "title": "Data Case Prep",
    "section": "",
    "text": "This exercise aims to gauge your skill on data visualization using the ggplot2 package. After loading this package, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE object, create a ggplot object with the follow aes: the x component should refer to the date column, the y axis should refer to the adjusted column, and the group argument should refer to the symbol column.\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for mapping your data to ggplot and adding further layers for adjusting aesthetics is:\n\nggplot(data=your_data, aes(x= variable_1, y=variable_2, group=your_group,...))+\ngeom_{yourgeom} +\nadditional_layer_1()+\nadditional_layer_2()+\nany_additional_layer_commands()\n\n\n\n\nAdd a geometry layer to your chart using the geom_line() function and call it in your terminal.\nChange the titles of your x and y axis, as well as chart title and subtitle using the labs() function.\nFacet your data by each symbol using the facet_wrap(symbol~.) function. Set the arguments of this function in such a way that you have a 2x3 orientation (i.e, nrow=2 and ncol=3).\nApply a predefined theme using the theme_minimal() function.\nSave your plot using the ggsave() function. To use it, call ggsave('myplot.jpg',width=10,height = 6) to save the last plot that has been prompted to your terminal using a 10x6 resolution (in units)."
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep/data-case-prep-solutions.html",
    "href": "quant-fin-datacases/data-case-prep/data-case-prep-solutions.html",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise list is a preparation for students of the Practical Applications in Quantitative Finance course held at FGV-EAESP. This document serves as a refresher on key R programming concepts covered in previous lectures, reinforcing the essential skills needed for quantitative finance applications. By revisiting these foundations, you’ll ensure a solid grasp of the tools and techniques required to analyze financial data effectively. As we progress, you’ll apply these concepts to real-world finance examples, including portfolio analysis, risk assessment, and asset pricing models. Mastering these skills now will prepare you to fully engage with the practical applications we’ll explore throughout the course.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®.\n\n\n\n\n\n\nDeliverable\n\n\n\nEach student is expected to deliver his/her assignment individually, although you can freely work in groups for solving the questions. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both your code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details.\n\n\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\n\n\n\nThis exercise aims to gauge your skill in fetching data in R through different sources. You are asked to work on the following questions:\n\nGo to our eClass® and download the Ibovespa_Sample.txt file inside the Data Cases folder. This file contains stock price information regarding 10 publicly traded companies that are part of the Brazilian market index, Ibovespa.\nRead the file into your session using the read_delim() function, and assign to an object called FILE_SAMPLE. Make sure you have the tidyverse package loaded. Depending on how your computer reads delimited data, you may need to set the argument sep to sep=';' or sep=' '.\n\n\nFILE_SAMPLE=read_delim('Ibovespa_Sample.txt',delim=' ')\n\n\nInspect the first 10 lines of the object you have just created using the head() function, setting n=10 as an argument.\n\n\nFILE_SAMPLE%&gt;%head(n=10)\n\n# A tibble: 10 × 8\n   symbol   date        open  high   low close   volume adjusted\n   &lt;chr&gt;    &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 PETR3.SA 2020-01-02  32.3  32.8  32.1  32.8  6660800     10.6\n 2 PETR3.SA 2020-01-03  33    33.2  32.0  32.0 20340400     10.4\n 3 PETR3.SA 2020-01-06  32    33.1  31.8  33.0 17549700     10.7\n 4 PETR3.SA 2020-01-07  33.0  33.0  32.4  32.6  5480400     10.6\n 5 PETR3.SA 2020-01-08  32.7  32.8  31.8  32.0 10030200     10.4\n 6 PETR3.SA 2020-01-09  32.1  32.4  31.8  32.2 15411600     10.4\n 7 PETR3.SA 2020-01-10  32.3  32.3  32.0  32.1  3867200     10.4\n 8 PETR3.SA 2020-01-13  32.2  32.3  31.9  32.1  6666500     10.4\n 9 PETR3.SA 2020-01-14  32.0  32.1  31.5  31.8  6752000     10.3\n10 PETR3.SA 2020-01-15  31.6  31.7  31.1  31.1  7113300     10.1\n\n\n\nHow many rows and columns does your object have? You can use the function dim(yourdata) to find the exact dimensions.\n\nThis object has 7464 rows and 8 columns.\n\nAlternatively, fetch the exact same stock level information directly from Yahoo! Finance using the tq_get() function. Collect the data beginning in 2020 (January \\(1^{st}\\) until the end of 2024 (December \\(31^{st}\\)) using the from and to arguments of the tq_get() function. Make sure you have the tidyquant package loaded. Assign this data to a new object in your R session called YAHOO_SAMPLE.\n\n\nassets = unique(FILE_SAMPLE$symbol)\n\nYAHOO_SAMPLE=assets%&gt;%tq_get(from='2020-01-01',to='2024-12-31')\n\n\nHow many rows and columns does this new object have?\n\nThis object has 7464 rows and 8 columns.\n\n\n\nThis exercise aims to gauge your skill on manipulating data using the tidyverse and the tidyquant. After loading these packages, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE you created before, create a new column, Year, that is defined as the year of a given timestamp found in the date column. You can use year(date) along with a mutate function to create this new variable. Update your YAHOO_SAMPLE object to make sure you have saved this variable.\n\n\nYAHOO_SAMPLE=YAHOO_SAMPLE%&gt;%\n  mutate(Year=year(date))\n\n\nUsing the updated YAHOO_SAMPLE object, use the filter function to filter for observations that occur in \\(2024\\), use group_by() to group the data by symbol, and use the summarize the function to create a new variable, avg_price, defined as the average adjusted prices for each year-symbol combination. Sort your dataset by avg_price (highest-to-lowest), storing this result in an object called 2023_2024_Summary and call it in your terminal. Which company had the highest price levels, and which one had the lowest?\n\n\nSummary_2023_2024=YAHOO_SAMPLE%&gt;%\n  filter(Year==2024)%&gt;%\n  group_by(symbol)%&gt;%\n  summarize(avg_price=mean(adjusted,na.rm=TRUE))%&gt;%\n  arrange(avg_price)\n\nSummary_2023_2024\n\n# A tibble: 6 × 2\n  symbol   avg_price\n  &lt;chr&gt;        &lt;dbl&gt;\n1 BEEF3.SA      6.48\n2 BBDC3.SA     11.8 \n3 BRFS3.SA     19.9 \n4 ITUB3.SA     28.0 \n5 PETR3.SA     36.1 \n6 WEGE3.SA     44.8 \n\n\n\nUsing the YAHOO_SAMPLE dataset, group your data by symbol and create a summary of yearly returns. To do that, first pipe the grouped data into into the tq_transmute function applying the yearlyReturn, assigning it to a new variable,yearly_return. Arrange your result by date and yearly_return (highest-to-lowest). Which stock had the best performance in 2020, and which stock had the worst?\n\n\nYAHOO_SAMPLE%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select='adjusted',\n            mutate_fun = yearlyReturn,\n            col_rename = 'yearly_return')%&gt;%\n  arrange(date,desc(yearly_return))\n\n# A tibble: 30 × 3\n# Groups:   symbol [6]\n   symbol   date       yearly_return\n   &lt;chr&gt;    &lt;date&gt;             &lt;dbl&gt;\n 1 WEGE3.SA 2020-12-30        1.17  \n 2 PETR3.SA 2020-12-30       -0.111 \n 3 ITUB3.SA 2020-12-30       -0.119 \n 4 BEEF3.SA 2020-12-30       -0.155 \n 5 BBDC3.SA 2020-12-30       -0.228 \n 6 BRFS3.SA 2020-12-30       -0.386 \n 7 PETR3.SA 2021-12-30        0.304 \n 8 BEEF3.SA 2021-12-30        0.161 \n 9 BRFS3.SA 2021-12-30        0.0218\n10 WEGE3.SA 2021-12-30       -0.119 \n# ℹ 20 more rows\n\n\nWEGE3.SA has the highest return in 2020, whereas BRFS3.SA had the worst.\n\n\n\nThis exercise aims to gauge your skill on data visualization using the ggplot2 package. After loading this package, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE object, create a ggplot object with the follow aes: the x component should refer to the date column, the y axis should refer to the adjusted column, and the group argument should refer to the symbol column.\n\n\nYAHOO_SAMPLE%&gt;%ggplot(aes(x=date,y=adjusted,group=symbol))\n\n\n\n\n\n\n\n\n\nAdd a geometry layer to your chart using the geom_line() function and call it in your terminal.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()\n\n\n\n\n\n\n\n\n\nChange the titles of your x and y axis, as well as chart title and subtitle using the labs() function.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')\n\n\n\n\n\n\n\n\n\nFacet your data by each symbol using the facet_wrap(symbol~.) function. Set the arguments of this function in such a way that you have a 2x3 orientation (i.e, nrow=2 and ncol=3).\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')+\n  facet_wrap(symbol~.,nrow=3,ncol=2)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nApply a predefined theme using the theme_minimal() function.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')+\n  facet_wrap(symbol~.,nrow=3,ncol=2)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSave your plot using the ggsave() function. To use it, call ggsave('myplot.jpg',width=10,height = 6) to save the last plot that has been prompted to your terminal using a 10x6 resolution (in units).\n\n\nggsave('myplot.jpg',width=10,height = 6)"
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep/data-case-prep-solutions.html#tech-setup",
    "href": "quant-fin-datacases/data-case-prep/data-case-prep-solutions.html#tech-setup",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))"
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep/data-case-prep-solutions.html#exercise-1",
    "href": "quant-fin-datacases/data-case-prep/data-case-prep-solutions.html#exercise-1",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise aims to gauge your skill in fetching data in R through different sources. You are asked to work on the following questions:\n\nGo to our eClass® and download the Ibovespa_Sample.txt file inside the Data Cases folder. This file contains stock price information regarding 10 publicly traded companies that are part of the Brazilian market index, Ibovespa.\nRead the file into your session using the read_delim() function, and assign to an object called FILE_SAMPLE. Make sure you have the tidyverse package loaded. Depending on how your computer reads delimited data, you may need to set the argument sep to sep=';' or sep=' '.\n\n\nFILE_SAMPLE=read_delim('Ibovespa_Sample.txt',delim=' ')\n\n\nInspect the first 10 lines of the object you have just created using the head() function, setting n=10 as an argument.\n\n\nFILE_SAMPLE%&gt;%head(n=10)\n\n# A tibble: 10 × 8\n   symbol   date        open  high   low close   volume adjusted\n   &lt;chr&gt;    &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 PETR3.SA 2020-01-02  32.3  32.8  32.1  32.8  6660800     10.6\n 2 PETR3.SA 2020-01-03  33    33.2  32.0  32.0 20340400     10.4\n 3 PETR3.SA 2020-01-06  32    33.1  31.8  33.0 17549700     10.7\n 4 PETR3.SA 2020-01-07  33.0  33.0  32.4  32.6  5480400     10.6\n 5 PETR3.SA 2020-01-08  32.7  32.8  31.8  32.0 10030200     10.4\n 6 PETR3.SA 2020-01-09  32.1  32.4  31.8  32.2 15411600     10.4\n 7 PETR3.SA 2020-01-10  32.3  32.3  32.0  32.1  3867200     10.4\n 8 PETR3.SA 2020-01-13  32.2  32.3  31.9  32.1  6666500     10.4\n 9 PETR3.SA 2020-01-14  32.0  32.1  31.5  31.8  6752000     10.3\n10 PETR3.SA 2020-01-15  31.6  31.7  31.1  31.1  7113300     10.1\n\n\n\nHow many rows and columns does your object have? You can use the function dim(yourdata) to find the exact dimensions.\n\nThis object has 7464 rows and 8 columns.\n\nAlternatively, fetch the exact same stock level information directly from Yahoo! Finance using the tq_get() function. Collect the data beginning in 2020 (January \\(1^{st}\\) until the end of 2024 (December \\(31^{st}\\)) using the from and to arguments of the tq_get() function. Make sure you have the tidyquant package loaded. Assign this data to a new object in your R session called YAHOO_SAMPLE.\n\n\nassets = unique(FILE_SAMPLE$symbol)\n\nYAHOO_SAMPLE=assets%&gt;%tq_get(from='2020-01-01',to='2024-12-31')\n\n\nHow many rows and columns does this new object have?\n\nThis object has 7464 rows and 8 columns."
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep/data-case-prep-solutions.html#exercise-2",
    "href": "quant-fin-datacases/data-case-prep/data-case-prep-solutions.html#exercise-2",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise aims to gauge your skill on manipulating data using the tidyverse and the tidyquant. After loading these packages, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE you created before, create a new column, Year, that is defined as the year of a given timestamp found in the date column. You can use year(date) along with a mutate function to create this new variable. Update your YAHOO_SAMPLE object to make sure you have saved this variable.\n\n\nYAHOO_SAMPLE=YAHOO_SAMPLE%&gt;%\n  mutate(Year=year(date))\n\n\nUsing the updated YAHOO_SAMPLE object, use the filter function to filter for observations that occur in \\(2024\\), use group_by() to group the data by symbol, and use the summarize the function to create a new variable, avg_price, defined as the average adjusted prices for each year-symbol combination. Sort your dataset by avg_price (highest-to-lowest), storing this result in an object called 2023_2024_Summary and call it in your terminal. Which company had the highest price levels, and which one had the lowest?\n\n\nSummary_2023_2024=YAHOO_SAMPLE%&gt;%\n  filter(Year==2024)%&gt;%\n  group_by(symbol)%&gt;%\n  summarize(avg_price=mean(adjusted,na.rm=TRUE))%&gt;%\n  arrange(avg_price)\n\nSummary_2023_2024\n\n# A tibble: 6 × 2\n  symbol   avg_price\n  &lt;chr&gt;        &lt;dbl&gt;\n1 BEEF3.SA      6.48\n2 BBDC3.SA     11.8 \n3 BRFS3.SA     19.9 \n4 ITUB3.SA     28.0 \n5 PETR3.SA     36.1 \n6 WEGE3.SA     44.8 \n\n\n\nUsing the YAHOO_SAMPLE dataset, group your data by symbol and create a summary of yearly returns. To do that, first pipe the grouped data into into the tq_transmute function applying the yearlyReturn, assigning it to a new variable,yearly_return. Arrange your result by date and yearly_return (highest-to-lowest). Which stock had the best performance in 2020, and which stock had the worst?\n\n\nYAHOO_SAMPLE%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select='adjusted',\n            mutate_fun = yearlyReturn,\n            col_rename = 'yearly_return')%&gt;%\n  arrange(date,desc(yearly_return))\n\n# A tibble: 30 × 3\n# Groups:   symbol [6]\n   symbol   date       yearly_return\n   &lt;chr&gt;    &lt;date&gt;             &lt;dbl&gt;\n 1 WEGE3.SA 2020-12-30        1.17  \n 2 PETR3.SA 2020-12-30       -0.111 \n 3 ITUB3.SA 2020-12-30       -0.119 \n 4 BEEF3.SA 2020-12-30       -0.155 \n 5 BBDC3.SA 2020-12-30       -0.228 \n 6 BRFS3.SA 2020-12-30       -0.386 \n 7 PETR3.SA 2021-12-30        0.304 \n 8 BEEF3.SA 2021-12-30        0.161 \n 9 BRFS3.SA 2021-12-30        0.0218\n10 WEGE3.SA 2021-12-30       -0.119 \n# ℹ 20 more rows\n\n\nWEGE3.SA has the highest return in 2020, whereas BRFS3.SA had the worst."
  },
  {
    "objectID": "quant-fin-datacases/data-case-prep/data-case-prep-solutions.html#exercise-3",
    "href": "quant-fin-datacases/data-case-prep/data-case-prep-solutions.html#exercise-3",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise aims to gauge your skill on data visualization using the ggplot2 package. After loading this package, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE object, create a ggplot object with the follow aes: the x component should refer to the date column, the y axis should refer to the adjusted column, and the group argument should refer to the symbol column.\n\n\nYAHOO_SAMPLE%&gt;%ggplot(aes(x=date,y=adjusted,group=symbol))\n\n\n\n\n\n\n\n\n\nAdd a geometry layer to your chart using the geom_line() function and call it in your terminal.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()\n\n\n\n\n\n\n\n\n\nChange the titles of your x and y axis, as well as chart title and subtitle using the labs() function.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')\n\n\n\n\n\n\n\n\n\nFacet your data by each symbol using the facet_wrap(symbol~.) function. Set the arguments of this function in such a way that you have a 2x3 orientation (i.e, nrow=2 and ncol=3).\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')+\n  facet_wrap(symbol~.,nrow=3,ncol=2)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nApply a predefined theme using the theme_minimal() function.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')+\n  facet_wrap(symbol~.,nrow=3,ncol=2)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSave your plot using the ggsave() function. To use it, call ggsave('myplot.jpg',width=10,height = 6) to save the last plot that has been prompted to your terminal using a 10x6 resolution (in units).\n\n\nggsave('myplot.jpg',width=10,height = 6)"
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-1",
    "href": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-1",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Use the tq_get() function from the tidyquant package to retrieve historical adjusted closing prices for the 10 largest publicly traded pharmaceutical companies in the U.S. from Yahoo! Finance. Your dataset should cover the period from January 1, 2020, to December 31, 2024. Using the functions from the tidyverse, ensure that your data includes only the timestamp column, as well as the column that contains the daily adjusted stock price information. Store this into an object called financial_data (or something similar). Store this data set for all the subsequent analysis - make sure not to override this dataset as you move along the data case to make sure you are always referring to the raw data pull!"
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-2",
    "href": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-2",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Using the tidyquant package, use the object you’ve just created with the tq_transmute function to compute the yearly returns for each stock over the analysis period. More specifically, pass the yearlyReturn function to adjusted column using the tq_transmute, labeling this new variable as yearly_return. Arrange your dataset by year and in descending order of yearly_return (highest-to-lowest). Store this into a new object called, for example, yearly_returns. Which stock had the highest return in 2024, and which one had the lowest? Prompt the results in your session."
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-3",
    "href": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-3",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "With your data.frame containing the yearly returns over time for each stock, use ggplot to create a line chart of the historical cumulative returns for each stock during the study period. Which stock had the highest cumulative return up-to-date? Recall that cumulative returns can be calculated from period returns as:\n\\[\n\\text{Cumulative Return}= (1+R_1)\\times(1+R_2)\\times ... \\times(1+R_T)-1\\equiv \\prod (1+R_t)-1\n\\]\nYour chart should map date to the x-axis, the yearly return variable to the y axis, and group the results by symbol. To make sure that you are plotting a line chart, use the geom_line() function after you have mapped your data. In addition to these two layers, add any customizations that you believe that are beneficial to convey the message - see the Data Visualization\n\n\n\n\n\n\nHint\n\n\n\n\nWith the data.frame you created to store yearly returns, group by symbol, and use tq_transmute() to apply the Return.cumulative function to the data.\nNow, your resulting data.frame contains the cumulative returns for all stocks. You can adjust the column names with the setNames() function and pipe that into a ggplot call, mapping the symbol to the x-axis, the cumulative return column to the y-axis, and the geom_col() function to create a bar chart. Add as many customizations you think are worth the effort."
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-4",
    "href": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-4",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "After reviewing your initial analysis, your fund manager at Atlas Capital liked the idea of examining yearly returns to get a broader perspective on performance. However, they pointed out that pharmaceutical companies vary significantly in terms of risk exposure, so it’s crucial to account for volatility as well. To complement the analysis, use the same rationale from the previous exercise to calculate the yearly volatility for each stock. How do the risk levels differ between firms? Store your results in a new object and prompt it in your session.\n\n\n\n\n\n\nHint\n\n\n\nAs opposed to the yearlyReturn function, the tidyquant package does not have a pre-built dailyStdev function. Instead, what you can do is to use a combination of functions to get the expected result:\n\nFirst, use tq_transmute() to calculate daily returns passing the dailyReturn function\nNow, your resulting data.frame contains daily returns for all stocks. It is now in a convenient format to chain this object again, in another tq_transmute() function, applying the StdDev.annualized function and assign to a new object, like yearly_volatility. Note, however, that if you simply use StdDev.annualized, it will calculate an annualized metric for each stock for the whole period, which is not what you want.\n\nTo make sure that you have calculating the annualized standard deviation for each year, you can do a composition of apply.yearly, which applies a given function at yearly intervals, and StdDev.annualized, using the following syntax:\n\nyour_daily_return_object%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=StdDev.annualized,\n               col_rename = 'yearly_volatility')\n\nHere, tq_transmute() will apply the function defined in FUN over each interval."
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-5",
    "href": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-5",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Building on your previous findings, since some companies exhibit higher returns but also greater risk, it might be a good idea to add a risk-adjusted performance metric to the analysis. The Sharpe ratio for stock \\(i\\) in period \\(t\\) measures the risk-adjusted return of an asset and is calculated as:\n\\[\n\\text{Sharpe Ratio}_{i,t}=\\dfrac{R_{i,t}-R_{f,t}}{\\sigma_{i,t}},\n\\]\nwhere \\(R_{i,t}\\) is the return of a given stock \\(i\\) in period \\(t\\), \\(R_{f,t}\\) is the risk-free return for the same period, and \\(\\sigma_{i,t}\\) is the volatility for stock \\(i\\) in period \\(t\\).\nYour task is to calculate the Sharpe Ratio for each pharmaceutical stock using yearly returns and yearly volatility. To simplify your calculations, assume a risk-free rate of \\(0\\%\\) per year (i.e, no risk-free premium). Compare the Sharpe ratios across companies. Do the highest-return stocks also have the best risk-adjusted performance? Are there any stocks that stand out as particularly efficient in generating returns relative to their risk? Are there companies that deliver strong returns but with disproportionately high volatility?\n\n\n\n\n\n\nHint\n\n\n\nThere are two ways you can use to create the Sharpe Ratio:\n\nUsing the previously created yearly_returns and yearly_volatility objects, use the left_join() function to merge them based on a common set of identifiers (in this case, date and symbol). After that, manipulate the resulting data.frame with mutate to generate the Sharpe Ratio.\nUsing tq_transmute in a very similar fashion to what you have done to calculate the yearly volatility, but now passing the the SharpeRatio.annualized function with arguments Rf=0 and scale=252.\n\nAlthough both approaches should yield similar results, potential differences might stem from rounding."
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-6",
    "href": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-6",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Way to go! As you delve deeper into your investment analysis, your fund manager emphasizes the importance of understanding how different pharmaceutical stocks interact with one another over time. To gain insights into the relationships between these companies, your next task is to calculate the correlation of daily stock returns for the selected pharmaceutical companies for the analysis period.\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the daily returns for each stock. You can use the tq_transmute function into your dataset and apply the dailyReturn function.\nAfter that, you need to pivot your data in such a way that each column is a specific ticker with information on daily returns. You can do that by calling pivot_wider(names_from='symbol',values_from='daily_return'), assuming that your daily return variable is called daily_return.\nWith that, you’ll achieve a data frame that now has \\(11\\) columns, namely, the date and the \\(10\\) individual ticker columns with daily return information.\nTo make sure that you are calculating the correlation using a \\(10\\times10\\) matrix, use select(-date) to get rid of the date column and pipe that into cor(), which calculates the correlation across all pairs of variables within a data.frame, and outputs a correlation matrix.\n\nIf you want, you can pipe the result into ggcorplot(), a function from the ggcorplot package that provides meaningful visualizations of correlation matrices."
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1.html#challenge",
    "href": "quant-fin-datacases/data-case-1/data-case-1.html#challenge",
    "title": "Data Case I - Practical Applications in Quantitative Finance",
    "section": "",
    "text": "After analyzing the equally weighted pharmaceutical portfolio, the fund manager was impressed with the performance results. However, they remain skeptical about whether the portfolio truly provides better risk-adjusted returns compared to simply picking one of the best-performing stocks in the industry.\nAs final step, your job is to prove whether the portfolio offers superior risk-adjusted returns by computing the Sharpe Ratio for both the portfolio and its individual stocks in 2024. If the portfolio has a higher Sharpe ratio, it means that diversification helps maximize returns while controlling for risk — an essential argument when managing institutional funds.\nIn order to do that, your task is to provide a visualization of the Sharpe Ratio of the equally-weighted portfolio you’ve just created and compare that to those of the individual stocks. Note that you have already calculated the yearly Sharpe Ratio for each individual stock in Exercise 3, so you just need to filter out for observations where year(date)==2024.\n\n#Individual Stocks\nindividual_sharpe_2024=yearly_returns%&gt;%\n  filter(year(date)==2024)%&gt;%\n  left_join(yearly_volatility)\n\n#Portfolio \nportfolio_returns_2024=financial_data%&gt;%\n  filter(year(date)==2024)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  group_by(date)%&gt;%\n  summarize(daily_return=mean(daily_return,na.rm=TRUE))%&gt;%as.xts()\n\nportfolio_sharpe_2024=data.frame(symbol='Portfolio',\n                                 yearly_return=as.numeric(Return.annualized(portfolio_returns_2024)),\n                                 yearly_volatility=as.numeric(StdDev.annualized(portfolio_returns_2024)))\n\nlibrary(ggrepel)\n\nindividual_sharpe_2024%&gt;%\n  rbind(portfolio_sharpe_2024)%&gt;%\n  mutate(sharpe_ratio=yearly_return/yearly_volatility,\n         color=ifelse(symbol=='Portfolio','Portfolio','Individual Stocks'))%&gt;%\n  ggplot(aes(y=yearly_return,x=yearly_volatility))+\n  geom_abline(slope = 0,intercept = 0,linetype='dashed')+\n  geom_point(aes(size=sharpe_ratio,color=color))+\n  geom_text_repel(aes(label=glue('{symbol}: {round(sharpe_ratio,2)}'),vjust=3),size=3)+\n  theme_minimal()+\n  labs(x='Yearly Volatility',\n       y='Yearly Return',\n       title='A comparison of individual returns and volatilities vis-a-vis portfolio results',\n       subtitle = 'Source: Yahoo! Finance')+\n  scale_y_continuous(labels=percent,limits = c(-0.5,0.5))+\n  scale_x_continuous(labels=percent)+\n  theme(legend.position = 'none')"
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-7",
    "href": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-7",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Based on your analysis of the correlation between each stock, it seems that these pharmaceutical firms are relatively trending together. Notwithstanding, there might be gains from diversification if instead of choosing a specific firm, we decide to hold a portfolio of pharmaceutical stocks.\nInvesting in a single stock exposes an investor to company-specific (idiosyncratic) risk, such as lawsuits, failed drug trials, or regulatory changes. However, constructing a diversified portfolio of multiple stocks within the same industry can help smooth out these risks while still capturing the overall sector trends. For instance, while one pharmaceutical company may experience a stock price drop due to a failed drug trial, another might gain due to a successful FDA approval. By equally weighting multiple stocks, investors can reduce the impact of any single company’s negative performance while still benefiting from the broader industry’s growth.\nYour manager liked your idea and wanted to test it out by creating an equally-weighted portfolio of all pharmaceutical companies over time. Using the tq_transmute() function, create an object, portfolio_returns, that contains the yearly returns of a portfolio that assigns equal weights - in this case, 10% - on each stock, and compare that to the yearly returns of the S&P 500 Index. Would the fund manager be better-off by investing in the portfolio relative to the S&P500?\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the yearly returns for each stock using the tq_transmute() function as before, grouping the data by symbol and creating a new variable, yearly_return.\nKnowing that you have an equally-weighted portfolio, group your data my date and pipe the result into a summarize() function to create a new variable, portfolio_return as the average across all stocks. Assign this result to an object called portfolio_returns\nFetch S&P 500 data using a similar call to tq_get() like you did in the beginning of the exercise, but now collecting data for ^GSPC. Calculate the yearly returns and assign to a new variable, index_return. Store the result in another data.frame, index_returns.\nMerge both datasets using left_join()."
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1.html#wrapping-up-your-analysis",
    "href": "quant-fin-datacases/data-case-1/data-case-1.html#wrapping-up-your-analysis",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Now that you have analyzed the Sharpe ratios of both individual pharmaceutical stocks and the equally weighted portfolio, take a step back and summarize your insights. Did the portfolio offer a better risk-adjusted return compared to individual stocks? If so, why? If not, what might explain the results?\nBased on your findings, what would you recommend to the fund manager? Would you suggest investing in the diversified portfolio, or do certain individual stocks offer superior risk-adjusted returns? Would you propose an alternative weighting scheme, such as a market cap-weighted portfolio, to further improve performance?\nWrite a short conclusion summarizing your key takeaways and justify your investment recommendation using data-driven insights."
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-8",
    "href": "quant-fin-datacases/data-case-1/data-case-1.html#exercise-8",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "After analyzing the equally weighted pharmaceutical portfolio, the fund manager was impressed with the performance results. However, they remain skeptical about whether the portfolio truly provides better risk-adjusted returns compared to simply picking one of the best-performing stocks in the industry.\nAs final step, your job is to prove whether the portfolio offers superior risk-adjusted returns by computing the Sharpe Ratio for both the portfolio and its individual stocks in 2024. If the portfolio has a higher Sharpe ratio, it means that diversification helps maximize returns while controlling for risk — an essential argument when managing institutional funds.\nIn order to do that, your task is to provide a visualization of the Sharpe Ratio of the equally-weighted portfolio you’ve just created and compare that to those of the individual stocks. I have already created the portfolio results for you, so you can copy-paste that to your session:\n\nportfolio_sharpe=data.frame(symbol='Portfolio',\n                            yearly_return=0.03705213,\n                            yearly_volatility=0.1223983)\n\n\n\n\n\n\n\nHint\n\n\n\n\nCreate the portfolio_sharpe in your session using the code chunk above.\nUsing the yearly_returns object you’ve created in Exercise 2, filter by year(date)==2024 and left_join() with the yearly_volatility object you have created in Exercise 3, assigning the result to a new object\nFinally, bind portfolio_sharpe to the resulting data.frame in a rowwise manner using rbind(dataframe1,dataframe2)\nFinally, call ggplot() and adjust the aesthetics to show the relationship between risk (x-axis) and return (y-axis) for all individual stocks and the portfolio."
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1.html#case-outline",
    "href": "quant-fin-datacases/data-case-1/data-case-1.html#case-outline",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "The pharmaceutical industry is a critical sector in financial markets, influenced by regulatory approvals, drug developments, and global health events. In this first Data Case, you will analyze stock performance for a set of 10 pharmaceutical companies over time, applying the tidyverse and tidyquant packages to extract and interpret insights from the data.\nYou are a junior analyst at Atlas Capital, a leading buyside investment firm specializing in sector-focused strategies. The firm is considering increasing its exposure to the pharmaceutical industry, given its long-term growth potential and resilience in volatile markets. In the latest investment committee meeting, your fund manager raised an important question: “How has the pharmaceutical industry performed over time? We need to identify whether now is the right time to increase our position.”\nYour team has been tasked with conducting an in-depth financial analysis of the pharmaceutical sector. The goal is to assess industry-wide trends, identify risks and opportunities, and ultimately recommend an investment stance. More specifically, your task will involve:\n\nCollecting stock price data and compute returns\nVisualizing key trends in returns and volatility\nInterpreting findings and suggest investment insights\n\nTo streamline our research, you will focus on the 10 largest publicly traded pharmaceutical companies in the U.S, analyze their performance, risks, and potential catalysts that could drive returns in the near future. As of February 2025, the 10 largest pharmaceutical companies traded in the U.S., along with their ticker symbols, are:\n\nEli Lilly and Co. (LLY): A leading pharmaceutical company known for its innovative treatments in diabetes and oncology.\nNovo Nordisk A/S (NVO): Specializing in diabetes care, Novo Nordisk has a significant presence in the U.S. market.\nJohnson & Johnson (JNJ): A diversified healthcare company with a strong pharmaceutical division.\nAbbVie Inc. (ABBV): Known for its immunology and oncology products, AbbVie is a major player in the pharmaceutical industry.\nMerck & Co., Inc. (MRK): Merck offers a wide range of prescription medicines, vaccines, and therapies.\nPfizer Inc. (PFE): A global pharmaceutical corporation recognized for its vaccines and therapeutics.\nBristol-Myers Squibb Company (BMY): Focused on oncology, cardiovascular, and immunology, Bristol-Myers Squibb is a key industry player.\nAstraZeneca PLC (AZN): A biopharmaceutical company with a strong portfolio in oncology and respiratory diseases.\nAmgen Inc. (AMGN): Specializing in biotechnology, Amgen develops therapies for serious illnesses.\nGilead Sciences, Inc. (GILD): Known for its antiviral drugs, Gilead has a significant market presence.\n\nNow, it’s up to you and your team to dive into the data, extract key insights, and present your data-driven investment thesis. Good luck—your next career milestone at Atlas Capital depends on it. 🚀\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®."
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1-solutions.html",
    "href": "quant-fin-datacases/data-case-1/data-case-1-solutions.html",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "This Data Case is part of the Practical Applications in Quantitative Finance course, held at FGV-EAESP’s undergraduate course in business. Carefully follow the instructions contained in the data case as well as eClass® before you make your submission.\n\n\nThe pharmaceutical industry is a critical sector in financial markets, influenced by regulatory approvals, drug developments, and global health events. In this first Data Case, you will analyze stock performance for a set of 10 pharmaceutical companies over time, applying the tidyverse and tidyquant packages to extract and interpret insights from the data.\nYou are a junior analyst at Atlas Capital, a leading buyside investment firm specializing in sector-focused strategies. The firm is considering increasing its exposure to the pharmaceutical industry, given its long-term growth potential and resilience in volatile markets. In the latest investment committee meeting, your fund manager raised an important question: “How has the pharmaceutical industry performed over time? We need to identify whether now is the right time to increase our position.”\nYour team has been tasked with conducting an in-depth financial analysis of the pharmaceutical sector. The goal is to assess industry-wide trends, identify risks and opportunities, and ultimately recommend an investment stance. More specifically, your task will involve:\n\nCollecting stock price data and compute returns\nVisualizing key trends in returns and volatility\nInterpreting findings and suggest investment insights\n\nTo streamline our research, you will focus on the 10 largest publicly traded pharmaceutical companies in the U.S, analyze their performance, risks, and potential catalysts that could drive returns in the near future. As of February 2025, the 10 largest pharmaceutical companies traded in the U.S., along with their ticker symbols, are:\n\nEli Lilly and Co. (LLY): A leading pharmaceutical company known for its innovative treatments in diabetes and oncology.\nNovo Nordisk A/S (NVO): Specializing in diabetes care, Novo Nordisk has a significant presence in the U.S. market.\nJohnson & Johnson (JNJ): A diversified healthcare company with a strong pharmaceutical division.\nAbbVie Inc. (ABBV): Known for its immunology and oncology products, AbbVie is a major player in the pharmaceutical industry.\nMerck & Co., Inc. (MRK): Merck offers a wide range of prescription medicines, vaccines, and therapies.\nPfizer Inc. (PFE): A global pharmaceutical corporation recognized for its vaccines and therapeutics.\nBristol-Myers Squibb Company (BMY): Focused on oncology, cardiovascular, and immunology, Bristol-Myers Squibb is a key industry player.\nAstraZeneca PLC (AZN): A biopharmaceutical company with a strong portfolio in oncology and respiratory diseases.\nAmgen Inc. (AMGN): Specializing in biotechnology, Amgen develops therapies for serious illnesses.\nGilead Sciences, Inc. (GILD): Known for its antiviral drugs, Gilead has a significant market presence.\n\nNow, it’s up to you and your team to dive into the data, extract key insights, and present your data-driven investment thesis. Good luck—your next career milestone at Atlas Capital depends on it. 🚀\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®.\n\n\n\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\",\"ggcorrplot\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)\n\n\n\n\nUse the tq_get() function from the tidyquant package to retrieve historical adjusted closing prices for the 10 largest publicly traded pharmaceutical companies in the U.S. from Yahoo! Finance. Your dataset should cover the period from January 1, 2020, to December 31, 2024. Using the functions from the tidyverse, ensure that your data includes only the timestamp column, as well as the column that contains the daily adjusted stock price information. Store this into an object called financial_data (or something similar). Store this data set for all the subsequent analysis - make sure not to override this dataset as you move along the data case to make sure you are always referring to the raw data pull!\n\n#Define the list of assets\nassets &lt;- c('LLY','NVO','JNJ','ABBV','MRK','PFE','BMY','AZN','AMGN','GILD')\nstart_date &lt;- '2020-01-01'\nend_date &lt;- '2024-12-31'\n\nfinancial_data=assets%&gt;%\n  tq_get(from=start_date,\n         to= end_date)%&gt;%\n  select(date,symbol,adjusted)\n\n\n\n\nUsing the tidyquant package, use the object you’ve just created with the tq_transmute function to compute the yearly returns for each stock over the analysis period. More specifically, pass the yearlyReturn function to adjusted column using the tq_transmute, labeling this new variable as yearly_return. Arrange your dataset by year and in descending order of yearly_return (highest-to-lowest). Store this into a new object called, for example, yearly_returns. Which stock had the highest return in 2024, and which one had the lowest? Prompt the results in your session.\n\nyearly_returns=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = yearlyReturn,\n               col_rename = 'yearly_return')%&gt;%\n  arrange(year(date),desc(yearly_return))\n\n#Full analysis\nyearly_returns\n\n# A tibble: 50 × 3\n# Groups:   symbol [10]\n   symbol date       yearly_return\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;\n 1 LLY    2020-12-31        0.303 \n 2 ABBV   2020-12-31        0.263 \n 3 NVO    2020-12-31        0.206 \n 4 JNJ    2020-12-31        0.107 \n 5 PFE    2020-12-31        0.0317\n 6 AZN    2020-12-31        0.0204\n 7 BMY    2020-12-31        0.0104\n 8 AMGN   2020-12-31       -0.0159\n 9 GILD   2020-12-31       -0.0706\n10 MRK    2020-12-31       -0.0830\n# ℹ 40 more rows\n\n#Best and worst performance\nyearly_returns%&gt;%\n  #Ungroup the data to make sure calculations are done rowwise\n  ungroup()%&gt;%\n  #Select only the last year\n  filter(year(date)==2024)%&gt;%\n  #Select only the 1st and 10th \n  filter(rank(yearly_return) %in% c(1,10))\n\n# A tibble: 2 × 3\n  symbol date       yearly_return\n  &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;\n1 LLY    2024-12-30         0.336\n2 NVO    2024-12-30        -0.162\n\n\n\n\n\nWith your data.frame containing the yearly returns over time for each stock, use ggplot to create a line chart of the historical cumulative returns for each stock during the study period. Which stock had the highest cumulative return up-to-date? Recall that cumulative returns can be calculated from period returns as:\n\\[\n\\text{Cumulative Return}= (1+R_1)\\times(1+R_2)\\times ... \\times(1+R_T)-1\\equiv \\prod (1+R_t)-1\n\\]\nYour chart should map date to the x-axis, the yearly return variable to the y axis, and group the results by symbol. To make sure that you are plotting a line chart, use the geom_line() function after you have mapped your data. In addition to these two layers, add any customizations that you believe that are beneficial to convey the message - see the Data Visualization\n\n\n\n\n\n\nHint\n\n\n\n\nWith the data.frame you created to store yearly returns, group by symbol, and use tq_transmute() to apply the Return.cumulative function to the data.\nNow, your resulting data.frame contains the cumulative returns for all stocks. You can adjust the column names with the setNames() function and pipe that into a ggplot call, mapping the symbol to the x-axis, the cumulative return column to the y-axis, and the geom_col() function to create a bar chart. Add as many customizations you think are worth the effort.\n\n\n\n\nyearly_returns%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = yearly_return,\n               mutate_fun = Return.cumulative)%&gt;%\n  setNames(c('symbol','cum_returns'))%&gt;%\n  ggplot(aes(x=reorder(symbol,desc(cum_returns)),y=cum_returns,fill=symbol))+\n  geom_col()+\n  geom_text(aes(label=percent(cum_returns),vjust=-1))+\n  theme_minimal()+\n  labs(x='Stocks',\n       y='Cumulative Return',\n       title='A comparison of individual cumulative returns from selected stocks',\n       subtitle = 'Source: Yahoo! Finance')+\n  scale_y_continuous(labels=percent,limits = c(-0.5,6))+\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n\n\n\nAfter reviewing your initial analysis, your fund manager at Atlas Capital liked the idea of examining yearly returns to get a broader perspective on performance. However, they pointed out that pharmaceutical companies vary significantly in terms of risk exposure, so it’s crucial to account for volatility as well. To complement the analysis, use the same rationale from the previous exercise to calculate the yearly volatility for each stock. How do the risk levels differ between firms? Store your results in a new object and prompt it in your session.\n\n\n\n\n\n\nHint\n\n\n\nAs opposed to the yearlyReturn function, the tidyquant package does not have a pre-built dailyStdev function. Instead, what you can do is to use a combination of functions to get the expected result:\n\nFirst, use tq_transmute() to calculate daily returns passing the dailyReturn function\nNow, your resulting data.frame contains daily returns for all stocks. It is now in a convenient format to chain this object again, in another tq_transmute() function, applying the StdDev.annualized function and assign to a new object, like yearly_volatility. Note, however, that if you simply use StdDev.annualized, it will calculate an annualized metric for each stock for the whole period, which is not what you want.\n\nTo make sure that you have calculating the annualized standard deviation for each year, you can do a composition of apply.yearly, which applies a given function at yearly intervals, and StdDev.annualized, using the following syntax:\n\nyour_daily_return_object%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=StdDev.annualized,\n               col_rename = 'yearly_volatility')\n\nHere, tq_transmute() will apply the function defined in FUN over each interval.\n\n\n\nyearly_volatility=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=StdDev.annualized,\n               col_rename = 'yearly_volatility')\n\n#Full analysis\nyearly_volatility\n\n# A tibble: 50 × 3\n# Groups:   symbol [10]\n   symbol date       yearly_volatility\n   &lt;chr&gt;  &lt;date&gt;                 &lt;dbl&gt;\n 1 LLY    2020-12-31             0.426\n 2 LLY    2021-12-31             0.317\n 3 LLY    2022-12-30             0.273\n 4 LLY    2023-12-29             0.288\n 5 LLY    2024-12-30             0.303\n 6 NVO    2020-12-31             0.315\n 7 NVO    2021-12-31             0.244\n 8 NVO    2022-12-30             0.325\n 9 NVO    2023-12-29             0.306\n10 NVO    2024-12-30             0.357\n# ℹ 40 more rows\n\n#Best and worst performance\nyearly_volatility%&gt;%\n  #Ungroup the data to make sure calculations are done rowwise\n  ungroup()%&gt;%\n  #Select only the last year\n  filter(year(date)==2024)%&gt;%\n  #Select only the 1st and 10th \n  filter(rank(yearly_volatility) %in% c(1,10))\n\n# A tibble: 2 × 3\n  symbol date       yearly_volatility\n  &lt;chr&gt;  &lt;date&gt;                 &lt;dbl&gt;\n1 NVO    2024-12-30             0.357\n2 JNJ    2024-12-30             0.151\n\n\n\n\n\nBuilding on your previous findings, since some companies exhibit higher returns but also greater risk, it might be a good idea to add a risk-adjusted performance metric to the analysis. The Sharpe ratio for stock \\(i\\) in period \\(t\\) measures the risk-adjusted return of an asset and is calculated as:\n\\[\n\\text{Sharpe Ratio}_{i,t}=\\dfrac{R_{i,t}-R_{f,t}}{\\sigma_{i,t}},\n\\]\nwhere \\(R_{i,t}\\) is the return of a given stock \\(i\\) in period \\(t\\), \\(R_{f,t}\\) is the risk-free return for the same period, and \\(\\sigma_{i,t}\\) is the volatility for stock \\(i\\) in period \\(t\\).\nYour task is to calculate the Sharpe Ratio for each pharmaceutical stock using yearly returns and yearly volatility. To simplify your calculations, assume a risk-free rate of \\(0\\%\\) per year (i.e, no risk-free premium). Compare the Sharpe ratios across companies. Do the highest-return stocks also have the best risk-adjusted performance? Are there any stocks that stand out as particularly efficient in generating returns relative to their risk? Are there companies that deliver strong returns but with disproportionately high volatility?\n\n\n\n\n\n\nHint\n\n\n\nThere are two ways you can use to create the Sharpe Ratio:\n\nUsing the previously created yearly_returns and yearly_volatility objects, use the left_join() function to merge them based on a common set of identifiers (in this case, date and symbol). After that, manipulate the resulting data.frame with mutate to generate the Sharpe Ratio.\nUsing tq_transmute in a very similar fashion to what you have done to calculate the yearly volatility, but now passing the the SharpeRatio.annualized function with arguments Rf=0 and scale=252.\n\nAlthough both approaches should yield similar results, potential differences might stem from rounding.\n\n\n\n#Option 1\nyearly_sharpe_1=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=SharpeRatio.annualized,\n               Rf=0,\n               scale=252,\n               col_rename = 'yearly_sharpe')\n\n#Option 2\nyearly_sharpe_2=yearly_returns%&gt;%\n  left_join(yearly_volatility)%&gt;%\n  group_by(symbol)%&gt;%\n  mutate(yearly_sharpe=yearly_return/yearly_volatility)\n\nyearly_sharpe_1\n\n# A tibble: 50 × 3\n# Groups:   symbol [10]\n   symbol date       yearly_sharpe\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;\n 1 LLY    2020-12-31         0.708\n 2 LLY    2021-12-31         2.09 \n 3 LLY    2022-12-30         1.26 \n 4 LLY    2023-12-29         2.13 \n 5 LLY    2024-12-30         1.11 \n 6 NVO    2020-12-31         0.652\n 7 NVO    2021-12-31         2.60 \n 8 NVO    2022-12-30         0.701\n 9 NVO    2023-12-29         1.81 \n10 NVO    2024-12-30        -0.456\n# ℹ 40 more rows\n\nyearly_sharpe_2\n\n# A tibble: 50 × 5\n# Groups:   symbol [10]\n   symbol date       yearly_return yearly_volatility yearly_sharpe\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n 1 LLY    2020-12-31        0.303              0.426        0.711 \n 2 ABBV   2020-12-31        0.263              0.349        0.754 \n 3 NVO    2020-12-31        0.206              0.315        0.654 \n 4 JNJ    2020-12-31        0.107              0.303        0.355 \n 5 PFE    2020-12-31        0.0317             0.357        0.0889\n 6 AZN    2020-12-31        0.0204             0.360        0.0566\n 7 BMY    2020-12-31        0.0104             0.296        0.0352\n 8 AMGN   2020-12-31       -0.0159             0.383       -0.0416\n 9 GILD   2020-12-31       -0.0706             0.375       -0.188 \n10 MRK    2020-12-31       -0.0830             0.318       -0.261 \n# ℹ 40 more rows\n\n\n\n\n\nWay to go! As you delve deeper into your investment analysis, your fund manager emphasizes the importance of understanding how different pharmaceutical stocks interact with one another over time. To gain insights into the relationships between these companies, your next task is to calculate the correlation of daily stock returns for the selected pharmaceutical companies for the analysis period.\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the daily returns for each stock. You can use the tq_transmute function into your dataset and apply the dailyReturn function.\nAfter that, you need to pivot your data in such a way that each column is a specific ticker with information on daily returns. You can do that by calling pivot_wider(names_from='symbol',values_from='daily_return'), assuming that your daily return variable is called daily_return.\nWith that, you’ll achieve a data frame that now has \\(11\\) columns, namely, the date and the \\(10\\) individual ticker columns with daily return information.\nTo make sure that you are calculating the correlation using a \\(10\\times10\\) matrix, use select(-date) to get rid of the date column and pipe that into cor(), which calculates the correlation across all pairs of variables within a data.frame, and outputs a correlation matrix.\n\nIf you want, you can pipe the result into ggcorplot(), a function from the ggcorplot package that provides meaningful visualizations of correlation matrices.\n\n\n\ncorr_returns=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  pivot_wider(names_from='symbol',values_from = 'daily_return')%&gt;%\n  select(-date)%&gt;%\n  cor()\n\ncorr_returns%&gt;%ggcorrplot(hc.order = TRUE, type = \"lower\",lab = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nBased on your analysis of the correlation between each stock, it seems that these pharmaceutical firms are relatively trending together. Notwithstanding, there might be gains from diversification if instead of choosing a specific firm, we decide to hold a portfolio of pharmaceutical stocks.\nInvesting in a single stock exposes an investor to company-specific (idiosyncratic) risk, such as lawsuits, failed drug trials, or regulatory changes. However, constructing a diversified portfolio of multiple stocks within the same industry can help smooth out these risks while still capturing the overall sector trends. For instance, while one pharmaceutical company may experience a stock price drop due to a failed drug trial, another might gain due to a successful FDA approval. By equally weighting multiple stocks, investors can reduce the impact of any single company’s negative performance while still benefiting from the broader industry’s growth.\nYour manager liked your idea and wanted to test it out by creating an equally-weighted portfolio of all pharmaceutical companies over time. Using the tq_transmute() function, create an object, portfolio_returns, that contains the yearly returns of a portfolio that assigns equal weights - in this case, 10% - on each stock, and compare that to the yearly returns of the S&P 500 Index. Would the fund manager be better-off by investing in the portfolio relative to the S&P500?\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the yearly returns for each stock using the tq_transmute() function as before, grouping the data by symbol and creating a new variable, yearly_return.\nKnowing that you have an equally-weighted portfolio, group your data my date and pipe the result into a summarize() function to create a new variable, portfolio_return as the average across all stocks. Assign this result to an object called portfolio_returns\nFetch S&P 500 data using a similar call to tq_get() like you did in the beginning of the exercise, but now collecting data for ^GSPC. Calculate the yearly returns and assign to a new variable, index_return. Store the result in another data.frame, index_returns.\nMerge both datasets using left_join().\n\n\n\n\nportfolio_returns=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = yearlyReturn,\n               col_rename = 'yearly_return')%&gt;%\n  group_by(date)%&gt;%\n  summarize(portfolio_return=mean(yearly_return,na.rm=TRUE))\n\nindex_returns=tq_get('^GSPC',from=start_date,to=end_date)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = yearlyReturn,\n               col_rename = 'index_return')\n\nleft_join(portfolio_returns,index_returns)\n\nJoining with `by = join_by(date)`\n\n\n# A tibble: 5 × 3\n  date       portfolio_return index_return\n  &lt;date&gt;                &lt;dbl&gt;        &lt;dbl&gt;\n1 2020-12-31           0.0772        0.153\n2 2021-12-31           0.295         0.269\n3 2022-12-30           0.208        -0.194\n4 2023-12-29           0.0534        0.242\n5 2024-12-30           0.0449        0.238\n\n\n\n\n\nAfter analyzing the equally weighted pharmaceutical portfolio, the fund manager was impressed with the performance results. However, they remain skeptical about whether the portfolio truly provides better risk-adjusted returns compared to simply picking one of the best-performing stocks in the industry.\nAs final step, your job is to prove whether the portfolio offers superior risk-adjusted returns by computing the Sharpe Ratio for both the portfolio and its individual stocks in 2024. If the portfolio has a higher Sharpe ratio, it means that diversification helps maximize returns while controlling for risk — an essential argument when managing institutional funds.\nIn order to do that, your task is to provide a visualization of the Sharpe Ratio of the equally-weighted portfolio you’ve just created and compare that to those of the individual stocks. I have already created the portfolio results for you, so you can copy-paste that to your session:\n\nportfolio_sharpe=data.frame(symbol='Portfolio',\n                            yearly_return=0.03705213,\n                            yearly_volatility=0.1223983)\n\n\n\n\n\n\n\nHint\n\n\n\n\nCreate the portfolio_sharpe in your session using the code chunk above.\nUsing the yearly_returns object you’ve created in Exercise 2, filter by year(date)==2024 and left_join() with the yearly_volatility object you have created in Exercise 3, assigning the result to a new object\nFinally, bind portfolio_sharpe to the resulting data.frame in a rowwise manner using rbind(dataframe1,dataframe2)\nFinally, call ggplot() and adjust the aesthetics to show the relationship between risk (x-axis) and return (y-axis) for all individual stocks and the portfolio.\n\n\n\n\n#Individual Stocks\nindividual_sharpe_2024=yearly_returns%&gt;%\n  filter(year(date)==2024)%&gt;%\n  left_join(yearly_volatility)\n\n#Portfolio \nportfolio_returns_2024=financial_data%&gt;%\n  filter(year(date)==2024)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  group_by(date)%&gt;%\n  summarize(daily_return=mean(daily_return,na.rm=TRUE))%&gt;%as.xts()\n\nportfolio_sharpe_2024=data.frame(symbol='Portfolio',\n                                 yearly_return=as.numeric(Return.annualized(portfolio_returns_2024)),\n                                 yearly_volatility=as.numeric(StdDev.annualized(portfolio_returns_2024)))\n\n\nlibrary(ggrepel)\n\nindividual_sharpe_2024%&gt;%\n  rbind(portfolio_sharpe_2024)%&gt;%\n  mutate(sharpe_ratio=yearly_return/yearly_volatility,\n         color=ifelse(symbol=='Portfolio','Portfolio','Individual Stocks'))%&gt;%\n  ggplot(aes(y=yearly_return,x=yearly_volatility))+\n  geom_abline(slope = 0,intercept = 0,linetype='dashed')+\n  geom_point(aes(size=sharpe_ratio,color=color))+\n  geom_text_repel(aes(label=glue('{symbol}: {round(sharpe_ratio,2)}'),vjust=3),size=3)+\n  theme_minimal()+\n  labs(x='Yearly Volatility',\n       y='Yearly Return',\n       title='A comparison of individual returns and volatilities vis-a-vis portfolio results',\n       subtitle = 'Source: Yahoo! Finance')+\n  scale_y_continuous(labels=percent,limits = c(-0.5,0.5))+\n  scale_x_continuous(labels=percent)+\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n\n\n\nNow that you have analyzed the Sharpe ratios of both individual pharmaceutical stocks and the equally weighted portfolio, take a step back and summarize your insights. Did the portfolio offer a better risk-adjusted return compared to individual stocks? If so, why? If not, what might explain the results?\nBased on your findings, what would you recommend to the fund manager? Would you suggest investing in the diversified portfolio, or do certain individual stocks offer superior risk-adjusted returns? Would you propose an alternative weighting scheme, such as a market cap-weighted portfolio, to further improve performance?\nWrite a short conclusion summarizing your key takeaways and justify your investment recommendation using data-driven insights."
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#case-outline",
    "href": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#case-outline",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "The pharmaceutical industry is a critical sector in financial markets, influenced by regulatory approvals, drug developments, and global health events. In this first Data Case, you will analyze stock performance for a set of 10 pharmaceutical companies over time, applying the tidyverse and tidyquant packages to extract and interpret insights from the data.\nYou are a junior analyst at Atlas Capital, a leading buyside investment firm specializing in sector-focused strategies. The firm is considering increasing its exposure to the pharmaceutical industry, given its long-term growth potential and resilience in volatile markets. In the latest investment committee meeting, your fund manager raised an important question: “How has the pharmaceutical industry performed over time? We need to identify whether now is the right time to increase our position.”\nYour team has been tasked with conducting an in-depth financial analysis of the pharmaceutical sector. The goal is to assess industry-wide trends, identify risks and opportunities, and ultimately recommend an investment stance. More specifically, your task will involve:\n\nCollecting stock price data and compute returns\nVisualizing key trends in returns and volatility\nInterpreting findings and suggest investment insights\n\nTo streamline our research, you will focus on the 10 largest publicly traded pharmaceutical companies in the U.S, analyze their performance, risks, and potential catalysts that could drive returns in the near future. As of February 2025, the 10 largest pharmaceutical companies traded in the U.S., along with their ticker symbols, are:\n\nEli Lilly and Co. (LLY): A leading pharmaceutical company known for its innovative treatments in diabetes and oncology.\nNovo Nordisk A/S (NVO): Specializing in diabetes care, Novo Nordisk has a significant presence in the U.S. market.\nJohnson & Johnson (JNJ): A diversified healthcare company with a strong pharmaceutical division.\nAbbVie Inc. (ABBV): Known for its immunology and oncology products, AbbVie is a major player in the pharmaceutical industry.\nMerck & Co., Inc. (MRK): Merck offers a wide range of prescription medicines, vaccines, and therapies.\nPfizer Inc. (PFE): A global pharmaceutical corporation recognized for its vaccines and therapeutics.\nBristol-Myers Squibb Company (BMY): Focused on oncology, cardiovascular, and immunology, Bristol-Myers Squibb is a key industry player.\nAstraZeneca PLC (AZN): A biopharmaceutical company with a strong portfolio in oncology and respiratory diseases.\nAmgen Inc. (AMGN): Specializing in biotechnology, Amgen develops therapies for serious illnesses.\nGilead Sciences, Inc. (GILD): Known for its antiviral drugs, Gilead has a significant market presence.\n\nNow, it’s up to you and your team to dive into the data, extract key insights, and present your data-driven investment thesis. Good luck—your next career milestone at Atlas Capital depends on it. 🚀\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®."
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#tech-setup",
    "href": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#tech-setup",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\",\"ggcorrplot\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)"
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-1",
    "href": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-1",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Use the tq_get() function from the tidyquant package to retrieve historical adjusted closing prices for the 10 largest publicly traded pharmaceutical companies in the U.S. from Yahoo! Finance. Your dataset should cover the period from January 1, 2020, to December 31, 2024. Using the functions from the tidyverse, ensure that your data includes only the timestamp column, as well as the column that contains the daily adjusted stock price information. Store this into an object called financial_data (or something similar). Store this data set for all the subsequent analysis - make sure not to override this dataset as you move along the data case to make sure you are always referring to the raw data pull!\n\n#Define the list of assets\nassets &lt;- c('LLY','NVO','JNJ','ABBV','MRK','PFE','BMY','AZN','AMGN','GILD')\nstart_date &lt;- '2020-01-01'\nend_date &lt;- '2024-12-31'\n\nfinancial_data=assets%&gt;%\n  tq_get(from=start_date,\n         to= end_date)%&gt;%\n  select(date,symbol,adjusted)"
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-2",
    "href": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-2",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Using the tidyquant package, use the object you’ve just created with the tq_transmute function to compute the yearly returns for each stock over the analysis period. More specifically, pass the yearlyReturn function to adjusted column using the tq_transmute, labeling this new variable as yearly_return. Arrange your dataset by year and in descending order of yearly_return (highest-to-lowest). Store this into a new object called, for example, yearly_returns. Which stock had the highest return in 2024, and which one had the lowest? Prompt the results in your session.\n\nyearly_returns=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = yearlyReturn,\n               col_rename = 'yearly_return')%&gt;%\n  arrange(year(date),desc(yearly_return))\n\n#Full analysis\nyearly_returns\n\n# A tibble: 50 × 3\n# Groups:   symbol [10]\n   symbol date       yearly_return\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;\n 1 LLY    2020-12-31        0.303 \n 2 ABBV   2020-12-31        0.263 \n 3 NVO    2020-12-31        0.206 \n 4 JNJ    2020-12-31        0.107 \n 5 PFE    2020-12-31        0.0317\n 6 AZN    2020-12-31        0.0204\n 7 BMY    2020-12-31        0.0104\n 8 AMGN   2020-12-31       -0.0159\n 9 GILD   2020-12-31       -0.0706\n10 MRK    2020-12-31       -0.0830\n# ℹ 40 more rows\n\n#Best and worst performance\nyearly_returns%&gt;%\n  #Ungroup the data to make sure calculations are done rowwise\n  ungroup()%&gt;%\n  #Select only the last year\n  filter(year(date)==2024)%&gt;%\n  #Select only the 1st and 10th \n  filter(rank(yearly_return) %in% c(1,10))\n\n# A tibble: 2 × 3\n  symbol date       yearly_return\n  &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;\n1 LLY    2024-12-30         0.336\n2 NVO    2024-12-30        -0.162"
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-3",
    "href": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-3",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "With your data.frame containing the yearly returns over time for each stock, use ggplot to create a line chart of the historical cumulative returns for each stock during the study period. Which stock had the highest cumulative return up-to-date? Recall that cumulative returns can be calculated from period returns as:\n\\[\n\\text{Cumulative Return}= (1+R_1)\\times(1+R_2)\\times ... \\times(1+R_T)-1\\equiv \\prod (1+R_t)-1\n\\]\nYour chart should map date to the x-axis, the yearly return variable to the y axis, and group the results by symbol. To make sure that you are plotting a line chart, use the geom_line() function after you have mapped your data. In addition to these two layers, add any customizations that you believe that are beneficial to convey the message - see the Data Visualization\n\n\n\n\n\n\nHint\n\n\n\n\nWith the data.frame you created to store yearly returns, group by symbol, and use tq_transmute() to apply the Return.cumulative function to the data.\nNow, your resulting data.frame contains the cumulative returns for all stocks. You can adjust the column names with the setNames() function and pipe that into a ggplot call, mapping the symbol to the x-axis, the cumulative return column to the y-axis, and the geom_col() function to create a bar chart. Add as many customizations you think are worth the effort.\n\n\n\n\nyearly_returns%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = yearly_return,\n               mutate_fun = Return.cumulative)%&gt;%\n  setNames(c('symbol','cum_returns'))%&gt;%\n  ggplot(aes(x=reorder(symbol,desc(cum_returns)),y=cum_returns,fill=symbol))+\n  geom_col()+\n  geom_text(aes(label=percent(cum_returns),vjust=-1))+\n  theme_minimal()+\n  labs(x='Stocks',\n       y='Cumulative Return',\n       title='A comparison of individual cumulative returns from selected stocks',\n       subtitle = 'Source: Yahoo! Finance')+\n  scale_y_continuous(labels=percent,limits = c(-0.5,6))+\n  theme(legend.position = 'none')"
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-4",
    "href": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-4",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "After reviewing your initial analysis, your fund manager at Atlas Capital liked the idea of examining yearly returns to get a broader perspective on performance. However, they pointed out that pharmaceutical companies vary significantly in terms of risk exposure, so it’s crucial to account for volatility as well. To complement the analysis, use the same rationale from the previous exercise to calculate the yearly volatility for each stock. How do the risk levels differ between firms? Store your results in a new object and prompt it in your session.\n\n\n\n\n\n\nHint\n\n\n\nAs opposed to the yearlyReturn function, the tidyquant package does not have a pre-built dailyStdev function. Instead, what you can do is to use a combination of functions to get the expected result:\n\nFirst, use tq_transmute() to calculate daily returns passing the dailyReturn function\nNow, your resulting data.frame contains daily returns for all stocks. It is now in a convenient format to chain this object again, in another tq_transmute() function, applying the StdDev.annualized function and assign to a new object, like yearly_volatility. Note, however, that if you simply use StdDev.annualized, it will calculate an annualized metric for each stock for the whole period, which is not what you want.\n\nTo make sure that you have calculating the annualized standard deviation for each year, you can do a composition of apply.yearly, which applies a given function at yearly intervals, and StdDev.annualized, using the following syntax:\n\nyour_daily_return_object%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=StdDev.annualized,\n               col_rename = 'yearly_volatility')\n\nHere, tq_transmute() will apply the function defined in FUN over each interval.\n\n\n\nyearly_volatility=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=StdDev.annualized,\n               col_rename = 'yearly_volatility')\n\n#Full analysis\nyearly_volatility\n\n# A tibble: 50 × 3\n# Groups:   symbol [10]\n   symbol date       yearly_volatility\n   &lt;chr&gt;  &lt;date&gt;                 &lt;dbl&gt;\n 1 LLY    2020-12-31             0.426\n 2 LLY    2021-12-31             0.317\n 3 LLY    2022-12-30             0.273\n 4 LLY    2023-12-29             0.288\n 5 LLY    2024-12-30             0.303\n 6 NVO    2020-12-31             0.315\n 7 NVO    2021-12-31             0.244\n 8 NVO    2022-12-30             0.325\n 9 NVO    2023-12-29             0.306\n10 NVO    2024-12-30             0.357\n# ℹ 40 more rows\n\n#Best and worst performance\nyearly_volatility%&gt;%\n  #Ungroup the data to make sure calculations are done rowwise\n  ungroup()%&gt;%\n  #Select only the last year\n  filter(year(date)==2024)%&gt;%\n  #Select only the 1st and 10th \n  filter(rank(yearly_volatility) %in% c(1,10))\n\n# A tibble: 2 × 3\n  symbol date       yearly_volatility\n  &lt;chr&gt;  &lt;date&gt;                 &lt;dbl&gt;\n1 NVO    2024-12-30             0.357\n2 JNJ    2024-12-30             0.151"
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-5",
    "href": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-5",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Building on your previous findings, since some companies exhibit higher returns but also greater risk, it might be a good idea to add a risk-adjusted performance metric to the analysis. The Sharpe ratio for stock \\(i\\) in period \\(t\\) measures the risk-adjusted return of an asset and is calculated as:\n\\[\n\\text{Sharpe Ratio}_{i,t}=\\dfrac{R_{i,t}-R_{f,t}}{\\sigma_{i,t}},\n\\]\nwhere \\(R_{i,t}\\) is the return of a given stock \\(i\\) in period \\(t\\), \\(R_{f,t}\\) is the risk-free return for the same period, and \\(\\sigma_{i,t}\\) is the volatility for stock \\(i\\) in period \\(t\\).\nYour task is to calculate the Sharpe Ratio for each pharmaceutical stock using yearly returns and yearly volatility. To simplify your calculations, assume a risk-free rate of \\(0\\%\\) per year (i.e, no risk-free premium). Compare the Sharpe ratios across companies. Do the highest-return stocks also have the best risk-adjusted performance? Are there any stocks that stand out as particularly efficient in generating returns relative to their risk? Are there companies that deliver strong returns but with disproportionately high volatility?\n\n\n\n\n\n\nHint\n\n\n\nThere are two ways you can use to create the Sharpe Ratio:\n\nUsing the previously created yearly_returns and yearly_volatility objects, use the left_join() function to merge them based on a common set of identifiers (in this case, date and symbol). After that, manipulate the resulting data.frame with mutate to generate the Sharpe Ratio.\nUsing tq_transmute in a very similar fashion to what you have done to calculate the yearly volatility, but now passing the the SharpeRatio.annualized function with arguments Rf=0 and scale=252.\n\nAlthough both approaches should yield similar results, potential differences might stem from rounding.\n\n\n\n#Option 1\nyearly_sharpe_1=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=SharpeRatio.annualized,\n               Rf=0,\n               scale=252,\n               col_rename = 'yearly_sharpe')\n\n#Option 2\nyearly_sharpe_2=yearly_returns%&gt;%\n  left_join(yearly_volatility)%&gt;%\n  group_by(symbol)%&gt;%\n  mutate(yearly_sharpe=yearly_return/yearly_volatility)\n\nyearly_sharpe_1\n\n# A tibble: 50 × 3\n# Groups:   symbol [10]\n   symbol date       yearly_sharpe\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;\n 1 LLY    2020-12-31         0.708\n 2 LLY    2021-12-31         2.09 \n 3 LLY    2022-12-30         1.26 \n 4 LLY    2023-12-29         2.13 \n 5 LLY    2024-12-30         1.11 \n 6 NVO    2020-12-31         0.652\n 7 NVO    2021-12-31         2.60 \n 8 NVO    2022-12-30         0.701\n 9 NVO    2023-12-29         1.81 \n10 NVO    2024-12-30        -0.456\n# ℹ 40 more rows\n\nyearly_sharpe_2\n\n# A tibble: 50 × 5\n# Groups:   symbol [10]\n   symbol date       yearly_return yearly_volatility yearly_sharpe\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n 1 LLY    2020-12-31        0.303              0.426        0.711 \n 2 ABBV   2020-12-31        0.263              0.349        0.754 \n 3 NVO    2020-12-31        0.206              0.315        0.654 \n 4 JNJ    2020-12-31        0.107              0.303        0.355 \n 5 PFE    2020-12-31        0.0317             0.357        0.0889\n 6 AZN    2020-12-31        0.0204             0.360        0.0566\n 7 BMY    2020-12-31        0.0104             0.296        0.0352\n 8 AMGN   2020-12-31       -0.0159             0.383       -0.0416\n 9 GILD   2020-12-31       -0.0706             0.375       -0.188 \n10 MRK    2020-12-31       -0.0830             0.318       -0.261 \n# ℹ 40 more rows"
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-6",
    "href": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-6",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Way to go! As you delve deeper into your investment analysis, your fund manager emphasizes the importance of understanding how different pharmaceutical stocks interact with one another over time. To gain insights into the relationships between these companies, your next task is to calculate the correlation of daily stock returns for the selected pharmaceutical companies for the analysis period.\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the daily returns for each stock. You can use the tq_transmute function into your dataset and apply the dailyReturn function.\nAfter that, you need to pivot your data in such a way that each column is a specific ticker with information on daily returns. You can do that by calling pivot_wider(names_from='symbol',values_from='daily_return'), assuming that your daily return variable is called daily_return.\nWith that, you’ll achieve a data frame that now has \\(11\\) columns, namely, the date and the \\(10\\) individual ticker columns with daily return information.\nTo make sure that you are calculating the correlation using a \\(10\\times10\\) matrix, use select(-date) to get rid of the date column and pipe that into cor(), which calculates the correlation across all pairs of variables within a data.frame, and outputs a correlation matrix.\n\nIf you want, you can pipe the result into ggcorplot(), a function from the ggcorplot package that provides meaningful visualizations of correlation matrices.\n\n\n\ncorr_returns=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  pivot_wider(names_from='symbol',values_from = 'daily_return')%&gt;%\n  select(-date)%&gt;%\n  cor()\n\ncorr_returns%&gt;%ggcorrplot(hc.order = TRUE, type = \"lower\",lab = TRUE)"
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-7",
    "href": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-7",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Based on your analysis of the correlation between each stock, it seems that these pharmaceutical firms are relatively trending together. Notwithstanding, there might be gains from diversification if instead of choosing a specific firm, we decide to hold a portfolio of pharmaceutical stocks.\nInvesting in a single stock exposes an investor to company-specific (idiosyncratic) risk, such as lawsuits, failed drug trials, or regulatory changes. However, constructing a diversified portfolio of multiple stocks within the same industry can help smooth out these risks while still capturing the overall sector trends. For instance, while one pharmaceutical company may experience a stock price drop due to a failed drug trial, another might gain due to a successful FDA approval. By equally weighting multiple stocks, investors can reduce the impact of any single company’s negative performance while still benefiting from the broader industry’s growth.\nYour manager liked your idea and wanted to test it out by creating an equally-weighted portfolio of all pharmaceutical companies over time. Using the tq_transmute() function, create an object, portfolio_returns, that contains the yearly returns of a portfolio that assigns equal weights - in this case, 10% - on each stock, and compare that to the yearly returns of the S&P 500 Index. Would the fund manager be better-off by investing in the portfolio relative to the S&P500?\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the yearly returns for each stock using the tq_transmute() function as before, grouping the data by symbol and creating a new variable, yearly_return.\nKnowing that you have an equally-weighted portfolio, group your data my date and pipe the result into a summarize() function to create a new variable, portfolio_return as the average across all stocks. Assign this result to an object called portfolio_returns\nFetch S&P 500 data using a similar call to tq_get() like you did in the beginning of the exercise, but now collecting data for ^GSPC. Calculate the yearly returns and assign to a new variable, index_return. Store the result in another data.frame, index_returns.\nMerge both datasets using left_join().\n\n\n\n\nportfolio_returns=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = yearlyReturn,\n               col_rename = 'yearly_return')%&gt;%\n  group_by(date)%&gt;%\n  summarize(portfolio_return=mean(yearly_return,na.rm=TRUE))\n\nindex_returns=tq_get('^GSPC',from=start_date,to=end_date)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = yearlyReturn,\n               col_rename = 'index_return')\n\nleft_join(portfolio_returns,index_returns)\n\nJoining with `by = join_by(date)`\n\n\n# A tibble: 5 × 3\n  date       portfolio_return index_return\n  &lt;date&gt;                &lt;dbl&gt;        &lt;dbl&gt;\n1 2020-12-31           0.0772        0.153\n2 2021-12-31           0.295         0.269\n3 2022-12-30           0.208        -0.194\n4 2023-12-29           0.0534        0.242\n5 2024-12-30           0.0449        0.238"
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-8",
    "href": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#exercise-8",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "After analyzing the equally weighted pharmaceutical portfolio, the fund manager was impressed with the performance results. However, they remain skeptical about whether the portfolio truly provides better risk-adjusted returns compared to simply picking one of the best-performing stocks in the industry.\nAs final step, your job is to prove whether the portfolio offers superior risk-adjusted returns by computing the Sharpe Ratio for both the portfolio and its individual stocks in 2024. If the portfolio has a higher Sharpe ratio, it means that diversification helps maximize returns while controlling for risk — an essential argument when managing institutional funds.\nIn order to do that, your task is to provide a visualization of the Sharpe Ratio of the equally-weighted portfolio you’ve just created and compare that to those of the individual stocks. I have already created the portfolio results for you, so you can copy-paste that to your session:\n\nportfolio_sharpe=data.frame(symbol='Portfolio',\n                            yearly_return=0.03705213,\n                            yearly_volatility=0.1223983)\n\n\n\n\n\n\n\nHint\n\n\n\n\nCreate the portfolio_sharpe in your session using the code chunk above.\nUsing the yearly_returns object you’ve created in Exercise 2, filter by year(date)==2024 and left_join() with the yearly_volatility object you have created in Exercise 3, assigning the result to a new object\nFinally, bind portfolio_sharpe to the resulting data.frame in a rowwise manner using rbind(dataframe1,dataframe2)\nFinally, call ggplot() and adjust the aesthetics to show the relationship between risk (x-axis) and return (y-axis) for all individual stocks and the portfolio.\n\n\n\n\n#Individual Stocks\nindividual_sharpe_2024=yearly_returns%&gt;%\n  filter(year(date)==2024)%&gt;%\n  left_join(yearly_volatility)\n\n#Portfolio \nportfolio_returns_2024=financial_data%&gt;%\n  filter(year(date)==2024)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  group_by(date)%&gt;%\n  summarize(daily_return=mean(daily_return,na.rm=TRUE))%&gt;%as.xts()\n\nportfolio_sharpe_2024=data.frame(symbol='Portfolio',\n                                 yearly_return=as.numeric(Return.annualized(portfolio_returns_2024)),\n                                 yearly_volatility=as.numeric(StdDev.annualized(portfolio_returns_2024)))\n\n\nlibrary(ggrepel)\n\nindividual_sharpe_2024%&gt;%\n  rbind(portfolio_sharpe_2024)%&gt;%\n  mutate(sharpe_ratio=yearly_return/yearly_volatility,\n         color=ifelse(symbol=='Portfolio','Portfolio','Individual Stocks'))%&gt;%\n  ggplot(aes(y=yearly_return,x=yearly_volatility))+\n  geom_abline(slope = 0,intercept = 0,linetype='dashed')+\n  geom_point(aes(size=sharpe_ratio,color=color))+\n  geom_text_repel(aes(label=glue('{symbol}: {round(sharpe_ratio,2)}'),vjust=3),size=3)+\n  theme_minimal()+\n  labs(x='Yearly Volatility',\n       y='Yearly Return',\n       title='A comparison of individual returns and volatilities vis-a-vis portfolio results',\n       subtitle = 'Source: Yahoo! Finance')+\n  scale_y_continuous(labels=percent,limits = c(-0.5,0.5))+\n  scale_x_continuous(labels=percent)+\n  theme(legend.position = 'none')"
  },
  {
    "objectID": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#wrapping-up-your-analysis",
    "href": "quant-fin-datacases/data-case-1/data-case-1-solutions.html#wrapping-up-your-analysis",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Now that you have analyzed the Sharpe ratios of both individual pharmaceutical stocks and the equally weighted portfolio, take a step back and summarize your insights. Did the portfolio offer a better risk-adjusted return compared to individual stocks? If so, why? If not, what might explain the results?\nBased on your findings, what would you recommend to the fund manager? Would you suggest investing in the diversified portfolio, or do certain individual stocks offer superior risk-adjusted returns? Would you propose an alternative weighting scheme, such as a market cap-weighted portfolio, to further improve performance?\nWrite a short conclusion summarizing your key takeaways and justify your investment recommendation using data-driven insights."
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#welcome-to-the-course",
    "href": "quant-fin/coursework/Introduction/index.html#welcome-to-the-course",
    "title": "Introduction and Course Overview",
    "section": "Welcome to the Course",
    "text": "Welcome to the Course\n\n\nOverview and Course Organization\nGrading and Evaluations\nNavigating through the syllabus\nHow you can get the best of this course\nOverall Q&A"
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#overview",
    "href": "quant-fin/coursework/Introduction/index.html#overview",
    "title": "Introduction and Course Overview",
    "section": "Overview",
    "text": "Overview\n\nWith the recent advances in technical resources and the vast availability of financial information, finance practitioners are required to generate reproducible and scalable analysis in a timely fashion to guide decision-making:\n\nHow can I continuously optimize the risk-return trade-off of my portfolio over time?\nWhat is the sensitivity of my investment decisions to changes in growth rates, discount rates, and expected margins?\nHow can I assess the ability of a given trading strategy to outperform the market over time?\n\n\n\n\\(\\rightarrow\\) To that point, departing from the general tools, such as Excel, to more advanced tools, such as  and , is an imperative change!\n\nThe goal of this course: translate theoretical concepts learned on the core finance courses to practical applications that can guide decision making in real-world financial markets"
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#course-structure",
    "href": "quant-fin/coursework/Introduction/index.html#course-structure",
    "title": "Introduction and Course Overview",
    "section": "Course Structure",
    "text": "Course Structure\n\nThis is a hands-on, practical course on Quantitative Finance with applications using  and , two of the most widely used open-source software for data analysis. It will be structured in topics that are of interest to Finance practitioners, aiming to include, but not limited to:\n\nCollecting and organizing financial data\nCAPM, Fama-French, and multi-factor models of risk\nEquily Valuation, sensitivity analysis, and Simulation\nPortfolio optimization and strategy backtesting\nEvent Analysis\nSentiment Analysis\n\nStudents are also expected to interact with leading industry practitioners focused on financial applications using open-source languages, aiming to discover more about the possibilities of applying the skills learned in this course in the financial industry"
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#bibliography",
    "href": "quant-fin/coursework/Introduction/index.html#bibliography",
    "title": "Introduction and Course Overview",
    "section": "Bibliography",
    "text": "Bibliography\n\n\nCoursework: we will mostly follow Tidy Finance (Scheuch, Voigt, and Weiss 2023), as our text-book. Other relevant reading materials include:\n\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\nMastering Shiny (Wickham 2021)\n\nSupplementary Reading:\n\nCorporate Finance (Berk and DeMarzo 2023) - a companion for finance-related topics\nOther optional contents, such as Harvard Business School (HBS) Cases\n\n\n\n\n\n\n\n\n\nImportant\n\n\nMost of the references listed in this bibliography have an open-source version that is hosted online, where you can copy-paste code chunks directly into your  session. All contents with restricted access will be provided upfront."
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#grading-and-evaluations",
    "href": "quant-fin/coursework/Introduction/index.html#grading-and-evaluations",
    "title": "Introduction and Course Overview",
    "section": "Grading and Evaluations",
    "text": "Grading and Evaluations\n\nGrading will be composed of the following activities:\n\nHandout Data Cases (40%)\nCapstone Project (40%)\nProject Showcase (15%)\nIn-class Participation (5%)\n\n\n\n\nYou can find the details of any of these activities in the official syllabus (available on eClass®)\nIn case of any questions, feel free to reach out to lucas.macoris@fgv.br\n\n\n\n\n\n\n\n\n\nOffice-hours\n\n\nI also host office-hours (by appointment) on Thursdays, 5PM-6PM. In these sessions, I’ll be more than happy to help you with anything you need from this course. Use the Office-hour Appointments link at the bottom of this slide to schedule an appointment (or click here)."
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#getting-the-best-of-this-course",
    "href": "quant-fin/coursework/Introduction/index.html#getting-the-best-of-this-course",
    "title": "Introduction and Course Overview",
    "section": "Getting the best of this course",
    "text": "Getting the best of this course"
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#getting-the-best-of-this-course-continued",
    "href": "quant-fin/coursework/Introduction/index.html#getting-the-best-of-this-course-continued",
    "title": "Introduction and Course Overview",
    "section": "Getting the best of this course, continued",
    "text": "Getting the best of this course, continued\n\nTech setup: in the official page of your course, you will find instructions on how to properly set up your computer in terms of downloading all necessary softwares, packages, and customizing your  session1\nCode Replication: right after we are done with a given topic, try to replicate the in-class handouts on your end and check if you are able to yield the same outputs\nShowcase: programming, data science, analytics, machine learning, and so on…these terms are on the hype of today’s job market - although few people really know how to make meaningful impact with it. Use this course as an opportunity to differentiate and showcase the skills you’ve learned and stand out to potential employers2\n\nWhile we will be using FGV-EAESP’s facilities, where you will have immediate access to all the necessary configurations, ensure to setup your personal computer so as to ensure that you can replicate the contents from the lectures.See Artificial Intelligence is losing hype (The Economist)"
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#on-the-usage-of-gen-ai",
    "href": "quant-fin/coursework/Introduction/index.html#on-the-usage-of-gen-ai",
    "title": "Introduction and Course Overview",
    "section": "On the usage of gen-AI",
    "text": "On the usage of gen-AI\n\n\n\n\n\n\nOn the usage of ChatGPT and other gen-AI tools\n\n\nGenerative Artificial Intelligent (gen-AI) adoption is quickly spreading through corporate life and universities. At this point, it is worth the question…am I allowed to use gen-AI tools in this course?\n\n\n\n\nThe answer is yes! Not only you are allowed, but also encouraged to do so:\n\nUse gen-AI tools to proof-read your work, get insights, and troubleshoot errors\nLearn to be skeptical around the solutions you have been provided with\nEvaluations will be based on how you can interpret, understand, and showcase your solution to the broader audience!\n\n\n\n\\(\\rightarrow\\) See AI-powered coding pulls in almost $1bn of funding to claim ‘killer app’ status (Financial Times)"
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#how-to-use-these-slides",
    "href": "quant-fin/coursework/Introduction/index.html#how-to-use-these-slides",
    "title": "Introduction and Course Overview",
    "section": "How to use these slides",
    "text": "How to use these slides\n\nThese slides leverage Quarto, an open-source scientific and technical publishing system from Posit (formerly RStudio):\n\nCreate dynamic content with , , , among other programming languages\nPublish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, Word, ePub, and more\nWrite beatufil, clean technical documents using markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more\n\nFor our course, we’ll use the following notation:\n\n\nLinks will be colored in blue\nInline equations and variables will be rendered in gray\nCode chunks will be provided along with outputs (R)"
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#an-example-of-a-code-chunk",
    "href": "quant-fin/coursework/Introduction/index.html#an-example-of-a-code-chunk",
    "title": "Introduction and Course Overview",
    "section": "An example of a code chunk",
    "text": "An example of a code chunk\n\nResultR\n\n\n\n\n\n\n\n\nNote\n\n\nIn the R panel, hit Show the Code to display the code inside the tabset. Hit the  button at the top-right to copy it to your session.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\ndat &lt;- data.frame(cond = rep(c(\"A\", \"B\"), each=10),\n                  xvar = 1:20 + rnorm(20,sd=3),\n                  yvar = 1:20 + rnorm(20,sd=3))\n\nggplot(dat, aes(x=xvar, y=yvar)) +\n  geom_point(shape=1) + \n  geom_smooth()"
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#interactive-learning",
    "href": "quant-fin/coursework/Introduction/index.html#interactive-learning",
    "title": "Introduction and Course Overview",
    "section": "Interactive Learning",
    "text": "Interactive Learning\n\n\n\nListing 1: We will extensively leverage interactive learning whenever possible. In selected sections, called Listings, you will be prompted with an interactive R console that you can use to run existing and new code to a “virtual” session. Try changing the ticker to NVDA and check if anything has changed."
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#interactive-learning-continued",
    "href": "quant-fin/coursework/Introduction/index.html#interactive-learning-continued",
    "title": "Introduction and Course Overview",
    "section": "Interactive Learning, continued",
    "text": "Interactive Learning, continued\n\nExerciseHintsSolution\n\n\n\n\n\nListing 2: You can use the Hints and Solution buttons to interact with the prompt. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution. For example, complete the code to read Microsoft price (MSFT.csv) data and select the latest 10 OHLC (Open, High, Low, Close) information. The dataset is arranged in descendant format (latest price information is at the bottom of the table).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can access the names of the columns using names(Data) to find the column that refers to closing prices:\nnames(Data)\nYou can also select specific columns by using the select() function:\nselect(Data,c(column1,column2,...,column_n))`\n\n\n\n\nFirst, use the names() function to retrieve the names of the columns available in the dataset.\nAfter that, use the tail() function to retrieve only the latest 10 observations.\n\n\n\n#Set the ticker\nData=read.csv(glue('Assets/MSFT.csv'))\nData=select(Data,c(Timestamp,Open,High,Low,Close))\nData=tail(Data,10)\nData\n\n#Set the ticker\nData=read.csv(glue('Assets/MSFT.csv'))\nData=select(Data,c(Timestamp,Open,High,Low,Close))\nData=tail(Data,10)\nData"
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#why-r",
    "href": "quant-fin/coursework/Introduction/index.html#why-r",
    "title": "Introduction and Course Overview",
    "section": "Why R?",
    "text": "Why R?\n\nWe will be using  throughout the course. Although you should feel that this course is language-agnostic, R is among one of the best choices for the task:\n\n\n\nFree and open-source\nDiverse and active community working on a broad range of tools\nActively maintained packages for various purposes - e.g., data manipulation, visualization, modeling, etc)\nPowerful tools for communication, such as Quarto and Shiny\nSmooth integration with other programming languages, e.g., SQL, Python, C, C++, Fortran\nRStudio is one of the best development environments for interactive data analysis\nTop-notch tools and packages for handling financial data and analysis1\n\n\nFor a comprehensive list of R packages designed for finance, please refer to the R Task View: Empirical Finance index - access here."
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#setting-up-your-environment",
    "href": "quant-fin/coursework/Introduction/index.html#setting-up-your-environment",
    "title": "Introduction and Course Overview",
    "section": "Setting up your environment",
    "text": "Setting up your environment\n\nAs we get started, there are a couple of things you should remember:\n\n works with libraries, which consists of a bundle of functions, methods, data and other components that can be loaded in your session (i.e, as you open RStudio or any other IDE of your preference)\nTo load a library, you call library(x), where x refers to the package name\n\n\nIf x is already installed in your computer, you are good to go\nIf, on the other hand, x is not installed, you need to call install.packages('x') before you attempt to load it\n\n\ninstall.packages() needs to be called once; library() needs to be called at the beginning of each session!"
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#setting-up-your-environment-continued",
    "href": "quant-fin/coursework/Introduction/index.html#setting-up-your-environment-continued",
    "title": "Introduction and Course Overview",
    "section": "Setting up your environment, continued",
    "text": "Setting up your environment, continued\n\nTo make things easier, ensure to install these packages in your computer and load it at the beginning of every session - I’ll make sure to update this list whenever needed throughout the sessions in the course’s official webpage\n\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\",\"glue\",\"scales\",\"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\nlapply(packages, library, character.only = TRUE)\n\n#Note that you could simply do it iteratively:\n\n#Install if not already available\n  #install.packages('tidyverse')\n  #install.packages('tidyquant')\n  #install.packages('tidymodels')\n  #install.packages('xts')\n  #install.packages('glue')\n  #install.packages('scales')\n  #install.packages('ggthemes')\n\n#Load\n  #library(tidyverse)\n  #library(tidyquant)\n  #library(tidymodels)\n  #library(xts)\n  #library(glue)\n  #library(scales)"
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#other-things-you-should-consider-quarto",
    "href": "quant-fin/coursework/Introduction/index.html#other-things-you-should-consider-quarto",
    "title": "Introduction and Course Overview",
    "section": "Other things you should consider: Quarto",
    "text": "Other things you should consider: Quarto\n\nIn this course, you’ll be assigned with three data cases, where you’ll need to manipulate code and write your insights altogether. You may have heard of Jupyter Notebooks before as a way to do it. I want to encourage you to give Quarto a try"
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#other-things-you-should-consider-quarto-continued",
    "href": "quant-fin/coursework/Introduction/index.html#other-things-you-should-consider-quarto-continued",
    "title": "Introduction and Course Overview",
    "section": "Other things you should consider: Quarto, continued",
    "text": "Other things you should consider: Quarto, continued\n\nTo install Quarto, follow this link and choose your Operating System. RStudio will automatically locate it and make it as an option:\n\n\n\n\n\n\nKey Highlights:\n\nMulti-language support (Python, R, Julia, JavaScript) and seamless integration with GitHub\nAdvanced document formatting and output options: you can choose pdf, html, docx, or even a reveal.js presentation (like the one you’re reading right now)"
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#getting-started",
    "href": "quant-fin/coursework/Introduction/index.html#getting-started",
    "title": "Introduction and Course Overview",
    "section": "Getting started",
    "text": "Getting started\n\nTo get started, you’ll need to:\n\n\n\nInstall R using this link\nInstall RStudio using this link\nInstall Quarto using this link\n\n\n\n\nIf you’re new to R or need a refresher on the basics, please refer to Projects I and II of the Hands-On Programming with R (Grolemund 2014)\n\n\n\n\n\n\n\n\n\nTech-setup\n\n\nIn the official webpage of this course, I have outlined all necessary steps to get started using R, as well as some useful tips for those that want to get up to speed on the course’s requirements - please follow this link and carefully read the instructions."
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#course-pre-assessment",
    "href": "quant-fin/coursework/Introduction/index.html#course-pre-assessment",
    "title": "Introduction and Course Overview",
    "section": "Course pre-assessment",
    "text": "Course pre-assessment\n\nFill out the form below, share your thoughts, and help me tailor the course to meet your needs and track your progress 🙂"
  },
  {
    "objectID": "quant-fin/coursework/Introduction/index.html#references",
    "href": "quant-fin/coursework/Introduction/index.html#references",
    "title": "Introduction and Course Overview",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with R. Sebastopol, CA: O’Reilly Media.\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley. 2021. Mastering Shiny. O’Reilly Media. https://mastering-shiny.org/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#outline",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#outline",
    "title": "Bridging Finance with Programming",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\nTidy Finance (Scheuch, Voigt, and Weiss 2023)\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\n\n\n\n\n\n\n\n\nCoding Replications\n\n\nFor coding replications, whenever applicable, please follow this page or hover on the specific slides with containing coding chunks.\n\nEnsure that you have your  session properly set-up according to the instructions outlined in the course webpage\nAlong with the slides, this lecture will also contain a replication file, in .qmd format, containing a thorough discussion for all examples that have been showcased. This file, that will be posted on eClass®, can be downloaded and replicated on your side. To do that, download the file, open it up in RStudio, and render the Quarto document using the Render button (shortcut: Ctrl+Shift+K).\nAt the end of this lecture, you will be prompted with a hands-on exercise to test your skills using the tools you’ve learned as you made your way through the slides. A suggested solution will be provided in the replication file."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tools-of-the-trade-part-i-the-data",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tools-of-the-trade-part-i-the-data",
    "title": "Bridging Finance with Programming",
    "section": "The Tools of the Trade, part I: the data",
    "text": "The Tools of the Trade, part I: the data\n\nFor most of the topics within the study of finance, there is a well-grounded, established use of statistical, economic, and mathematical concepts that set the stage for data analysis:\n\nMacroeconomic analysts use time-series models to predict future interest rates\nFinancial analysts study the potential effects in stock prices of issuing equity\nHedge Fund Managers use models to predict inflation and adjust their positions\n\n\n\n\nBack in the pre-internet era, the use of technology to support those activities was limited to a smaller set of players (e.g, hedge funds, banks, investment trusts). Nowadays, financial information is accessible to the broader public almost in real time:\n\nYahoo! Finance provides data on stocks, ETFs, and market indices\nEDGAR provides information on all periodic fillings provided by US-listed companies\nA wide range of social media platforms, such as X (formerly Twitter) and Reddit, have been recently use as a way to spread and collect financial information"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tools-of-the-trade-part-ii-the-technology",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tools-of-the-trade-part-ii-the-technology",
    "title": "Bridging Finance with Programming",
    "section": "The Tools of the Trade, part II: the technology",
    "text": "The Tools of the Trade, part II: the technology\n\nNot only the availability of financial data, but also the necessary technology to process it, were among the bottlenecks for the adoption of such methods in financial practice\nNowadays, the widespread adoption of open-source technologies, such as  and , helped bridging the gap towards a more inclusive environment for those methods\nDespite such advances, one quickly learns that the actual implementation of models to solve problems in the area of financial economics is typically rather opaque:\n\nThere is lack of public, centralized code readily available for use\nAnalysts employ a substantial amount of wasteful efforts trying to replicate results\n\n\n\n\nIt is often said that more than 80 percent of data analysis is spent on preparing data rather than analyzing it. How do you solve for that?"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#why-tidy",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#why-tidy",
    "title": "Bridging Finance with Programming",
    "section": "Why Tidy?",
    "text": "Why Tidy?\n\nIt is often said that more than 80 percent of data analysis is spent on preparing data rather than analyzing it\nAs you start working with data, you quickly realize that you indeed spend a lot of time reading, cleaning, and transforming your data just\n\n\n\n\n\n\n\nA note on Tidy Data\n\n\n“Tidy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).”\n\n\n\n\nIn its essence, tidy data mainly follows three principles:\n\nEvery column is a variable\nEvery row is an observation\nEvery cell is a single value"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#why-tidy-continued",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#why-tidy-continued",
    "title": "Bridging Finance with Programming",
    "section": "Why Tidy? Continued",
    "text": "Why Tidy? Continued\n\nIn addition to the data layer, there are also tidy coding principles outlined in the tidy tools manifesto that we’ll try to follow:\n\nReuse existing data structures\nCompose simple functions with chaining methods\nEmbrace functional programming\nDesign for humans, improved readability\n\nLuckily, the  community has already took a stab at creating tools and organizing a unified approach towards tidy analysis\nAmongst a diverse set of option for tidy data manipulation, the tidyverse contains packages that follow a unified approach"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#introducing-the-tidyverse",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#introducing-the-tidyverse",
    "title": "Bridging Finance with Programming",
    "section": "Introducing: the tidyverse",
    "text": "Introducing: the tidyverse\n\n\n\n\nThe tidyverse is an opinionated collection of  packages designed for data science\nAll packages share an underlying design philosophy, grammar, and data structures\nIt is supported by Posit, the maintainer of RStudio and R’s largest open-source contributor1\nYou can install the complete tidyverse using:\n\n\ninstall.packages(\"tidyverse\")\n\n\nTo load tidyverse in your session, simply run:\n\n\nlibrary(tidyverse)\n\n\n\n\n\n\n\n\n\nPosit recently hired Wes McKinney, the creator of pandas, highlighting its efforts to harmonize innovations among Python users."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-dplyr",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-dplyr",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: dplyr",
    "text": "The tidyverse packages: dplyr\n\n\ndplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges:\n\n\n\n\n\n\nmutate() adds new variables that are functions of existing variables\nselect() picks variables based on their names\nfilter() picks cases based on their values\nsummarise() reduces multiple values down to a single summary\narrange() changes the ordering of the rows\n\n\nKey Highlights\n\nThese all combine with group_by(), allowing users to perform operations groupwise\nLazy evaluation methods, as well as the pipe operator, %&gt;%, increases code readability and reproducibility"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#using-dplyr",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#using-dplyr",
    "title": "Bridging Finance with Programming",
    "section": "Using dplyr",
    "text": "Using dplyr"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-ggplot2",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-ggplot2",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: ggplot2",
    "text": "The tidyverse packages: ggplot2\n\n\nThe core tidyverse includes the packages that you’re likely to use in everyday data analyses. As of its 1.3.0 version, the following packages are included in the core tidyverse:\n\n\n\n\n\n\nggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics\nYou provide the data, tell how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details\n\n\nKey Highlights\n\nIt is, by and large, the richest and most widely used plotting ecosystem in the  language\nggplot2 has a rich ecosystem of extensions - ranging from annotations and interactive visualizations to specialized genomics - click here a community maintained list"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#using-ggplot2",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#using-ggplot2",
    "title": "Bridging Finance with Programming",
    "section": "Using ggplot2",
    "text": "Using ggplot2"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-tidyr",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-tidyr",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: tidyr",
    "text": "The tidyverse packages: tidyr\n\n\nThe goal of tidyr is to help you create tidy data. Tidy data is data where:\n\n\n\n\n\n\nEach variable is a column; each column is a variable\nEach observation is a row; each row is an observation\nEach value is a cell; each cell is a single value\n\n\nKey Highlights\n\nTidy data describes a standard way of storing data that is used wherever possible throughout the tidyverse\nIt makes it easier to put reshape data in a way that it can be used as an input to other tidyverse packages"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#using-tidyr",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#using-tidyr",
    "title": "Bridging Finance with Programming",
    "section": "Using tidyr",
    "text": "Using tidyr\n\n\n\n\n\n\n\n\n\n\n Download raw data"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#accessing-and-managing-financial-data-1",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#accessing-and-managing-financial-data-1",
    "title": "Bridging Finance with Programming",
    "section": "Accessing and Managing Financial Data",
    "text": "Accessing and Managing Financial Data\n\nEverybody who has experience working with data is also familiar with storing and reading data in formats like .csv, .xls, .xlsx or other delimited value storage\nHowever, if your goal is to replicate a common task at a predefined time interval, like charting weekly stock prices for a selected bundle of stocks every end-of-week, it might be overwhelming to manually perform these tasks every week\nIn what follows, we’ll dive in the various sources of financial data - both global as well as specific to the Brazilian financial markets that can be directly fed into your R session:\n\nWe will cover the most widely used free data resources for Finance, like Yahoo! Finance\nWe will also discuss linkages to private information sources, such as Bloomberg\nFinally, we will take a look at some output data examples from some data providers"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-basics-stock-level-information",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-basics-stock-level-information",
    "title": "Bridging Finance with Programming",
    "section": "The basics: stock-level information",
    "text": "The basics: stock-level information\n\nSo… you have been prompted with the task of collecting daily stock price information for a subset of the U.S Big Techs. How should you do it?\nIn a nutshell, Yahoo! Finance is your go-to guy:\n\nIt provides financial news, data and commentary including stock quotes, press releases, financial reports, and original content\nIt has an extensive list of open-source solutions that enables users to retrieve financial information using several coding languages\n\nHighlights: free, quick and easy to setup, with an impressive range of data containing stock prices, dividends, and splits. There is an extensive list of R packages can be used to retrieve Yahoo! Finance information - including, but not limited to, tidyquant, quantmod and yfR\nDrawbacks: its API is no longer a fully official API: as a consequence, solutions tipically used to retrieve information may not work in the future if Yahoo Finance change its structure. Importantly, data is not in real-time - often, it comes with a 15-minute delay (see here)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#using-yahoo-finance-continued",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#using-yahoo-finance-continued",
    "title": "Bridging Finance with Programming",
    "section": "Using Yahoo! Finance, continued",
    "text": "Using Yahoo! Finance, continued\n\n\nBelow, you can find an example of how to use tq_get(), from the tidyquant package, to download both single and multiple stock price information\nData is stored in a convenient way that allows users to manipulate data seamlessly - hit Download Data and see how the output would look like in Excel format\n\n\n#Load tidyquant\nlibrary(tidyquant)\n\n#Using tidyquant to download single stock prices\ntq_get('AAPL',from='2020-01-01',to='2024-12-31')\n\n#Using tidyquant to download multiple stock prices\ntq_get(c('AAPL','GOOGL','NVDA'),from='2020-01-01',to='2024-12-31')\n\n\n\n Download Data\n\n\n\n\n\n\n\n\nImportant\n\n\nYahoo! Finance provides Open, High, Low, Close, and Adjusted Close trading prices for each asset that is being tracked, where Adjusted Close is defined by the closing price adjusted for dividends and stock splits. If you are using R, Python, or any API to pull this data, ensure to use the information adjusted by dividends and splits."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#macroeconomic-data-providers",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#macroeconomic-data-providers",
    "title": "Bridging Finance with Programming",
    "section": "Macroeconomic data providers",
    "text": "Macroeconomic data providers\nApart from price-level information, there are plenty of available resources to efficiently download the most commonly used macroeconomic variables directly within an R session:\n\nThe Federal Reserve Bank of St. Louis has as extense set of U.S and international time series from more than 100 sources via its API, FRED, for free\n\n\\(\\rightarrow\\) Related packages: tidyquant, FredR, quantmod, and quandl\n\nThe World Bank’s International Debt Statistics (IDS) provides creditor-debtor relationships between countries, regions, and institutions\n\n\\(\\rightarrow\\) Related packages: wbids\n\nThe European Central Bank’s Statistical Data Warehouse provides data on Euro area monetary policy, financial stability, and other relevant topics\n\n\\(\\rightarrow\\) Related packages: ecb"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#macroeconomic-data-providers-examples",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#macroeconomic-data-providers-examples",
    "title": "Bridging Finance with Programming",
    "section": "Macroeconomic data providers, examples",
    "text": "Macroeconomic data providers, examples\n\nFREDIBSECB\n\n\n\n#Load the tidyquant library\nlibrary(tidyquant)\n\n#Go to FRED's website, search for a time series, and copy-paste its code\nseries='CUSR0000SETB01'\n\n#Use the tq_get() function to retrieve the information\ntq_get(series,get='economic.data')\n\n\n\n Download Data\n\n\n\\(\\rightarrow\\) For full details and implementation of the R package tidyquant, click here\n\n\n\n#Load the wbids package\nlibrary(wbids)\n\n#Get information for Brasil, Russia, \nids_get(\n  geographies = c(\"BRA\", \"ARG\"),\n  series = c(\"DT.MAT.DPPG\"), #Average maturity on new external debt commitments (years)\n  counterparts = c(\"302\"), #United States\n  start_year = 2000,\n  end_year = 2023\n)\n\n\n\n Download Data\n\n\n\\(\\rightarrow\\) For full details and implementation of the R package wbids, click here\n\n\n\n#Load the ecb package\nlibrary(ecb)\n\n#Get information of headline and core inflation for Eurozone countries\nkey &lt;- \"ICP.M.DE+FR+ES+IT+NL+U2.N.000000+XEF000.4.ANR\"\n\n#Get the latest 12 observations\nfilter &lt;- list(lastNObservations = 12, detail = \"full\")\n\n#Retrieve the data\nhicp &lt;- get_data(key, filter)\n\n#Parse time component to proper format\nhicp$obstime &lt;- convert_dates(hicp$obstime)\n\n\\(\\rightarrow\\) For full details and implementation of the R package ecb, click here"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#financial-data-providers",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#financial-data-providers",
    "title": "Bridging Finance with Programming",
    "section": "Financial data providers",
    "text": "Financial data providers\n\nFor some widely known paid data providers, there are interfaces that enable analysts to retrieve information directly within an R session through the provider’s official API1\n\n\nBloomberg: the Rblpapi provides access to data and calculations from Bloomberg\nRefinitiv Eikon: the DatastreamDSWS2R provides a set of functions and a class to connect, extract and upload information from the LSEG Datastream database\nQuandl: publishes free/paid data, scraped from many different sources from the web. The Quandl package can be used to retrieve data\nSimfin: fundamental financial data freely available to private investors, researchers, and students. The simfinapi package can be used to retrieve data\nFMP: accurate financial data (balance-sheet, income statements, etc), with historical information dating back 30 years in history. The fmpapi package can be used to retrieve data\n\nFor paid data providers, you must provide your API key in order to successfully download data."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#other-data-providers-and-r-packages",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#other-data-providers-and-r-packages",
    "title": "Bridging Finance with Programming",
    "section": "Other data providers (and R packages)",
    "text": "Other data providers (and R packages)\n\nBanco Central do Brasil (BACEN): interface to the Brazilian Central Bank web services - see package rbcb\nTesouro Direto (Brazilian Government Bonds): prices and yields of bonds issued by the Brazilian government - see package GetTDData\nCoinMarketCap: provides cryptocurrency information and historical prices - see package crypto2\nAlpha Vantage: free and paid subscriptions for financial data (including intraday) - see package alphavantager\n\n\n\n\n\n\n\nWrapping up on data providers\n\n\nWhile some data providers provide their official API for developers, other solutions rely on scraping historical data from the web. As such, some solutions can deprecated after some time if, for example, access is blocked. It is always important to check whether an R package is not deprecated by looking into the Comprehensive R Archive Network (CRAN) or its GitHub repository."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-purrr",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-purrr",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: purrr",
    "text": "The tidyverse packages: purrr\n\n\nThe goal of purrr is to enhances R’s functional programming toolkit by providing a complete and consistent set of tools for working with functions and vectors\n\n\n\n\n\n\nFunctional programming allows you to replace many for loops with code that is both more succinct and easier to read\nYou provide a function and a list of elements to map to, and purrr takes care of the nitty-gritty details\n\n\nKey Highlights\n\nIt seamlessly integrates with all tidyverse packages and functions, allowing users to apply functional programming in the most straightforward way possible\nSimplifies the code pipeline to solve fairly realistic problems - e.g, estimating the CAPM for 100+ industries where we have a different number of observations per industry"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-readr",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-readr",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: readr",
    "text": "The tidyverse packages: readr\n\n\nThe goal of readr is to provide a fast and friendly way to read rectangular data from delimited files, such as comma-separated values (.csv) and tab-separated values (.tsv)\n\n\n\n\n\n\nIt is designed to parse many types of data found in the wild, while providing an informative problem report when parsing leads to unexpected results\nHandles column-type guessing, allowing users to specify how it should parse information, providing informative problem reports when parsing leads to unexpected results\n\n\nKey Highlights\n\nIs generally much faster than base R functions (up to 10x-100x), depending on the dataset\nAll functions work exactly the same way regardless of the current locale (e.g., thousands and decimal separators)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-tibble",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-tibble",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: tibble",
    "text": "The tidyverse packages: tibble\n\n\nThe tibble package provides a modern reimagining of a data.frame, keeping what time has proven to be effective, and throwing out what is not\n\n\n\n\n\n\nTibbles are a modern take on data frames. They keep the features that have stood the test of time, and drop the features that used to be convenient but are now frustrating\nIt is a nice way to create data frames. It encapsulates best practices for data frames and handles various data formats in an easier way\n\n\nKey Highlights\n\nTibbles also have an enhanced print() method which makes them easier to use with large datasets containing complex objects.\nIt can store various data formats in a data-frame-like format (e.g, store a whole list as a column)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-stringr",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-stringr",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: stringr",
    "text": "The tidyverse packages: stringr\n\n\nThe stringr package provides a cohesive set of functions designed to make working with strings (e.g, qualitative data, such as stock tickers, names, etc) as easy as possible:\n\n\n\n\n\n\nstr_detect() tells you if there’s any match to the pattern\nstr_locate() gives the position of the match\nstr_count() counts the number of pattern\nstr_subset() extracts the matching components\nstr_extract() extracts the text of the match\nstr_match() extracts parts of the match defined by parentheses\nstr_replace() replaces the matches with new text\nstr_split() splits up a string into multiple pieces"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-forcats",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#the-tidyverse-packages-forcats",
    "title": "Bridging Finance with Programming",
    "section": "The tidyverse packages: forcats",
    "text": "The tidyverse packages: forcats\n\n\nThe goal of the forcats package is to provide a suite of tools that solve common problems with factors, variables that have a fixed and known set of possible values (e.g, a vector that contains all possible days in a week)\n\n\n\n\n\n\nfct_reorder() reorders a factor by another variable\nfct_infreq() reorders a factor by the frequency of values\nfct_relevel() changes the order of a factor by hand\nfct_lump() collapses the least/most frequent values of a factor into a consolidated group\n\n\nKey Highlights\n\nWorking with factors makes it easier to display, visualize, and communicate data\nExplicitly defining a variable as a factor handles several issues regarding inserting new data"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#references",
    "href": "quant-fin/coursework/Lecture 1 - Bridging Finance with Programming/index.html#references",
    "title": "Bridging Finance with Programming",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#outline",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#outline",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\nTidy Finance (Scheuch, Voigt, and Weiss 2023)\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\n\n\n\n\n\n\n\n\nCoding Replications\n\n\nFor coding replications, whenever applicable, please follow this page or hover on the specific slides with containing coding chunks.\n\nEnsure that you have your  session properly set-up according to the instructions outlined in the course webpage\nAlong with the slides, this lecture will also contain a replication file, in .qmd format, containing a thorough discussion for all examples that have been showcased. This file, that will be posted on eClass®, can be downloaded and replicated on your side. To do that, download the file, open it up in RStudio, and render the Quarto document using the Render button (shortcut: Ctrl+Shift+K).\nAt the end of this lecture, you will be prompted with a hands-on exercise to test your skills using the tools you’ve learned as you made your way through the slides. A suggested solution will be provided in the replication file."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#organizing-financial-data",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#organizing-financial-data",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Organizing Financial Data",
    "text": "Organizing Financial Data\n\n\n\n\n\n\nA note on Tidy Data\n\n\n“Tidy datasets are all alike, but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).”\n\n\n\n\nStock price information is an example of raw data that can be easily pulled from providers such as Yahoo! Finance. However, it is not often structured in a tidy and convenient way\n\nIn this lecture, we will be working with daily stock price data from the Magnificent Seven (AAPL, GOOG, MSFT, NVDA, TSLA, AMZN, and META)\nI have already downloaded the data for you using the tidyquant package, which allows us to pull stock price data from multiple securities in a convenient format. You can hit the Download button to get a grasp on how the data looks like\n\n\n\n\n Download"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-basic-dplyr-verbs-recap",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-basic-dplyr-verbs-recap",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The basic dplyr verbs: recap",
    "text": "The basic dplyr verbs: recap\n\n\ndplyr is a grammar of data manipulation, contained in the tidyverse, providing a consistent set of verbs that help you solve the most common data manipulation challenges:\n\n\n\n\n\n\nmutate() adds new variables that are functions of existing variables\nselect() picks variables based on their names\nfilter() picks cases based on their values\nsummarise() reduces multiple values down to a single summary\narrange() changes the ordering of the rows"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-mutate-function",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-mutate-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The mutate() function",
    "text": "The mutate() function\n\n\n\n\n\n\nDefinition\n\n\nThe mutate() function adds new variables that are functions of existing variables:\n\nmutate(.data, #The object you are performing the calculations \n       new_variable_1 = var1 * 2, #Can use basic operations...\n       new_variable_2 = median(var2), #Or predefined functions)\n       variable_3 = as.character(var3) #And can be used to modify existing variables)\n       ) \n\n\n\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nIt sequentially creates the columns you asked for and place them to the right of your data.frame (or tibble)\nYou can use any function, predefined or custom, and apply it to mutate()\nIt can also modify any columns you want (if the name is the same as an existing column)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-mutate-function-practice",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-mutate-function-practice",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The mutate() function, practice",
    "text": "The mutate() function, practice\n\nExerciseSolution\n\n\n\n\n\nListing 1: Use columns high and low and create a new column, mid, defined as the average between daily high and low prices. The M7 dataset has been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, use the mutate() function to create the mid column.\nAfter that, define mid as (high+low)/2 to calculate the average between the two values.\n\n\n\n#Apply function to the data\nM7=mutate(M7, mid= (high+low)/2)\n\n#Show the first 10 observations\nhead(M7)\n\n#Apply function to the data\nM7=mutate(M7, mid= (high+low)/2)\n\n#Show the first 10 observations\nhead(M7)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-select-function",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-select-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The select() function",
    "text": "The select() function\n\n\n\n\n\n\nDefinition\n\n\nThe select() function select (and optionally rename) variables in a data frame, using a concise mini-language that makes it easy to refer to variables based on their name (e.g. a:f selects all columns from a on the left to f on the right) or type (e.g. where(is.numeric) selects all numeric columns)\n\nselect(.data, #The object which you are performing the operations \n       variable_3, #Can reorder columns\n       variable_1, \n       variable_2:variable_4, #Matches position patterns \n       where(is.numeric) #Can select all columns that match a given pattern\n       ) \n\n\n\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd select only the columns you’ve asked for\nYou can also use select(.data,-variable) to remove a variable\nIt keeps the structure of the data.frame intact - no rows are affected"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-select-function-continued",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-select-function-continued",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The select() function, continued",
    "text": "The select() function, continued\n\nThe select() function also comes with a handy companion of selectors, which are functions that help you cherry pick columns in a concise way, rather than hardcoding them altogether:\n\n: for selecting a range of consecutive variables.\nstarts_with() starts with a string\nends_with() ends with a string\ncontains() contains a string\nmatches()matches a regular expression.\nwhere()a function to all variables and selects those for which the function returns TRUE"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-select-function-practice",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-select-function-practice",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The select() function, practice",
    "text": "The select() function, practice\n\nExerciseSolution\n\n\n\n\n\nListing 2: Select only the symbol, date, volume, and adjusted, in that order. The M7 dataset has been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the select() function to select only a subset of the available columns:\n\n\n\n#Apply function to the data\nM7=select(M7,symbol,date,volume,adjusted)\n\n#Show the first 10 observations\nhead(M7)\n\n#Apply function to the data\nM7=select(M7,symbol,date,volume,adjusted)\n\n#Show the first 10 observations\nhead(M7)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-filter-function",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-filter-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The filter() function",
    "text": "The filter() function\n\n\n\n\n\n\nDefinition\n\n\nThe filter() function is used to subset a data frame, retaining all rows that satisfy your conditions. To be retained, the row must produce a value of TRUE for all conditions.\n\nfilter(.data, #The object which you are performing the operations\n       variable_1 &gt;10, #Simple arithmetic operators\n       variable_2 %in% c('AAPL','MSFT','FORD'), #Pattern search\n       !(variable_3 %in% c('Boston','Mass','Silicon Valley')), #Negate pattern search\n       variable_4 &gt;=10 & variable_3&lt;= 4 | is.na(variable_4) #IF and OR conditions\n       ) \n\n\n\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd filter the rows based on the conditions outlined\nYou can use any function, predefined or custom, and apply it to filter()\nIt returns a subset of the whole object, keeping the columns and the data structure intact"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-filter-function-practice",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-filter-function-practice",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The filter() function, practice",
    "text": "The filter() function, practice\n\nExerciseSolution\n\n\n\n\n\nListing 3: Filter for observations that occurred in 2025, only. You can use the year() function with the date variable to retrieve the year. The M7 dataset has been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the filter() together with year(date):\n\n\n\n#Apply function to the data\nM7=filter(M7,year(date)==2025)\n\n#Show the first 10 observations\nhead(M7)\n\n#Apply function to the data\nM7=filter(M7,year(date)==2025)\n\n#Show the first 10 observations\nhead(M7)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-arrange-function",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-arrange-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The arrange() function",
    "text": "The arrange() function\n\n\n\n\n\n\nDefinition\n\n\nThe arrange() function reorders the rows of a data frame by the values of selected columns:\n\n#Some Options, always in the following format: the object you are rearranging + the reordering scheme\narrange(.data, variable1) #Ascending by variable_1\narrange(.data, variable1, variable_2) #Ascending by variable_1 and then variable_2\narrange(.data, variable2, variable_1) #Ascending by variable_2 and then variable 1\narrange(.data, variable1, desc(variable_2)) #Ascending by variable_1, and then descending by variable_2\n\n\n\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd reorders the rows of your data.frame (or tibble)\nThis can be useful for visualization, but also for applying position-dependent functions, like lag(), lead(), head(), and tail()"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-arrange-function-practice",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-arrange-function-practice",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The arrange() function, practice",
    "text": "The arrange() function, practice\n\nExerciseSolution\n\n\n\n\n\nListing 4: Arrange the dataset by descending date (newest to oldest) and symbol. The M7 dataset has been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the arrange() together with desc(date) and symbol:\n\n\n\n#Apply function to the data\nM7=arrange(M7,desc(date),symbol)\n\n#Show the first 10 observations\nhead(M7)\n\n#Apply function to the data\nM7=arrange(M7,desc(date),symbol)\n\n#Show the first 10 observations\nhead(M7)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-summarize-function",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-summarize-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The summarize() function",
    "text": "The summarize() function\n\n\n\n\n\n\nDefinition\n\n\nThe summarise() - or summarize() - function creates a new data frame. It returns one row for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarising all observations in the input. It will contain one column for each grouping variable and one column for each of the summary statistics that you have specified.\n\nsummarize(.data, #The object which you are performing the operations \n       new_variable_1 = mean(var1,na.rm=TRUE), #Average of var1, removing NA values\n       new_variable_2 = median(var2,na.rm=TRUE), #Median of var1, removing, NA values\n       new_variable_3 = n_distinct(var2) #Number of unique values of var2\n       ) \n\n\n\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd reshapes the data.frame (or tibble) by the aggregation functions\nAs the name suggests, it is used to summarize a table"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-summarize-function-practice",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#the-summarize-function-practice",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "The summarize() function, practice",
    "text": "The summarize() function, practice\n\nExerciseSolution\n\n\n\n\n\nListing 5: Summarize the dataset by creating an average column, defined as the average adjusted prices. You can use the mean() function to get the average. Use the option na.rm=TRUE inside the mean function to make sure that NA values are disregarded. The M7 dataset has been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the summarize() together with mean():\n\n\n\n#Apply function to the data\nSummary=summarize(M7,average=mean(adjusted,na.rm=TRUE))\n\n#Show the first observations\nhead(Summary)\n\n#Apply function to the data\nSummary=summarize(M7,average=mean(adjusted,na.rm=TRUE))\n\n#Show the first observations\nhead(Summary)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#slice-and-dice-through-group_by",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#slice-and-dice-through-group_by",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Slice and dice through group_by()",
    "text": "Slice and dice through group_by()\n\nYou saw how the tidyverse verbs helps us getting ahead of the game when it comes to data operations. In most cases, however, you may need to add an extra layer of complexity: perform operations groupwise:\n\nGet the average returns by each stock\nFilter for the 10 highest prices for each year\nCalculate the median return for each industry\n\nFor cases like this, we need to find a convenient way of repeating the same operation across subsets of our data\nIt goes without saying that there should be a function in the tidyverse that makes this operation straightforward: fortunately, you can use group_by() together with all previous dplyr verbs!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#slice-and-dice-through-group_by-continued",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#slice-and-dice-through-group_by-continued",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Slice and dice through group_by(), continued",
    "text": "Slice and dice through group_by(), continued\n\n\n\n\n\n\nDefinition\n\n\nThe group_by() function takes an existing table and converts it into a grouped table where operations are performed “by group”. Using ungroup() removes grouping.\n\nData=group_by(Data,v1,v2,v3)\nData=summarize(avg=mean(x,na.rm=TRUE))\n\n\n\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd creates the avg variable taking the average of x within each tuple defined by the grouping variables (in this case, v1,v2, and v3 )\nIt returns a grouped dataframe, with the results of avg displayed for each unique combination of v1,v2, and v3"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#revisiting-average-prices",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#revisiting-average-prices",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Revisiting average prices",
    "text": "Revisiting average prices\n\nLet’s try the latest summarize() call again, but now grouping the data by symbol first"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#combining-multiple-operations",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#combining-multiple-operations",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Combining multiple operations",
    "text": "Combining multiple operations\n\nIn the previous exercises, you have used the main dplyr verbs to create, select, arrange, filter, and summarize data, one by one. In practical applications, however it is likely that you need more than one of these functionalities at the same time\nIt is tempting to do it piecewise:\n\n\n#Start with the data\nData = read.csv('Data.csv')\n#Mutate\nData = mutate(Data, new_var_1=var_1*10)\n#Select\nData = select(Data, var_1,var_2,new_var_1,where(is.numeric))\n#Filter\nData = filter(Data, new_var_1&gt;5)\n#Arrange\nData = arrange(Data, new_var_1,desc(var2))\n#Summarize\nData = summarize(Data, new_var=mean(new_var_1,na.rm=TRUE))\n\n\nAlthough organized, it is wildly inefficient: you are sequentially (re)creating the same object all over again - not to mention that you called Data \\(10\\) times!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#pipe-your-way-through-the-code",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#pipe-your-way-through-the-code",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Pipe your way through the code %>%",
    "text": "Pipe your way through the code %&gt;%\n\nThe dplyr verbs, in isolation, are a great tool for data analysts, but what really makes them to shine is what glues them together: I introduce you the pipe (%&gt;% or |&gt;)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#pipe-your-way-through-the-code-continued",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#pipe-your-way-through-the-code-continued",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Pipe your way through the code %>%, continued",
    "text": "Pipe your way through the code %&gt;%, continued\n\nRené Magritte was right when he claimed that, in The Treachery of Images (La Trahison des images), there was not a pipe\nIn fact, the pipe that is relevant for us, R users, was only introduced recently, in the magrittr package, which makes clear allusion to the artist\nThe pipe operator (%&gt;% or |&gt;) helps you chain operations sequentially, in such a way that the output of one operation serves as the input of the subsequent one!\n\nInstead of repeating the .data input multiple times, you chain the operations using the pipe operator\nIt tremendously improve code readability and minimizes spelling errors (now you only need to type in the .data argument once!)\nFinally, it makes your code much more efficient: you don’t need to allocate memory to (re)create the same object in each step"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#pipe-your-way-through-the-code-continued-1",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#pipe-your-way-through-the-code-continued-1",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Pipe your way through the code %>%, continued",
    "text": "Pipe your way through the code %&gt;%, continued\n\n#Instead of \nData = read.csv('Data.csv') #Start with the data\nData = mutate(Data, new_var_1=var_1*10)#Mutate\nData = select(Data, var_1,var_2,new_var_1,where(is.numeric))#Select\nData = filter(Data, new_var_1&gt;5)#Filter\nData = arrange(Data, new_var_1,desc(var2))#Arrange\nData = summarize(Data, new_var=mean(new_var_1,na.rm=TRUE))#Summarize\n\n#Do\nData = read.csv('Data.csv')%&gt;% #Start with the data\n        mutate(new_var_1=var_1*10)%&gt;% #Mutate\n        select(var_1,var_2,new_var_1,where(is.numeric))%&gt;% #Select\n        filter(new_var_1&gt;5)%&gt;% #Filter\n        arrange(new_var_1,desc(var2))%&gt;% #Arrange\n        summarize(new_var=mean(new_var_1,na.rm=TRUE))#Summarize\n\n\nThe pipe operator lets you pass the object on its left-hand side to the first argument of the function on the right-hand side\nAnother nice feature in R is lazy evaluation: function arguments are only evaluated if (and when) they are accessed. This allows us to refer to variables that will only be created within the pipe without breaking the code!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#practical-exercise",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#practical-exercise",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Practical Exercise",
    "text": "Practical Exercise\n\nOn January \\(25^{th}\\), chinese startup DeepSeek disrupted the tech stock market as investors reassessed the likely future investment in Artificial Intelligence hardware\n\n\n\n\n\n\n\\(\\rightarrow\\) Read: Tech stocks slump as China’s DeepSeek stokes fears over AI spending (Financial Times)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#hands-on-exercise",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#hands-on-exercise",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\nAs part of your work as a buy-side analyst, you were asked to analyze how the Magnificent 7 performed after the DeepSeek\nFollow the instructions and answer to the following question: which stock suffered the most during January 2025?\n\n\nTo answer this question, you will be using all dplyr verbs you’ve practiced so far\nFurthermore, you will be also using some common base R and ther dplyr functions, like lag(), prod(), as.Date() and drop_na()\n\n\nThe expected result is a data.frame object that shows, for each symbol, the monthly return on January, 2025, ordered from lowest-to-highest\n\n\\(\\rightarrow\\) Suggested solution will be provided in the replication file for this lecture."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#hands-on-exercise-continued",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#hands-on-exercise-continued",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Hands-On Exercise, continued",
    "text": "Hands-On Exercise, continued\n\n\n\n\n\n\nInstructions\n\n\nThe data, stored in M7.csv, can be loaded using read.csv('M7.csv'). You can download it using the link shown in Slide 4.\n\nSelect only the symbol, date, and adjusted columns, and arrange the dataset from oldest to newest\nMutate your date variable, making sure to read it as a Date object using as.Date()\nCreate a Year variable and filter only on observations happening in 2025. You can use the year() function to retrieve the year of a given Date column.\nGroup data by symbol\nCreate, for each different symbol, a Return variable that is defined as \\(P_{t+1}/P_{t}\\), where \\(t\\) refers to a date. You can use the lag() function for this\nYou will see that lag produces an NA whenever you try to lag the first observation. To make sure your data does not contain any NA, call drop_na()\nCreate, for each different symbol, a Cum_Return variable that is defined as the cumulative return. Compounded returns over time can be written as \\(\\small \\prod(1+R_t)=(1+R_1)\\times(1+R_2)\\times...\\times(1+R_t)\\). For this, you can use the prod() function.\nPick the latest observation from each symbol and arrange the table from lowest-to-highest return. The function slice_tail(n=x) retrieves the bottom x observations, whereas slice_head(n=y) retrieves the top y."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#solution-5",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#solution-5",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Solution",
    "text": "Solution\n\nCodeOutput\n\n\n\n#Read the Data\nM7%&gt;%\n#Select only the columns of interest\nselect(symbol,date,adjusted)%&gt;%\n#Make sure date is read as a Date object\nmutate(date=as.Date(date))%&gt;%\n#Filter for observations happening in 2025\nfilter(year(date)==2025)%&gt;%\n#Arrange from chronological order\narrange(date)%&gt;%\n#Group by Symbol to perform the calculations\ngroup_by(symbol)%&gt;%\n#Create the return\nmutate(Return = adjusted/lag(adjusted,default = NA))%&gt;%\n#Remove NAs before doing the cumulative product\ndrop_na()%&gt;%\nmutate(Cum_Return = cumprod(Return)-1)%&gt;%\n#Select the latest observation from each symbol\nslice_tail(n=1)%&gt;%\n#Select symbol, date, and cumulative return\nselect(symbol,date,Cum_Return)%&gt;%\n#Arrange from lowest-to-highest\narrange(Cum_Return)\n\n\n\n\n\n# A tibble: 7 × 3\n# Groups:   symbol [7]\n  symbol date       Cum_Return\n  &lt;chr&gt;  &lt;date&gt;          &lt;dbl&gt;\n1 NVDA   2025-01-29    -0.106 \n2 AAPL   2025-01-29    -0.0184\n3 TSLA   2025-01-29     0.0259\n4 GOOG   2025-01-29     0.0344\n5 MSFT   2025-01-29     0.0567\n6 AMZN   2025-01-29     0.0765\n7 META   2025-01-29     0.129"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#helpful-tips-while-using-r-and-quarto",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#helpful-tips-while-using-r-and-quarto",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Helpful tips while using R and Quarto",
    "text": "Helpful tips while using R and Quarto\n\nAs you pave your way through the coding exercises, there are a couple of best practices that will make your life easier when dealing with data in an R session:\n\nWhenever you are loading data, make sure to refer to the correct path where the file is located. You can use the function getwd() without any arguments to retrieve the current path, and setwd('C:/Users/you/newpath/') to set up a new working directory\nThe easiest way to make this logic redundant is to place the .R (or .qmd) script in the same folder as the data file (in our case, M7.csv). When you open the script, it will point to its own directory as the working directory - which will coincide with the data directory\n\n\n\ngetwd() #Gets the current directory. For Windows users, this is generally defaulted to C:/Users/USER/Documents\nsetwd('C:/Users/Lucas/Desktop') #changing this to desktop\nlist.files() #You can confirm if your data is listed in this directory\nread.csv('M7.csv') #It works without entering the full path since M7 is in the current path"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#helpful-tips-while-using-r-and-quarto-continued",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#helpful-tips-while-using-r-and-quarto-continued",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "Helpful tips while using R and Quarto, continued",
    "text": "Helpful tips while using R and Quarto, continued\n\nRStudio allows you to interact with your session piece wise - this means that you don’t need to run your code chunks all at once\n\nTo run only a specific portion of your code, select the specific code lines you want to run and hit Ctrl + Enter\nAs an effect, R will run only the selected lines directly into your session, and the output from these lines will be prompted into your session\n\nIf you are working on a Quarto notebook, you can use the same strategy to run specific lines directly into your session, or you can hit Ctrl+Shift+K to render the output altogether\n\n\nTry this copy-pasting the code from the next slide in your R Session - we’ll dive into the details later on, but note how the output printed in your session changes as you increase the selected lines"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#references",
    "href": "quant-fin/coursework/Lecture 2 - Collecting, Organizing, and Manipulating Financial Data/index.html#references",
    "title": "Collecting, Organizing, and Manipulating Financial Data",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#outline",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#outline",
    "title": "Manipulating time series Data",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\nTidy Finance (Scheuch, Voigt, and Weiss 2023)\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\n\n\n\n\n\n\n\n\nCoding Replications\n\n\nFor coding replications, whenever applicable, please follow this page or hover on the specific slides with containing coding chunks.\n\nEnsure that you have your  session properly set-up according to the instructions outlined in the course webpage\nAlong with the slides, this lecture will also contain a replication file, in .qmd format, containing a thorough discussion for all examples that have been showcased. This file, that will be posted on eClass®, can be downloaded and replicated on your side. To do that, download the file, open it up in RStudio, and render the Quarto document using the Render button (shortcut: Ctrl+Shift+K).\nAt the end of this lecture, you will be prompted with a hands-on exercise to test your skills using the tools you’ve learned as you made your way through the slides. A suggested solution will be provided in the replication file."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#organizing-financial-data",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#organizing-financial-data",
    "title": "Manipulating time series Data",
    "section": "Organizing Financial Data",
    "text": "Organizing Financial Data\n\nIn the previous lecture, you worked your way through the exercises by using the amazing dplyr functionalities on data.frames\nIn some cases, you had to do some workarounds with drop_na(), slice_tail() and lag() simply because you were manipulating time series data\nIn this lecture, you will be introduced to a particular type of class in R: xts\n\n\n\n\n\n\n\n\nDefinition\n\n\nxts is an R package that provides an extension of the zoo class, a class with methods for totally ordered indexed observations - in special, time series\n\nWith xts, you get a lot of flexibility in handling time series observations that are of interest of financial analysts, such as:\n\nSubsetting data by years/months/days\nCalculating rolling functions (e.g, yearly averages)\nAggregating data at different intervals (e.g, convert daily to weekly prices)\n\n\n\n\n\n\n\nQuestion: but wait, why are we departing from dplyr?"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#bridging-the-tidyverse-with-time-series",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#bridging-the-tidyverse-with-time-series",
    "title": "Manipulating time series Data",
    "section": "Bridging the tidyverse with time series",
    "text": "Bridging the tidyverse with time series\n\nUnfortunately, there is an issue: the tidyverse is not fully designed to work with time series classes, such as xts and zoo\nAs a consequence, you won’t be able to use a lot of interesting functionalities that would perfectly apply for time series\n\nBut don’t you worry, I got you covered: the tidyquant package1 integrates the best resources for collecting and analyzing financial data\nIt integrates several financial packages, like zoo, xts, quantmod, TTR, and PerformanceAnalytics, with the tidy data infrastructure of the tidyverse, allowing for seamless interaction between each\n\nYou can now perform complete financial analyses using the same functionalities you’ve learned so far!\n\nClick here for a thorough documentation on the tidyquant package."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#the-tidyquant-package-a-short-video",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#the-tidyquant-package-a-short-video",
    "title": "Manipulating time series Data",
    "section": "The tidyquant package, a short-video",
    "text": "The tidyquant package, a short-video"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#the-tq_mutate-function",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#the-tq_mutate-function",
    "title": "Manipulating time series Data",
    "section": "The tq_mutate() function",
    "text": "The tq_mutate() function\n\n\n\n\n\n\nDefinition\n\n\nThe tq_mutate() function adds adds new variables to an existing tibble:\n\ntq_mutate(.data, #The object you are performing the calculations \n       selected_variables, #The columns to send to the mutation function\n       mutate_fun, #The mutation function from either the xts, quantmod, or TTR package.\n       col_rename #A string or character vector containing names that can be used to quickly rename columns\n       ) \n\n\n\n\n\nThe main advantage is the results are returned as a tibble and the function can be used with the tidyverse\nIt is used when you expected additional columns to be added to the resulting data frame\nYou can use several time series related functions from other R packages - call tq_mutate_fun_options() to see the list of available options\nAll in all, it is similar in spirit to mutate()"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#the-tq_transmute-function",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#the-tq_transmute-function",
    "title": "Manipulating time series Data",
    "section": "The tq_transmute() function",
    "text": "The tq_transmute() function\n\n\n\n\n\n\nDefinition\n\n\nThe tq_transmute() returns only newly created columns and is typically used when periodicity changes.\n\ntq_transmute(.data, #The object you are performing the calculations \n       selected_variables, #The columns to send to the mutation function\n       mutate_fun, #The mutation function from either the xts, quantmod, or TTR package.\n       col_rename #A string or character vector containing names that can be used to quickly rename columns\n       )\n\n\n\n\n\ntq_transmute() works exactly like tq_mutate() except it only returns the newly created columns\nThis is helpful when changing periodicity where the new columns would not have the same number of rows as the original tibble\nAll in all, it is similar in spirit to summarize()"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-i",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-i",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise I",
    "text": "Working with time series objects, Exercise I\n\nAn immediate useful example of using a time series specific functionality with a tidyverse logic relates to filtering:\n\nSometimes, we may be interested in getting only a subset of the data (for example, only GOOG information)\nFurthermore, we may be interested in subsetting only a specific time frame for our analysis\n\nIt is relatively straightforward to do it with tidyquant:\n\nUse filter() to select only rows where symbol=='GOOG'\nIn the same call, filter for date&gt;= min_date and date&lt;=max_date"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-i-1",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-i-1",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise I",
    "text": "Working with time series objects, Exercise I\n\nCodeOutput\n\n\n\nlibrary(tidyquant)\nlibrary(tidyverse)\n\n#Set up the list of assets\nassets=c('AMZN','GOOG','META','GME')\n\n#Filter out\nassets%&gt;%\n  tq_get()%&gt;%\n  filter(symbol=='GOOG',\n         date&gt;='2020-01-01',\n         date&lt;='2024-12-31')\n\n\n\n\n\n# A tibble: 1,258 × 8\n   symbol date        open  high   low close   volume adjusted\n   &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 GOOG   2020-01-02  67.1  68.4  67.1  68.4 28132000     68.1\n 2 GOOG   2020-01-03  67.4  68.6  67.3  68.0 23728000     67.8\n 3 GOOG   2020-01-06  67.5  69.8  67.5  69.7 34646000     69.5\n 4 GOOG   2020-01-07  69.9  70.1  69.5  69.7 30054000     69.4\n 5 GOOG   2020-01-08  69.6  70.6  69.5  70.2 30560000     70.0\n 6 GOOG   2020-01-09  71.0  71.4  70.5  71.0 30018000     70.7\n 7 GOOG   2020-01-10  71.4  71.7  70.9  71.5 36414000     71.2\n 8 GOOG   2020-01-13  71.8  72.0  71.3  72.0 33046000     71.7\n 9 GOOG   2020-01-14  72.0  72.1  71.4  71.5 31178000     71.3\n10 GOOG   2020-01-15  71.5  72.1  71.5  72.0 25654000     71.7\n# ℹ 1,248 more rows"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-ii",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-ii",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise II",
    "text": "Working with time series objects, Exercise II\n\nAnother example of using a time series specific functionality is working with leads and lags:\n\nSometimes, we need to shift our variables by a specific interval, like getting the previous day’s price\nSay, for example, that you want to understand how S&P returns levels relate to NFLX returns one-week ahead\n\nIt is relatively straightforward to do it with tidyquant:\n\nDownload S&P 500 and NFLX data using the tq_get() function\nUse tq_transmute() to compute the weekly returns for each security based on daily data\nUse tq_mutate() to generate a lagged series of S&P 500 returns"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-ii-1",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-ii-1",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise II",
    "text": "Working with time series objects, Exercise II\n\nCodeOutput\n\n\n\n#Assuming you have already loaded the tidyquant and the tidyverse packages\n\n#Netflix Data\nNFLX=tq_get('NFLX')%&gt;%\n  #Select only the necessary columns\n  select(date,symbol,adjusted)%&gt;%\n  #Apply the weeklyReturn function and call the new column 'NFLX'\n  tq_transmute(mutate_fun = weeklyReturn,\n               col_rename = 'NFLX')\n\n#S&P Data\nSP500=tq_get('^GSPC')%&gt;%\n  #Select only the necessary columns\n  select(date,symbol,adjusted)%&gt;%\n  #Apply the weeklyReturn function and call the new column 'SP500'\n  tq_transmute(mutate_fun = weeklyReturn,\n               col_rename = 'SP500')%&gt;%\n  #Apply the lag function for n=1 week and call the new column 'SP500'\n  tq_transmute(mutate_fun = lag.xts,\n            n=1,\n            col_rename = 'SP500')%&gt;%\n  #Drop all rows with NA information (row 1, in this case)\n  drop_na()\n\n#Merge Data \ninner_join(NFLX,SP500)\n\n\n\n\n\n# A tibble: 530 × 3\n   date           NFLX    SP500\n   &lt;date&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 2015-01-09 -0.0563   0      \n 2 2015-01-16  0.0244  -0.00651\n 3 2015-01-23  0.297   -0.0124 \n 4 2015-01-30  0.00992  0.0160 \n 5 2015-02-06  0.00579 -0.0277 \n 6 2015-02-13  0.0489   0.0303 \n 7 2015-02-20  0.0260   0.0202 \n 8 2015-02-27 -0.00688  0.00635\n 9 2015-03-06 -0.0438  -0.00275\n10 2015-03-13 -0.0346  -0.0158 \n# ℹ 520 more rows"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iii",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iii",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise III",
    "text": "Working with time series objects, Exercise III\n\nFinance practitioners are often asked to perform analysis on a rolling basis:\n\nWe may want to calculate a given signal on day \\(t\\) based on past \\(x\\) periods of information\nSay, for example, that you want to calculate a simple and exponential moving average of adjusted prices from 5 days back for a given stock\n\nIt is relatively straightforward to do it with tidyquant:\n\nDownload stock data using the tq_get() function\nUse tq_mutate() twice along with the SMA() and EMA() functions setting n=5"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iii-1",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iii-1",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise III",
    "text": "Working with time series objects, Exercise III\n\nCodeOutput\n\n\n\n#Assuming you have already loaded the tidyquant and the tidyverse packages\n\n#Set up the list of assets\nassets=c('AMZN')\n\nassets%&gt;%\n  tq_get()%&gt;%\n  select(date,symbol,adjusted)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_mutate(adjusted, mutate_fun = SMA, n = 5)%&gt;%\n  tq_mutate(adjusted, mutate_fun = EMA, n = 5)\n\n\n\n\n\n# A tibble: 2,551 × 5\n# Groups:   symbol [1]\n   symbol date       adjusted   SMA   EMA\n   &lt;chr&gt;  &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 AMZN   2015-01-02     15.4  NA    NA  \n 2 AMZN   2015-01-05     15.1  NA    NA  \n 3 AMZN   2015-01-06     14.8  NA    NA  \n 4 AMZN   2015-01-07     14.9  NA    NA  \n 5 AMZN   2015-01-08     15.0  15.0  15.0\n 6 AMZN   2015-01-09     14.8  14.9  15.0\n 7 AMZN   2015-01-12     14.6  14.8  14.8\n 8 AMZN   2015-01-13     14.7  14.8  14.8\n 9 AMZN   2015-01-14     14.7  14.8  14.8\n10 AMZN   2015-01-15     14.3  14.6  14.6\n# ℹ 2,541 more rows"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iv",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iv",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise IV",
    "text": "Working with time series objects, Exercise IV\n\nLastly, financial analysts often cover a collection of securities on a rolling basis\nFor example, a buy-side analyst will monitor stocks from a given industry so as to understand which ones are overvalued, and which ones are undervalued\nSay, for example, that you want to focus on a subset of 4 stocks, and you need to compare the cumulative return up to the latest closing price\nIt is easy to integrate the tidyquant functions along with the group_by() function you’ve learned when working with dplyr:\n\nGet the information using tq_get()\nGroup the data by symbol\nApply the tq_mutate and tq_transmute functions to pass time series functions to the data - in this case, the dailyReturn() and the Return.cumulative() function"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iv-1",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#working-with-time-series-objects-exercise-iv-1",
    "title": "Manipulating time series Data",
    "section": "Working with time series objects, Exercise IV",
    "text": "Working with time series objects, Exercise IV\n\nCodeOutput\n\n\n\n#Assuming you have already loaded the tidyquant and the tidyverse packages\n\n#Set up the list of assets\nassets=c('AMZN','GOOG','META','GME')\n\nassets%&gt;%\n  tq_get()%&gt;%\n  select(date,symbol,adjusted)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_mutate(adjusted, mutate_fun = dailyReturn,col_rename = 'daily_return')%&gt;%\n  tq_transmute(daily_return,mutate_fun = Return.cumulative)%&gt;%\n  mutate(across(where(is.numeric),percent,big.mark='.'))%&gt;%\n  setNames(c('Ticker','Cumulative Return up-to-date'))\n\n\n\n\n\n# A tibble: 4 × 2\n# Groups:   Ticker [4]\n  Ticker `Cumulative Return up-to-date`\n  &lt;chr&gt;  &lt;chr&gt;                         \n1 AMZN   1 279%                        \n2 GOOG   595%                          \n3 META   755%                          \n4 GME    298%"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#wrapping-up-on-tidyquant",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#wrapping-up-on-tidyquant",
    "title": "Manipulating time series Data",
    "section": "Wrapping-up on tidyquant",
    "text": "Wrapping-up on tidyquant\n\nThere is so much you can use from tidyquant in your journey as a quantitative financial analyst:\n\nI strongly recommend looking at all the predefined functions supported by tidyquant - click here for a detailed discussion around all supported functions\nYou can also customize your own functions that work with time series (for example, your secret trading indicator) and pass it over through tq_mutate() or tq_transmute()\n\nIn the package’s official website, you can find a variety of examples to nurture your creativity around what you can do using this package\nAll in all, that’s the motto: time series analysis made easy with tidyquant and the tidyverse"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#does-getting-ripped-increase-returns",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#does-getting-ripped-increase-returns",
    "title": "Manipulating time series Data",
    "section": "Does getting ripped increase returns?",
    "text": "Does getting ripped increase returns?\n\n\n\n\n\n\n\\(\\rightarrow\\) See Deadlift: The ETF World’s Latest Headscratcher"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#hands-on-exercise",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#hands-on-exercise",
    "title": "Manipulating time series Data",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\nYour manager (who did not lift any weights past the last 5 years) wanted to replicate the returns of the Deadlift ETF from 2020 to 2024. You job is to create a simple table of yearly returns comparing the Deadlift ETF vis-a-vis the S&P 500 Index\nFollow the instructions and answer to the following questions:\n\nWhen looking at the yearly results from both the Deadlift ETF and S&P 500, which one did perform better?\nWhat are the potential explanations for the result you have found?\n\nTo answer to these questions, you will be using the a combination of dplyr and tidyquant functions you have learned so far\nThe expected result is a data.frame object that shows both the Deadlift ETF as well as the S&P 500 returns (columns) on a yearly basis (rows)\n\n\\(\\rightarrow\\) Suggested solution will be provided in the replication file for this lecture."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#hands-on-exercise-continued",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#hands-on-exercise-continued",
    "title": "Manipulating time series Data",
    "section": "Hands-On Exercise, continued",
    "text": "Hands-On Exercise, continued\n\n\n\n\n\n\nExercise\n\n\nBefore you start, make sure to have the tidyverse and the tidyquant packages loaded in your session. Following the instructions from the previous lectures, you can either make a direct call to each package, library(tidyverse) and library(tidyquant), or copy-paste the script from the course’s official website.\n\nUse tq_get() to load information from the S&P Index and the Deadlift ETF constituents in two separate objects. You can use the code ^GSPC to retrieve information for the index, and you can pass a vector c('ticker1','ticker2',...,'ticker_n') to get information on the Deadlift ETF constituents\nFilter for observations starting between 2020 (beginning of) and 2024 (end of) using the from and to arguments of the tq_get() function\nGroup the Deadlift ETF data by symbol using the group_by() function\nFor both data sets, create a yearly_ret variable that calculates the yearly return of a given security. You can use the tq_transmute() function, passing the yearlyReturn() function along the chain\nFor the Deadlift data set, regroup the data by date and calculate the Deadlift returns using a mutate() function (Hint: it is an equally weighted portfolio)\nMerge both datasets using inner_join()"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#solution",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#solution",
    "title": "Manipulating time series Data",
    "section": "Solution",
    "text": "Solution\n\nCodeOutput\n\n\n\n# Set up the list of assets\ndeadlift=c('META','AMZN','GS','UBER','MSFT','AAPL','BLK','NVDA')\n\n#Set up the starting date\nstart='2020-01-01'\nend='2024-12-31'\n\n#Step 1: Read the Deadlift data using tidyquant\nDeadlift_Performance=deadlift%&gt;%\n  tq_get(from=start,to=end)%&gt;%\n  #Select only the columns of interest\n  select(symbol,date,adjusted)%&gt;%\n  #Group by symbol and date\n  group_by(symbol)%&gt;%\n  #Use tq_transmute to aggregate and calculate weekly returns\n  tq_transmute(selected=adjusted,\n               mutate_fun=yearlyReturn,\n               col_rename = 'Deadlift')%&gt;%\n  #Group by date\n  group_by(date)%&gt;%\n  #Summarize average return (since it is an equally-weighted portfolio)\n  summarize(Deadlift=mean(Deadlift,na.rm=TRUE))\n\n#Step 2: Read the S&P 500 data using tidyquant\nSP500_Performance=tq_get('^GSPC',from=start,to=end)%&gt;%\n  #Select only the columns of interest\n  select(symbol,date,adjusted)%&gt;%\n  #Group by symbol and date\n  group_by(symbol)%&gt;%\n  #Use tq_transmute to aggregate and calculate weekly returns\n  tq_transmute(selected=adjusted,\n               mutate_fun=yearlyReturn,\n               col_rename = 'SP500')%&gt;%\n  ungroup()%&gt;%\n  select(-symbol)\n    \n#Merge\nSP500_Performance%&gt;%\n  inner_join(Deadlift_Performance)%&gt;%\n  mutate(across(where(is.numeric),percent))%&gt;%\n  mutate(date=year(date))%&gt;%\n  setNames(c('Year','S&P 500','DeadLift ETF'))\n\n\n\n\n\n# A tibble: 5 × 3\n   Year `S&P 500` `DeadLift ETF`\n  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;         \n1  2020 15.29%    57.9%         \n2  2021 26.89%    37.2%         \n3  2022 -19.44%   -36.0%        \n4  2023 24.23%    100.5%        \n5  2024 23.84%    52.1%"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#references",
    "href": "quant-fin/coursework/Lecture 3 - Manipulating Time Series/index.html#references",
    "title": "Manipulating time series Data",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#outline",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#outline",
    "title": "Data Visualization",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\nTidy Finance (Scheuch, Voigt, and Weiss 2023)\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\nThe Grammar of Graphics (Wilkinson 2005)\n\n\n\n\n\n\n\n\nCoding Replications\n\n\nFor coding replications, whenever applicable, please follow this page or hover on the specific slides with containing coding chunks.\n\nEnsure that you have your  session properly set-up according to the instructions outlined in the course webpage\nAlong with the slides, this lecture will also contain a replication file, in .qmd format, containing a thorough discussion for all examples that have been showcased. This file, that will be posted on eClass®, can be downloaded and replicated on your side. To do that, download the file, open it up in RStudio, and render the Quarto document using the Render button (shortcut: Ctrl+Shift+K).\nAt the end of this lecture, you will be prompted with a hands-on exercise to test your skills using the tools you’ve learned as you made your way through the slides. A suggested solution will be provided in the replication file."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#visualizing-financial-data",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#visualizing-financial-data",
    "title": "Data Visualization",
    "section": "Visualizing Financial Data",
    "text": "Visualizing Financial Data\n\nPart of your work as a Financial Analyst is to convey the information to a broader public:\n\nOften times, Fund managers do not use R, Python, and won’t be proof-reading your .xlsx file\nInvestors, Journalists, or the general audience all alike may not have the set of skills to ingest and analyze data like you do\n\nIt is imperative to think about how you should structure your message to effectively communicate it to your audience’s needs\n\n\n\n\n\n\n\nIntroducing: the Grammar of Graphics\n\n\nThe Grammar of Graphics sets up the foundations that underlie the production of all types of charts, ranging from pie charts, bar charts, scatterplots, and many more. To that matter, the Grammar of Graphics presents a unique foundation for producing charts from quantitative information that are widely used in scientific journals, newspapers, statistical packages, and data visualization systems.\n\n\n\n\nHow can you implement these foundations to showcase your data analysis? Luckily, there is a way to do that in R: I introduce you to the wonderful world of ggplot2"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#introducing-ggplot2",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#introducing-ggplot2",
    "title": "Data Visualization",
    "section": "Introducing ggplot2",
    "text": "Introducing ggplot2\n\nggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics\n\n\n\n\n\n\nYou start with a call to ggplot(), supplying the data and a aesthetic mapping (aes), like x and y axis, groupings, etc\nAfter that, you choose the geometry (geom), the shape of the visual elements contained in the visualization\nFinally, you add layers on top on the geometry (titles, annotations, etc) and customize your theme (font size, background color, etc)\n\n\n\nKey Highlights\n\nIt is, by and large, the richest and most widely used plotting ecosystem in the  language\nggplot2 has a rich ecosystem of extensions - ranging from annotations and interactive visualizations to specialized genomics - click here a community maintained list"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#ggplot2-foundations",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#ggplot2-foundations",
    "title": "Data Visualization",
    "section": "ggplot2 foundations",
    "text": "ggplot2 foundations\n\nWe will illustrate the use of ggplot2 to replicate the Grammar of Graphics foundations using the FANG dataset, which is loaded together with your slides - if you prefer to do it direclty in R, hit the download button and load it using read_delim('FANG.txt')\nTo get ggplot2 in your session, either load tidyverse altogether of directly load the library:\n\n\n#Load the tidyquant package\nlibrary(tidyquant)\n\n#Option 1: load the tidyverse, which includes ggplot2\nlibrary(tidyverse)\n\n#Option 2: load ggplot2 directly\nlibrary(ggplot2)\n\n\nIn what follows, we will be working with a step-by-step example of how to use ggplot2 for data visualizations\n\n\n\n Download Raw data"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#step-1-the-data",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#step-1-the-data",
    "title": "Data Visualization",
    "section": "Step 1: the data",
    "text": "Step 1: the data\n\nWe will be using the FANG dataset, which contains basic stock information from popular U.S. techonology firms: Facebook (Meta), Amazon, Netflix, and Google (Alphabet)\nThe first step in using ggplot2 is to call your data dataframe and supply the aesthetic mapping, which we’ll refer to as aes\n\n\nggplot(data=your_data, aes(x= variable_1, y=variable_2, ...))\n\n\nThe data argument refers to the dataset used\nThe aes argument contains all the aesthetic mappings that will be used\n\n\nTogether, these constitute the backbone of your visualization: they tell ggplot2 what the raw information to be used and where it should be mapped!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#step-1-the-data-practice",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#step-1-the-data-practice",
    "title": "Data Visualization",
    "section": "Step 1: the data, practice",
    "text": "Step 1: the data, practice\n\nExerciseSolution\n\n\n\n\n\nListing 1: Use the newly created META dataset and call ggplot, mapping the date variable in the x axis, adjusted variable in the y axis, and symbol in the group aesthetic. The FANG dataset and ggplot2 have been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the ggplot() function together with aes(x, y, group):\n\n\n#Let's use Apple (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol))\n#Let's use Apple (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#step-2-adding-your-geom",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#step-2-adding-your-geom",
    "title": "Data Visualization",
    "section": "Step 2: adding your geom",
    "text": "Step 2: adding your geom\n\nYou probably thought you did something wrong when you saw an empty chart with the named axis, right? However, I can assure: you did great!\n\nIt is all about the philosophy embedded in the Grammar of Graphics: you first provide the data and the aes(thetic) mapping to your data\nNow, ggplot knows exactly which information to select and where to place it. However, it is still agnostic about how to display it\n\nWe will now add a geometry layer - in short, a geom:\n\nYou can add layers on top of ggplot object addition symbol (+)\nThere are many types of potential geometries, to name a few: geom_point(), geom_col(), geom_line() - access here for a complete list\n\n\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted)) +\n#Map a given geometry\ngeom_{yourgeomhere}()"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#step-2-adding-your-geom-practice",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#step-2-adding-your-geom-practice",
    "title": "Data Visualization",
    "section": "Step 2: adding your geom, practice",
    "text": "Step 2: adding your geom, practice\n\nExerciseSolution\n\n\n\n\n\nListing 2: In your ggplot object, try out the following geoms: geom_point(), geom_col(), and geom_line(). Which one do you think is the best for the task? The FANG dataset and ggplot2 have been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn general, using geom_line() suits the best for time series\n\n\n#Let's use Meta (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()\n#Let's use Meta (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#step-3-be-creative-with-additional-layers",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#step-3-be-creative-with-additional-layers",
    "title": "Data Visualization",
    "section": "Step 3: be creative with additional layers",
    "text": "Step 3: be creative with additional layers\n\nYour main chart is now all set:\n\nIt contains the data and the necessary aes(thetic) mappings to the chart;\nIt also contains a shape, or geom(metry), that was selected to display the data\n\nThe philosophy behind the Grammar of Graphics is now to add layers of information on top of the base chart using the + operator, like before\nWe will proceed by including several layers of information that will either add or modify the behavior of the chart, making it more appealing to our audience:\n\nAdding trend-lines using geom_smooth()\nAdding annotations and labels using annotation and labs\nModifying the behavior of the scales using scale_y and scale_x\n\nTry to sequentially add these layers and re-run the code to see how it reflects on the output!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#step-3-be-creative-with-additional-layers-1",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#step-3-be-creative-with-additional-layers-1",
    "title": "Data Visualization",
    "section": "Step 3: be creative with additional layers",
    "text": "Step 3: be creative with additional layers\n\nExerciseSolution\n\n\n\n\n\nListing 3: In your previously created ggplot object, add a smoothed trend of adjusted prices using the geom_smooth(method='loess') geometry and adjust the labels of your axis, chart title, and subtitle. You can pass additional layers using the + operator. For changing the labels, you can use the labs(x='Your X Label',y='Your Y Label', title='Your Title', subtitle='Your subtitle') syntax. The x-axis should be called “Date,” y-axis should be called “Adjusted Prices”, the title should be called “META Prices Over Time”, and the subtitle should be called “Source: Yahoo! Finance”. The FANG dataset and ggplot2 have been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can call the geom_smooth() along with method='loess' to have a smoothed trend added on top of your chart, and customize your labels by calling the labs() argument. You can chain these operations on top of your chart using the + sign.\n\n\n#Let's use Meta (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()+\n#Adding a trend\ngeom_smooth(method='loess')+\n#Adding Annotations\nlabs(title='META adjusted prices',\n     subtitle = 'Source: Yahoo! Finance',\n     x = 'Year',\n     y = 'Adjusted Prices')\n#Let's use Meta (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()+\n#Adding a trend\ngeom_smooth(method='loess')+\n#Adding Annotations\nlabs(title='META adjusted prices',\n     subtitle = 'Source: Yahoo! Finance',\n     x = 'Year',\n     y = 'Adjusted Prices')"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#more-annotations",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#more-annotations",
    "title": "Data Visualization",
    "section": "More annotations",
    "text": "More annotations\n\nApart from simply changing the labels of your axis, titles and subtitles, you can also use ggplot2 to customize the appearance of your axis:\n\nThe family of functions scale_x_{} apply a given structure to the x-axis - e.g, scale_x_date(),scale_x_continuous()\nThe family of functions scale_y_{} apply a given structure to the y-axis - e.g, scale_y_continuous() etc\n\nWith that, you can, for example:\n\nForce the x-axis to be formatted as a date, adjusting how it is being displayed\nForce the y-axis to be formatted in terms of dollar amounts\n\nIn this way, you can impose meaningful structures in your chart depending on the type of data you are considering in your mapping to x and y axis!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#more-annotations-continued",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#more-annotations-continued",
    "title": "Data Visualization",
    "section": "More annotations, continued",
    "text": "More annotations, continued\n\nFor example, the code snippet below formats the x-axis to show breaks at the year level, and formats the y-axis in such a way that it goes from \\(\\small\\$0\\) to \\(\\small\\$1,000\\) by increments of \\(\\small\\$50\\)\n\n\n  #Your previous ggplot call up to now\n  {your_previous_ggplot} +\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar, breaks = seq(from=0,to=1000,by=50))\n\n\nClick here to see comprehensive list of all customizations that can be done across both x-axis and y-axis for continuous scales (scale_x_continuous() and scale_y_continuous())\nClick here to see comprehensive list of all customizations that can be done across both x-axis and y-axis for date scales (scale_x_date() and scale_y_date())\n\n\n\n\n\n\n\nFormatting scales\n\n\nTo properly format the appearance of your axis, make sure to have the scales package properly installed and loaded. You can do so by calling install.packages('scales') and library(scales)."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#step-4-customize-your-axis",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#step-4-customize-your-axis",
    "title": "Data Visualization",
    "section": "Step 4: customize your axis",
    "text": "Step 4: customize your axis\n\nExerciseSolution\n\n\n\n\n\nListing 4: Using your previously created ggplot object, customize the appearance of the x-axis and y-axis in the following way: the x-axis shoudl be formatted as a date using an appropriate function that shows each year as a breakpoint, whereas the y-axis should be formatted in dollar terms, ranging from zero to one thousand dollars, by increments of 50, using an appropriate function. You can pass additional layers using the + operator. The FANG dataset and ggplot2 have been already loaded for you. Even if you submit the wrong answer, a live-tutoring feature will provide you with a handful of tips to adjust your code and resubmit your solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse scale_x_date() with the appropriate arguments to format the x-axis, doing the same thing for the y-axis using scale_y_continuous():\n\n\n#Let's use Meta (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()+\n#Adding a trend\ngeom_smooth(method='loess')+\n#Adding Annotations\nlabs(title='META adjusted prices',\n     subtitle = 'Source: Yahoo! Finance',\n     x = 'Year',\n     y = 'Adjusted Prices')+\n#Changing the behavior of scales\nscale_x_date(date_breaks = '1 year',labels = year) +\nscale_y_continuous(labels = dollar, breaks = seq(from=0,to=1000,by=50))\n#Let's use Meta (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()+\n#Adding a trend\ngeom_smooth(method='loess')+\n#Adding Annotations\nlabs(title='META adjusted prices',\n     subtitle = 'Source: Yahoo! Finance',\n     x = 'Year',\n     y = 'Adjusted Prices')+\n#Changing the behavior of scales\nscale_x_date(date_breaks = '1 year',labels = year) +\nscale_y_continuous(labels = dollar, breaks = seq(from=0,to=1000,by=50))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#adding-multiple-data-points",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#adding-multiple-data-points",
    "title": "Data Visualization",
    "section": "Adding multiple data points",
    "text": "Adding multiple data points\nQuestion: what if we wanted to add more data?\n\n\nIn our first example, we set filter(symbol)=='META' to select only information from Meta to your chart\nHowever, one might be interested in understanding how did Meta perform relative to its FANG peers\n\n\n\n\nIt is easy to do it with ggplot:\n\nBecause you have set group=symbol, ggplot already knows that it needs to group by each different string contained in the ticker column\nIn such a way, all you need to do is to add a new aes mapping, colour=symbol, so that ggplot knows that each symbol needs to have a different color!\n\nIn what follows, we will be charting all four FANG stocks in the same chart, adjusting the layers to try keeping aesthetics as good as possible"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#adding-multiple-data-points-practice",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#adding-multiple-data-points-practice",
    "title": "Data Visualization",
    "section": "Adding multiple data points, practice",
    "text": "Adding multiple data points, practice"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#facet-it-until-you-make-it",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#facet-it-until-you-make-it",
    "title": "Data Visualization",
    "section": "Facet it until you make it",
    "text": "Facet it until you make it\n\nWe have included all FANG stocks into the same chart. Easy peasy, lemon squeezy!\nAs far as we could go on adjusting the layers, it seems that the chart conveys too much information:\n\nBecause of the different scales, you can hardly tell the different between AMZN AND GOOG during 2015-2018\nFurthermore, trend lines are, in some cases, effectively hiding the data undernearth\n\nAlthough you could easily remove the trend lines, ggplot2 also comes with a variety of alternatives when it comes to charting multiple data that may come in handy:\n\nYou can facet your chart using facet_wrap, controlling the axis as well as the number of rows and columns\nYou can grid your chart, making the comparison easier with fixed axes"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#option-1-using-facet_wrap",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#option-1-using-facet_wrap",
    "title": "Data Visualization",
    "section": "Option 1: using facet_wrap()",
    "text": "Option 1: using facet_wrap()"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#option-2-using-facet_grid",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#option-2-using-facet_grid",
    "title": "Data Visualization",
    "section": "Option 2: using facet_grid()",
    "text": "Option 2: using facet_grid()"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#adding-themes-youre-in-full-control-of-your-message",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#adding-themes-youre-in-full-control-of-your-message",
    "title": "Data Visualization",
    "section": "Adding themes: you’re in full control of your message!",
    "text": "Adding themes: you’re in full control of your message!\n\nBy now, you are already looking like a data manipulation wizard in your firm:\n\nYou have created a fully automated data ingestion process using tq_get() to get live FANG prices.\nSet up ggplot to automatically update the chart;\nFinally, you have adjusted all aesthetics to make it more much more professional\n\nPlus: you haven’t even opened Excel! Well, what’s next?\n\n\n\nA lot of the ggplot adoption throughout the R usiverse relates to themes: complete configurations which control all non-data display\n\nThere are a lot of available themes that you can pass to your ggplot, like theme_minimal(), theme_bw()\nAlternatively, you can pass theme() if you just need to tweak the display of an existing theme"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#adding-themes-to-your-chart",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#adding-themes-to-your-chart",
    "title": "Data Visualization",
    "section": "Adding themes() to your chart",
    "text": "Adding themes() to your chart"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#the-r-community-is-on-your-side",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#the-r-community-is-on-your-side",
    "title": "Data Visualization",
    "section": "The R community is on your side!",
    "text": "The R community is on your side!\n\nThere are endless customizations that you could think of that could be applied to a theme\nIn special, the package ggthemes provides extra themes, geoms, and scales for ggplot2 that replicate the look of famous aesthetics that you have often looked and said: “how could I replicate that?”\nTo get access to these additional graphical resources in your R session, install and load the package using:\n\n\ninstall.packages('ggthemes') #Install if not available\nlibrary(ggthemes) #Load\n\nyour_previous_ggplot_object + theme_{insertyourtheme}\n\n\nTo check all available themes, check the ggthemes library here website"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#endless-possibilities-for-theme-customization",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#endless-possibilities-for-theme-customization",
    "title": "Data Visualization",
    "section": "Endless possibilities for theme customization",
    "text": "Endless possibilities for theme customization\n\nWSJThe EconomistExcelFiveThirtyEightGoogle Docs"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#further-theme-customization",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#further-theme-customization",
    "title": "Data Visualization",
    "section": "Further theme customization",
    "text": "Further theme customization\n\nEven with customized themes, you might still want to do your own customizations\nIt is easy to access each and every component of the chart by adding theme (using the + operator):\n\n\ninstall.packages('ggthemes') #Install if not available\nlibrary(ggthemes) #Load\n\nyour_previous_ggplot_object +\n  theme_{insertyourtheme}+\n  theme(component_1 = configuration_1,\n        component_2 = configuration_2,\n        ...\n        component_n = configuration_n)\n\n\nIn what follows, we will be using the theme() function to adjust some aspects of our chart, such as font size, angle, and text width, to make it look more professional"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#doing-custom-theme-adjustments-to-the-chart",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#doing-custom-theme-adjustments-to-the-chart",
    "title": "Data Visualization",
    "section": "Doing custom theme() adjustments to the chart",
    "text": "Doing custom theme() adjustments to the chart"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#integrating-tidyquant",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#integrating-tidyquant",
    "title": "Data Visualization",
    "section": "Integrating tidyquant",
    "text": "Integrating tidyquant\n\nLike in our previous lecture, tidyquant added very important functionalities for those who work in finance to easily manage financial time series using the well-established foundations of the tidyverse\nWhen it comes to data visualization, tidyquant also provides a handful of integrations that can be inserted into your ggplot call:\n\nPossibility of using geom_barchart and geom_candlestick\nMoving average visualizations and Bollinger Bands available using geom_ma and geom_bbands\nA new theme, theme_tq, available\n\n\n\\(\\rightarrow\\) For a thorough discussion, see a detailed discussion on tidyquant’s charting capabilities here"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#integrating-tidyquant-continued",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#integrating-tidyquant-continued",
    "title": "Data Visualization",
    "section": "Integrating tidyquant, continued",
    "text": "Integrating tidyquant, continued\n\nCodeOutput\n\n\n\n#Set up start and end dates\nend=Sys.Date()\nstart=end-weeks(5)\n\nFANG%&gt;%\n  #Make sure that date is read as a Date object\n  mutate(date=as.Date(date))%&gt;%\n  #Filter\n  filter(date &gt;= start, date&lt;=end)%&gt;%\n  #Basic layer - aesthetic mapping including fill\n  ggplot(aes(x=date,y=close,group=symbol))+\n  #Charting data - you could use geom_line(), geom_col(), geom_point(), and others\n  geom_candlestick(aes(open = open, high = high, low = low, close = close))+\n  geom_ma(ma_fun = SMA, n = 5, color = \"black\", size = 0.25)+\n  #Facetting\n  facet_wrap(symbol~.,scales='free_y')+\n  #DeepSeek date\n  geom_vline(xintercept=as.Date('2025-01-24'),linetype='dashed')+\n  #Annotations\n  labs(title='FANG adjusted prices before/after DeepSeek announcement',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Date',\n       y = 'Adjusted Prices')+\n  #Scales\n  scale_x_date(date_breaks = '3 days') +\n  scale_y_continuous(labels = dollar) +\n  #Custom 'The Economist' theme\n  theme_economist()+\n  #Adding further customizations\n  theme(legend.position='none',\n        axis.title.y = element_text(vjust=+4,face='bold'),\n        axis.title.x = element_text(vjust=-3,face='bold'),\n        plot.subtitle = element_text(size=8,vjust=-2,hjust=0,margin = margin(b=15)),\n        axis.text.y = element_text(size=8),\n        axis.text.x = element_text(angle=90,size=8))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#alternatives-to-ggplot2",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#alternatives-to-ggplot2",
    "title": "Data Visualization",
    "section": "Alternatives to ggplot2",
    "text": "Alternatives to ggplot2\n\nggplot2 is, by and large, the richest and most widely used plotting ecosystem in the  language\nHowever, there are also other interesting options, especially when it comes to interactive data visualization\n\nThe plotly ecosystem provides interactive charts for R, Python, Julia, Java, among others - you can install the R package using install.packages('plotly')\nThe Highcharts is another option whenever there is a need for interactive data visualization - you can install the R package using install.packages('highcharter')\n\nIn special, the highcharter package works seamlessly with time series data, especially those retrieved by the tidyquant’s tq_get() function"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#using-the-highcharter-package",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#using-the-highcharter-package",
    "title": "Data Visualization",
    "section": "Using the highcharter package",
    "text": "Using the highcharter package\n\nCodeSolution\n\n\n\n#Install the highcharter package (if not installed yet)\n#install.packages('highcharter')\n\n#Load the highcharter package (if not loaded yet)\nlibrary(highcharter)\n\n#Select the Google Stock with OHLC information and transform to an xts object\nGOOG=tq_get('GOOG')%&gt;%select(-symbol)%&gt;%as.xts()\n\n  #Initialize an empty highchart\n  highchart(type='stock')%&gt;%\n  #Add the Google Series\n  hc_add_series(GOOG,name='Google')%&gt;%\n  #Add title and subtitle\n  hc_title(text='A Dynamic Visualization of Google Stock Prices Over Time')%&gt;%\n  hc_subtitle(text='Source: Yahoo! Finance')%&gt;%\n  #Customize the tooltip\n  hc_tooltip(valueDecimals=2,valuePrefix='$')%&gt;%\n  #Convert it to a 'The Economist' theme\n  hc_add_theme(hc_theme_economist())"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#i-hope-you-are-excited-to-whats-next",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#i-hope-you-are-excited-to-whats-next",
    "title": "Data Visualization",
    "section": "I hope you are excited to what’s next!",
    "text": "I hope you are excited to what’s next!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#hands-on-exercise",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#hands-on-exercise",
    "title": "Data Visualization",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\nIn late January 2021, Reddit traders took on the short-sellers by forcing them to liquidate their short positions using GameStop stocks. This coordinated behavior had significant repercussions for various investment funds, such as Melvin Capital - see here and here\n\n\n\n\n\n\n\nExercise\n\n\n\nUse tq_get() to load information for GameStop (ticker: GME) and store it in a data.frame. Using the arguments from and to from tq_get(), filter for observations between occurring in between December 2020 (beginning of) and March 2021 (end of)\nUse ggplot(aes(x=date,group=symbol)), along with geom_candlestick() and its appropriate arguments, to chart the historical OHLC prices\nCreate a vertical line annotation using geom_vline, setting the xintercept argument to the date of the Reddit frenzy (as.Date('2021-01-25'))\nUse the theme from The Economist calling theme_economist(). Make sure to have the ggthemes package installed and loaded\nFinally, call theme() and labs() to adjust the aesthetics of your theme and labels as you think it would best convey your message. For example, you can use the scales package to format the appearance of your x and y labels (for example, displaying a dollar sign in front of adjusted prices)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#hands-on-exercise-solutions",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#hands-on-exercise-solutions",
    "title": "Data Visualization",
    "section": "Hands-On Exercise, solutions",
    "text": "Hands-On Exercise, solutions\n\nCodeOutput\n\n\n\n#Libraries\nlibrary(tidyquant)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(scales)\n\n#Setting start/end dates + reddit date\nstart='2020-12-01'\nend='2021-03-31'\nreddit_date=as.Date('2021-01-25')\n\n#Get the data\ntq_get('GME',from=start,to=end)%&gt;%\n  #Mapping\n  ggplot(aes(x=date,group=symbol))+\n  #Geom\n  geom_candlestick(aes(open = open, high = high, low = low, close = close))+\n  #Labels\n  labs(x='',\n       y='Adjusted Prices',\n       title='GameStop (ticker: GME) prices during the reddit (Wall St. Bets) frenzy',\n       subtitle='Source: Yahoo! Finance')+\n  #Annotation\n  geom_vline(xintercept=reddit_date,linetype='dashed')+\n  annotate(geom='text',x=reddit_date-5,y=75,label='Reddit Frenzy Starts',angle=90)+\n  #Scales\n  scale_x_date(date_breaks = '2 weeks') +\n  scale_y_continuous(labels = dollar) +\n  #Custom 'The Economist' theme\n  theme_economist()+\n  #Adding further customizations\n  theme(legend.position='none',\n        axis.title.y = element_text(vjust=+4,face='bold'),\n        axis.title.x = element_text(vjust=-3,face='bold'),\n        plot.title = element_text(size=10),\n        plot.subtitle = element_text(size=8,vjust=-2,hjust=0,margin = margin(b=15)),\n        axis.text.y = element_text(size=8),\n        axis.text.x = element_text(angle=45,size=8,vjust=0.75))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#references",
    "href": "quant-fin/coursework/Lecture 4 - Data Visualization/index.html#references",
    "title": "Data Visualization",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed. Statistics and Computing. New York, NY: Springer."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#outline",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#outline",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\nTidy Finance (Scheuch, Voigt, and Weiss 2023)\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\n\n\n\n\n\n\n\n\nCoding Replications\n\n\nFor coding replications, whenever applicable, please follow this page or hover on the specific slides with containing coding chunks.\n\nEnsure that you have your  session properly set-up according to the instructions outlined in the course webpage\nIn the webpage, you can also find a detailed discussion of the examples covered in this lecture"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#disclaimer",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#disclaimer",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\n\n\n\n\nDisclaimer\n\n\nThe information presented in this lecture is for educational and informational purposes only and should not be construed as investment advice. Nothing discussed constitutes a recommendation to buy, sell, or hold any financial instrument or security. Investment decisions should be made based on individual research and consultation with a qualified financial professional. The presenter assumes no responsibility for any financial decisions made based on this content.\nAll code used in this lecture is publicly available and is also shared on my GitHub page. Participants are encouraged to review, modify, and use the code for their own learning and research purposes. However, no guarantees are made regarding the accuracy, completeness, or suitability of the code for any specific application.\nFor any questions or concerns, please feel free to reach out via email at lucas.macoris@fgv.br"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#background",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#background",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Background",
    "text": "Background\n\nThe Capital Asset Pricing Model (CAPM) is a very practical, robust and straightforward implementation for modeling expected returns\nIt gets managers to think about risk in the correct way: instead of thinking about total risk, the CAPM shows us that we only the market risk (non-diversifiable) should be the concern\n\n\n\nThere are three simplifying assumptions around investor behavior that the CAPM establishes:\n\n\n\n\n\n\n\nThe Capital Asset Pricing Model (CAPM) Assumptions\n\n\n\nInvestors can buy and sell all securities at competitive market prices without incurring taxes or transactions costs can borrow and lend at the risk-free interest rate\nInvestors hold only efficient portfolios of traded securities\nInvestors have homogeneous expectations regarding the volatilities, correlations, and expected returns of securities\n\n\n\n\n\n\nQuestion: why these assumptions are important?"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#pricing-the-risk-premium-under-the-capm",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#pricing-the-risk-premium-under-the-capm",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Pricing the Risk Premium under the CAPM",
    "text": "Pricing the Risk Premium under the CAPM\n\nRecall that the expected return of any given asset \\(i\\) is given by:\n\n\\[\n\\small E[R_i]  =  R_f + \\beta_i^P  \\times (E[R_p] - R_f)\n\\]\n\nHow can we find \\(\\beta_i^P\\), the sensitivity of asset \\(i\\) returns to the efficient portfolio, \\(P\\)?\n\nTo identify the efficient portfolio (Markowitz 1952), we need to know the expected returns, volatilities, and correlations between all available investments!\nHowever, if the CAPM assumptions are valid, we can now identify the efficient portfolio: it is equal to the market portfolio!\n\nWhat does that mean for us in terms of determining expected equity returns? Until now, we were agnostic on what \\(P\\) was. Under the CAPM, we can change the subscript \\(P\\) to \\(M\\):\n\n\\[\n\\small E[R_i] =  R_f + \\beta_i^M  \\times (E[R_m] - R_f)\n\\]"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#capm-implication-1-the-capital-market-line-cml",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#capm-implication-1-the-capital-market-line-cml",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "CAPM Implication 1: the Capital Market Line (CML)",
    "text": "CAPM Implication 1: the Capital Market Line (CML)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#the-cml-shows-no-clear-relationship-between-risk-and-return",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#the-cml-shows-no-clear-relationship-between-risk-and-return",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "The CML shows no clear relationship between risk and return…",
    "text": "The CML shows no clear relationship between risk and return…"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#capm-implication-2-the-security-market-line-sml-makes-the-relationship-clear-when-focusing-only-betam_i",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#capm-implication-2-the-security-market-line-sml-makes-the-relationship-clear-when-focusing-only-betam_i",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "CAPM Implication 2: the Security Market Line (SML) makes the relationship clear when focusing only (\\(\\beta^M_i\\))!",
    "text": "CAPM Implication 2: the Security Market Line (SML) makes the relationship clear when focusing only (\\(\\beta^M_i\\))!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#hands-on-exercise",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#hands-on-exercise",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\nYou work as a buy-side analyst at Pierpoint Capital, focusing on the chemicals industry. You job is replicate the CAPM for a handful of securities from the Chemical (basic) industry and provide insights for the fund manager:\n\n\nWhich stocks, according to the CAPM, are undervalued and why?\nWhich stocks, according to the CAPM, are overvalued and why?\nIf the fund were to implement your strategy, what are the risks associated with?\n\n\n\n\n\n\n\nSpecific Instructions\n\n\n\nThe securities to be included in the analysis are: Dow (ticker: DOW), LyondellBasell (LYB), Perimeter (PRM), Flotek (FTK), Rayonier (RYAM), Albemarle (ALB), Celanese (CE),The Chemours (CC), Ginkgo Bioworks (DNA), and American Vanguard (AVD).\nCAPM estimation should be done at a weekly level using data from 2024"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#estimating-the-equity-cost-of-capital-in-practice",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#estimating-the-equity-cost-of-capital-in-practice",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Estimating the Equity Cost of Capital in practice",
    "text": "Estimating the Equity Cost of Capital in practice\n\nThe dynamics behind the pricing of securities under the CAPM are:\n\n\\[R_i = R_f + \\beta \\times (E[R_m] - R_f)\\]\n\nHowever, no one really told you from where the numbers came from\nRecall that, under the CAPM, we need to have estimates related to the market portfolio:\n\nIt is is equal to the risk-free interest rate, \\(R_f\\)…\nThe expected return on the market portfolio, \\(E[R_m]\\)…\nAnd a stock’s s sensitivity to the market portfolio, denoted by \\(\\beta\\)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-the-risk-free-rate",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-the-risk-free-rate",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Cost of Equity Components: the risk-free rate",
    "text": "Cost of Equity Components: the risk-free rate\n\nThe first ingredient of CAPM is risk-free rate, which is the interest rate that investors can earn while having zero to limited volatility\nSuggestions on how to pick the Risk-Free (\\(R_f\\)) rate to be used:\n\nThe yield on U.S. Treasury securities\nSurveys suggest most practitioners use 10- to 30-year treasuries\nHighest quality assets\n\nOften, we use a short-term risk-free rate to evaluate a short-term investment, and a long-term rate when evaluating a long-term investment\n\n\n\n\n\n\n\nCountry-specific risk-free rates\n\n\nWhenever modeling assets outside of the U.S, we can either use the yields for local treasuries (i.e, relatively safer assets) or use U.S treasuries by adjusting the calculations for country-specific risk premium - see, for example, Brazilian’s EMBI."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-the-market-risk-premium",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-the-market-risk-premium",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Cost of Equity Components: the market risk premium",
    "text": "Cost of Equity Components: the market risk premium\n\nAnother component of the Cost of Equity is the difference between \\(E[R_m]\\) and \\(R_f\\) (the market risk premium)\nWays to estimate the market risk premium:\n\nEstimate the risk premium (\\(E[R_m] − R_f\\)) using the historical average excess return of the market over the risk-free interest rate\nNotice that, even with long periods, we often have large standard errors\nImplicitly, you are assuming that the past is a good proxy for the future\n\n\n\n\n\n\n\n\nWatch-out!\n\n\nIndexes like the S&P500 and Ibovespa are not considered the market portfolio, but rather, they are proxies for the market portfolios - in other words, they are reasonable approximations of the market portfolio for a given set universe of securities"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-1-collecting-data",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-1-collecting-data",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 1: Collecting Data",
    "text": "Step 1: Collecting Data\n\nAs a suggestion, we will be collecting data on U.S. Treasury yields and Market Risk Premium using Kenneth French’s website, which hosts a data library with updated on U.S. returns from a wide varieaty of risk factors and asset classes\n\nTreasury yields (\\(R_F\\)) are defined as the daily returns on the 1-month Treasury Bill\nMarket Returns (\\(R_M\\)) are defined as the value-weighted returns on a bundle of U.S. stocks 1\n\nI have already worked on the data for you, and you can download it using the Download button - details on the code used to manipulate the data and put it into tidy format are presented in the next slide\nTo load the data in your session, call:\n\n\n#Assuming that you have the file in your working directory\nFF_Data=readRDS('FF_Data.RDS')\n\n\n\n Download Raw data\n\n\nClick here for details around the stock selection criteria."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-1-r_m-r_f-and-the-market-risk-premium",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-1-r_m-r_f-and-the-market-risk-premium",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 1: \\(R_M\\), \\(R_F\\), and the Market Risk Premium",
    "text": "Step 1: \\(R_M\\), \\(R_F\\), and the Market Risk Premium\n\nCodeOutput\n\n\n\n# Use Fama-French Data to retrieve Rf and MRP\nFF_url &lt;- \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_daily_CSV.zip\"\ntemp_file &lt;- tempfile()\ndownload.file(FF_url, temp_file)\n\n#Download and manipulate the data\nunzip(temp_file)%&gt;%\nread.csv(skip=3)%&gt;%\n#Select only the data, Excess Returns, and Risk-Free Columns\nselect(1,2,7)%&gt;%\n#Change the names of the variables\nsetNames(c('Date','MRP','Rf'))%&gt;%\n#Make sure the date columns is read as a Date object\nmutate(Date=as.Date(strptime(Date,format='%Y%m%d')))%&gt;%\n#Filter for 2024\nfilter(year(Date)==2024)%&gt;%\n#Manipulate data to aggregate\nmutate(across(where(is.numeric),\\(x) (1+x/100)))%&gt;%\n#Pivot to get only one column\npivot_longer(names_to = \"Key\",values_to = \"Value\",-Date)%&gt;%\n#Group by the newly created Key column\ngroup_by(Key)%&gt;%\n#Apply nest() for functional programming and perform aggregation\nnest()%&gt;%\nmutate(data = map(data,as.xts))%&gt;%\nmutate(data = map(data,apply.weekly,\\(x) prod(x)-1))%&gt;%\nmutate(data = map(data,as.data.frame))%&gt;%\nmutate(data = map(data,~rownames_to_column(.,'Date')))%&gt;%\nunnest(data)%&gt;%\n#Pivot back to wide format\npivot_wider(names_from = Key,values_from = Value)%&gt;%\n#Write as a csv\nsaveRDS('MRP_and_RF.rds')\n\n\n\n\n\n         Date           MRP           Rf\n1  2024-01-05 -1.880351e-02 0.0008802904\n2  2024-01-12  1.630595e-02 0.0011004841\n3  2024-01-19  9.912377e-03 0.0008802904\n4  2024-01-26  1.043348e-02 0.0011004841\n5  2024-02-02  1.185523e-02 0.0010804666\n6  2024-02-09  1.484388e-02 0.0010504411\n7  2024-02-16 -2.926148e-03 0.0010504411\n8  2024-02-23  1.254919e-02 0.0008402646\n9  2024-03-01  1.060239e-02 0.0010504411\n10 2024-03-08 -3.840512e-03 0.0010504411\n11 2024-03-15 -3.377924e-03 0.0010504411\n12 2024-03-22  2.306942e-02 0.0010504411\n13 2024-03-28  4.564621e-03 0.0008402646\n14 2024-04-05 -1.150557e-02 0.0010504411\n15 2024-04-12 -1.675444e-02 0.0010504411\n16 2024-04-19 -3.211752e-02 0.0010504411\n17 2024-04-26  2.707724e-02 0.0010504411\n18 2024-05-03  5.745092e-03 0.0010204162\n19 2024-05-10  1.656236e-02 0.0010004001\n20 2024-05-17  1.543225e-02 0.0010004001\n21 2024-05-24 -1.968430e-03 0.0010004001\n22 2024-05-31 -6.544949e-03 0.0008002400\n23 2024-06-07  9.163612e-03 0.0011004841\n24 2024-06-14  1.293156e-02 0.0011004841\n25 2024-06-21  5.376481e-03 0.0008802904\n26 2024-06-28  8.368114e-05 0.0011004841\n27 2024-07-05  1.761172e-02 0.0008402646\n28 2024-07-12  1.057404e-02 0.0010504411\n29 2024-07-19 -1.923631e-02 0.0010504411\n30 2024-07-26 -6.090287e-03 0.0010504411\n31 2024-08-02 -2.779540e-02 0.0010704580\n32 2024-08-09 -1.139424e-03 0.0011004841\n33 2024-08-16  3.955583e-02 0.0011004841\n34 2024-08-23  1.501644e-02 0.0011004841\n35 2024-08-30  9.239343e-04 0.0011004841\n36 2024-09-06 -4.546808e-02 0.0008002400\n37 2024-09-13  4.114533e-02 0.0010004001\n38 2024-09-20  1.494732e-02 0.0010004001\n39 2024-09-27  4.994023e-03 0.0010004001\n40 2024-10-04  1.992034e-03 0.0008803095\n41 2024-10-11  1.142536e-02 0.0008502890\n42 2024-10-18  8.262229e-03 0.0008502890\n43 2024-10-25 -1.139132e-02 0.0008502890\n44 2024-11-01 -1.142076e-02 0.0008803095\n45 2024-11-08  5.133601e-02 0.0010004001\n46 2024-11-15 -2.267490e-02 0.0010004001\n47 2024-11-22  2.167504e-02 0.0010004001\n48 2024-11-29  9.707892e-03 0.0008002400\n49 2024-12-06  1.021493e-02 0.0008502890\n50 2024-12-13 -9.249021e-03 0.0008502890\n51 2024-12-20 -2.273584e-02 0.0008502890\n52 2024-12-27  5.566751e-03 0.0006801734\n53 2024-12-31 -1.544986e-02 0.0003400289"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-2-collecting-stock-price-information",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-2-collecting-stock-price-information",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 2: collecting stock price information",
    "text": "Step 2: collecting stock price information\n\nNow that we have the right-hand side components of your CAPM equation, it is time to collect information on stock prices for the selected stocks. By now, you can pretty much apply the rationale you’ve done in previous lectures:\n\nCreate a vector assets containing the tickers that you wish to request information from\nCreate a start and end objects containing the analysis period\nUse tq_get() and pipe assets onto the function along with from=start and to=end arguments\nManipulate the data to calculate weekly returns, assigning it to an object called Stock_Data\n\nFinally, you can use left_join() to merge Stock_Data with FF_Data:\n\n\nFinal_Data=Stock_Data%&gt;%left_join(FF_Data)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-2-collecting-stock-price-information-1",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-2-collecting-stock-price-information-1",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 2: collecting stock price information",
    "text": "Step 2: collecting stock price information\n\nCodeOutput\n\n\n\n#Create the start and end dates\nstart=as.Date('2024-01-01')\nend=as.Date('2024-12-31')\n\n#Create the list of assets\nassets=c('DOW','LYB','PRM','FTK','RYAM',\n         'ALB','CE','CC','DNA','AVD')\n\n#Collect data, select necessary columns, and calculate weekly returns\nStock_Data=assets%&gt;%\n  tq_get(from=start,to=end)%&gt;%\n  select(symbol,date,adjusted)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = weeklyReturn,\n               col_rename = 'weekly_return')\n\n#Left join when column names are different\nFull_Data=Stock_Data%&gt;%left_join(FF_Data,by=c('date'='Date'))\n\n\n\n\n\n# A tibble: 530 × 5\n# Groups:   symbol [10]\n   symbol date       weekly_return      MRP       Rf\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 DOW    2024-01-05      -0.00922 -0.0188  0.000880\n 2 DOW    2024-01-12      -0.0265   0.0163  0.00110 \n 3 DOW    2024-01-19      -0.0105   0.00991 0.000880\n 4 DOW    2024-01-26       0.0237   0.0104  0.00110 \n 5 DOW    2024-02-02      -0.0118   0.0119  0.00108 \n 6 DOW    2024-02-09       0.0107   0.0148  0.00105 \n 7 DOW    2024-02-16       0.0276  -0.00293 0.00105 \n 8 DOW    2024-02-23       0.0164   0.0125  0.000840\n 9 DOW    2024-03-01       0.00146  0.0106  0.00105 \n10 DOW    2024-03-08       0.0151  -0.00384 0.00105 \n# ℹ 520 more rows\n\n\n\\(\\rightarrow\\) You now have the weekly returns for all selected stocks, the weekly risk-free returns, and the weekly market returns!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-beta-estimation",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-beta-estimation",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Cost of Equity Components: \\(\\beta\\) estimation",
    "text": "Cost of Equity Components: \\(\\beta\\) estimation\n\nAll that’s left is to estimate the stocks’s sensitivity to the returns of the market portfolio, \\(\\beta\\)\nFrom what we know on the theory on portfolio returns, a new asset \\(i\\) should be enhance the performance of a portfolio if:\n\n\\[\n\\small \\underbrace{\\frac{E[R_i] - R_f}{\\sigma_{i} \\times Corr(R_i,R_m)}}_{\\text{Sharpe Ratio of } i} &gt; \\underbrace{\\frac{E[R_m] - R_f}{\\sigma_{m}}}_{\\text{Sharpe Ratio of Market}}\n\\]\n\nWith that, we saw that the expected return from an asset \\(i\\) should be:\n\n\\[\n\\small R_i - R_f = \\underbrace{\\frac{\\sigma_{i} \\times Corr(R_i,R_m)}{\\sigma_{m}}}_{\\beta^M_i}  \\times (E[R_m] - R_f)\n\\]"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-beta-estimation-continued",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#cost-of-equity-components-beta-estimation-continued",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Cost of Equity Components: \\(\\beta\\) estimation, continued",
    "text": "Cost of Equity Components: \\(\\beta\\) estimation, continued\n\nBecause \\(\\small Corr(R_i,R_m)=\\frac{Cov(R_i,R_m)}{\\sigma_i\\sigma_m}\\), we have that:\n\n\\[\n\\small (R_i - R_f)=\\frac{\\sigma_{i} \\times Cov(R_i,R_m)}{\\sigma_i \\sigma_m\\sigma_{m}}  \\times (E[R_p] - R_f)\\rightarrow  (R_i - R_f)=  \\underbrace{\\frac{Cov(R_i,R_m)}{\\sigma^2_m}}_{\\text{OLS formula for slope}}\\times (E[R_p] - R_f)\n\\]\n\nWe can then estimate \\(\\beta\\) using an Ordinary Least Squares regression:\n\n\\[\n\\small \\underbrace{(R_i - R_f)}_{\\text{Excess Return}} = \\underbrace{\\alpha_i}_{\\text{Uncorrelated Return}} + \\underbrace{\\beta_i}_{\\text{Stock's Market Sensitivity}} \\times \\underbrace{(R_m - R_f)}_{\\text{Risk Premium}} + \\epsilon_i\n\\]\n\n\\(\\epsilon_i\\) is the error term (or the residual). It represents the deviations from the best-fitting line and is, by definition, zero on average (or else we could improve the fit), and represent firm-specific risk that is diversifiable and that averages out in a large portfolio"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-3-estimating-alpha-and-beta",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-3-estimating-alpha-and-beta",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 3: estimating \\(\\alpha\\) and \\(\\beta\\)",
    "text": "Step 3: estimating \\(\\alpha\\) and \\(\\beta\\)\n\nWe know need estimate the following equation for each stock in our analysis:\n\n\\[\n\\small Excess_t = \\alpha + \\beta \\times (R_m - R_f) + \\epsilon_t\n\\]\n\nThe naivest way to do it is to repeat the process \\(10\\) times, filtering each stock at a time, and running an OLS model with the lm() function:\n\nStart with the Full_Data object and pipe onto mutate to creat the excess_return variable\nselect only the symbol, the excess_return, and the MRP columns\nUse filter to work with a single ticker, say, symbol=='DOW'\nCall the lm() function to run an OLS regression of excess returns on market returns\n\nThe next slide shows the result of estimating the CAPM model for RYAM"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-3-estimating-alpha-and-beta-dow-only",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-3-estimating-alpha-and-beta-dow-only",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 3: estimating \\(\\alpha\\) and \\(\\beta\\) (DOW only)",
    "text": "Step 3: estimating \\(\\alpha\\) and \\(\\beta\\) (DOW only)\n\nCodeOutput\n\n\n\n#Manipulate data\nDOW=Full_Data%&gt;%\n  filter(symbol=='DOW')%&gt;%\n  mutate(excess_return=weekly_return-Rf)%&gt;%\n  select(symbol,excess_return,MRP)\n\n#Run the OLS regression\nOLS=lm(excess_return~MRP,data=DOW)\n\n#Inspect the results using summary()\nsummary(OLS)\n\n\n\n\n\n\nCall:\nlm(formula = excess_return ~ MRP, data = DOW)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.062717 -0.014611 -0.000301  0.016377  0.069557 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.007160   0.003631  -1.972   0.0541 .\nMRP          0.355167   0.198406   1.790   0.0795 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02562 on 50 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.06023,   Adjusted R-squared:  0.04143 \nF-statistic: 3.204 on 1 and 50 DF,  p-value: 0.07949"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#understanding-the-beta-term-inside-the-ols-estimation",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#understanding-the-beta-term-inside-the-ols-estimation",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Understanding the \\(\\beta\\) term inside the OLS estimation",
    "text": "Understanding the \\(\\beta\\) term inside the OLS estimation\n\\[\n\\small \\underbrace{(R_i - R_f)}_{\\text{Excess Return}} = \\underbrace{\\alpha}_{\\text{Uncorrelated Return}} + \\underbrace{\\beta}_{\\text{Stock's Market Sensitivity}} \\times \\underbrace{(R_m - R_f)}_{\\text{Risk Premium}} + \\epsilon\n\\]\n\n\\(\\beta\\) is the sensitivity to market risk. It measures the historical variation of the security relative to the market\nAccording to the CAPM, all assets should line on the Security Market Line (SML)\n\n\nIf \\(\\beta&gt;1\\), it means that a 1% variation in market returns implies a variation that is greater than 1% in stock returns (either up or down!)\nIf \\(\\beta&lt;1\\) it means that a 1% variation in market returns implies a variation that is less than 1% in stock returns (either up or down!)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#assessing-required-returns",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#assessing-required-returns",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Assessing Required Returns",
    "text": "Assessing Required Returns\n\nSuppose you need to price the long-run required returns for investing in an opportunity that has the same equity risk and as RYAM. It is now a straightforward application of the CAPM:\n\n\\[\n\\small \\text{Required Return} = R_f +  \\beta \\times (R_m - R_f)\n\\]\n\nSay, for example that you have the following information:\n\nThe historical long-run risk-free rate return, \\(\\small R_f\\), is \\(\\small 4.50\\%\\)\nThe historical long-run market return, \\(\\small R_m\\), is \\(\\small 9.94\\%\\)\nThe \\(\\beta\\) you’ve just found is \\(\\small 1.23\\)\n\nThen, the long-run required return is simply:\n\n\\[\n\\small \\text{Required Return} = 4.5\\%+ 1.23\\times(9.94\\%-4.5\\%)=11.20\\%\n\\]"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#understanding-the-alpha-term-inside-the-ols-estimation",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#understanding-the-alpha-term-inside-the-ols-estimation",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Understanding the \\(\\alpha\\) term inside the OLS estimation",
    "text": "Understanding the \\(\\alpha\\) term inside the OLS estimation\n\\[\n\\small \\underbrace{(R_i - R_f)}_{\\text{Excess Return}} = \\underbrace{\\alpha}_{\\text{Uncorrelated Return}} + \\underbrace{\\beta}_{\\text{Stock's Market Sensitivity}} \\times \\underbrace{(R_m - R_f)}_{\\text{Risk Premium}} + \\epsilon\n\\]\n\n\\(\\alpha_i\\) is the constant term. It measures the historical performance of the security relative to the expected return predicted by the security market line\nIt is the distance that the stock’s average return is above or below the SML. Thus, we can say \\(\\alpha_i\\) is a risk-adjusted measure of the stock’s historical performance.\nAccording to the CAPM, \\(\\alpha_i\\) should not be significantly different from zero\n\n\nIf \\(\\alpha&gt;0\\) consistently, it would mean that a security delivers a constant positive return and, by definition, independent from the market returns\nIf that is the case, investors would buy the security up to a point where price adjusts so that \\(\\alpha\\) goes to zero (recall Assumption #1)!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#interpreting-alpha",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#interpreting-alpha",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Interpreting \\(\\alpha\\)",
    "text": "Interpreting \\(\\alpha\\)\n\\[\\small \\alpha_i = \\underbrace{E[R_i]}_{\\text{Observed by the analyst}} - \\underbrace{R_i}_{\\text{Implied by the CAPM}}\\]\n\nA positive alpha means that the stock is above the SML\n\nIn words, the expected return is higher than its required return. Before prices adjust, investors will anticipate that the price will rise and will likely put in buy orders at the current prices\n\nA negative alpha means that the stock is below the SML\n\nThe expected return is lower than its required return. Before prices adjust, investors will anticipate that the price will fall and will likely put in sell orders at the current prices\n\n\n\\(\\rightarrow\\) In either case, we’ll be able to improve portfolio results. However, as we do so, prices will change and their alphas will shrink towards zero!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-4-putting-all-together",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-4-putting-all-together",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 4: putting all together",
    "text": "Step 4: putting all together\n\nOur final step is to use the CAPM to assess which stocks are overvalued, and which ones are undervalued\nYou could proceed by doing the same procedure as before, but now focusing on the \\(\\alpha\\) term that has been estimated for you\n\nTo do that for all 10 stocks, you could do a for loop, store the results, and analyze\nIn general, for loops are inefficient: they run sequentially, have slower performance, and are difficult to read\n\nAn alternative is to use the tidyverse excellent capabilities for functional programming using the map function from the purrr package, which breaks the problem into sub-pieces and estimate the models in parallel\nFor detailed information on functional programming, see purrr documentation here"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-4-putting-all-together-continued",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-4-putting-all-together-continued",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 4: putting all together, continued",
    "text": "Step 4: putting all together, continued\n\nFor LoopFunctional Programming\n\n\n\n#Start an empty data.frame\nStored_Data=data.frame()\n\nfor (i in assets){\n  \n  #Manipulate data\n  Filtered_Data=Full_Data%&gt;%\n    #Filter for the specific ticker\n    filter(symbol== i)%&gt;%\n    mutate(excess_return=weekly_return-Rf)%&gt;%\n    select(symbol,excess_return,MRP)\n  \n  #Run the OLS regression\n  OLS=lm(excess_return~MRP,data=Filtered_Data)\n\n  #Get the coefficients using the coefficients() function and add it to a temp data\n  \n  Temp_Data = data.frame(ticker=i,\n                         alpha=coefficients(OLS)[1],\n                         beta=coefficients(OLS)[2])\n  \n  #Bind it to the dataframe\n  Stored_Data=Stored_Data%&gt;%rbind(Temp_Data)\n  \n  }\n\n\n\n\nCAPM_Estimation=Full_Data%&gt;%\n  mutate(excess_return=weekly_return-Rf)%&gt;%\n  select(symbol,excess_return,MRP)%&gt;%\n  group_by(symbol)%&gt;%\n  nest()%&gt;%\n  mutate(CAPM = map(data,~ lm(excess_return~MRP,data=.)))%&gt;%\n  mutate(coefficients = map(CAPM,tidy))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#charting-the-result",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#charting-the-result",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Charting the result",
    "text": "Charting the result\n\nCodeOutput\n\n\n\nCAPM_Estimation%&gt;%\n  select(coefficients)%&gt;%\n  unnest()%&gt;%\n  filter(term=='(Intercept)')%&gt;%\n  select(symbol,estimate)%&gt;%\n  setNames(c('Ticker','Alpha'))%&gt;%\n  ggplot(aes(x=Ticker,y=Alpha))+\n  geom_point(size=5)+\n  geom_segment(aes(yend = 0), linetype = \"dashed\", color = \"gray50\")+\n  geom_hline(yintercept=0,linetype='dashed')+\n  #Annotations\n  labs(title='Using CAPM to analyze over/undervalued investment opportunities',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Ticker',\n       y = 'Alpha')+\n  #Scales\n  scale_y_continuous(labels = percent)+\n  #Custom 'TidyQuant' theme\n  theme_tq()+\n  #Adding further customizations\n  theme(legend.position='none',\n        axis.title = element_text(face='bold',size=15),\n        axis.text = element_text(size=10),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15,face='bold'))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#capm-shortcomings",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#capm-shortcomings",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "CAPM shortcomings",
    "text": "CAPM shortcomings\n\nIn our estimation, none of the \\(\\small \\alpha\\) results were statistically significant, meaning that we cannot reject the hypothesis that \\(\\alpha\\) is different from zero\n\nIn such a way, it implies that investors cannot really earn abnormal returns that are uncorrelated with the market\nHowever, many researchers have found market anomalies where it was possible to create a strategy that generated positive \\(\\alpha\\)\n\nIt is important to recall some of the model’s shortcomings:\n\nMarket Returns are really context-dependent, and the use of NYSE/DOW/Ibovespa etc is problem-specific\nThe sensitivity to market returns, \\(\\small \\beta\\), might not be stable over time\nSystematic Risk might not be the only factor that matters!\nAssumptions of the CAPM might not be that realistic"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#references",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#references",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nFama, Eugene F. 1970. “Efficient Capital Markets: A Review of Theory and Empirical Work.” The Journal of Finance 25 (2): 383–417. https://doi.org/10.2307/2325486.\n\n\nFama, Eugene F., and Kenneth R. French. 1993. “Common Risk Factors in the Returns on Stocks and Bonds.” Journal of Financial Economics 33 (1): 3–56.\n\n\nMarkowitz, Harry. 1952. “Portfolio Selection.” The Journal of Finance 7 (1): 77–91. http://www.jstor.org/stable/2975974.\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "quant-fin/datacases/data-case-prep/data-case-prep.html",
    "href": "quant-fin/datacases/data-case-prep/data-case-prep.html",
    "title": "Data Case Prep",
    "section": "",
    "text": "This exercise list is a preparation for students of the Practical Applications in Quantitative Finance course held at FGV-EAESP. This document serves as a refresher on key R programming concepts covered in previous lectures, reinforcing the essential skills needed for quantitative finance applications. By revisiting these foundations, you’ll ensure a solid grasp of the tools and techniques required to analyze financial data effectively. As we progress, you’ll apply these concepts to real-world finance examples, including portfolio analysis, risk assessment, and asset pricing models. Mastering these skills now will prepare you to fully engage with the practical applications we’ll explore throughout the course.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®.\n\n\n\n\n\n\nDeliverable\n\n\n\nEach student is expected to deliver his/her assignment individually, although you can freely work in groups for solving the questions. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both your code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details.\n\n\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)\n\n\n\n\n\n\n\nNote\n\n\n\nFor the remainder of the sessions, you will use financial data from multiple sources, either that being from a local file or a pull from Yahoo! Finance. Whenever you are working with local files, it is always important to make sure that your R is able to locate it. To check the working directory of your session, simply type getwd(), and it will prompt your current directory. If you want to change your directory, simply type setwd('C:/path/to/your/folder') with the specific path to your desired folder. To make sure that you switched directories, you can type getwd() to confirm the new directory.\nMost of the issues regarding not being able to load a specific file, like .csv and .xlsx spreadsheets can be easily solved by placing your R file (either a plain script, like myscript.R, or a quarto document, myquartodoc.qmd) in the same folder as of your data. When you open your R script or Quarto document, it will automatically set that folder (which coincides with the data folder) as the working directory. To confirm which files are available to you, you can simply type list.files() to get the list of all files that R can find in the working directory.\nIf you prefer, whenever you are calling a function that requires a path to your computer, you can always provide the full path of the file: for example, using \"C:/Users/Lucas/Documents/GitHub/Folder/test.csv' would find the test.csv even if Folder is not your working directory.\nWhenever you are unsure about how a specific function works, type the function in an R script and you will notice that RStudio will auto complete the function name for you. To get more information on a given function’s arguments, hit F1 to see a description at the bottom-right of your session.\n\n\n\n\n\nThis exercise aims to gauge your skill in fetching data in R through different sources. You are asked to work on the following questions:\n\nGo to our eClass® and download the Ibovespa_Sample.txt file inside the Data Cases folder. This file contains stock price information regarding 6 publicly traded companies that are part of the Brazilian market index, Ibovespa.\nRead the file into your session using the read_delim() function, and assign to an object called FILE_SAMPLE. Make sure you have the tidyverse package loaded. Depending on how your computer reads delimited data, you may need to set the argument sep to sep=';' or sep=' '.\nInspect the first 10 lines of the object you have just created using the head() function, setting n=10 as an argument.\nHow many rows and columns does your object have? You can use the function dim(yourdata) to find the exact dimensions.\nAlternatively, fetch the exact same stock level information directly from Yahoo! Finance using the tq_get() function. Collect the data beginning in 2020 (January \\(1^{st}\\) until the end of 2024 (December \\(31^{st}\\)) using the from and to arguments of the tq_get() function. Make sure you have the tidyquant package loaded. Assign this data to a new object in your R session called YAHOO_SAMPLE.\n\n\n\n\n\n\n\nHint\n\n\n\nRemember that the syntax for using tq_get() to collect stock prices is:\n\nmy_stocks &lt;- tq_get(c(\"stock1\", \"stock2\"),\n                      from = \"YYYY-MM-DD\",\n                      to   = \"YYYY-MM-DD\")\n\nFurthermore, you can use the unique(FILE_SAMPLE$symbol) to find the exact tickers contained in the file, and assign it to a new object, assets, which will then be piped onto a call to tq_get().\n\n\n\nHow many rows and columns does this new object have?\n\n\n\n\nThis exercise aims to gauge your skill on manipulating data using the tidyverse and the tidyquant. After loading these packages, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE you created before, create a new column, Year, that is defined as the year of a given timestamp found in the date column. You can use year(date) along with a mutate function to create this new variable. Update your YAHOO_SAMPLE object to make sure you have saved this variable.\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the mutate function is:\n\nmutate(.data, #The object you are performing the calculations \n       new_variable_1 = var1 * 2, #Can use basic operations...\n       new_variable_2 = median(var2), #Or predefined functions)\n       variable_3 = as.character(var3) #And can be used to modify existing variables)\n       ) \n\nFurthermore, recall that you can always use the pipe operator (%&gt;%) to chain operations along the way whenever you are using the tidyverse.\n\n\n\nUsing the updated YAHOO_SAMPLE object, use the filter function to filter for observations that occur in \\(2024\\), use group_by() to group the data by symbol, and use the summarize the function to create a new variable, avg_price, defined as the average adjusted prices for each year-symbol combination. Sort your dataset by avg_price (highest-to-lowest), storing this result in an object called 2023_2024_Summary and call it in your terminal. Which company had the highest price levels, and which one had the lowest?\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the filter function is:\n\nfilter(.data, #The object which you are performing the operations\n       variable_1 &gt;10, #Simple arithmetic operators\n       variable_2 %in% c('AAPL','MSFT','FORD'), #Pattern search\n       !(variable_3 %in% c('Boston','Mass','Silicon Valley')), #Negate pattern search\n       variable_4 &gt;=10 & variable_3&lt;= 4 | is.na(variable_4) #IF and OR conditions\n       ) \n\n\n\n\nUsing the YAHOO_SAMPLE dataset, group your data by symbol and create a summary of yearly returns. To do that, first pipe the grouped data into into the tq_transmute function applying the yearlyReturn, assigning it to a new variable,yearly_return. Arrange your result by date and yearly_return (highest-to-lowest). Which stock had the best performance in 2020, and which stock had the worst?\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the tq_transmute function is:\n\ntq_transmute(.data, #The object you are performing the calculations \n       selected_variables, #The columns to send to the mutation function\n       mutate_fun, #The mutation function from either the xts, quantmod, or TTR package.\n       col_rename #A string or character vector containing names that can be used to quickly rename columns\n       )\n\n\n\n\n\n\nThis exercise aims to gauge your skill on data visualization using the ggplot2 package. After loading this package, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE object, create a ggplot object with the follow aes: the x component should refer to the date column, the y axis should refer to the adjusted column, and the group argument should refer to the symbol column.\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for mapping your data to ggplot and adding further layers for adjusting aesthetics is:\n\nggplot(data=your_data, aes(x= variable_1, y=variable_2, group=your_group,...))+\ngeom_{yourgeom} +\nadditional_layer_1()+\nadditional_layer_2()+\nany_additional_layer_commands()\n\n\n\n\nAdd a geometry layer to your chart using the geom_line() function and call it in your terminal.\nChange the titles of your x and y axis, as well as chart title and subtitle using the labs() function.\nFacet your data by each symbol using the facet_wrap(symbol~.) function. Set the arguments of this function in such a way that you have a 2x3 orientation (i.e, nrow=2 and ncol=3).\nApply a predefined theme using the theme_minimal() function.\nSave your plot using the ggsave() function. To use it, call ggsave('myplot.jpg',width=10,height = 6) to save the last plot that has been prompted to your terminal using a 10x6 resolution (in units)."
  },
  {
    "objectID": "quant-fin/datacases/data-case-prep/data-case-prep.html#tech-setup",
    "href": "quant-fin/datacases/data-case-prep/data-case-prep.html#tech-setup",
    "title": "Data Case Prep",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)\n\n\n\n\n\n\n\nNote\n\n\n\nFor the remainder of the sessions, you will use financial data from multiple sources, either that being from a local file or a pull from Yahoo! Finance. Whenever you are working with local files, it is always important to make sure that your R is able to locate it. To check the working directory of your session, simply type getwd(), and it will prompt your current directory. If you want to change your directory, simply type setwd('C:/path/to/your/folder') with the specific path to your desired folder. To make sure that you switched directories, you can type getwd() to confirm the new directory.\nMost of the issues regarding not being able to load a specific file, like .csv and .xlsx spreadsheets can be easily solved by placing your R file (either a plain script, like myscript.R, or a quarto document, myquartodoc.qmd) in the same folder as of your data. When you open your R script or Quarto document, it will automatically set that folder (which coincides with the data folder) as the working directory. To confirm which files are available to you, you can simply type list.files() to get the list of all files that R can find in the working directory.\nIf you prefer, whenever you are calling a function that requires a path to your computer, you can always provide the full path of the file: for example, using \"C:/Users/Lucas/Documents/GitHub/Folder/test.csv' would find the test.csv even if Folder is not your working directory.\nWhenever you are unsure about how a specific function works, type the function in an R script and you will notice that RStudio will auto complete the function name for you. To get more information on a given function’s arguments, hit F1 to see a description at the bottom-right of your session."
  },
  {
    "objectID": "quant-fin/datacases/data-case-prep/data-case-prep.html#exercise-1",
    "href": "quant-fin/datacases/data-case-prep/data-case-prep.html#exercise-1",
    "title": "Data Case Prep",
    "section": "",
    "text": "This exercise aims to gauge your skill in fetching data in R through different sources. You are asked to work on the following questions:\n\nGo to our eClass® and download the Ibovespa_Sample.txt file inside the Data Cases folder. This file contains stock price information regarding 6 publicly traded companies that are part of the Brazilian market index, Ibovespa.\nRead the file into your session using the read_delim() function, and assign to an object called FILE_SAMPLE. Make sure you have the tidyverse package loaded. Depending on how your computer reads delimited data, you may need to set the argument sep to sep=';' or sep=' '.\nInspect the first 10 lines of the object you have just created using the head() function, setting n=10 as an argument.\nHow many rows and columns does your object have? You can use the function dim(yourdata) to find the exact dimensions.\nAlternatively, fetch the exact same stock level information directly from Yahoo! Finance using the tq_get() function. Collect the data beginning in 2020 (January \\(1^{st}\\) until the end of 2024 (December \\(31^{st}\\)) using the from and to arguments of the tq_get() function. Make sure you have the tidyquant package loaded. Assign this data to a new object in your R session called YAHOO_SAMPLE.\n\n\n\n\n\n\n\nHint\n\n\n\nRemember that the syntax for using tq_get() to collect stock prices is:\n\nmy_stocks &lt;- tq_get(c(\"stock1\", \"stock2\"),\n                      from = \"YYYY-MM-DD\",\n                      to   = \"YYYY-MM-DD\")\n\nFurthermore, you can use the unique(FILE_SAMPLE$symbol) to find the exact tickers contained in the file, and assign it to a new object, assets, which will then be piped onto a call to tq_get().\n\n\n\nHow many rows and columns does this new object have?"
  },
  {
    "objectID": "quant-fin/datacases/data-case-prep/data-case-prep.html#exercise-2",
    "href": "quant-fin/datacases/data-case-prep/data-case-prep.html#exercise-2",
    "title": "Data Case Prep",
    "section": "",
    "text": "This exercise aims to gauge your skill on manipulating data using the tidyverse and the tidyquant. After loading these packages, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE you created before, create a new column, Year, that is defined as the year of a given timestamp found in the date column. You can use year(date) along with a mutate function to create this new variable. Update your YAHOO_SAMPLE object to make sure you have saved this variable.\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the mutate function is:\n\nmutate(.data, #The object you are performing the calculations \n       new_variable_1 = var1 * 2, #Can use basic operations...\n       new_variable_2 = median(var2), #Or predefined functions)\n       variable_3 = as.character(var3) #And can be used to modify existing variables)\n       ) \n\nFurthermore, recall that you can always use the pipe operator (%&gt;%) to chain operations along the way whenever you are using the tidyverse.\n\n\n\nUsing the updated YAHOO_SAMPLE object, use the filter function to filter for observations that occur in \\(2024\\), use group_by() to group the data by symbol, and use the summarize the function to create a new variable, avg_price, defined as the average adjusted prices for each year-symbol combination. Sort your dataset by avg_price (highest-to-lowest), storing this result in an object called 2023_2024_Summary and call it in your terminal. Which company had the highest price levels, and which one had the lowest?\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the filter function is:\n\nfilter(.data, #The object which you are performing the operations\n       variable_1 &gt;10, #Simple arithmetic operators\n       variable_2 %in% c('AAPL','MSFT','FORD'), #Pattern search\n       !(variable_3 %in% c('Boston','Mass','Silicon Valley')), #Negate pattern search\n       variable_4 &gt;=10 & variable_3&lt;= 4 | is.na(variable_4) #IF and OR conditions\n       ) \n\n\n\n\nUsing the YAHOO_SAMPLE dataset, group your data by symbol and create a summary of yearly returns. To do that, first pipe the grouped data into into the tq_transmute function applying the yearlyReturn, assigning it to a new variable,yearly_return. Arrange your result by date and yearly_return (highest-to-lowest). Which stock had the best performance in 2020, and which stock had the worst?\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for using the tq_transmute function is:\n\ntq_transmute(.data, #The object you are performing the calculations \n       selected_variables, #The columns to send to the mutation function\n       mutate_fun, #The mutation function from either the xts, quantmod, or TTR package.\n       col_rename #A string or character vector containing names that can be used to quickly rename columns\n       )"
  },
  {
    "objectID": "quant-fin/datacases/data-case-prep/data-case-prep.html#exercise-3",
    "href": "quant-fin/datacases/data-case-prep/data-case-prep.html#exercise-3",
    "title": "Data Case Prep",
    "section": "",
    "text": "This exercise aims to gauge your skill on data visualization using the ggplot2 package. After loading this package, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE object, create a ggplot object with the follow aes: the x component should refer to the date column, the y axis should refer to the adjusted column, and the group argument should refer to the symbol column.\n\n\n\n\n\n\n\nHint\n\n\n\nRecall that the syntax for mapping your data to ggplot and adding further layers for adjusting aesthetics is:\n\nggplot(data=your_data, aes(x= variable_1, y=variable_2, group=your_group,...))+\ngeom_{yourgeom} +\nadditional_layer_1()+\nadditional_layer_2()+\nany_additional_layer_commands()\n\n\n\n\nAdd a geometry layer to your chart using the geom_line() function and call it in your terminal.\nChange the titles of your x and y axis, as well as chart title and subtitle using the labs() function.\nFacet your data by each symbol using the facet_wrap(symbol~.) function. Set the arguments of this function in such a way that you have a 2x3 orientation (i.e, nrow=2 and ncol=3).\nApply a predefined theme using the theme_minimal() function.\nSave your plot using the ggsave() function. To use it, call ggsave('myplot.jpg',width=10,height = 6) to save the last plot that has been prompted to your terminal using a 10x6 resolution (in units)."
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1.html",
    "href": "quant-fin/datacases/data-case-1/data-case-1.html",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "This Data Case is part of the Practical Applications in Quantitative Finance course, held at FGV-EAESP’s undergraduate course in business. Carefully follow the instructions contained in the data case as well as eClass® before you make your submission.\n\n\nThe pharmaceutical industry is a critical sector in financial markets, influenced by regulatory approvals, drug developments, and global health events. In this first Data Case, you will analyze stock performance for a set of 10 pharmaceutical companies over time, applying the tidyverse and tidyquant packages to extract and interpret insights from the data.\nYou are a junior analyst at Atlas Capital, a leading buyside investment firm specializing in sector-focused strategies. The firm is considering increasing its exposure to the pharmaceutical industry, given its long-term growth potential and resilience in volatile markets. In the latest investment committee meeting, your fund manager raised an important question: “How has the pharmaceutical industry performed over time? We need to identify whether now is the right time to increase our position.”\nYour team has been tasked with conducting an in-depth financial analysis of the pharmaceutical sector. The goal is to assess industry-wide trends, identify risks and opportunities, and ultimately recommend an investment stance. More specifically, your task will involve:\n\nCollecting stock price data and compute returns\nVisualizing key trends in returns and volatility\nInterpreting findings and suggest investment insights\n\nTo streamline our research, you will focus on the 10 largest publicly traded pharmaceutical companies in the U.S, analyze their performance, risks, and potential catalysts that could drive returns in the near future. As of February 2025, the 10 largest pharmaceutical companies traded in the U.S., along with their ticker symbols, are:\n\nEli Lilly and Co. (LLY): A leading pharmaceutical company known for its innovative treatments in diabetes and oncology.\nNovo Nordisk A/S (NVO): Specializing in diabetes care, Novo Nordisk has a significant presence in the U.S. market.\nJohnson & Johnson (JNJ): A diversified healthcare company with a strong pharmaceutical division.\nAbbVie Inc. (ABBV): Known for its immunology and oncology products, AbbVie is a major player in the pharmaceutical industry.\nMerck & Co., Inc. (MRK): Merck offers a wide range of prescription medicines, vaccines, and therapies.\nPfizer Inc. (PFE): A global pharmaceutical corporation recognized for its vaccines and therapeutics.\nBristol-Myers Squibb Company (BMY): Focused on oncology, cardiovascular, and immunology, Bristol-Myers Squibb is a key industry player.\nAstraZeneca PLC (AZN): A biopharmaceutical company with a strong portfolio in oncology and respiratory diseases.\nAmgen Inc. (AMGN): Specializing in biotechnology, Amgen develops therapies for serious illnesses.\nGilead Sciences, Inc. (GILD): Known for its antiviral drugs, Gilead has a significant market presence.\n\nNow, it’s up to you and your team to dive into the data, extract key insights, and present your data-driven investment thesis. Good luck—your next career milestone at Atlas Capital depends on it. 🚀\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®.\n\n\n\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\",\"ggcorrplot\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)\n\n\n\n\nUse the tq_get() function from the tidyquant package to retrieve historical adjusted closing prices for the 10 largest publicly traded pharmaceutical companies in the U.S. from Yahoo! Finance. Your dataset should cover the period from January 1, 2020, to December 31, 2024. Using the functions from the tidyverse, ensure that your data includes only the timestamp column, as well as the column that contains the daily adjusted stock price information. Store this into an object called financial_data (or something similar). Store this data set for all the subsequent analysis - make sure not to override this dataset as you move along the data case to make sure you are always referring to the raw data pull!\n\n\n\nUsing the tidyquant package, use the object you’ve just created with the tq_transmute function to compute the yearly returns for each stock over the analysis period. More specifically, pass the yearlyReturn function to adjusted column using the tq_transmute, labeling this new variable as yearly_return. Arrange your dataset by year and in descending order of yearly_return (highest-to-lowest). Store this into a new object called, for example, yearly_returns. Which stock had the highest return in 2024, and which one had the lowest? Prompt the results in your session.\n\n\n\nWith your data.frame containing the yearly returns over time for each stock, use ggplot to create a line chart of the historical cumulative returns for each stock during the study period. Which stock had the highest cumulative return up-to-date? Recall that cumulative returns can be calculated from period returns as:\n\\[\n\\text{Cumulative Return}= (1+R_1)\\times(1+R_2)\\times ... \\times(1+R_T)-1\\equiv \\prod (1+R_t)-1\n\\]\nYour chart should map date to the x-axis, the yearly return variable to the y axis, and group the results by symbol. To make sure that you are plotting a line chart, use the geom_line() function after you have mapped your data. In addition to these two layers, add any customizations that you believe that are beneficial to convey the message - see the Data Visualization\n\n\n\n\n\n\nHint\n\n\n\n\nWith the data.frame you created to store yearly returns, group by symbol, and use tq_transmute() to apply the Return.cumulative function to the data.\nNow, your resulting data.frame contains the cumulative returns for all stocks. You can adjust the column names with the setNames() function and pipe that into a ggplot call, mapping the symbol to the x-axis, the cumulative return column to the y-axis, and the geom_col() function to create a bar chart. Add as many customizations you think are worth the effort.\n\n\n\n\n\n\nAfter reviewing your initial analysis, your fund manager at Atlas Capital liked the idea of examining yearly returns to get a broader perspective on performance. However, they pointed out that pharmaceutical companies vary significantly in terms of risk exposure, so it’s crucial to account for volatility as well. To complement the analysis, use the same rationale from the previous exercise to calculate the yearly volatility for each stock. How do the risk levels differ between firms? Store your results in a new object and prompt it in your session.\n\n\n\n\n\n\nHint\n\n\n\nAs opposed to the yearlyReturn function, the tidyquant package does not have a pre-built dailyStdev function. Instead, what you can do is to use a combination of functions to get the expected result:\n\nFirst, use tq_transmute() to calculate daily returns passing the dailyReturn function\nNow, your resulting data.frame contains daily returns for all stocks. It is now in a convenient format to chain this object again, in another tq_transmute() function, applying the StdDev.annualized function and assign to a new object, like yearly_volatility. Note, however, that if you simply use StdDev.annualized, it will calculate an annualized metric for each stock for the whole period, which is not what you want.\n\nTo make sure that you have calculating the annualized standard deviation for each year, you can do a composition of apply.yearly, which applies a given function at yearly intervals, and StdDev.annualized, using the following syntax:\n\nyour_daily_return_object%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=StdDev.annualized,\n               col_rename = 'yearly_volatility')\n\nHere, tq_transmute() will apply the function defined in FUN over each interval.\n\n\n\n\n\nBuilding on your previous findings, since some companies exhibit higher returns but also greater risk, it might be a good idea to add a risk-adjusted performance metric to the analysis. The Sharpe ratio for stock \\(i\\) in period \\(t\\) measures the risk-adjusted return of an asset and is calculated as:\n\\[\n\\text{Sharpe Ratio}_{i,t}=\\dfrac{R_{i,t}-R_{f,t}}{\\sigma_{i,t}},\n\\]\nwhere \\(R_{i,t}\\) is the return of a given stock \\(i\\) in period \\(t\\), \\(R_{f,t}\\) is the risk-free return for the same period, and \\(\\sigma_{i,t}\\) is the volatility for stock \\(i\\) in period \\(t\\).\nYour task is to calculate the Sharpe Ratio for each pharmaceutical stock using yearly returns and yearly volatility. To simplify your calculations, assume a risk-free rate of \\(0\\%\\) per year (i.e, no risk-free premium). Compare the Sharpe ratios across companies. Do the highest-return stocks also have the best risk-adjusted performance? Are there any stocks that stand out as particularly efficient in generating returns relative to their risk? Are there companies that deliver strong returns but with disproportionately high volatility?\n\n\n\n\n\n\nHint\n\n\n\nThere are two ways you can use to create the Sharpe Ratio:\n\nUsing the previously created yearly_returns and yearly_volatility objects, use the left_join() function to merge them based on a common set of identifiers (in this case, date and symbol). After that, manipulate the resulting data.frame with mutate to generate the Sharpe Ratio.\nUsing tq_transmute in a very similar fashion to what you have done to calculate the yearly volatility, but now passing the the SharpeRatio.annualized function with arguments Rf=0 and scale=252.\n\nAlthough both approaches should yield similar results, potential differences might stem from rounding.\n\n\n\n\n\nWay to go! As you delve deeper into your investment analysis, your fund manager emphasizes the importance of understanding how different pharmaceutical stocks interact with one another over time. To gain insights into the relationships between these companies, your next task is to calculate the correlation of daily stock returns for the selected pharmaceutical companies for the analysis period.\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the daily returns for each stock. You can use the tq_transmute function into your dataset and apply the dailyReturn function.\nAfter that, you need to pivot your data in such a way that each column is a specific ticker with information on daily returns. You can do that by calling pivot_wider(names_from='symbol',values_from='daily_return'), assuming that your daily return variable is called daily_return.\nWith that, you’ll achieve a data frame that now has \\(11\\) columns, namely, the date and the \\(10\\) individual ticker columns with daily return information.\nTo make sure that you are calculating the correlation using a \\(10\\times10\\) matrix, use select(-date) to get rid of the date column and pipe that into cor(), which calculates the correlation across all pairs of variables within a data.frame, and outputs a correlation matrix.\n\nIf you want, you can pipe the result into ggcorplot(), a function from the ggcorplot package that provides meaningful visualizations of correlation matrices.\n\n\n\n\n\nBased on your analysis of the correlation between each stock, it seems that these pharmaceutical firms are relatively trending together. Notwithstanding, there might be gains from diversification if instead of choosing a specific firm, we decide to hold a portfolio of pharmaceutical stocks.\nInvesting in a single stock exposes an investor to company-specific (idiosyncratic) risk, such as lawsuits, failed drug trials, or regulatory changes. However, constructing a diversified portfolio of multiple stocks within the same industry can help smooth out these risks while still capturing the overall sector trends. For instance, while one pharmaceutical company may experience a stock price drop due to a failed drug trial, another might gain due to a successful FDA approval. By equally weighting multiple stocks, investors can reduce the impact of any single company’s negative performance while still benefiting from the broader industry’s growth.\nYour manager liked your idea and wanted to test it out by creating an equally-weighted portfolio of all pharmaceutical companies over time. Using the tq_transmute() function, create an object, portfolio_returns, that contains the yearly returns of a portfolio that assigns equal weights - in this case, 10% - on each stock, and compare that to the yearly returns of the S&P 500 Index. Would the fund manager be better-off by investing in the portfolio relative to the S&P500?\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the yearly returns for each stock using the tq_transmute() function as before, grouping the data by symbol and creating a new variable, yearly_return.\nKnowing that you have an equally-weighted portfolio, group your data my date and pipe the result into a summarize() function to create a new variable, portfolio_return as the average across all stocks. Assign this result to an object called portfolio_returns\nFetch S&P 500 data using a similar call to tq_get() like you did in the beginning of the exercise, but now collecting data for ^GSPC. Calculate the yearly returns and assign to a new variable, index_return. Store the result in another data.frame, index_returns.\nMerge both datasets using left_join().\n\n\n\n\n\n\nAfter analyzing the equally weighted pharmaceutical portfolio, the fund manager was impressed with the performance results. However, they remain skeptical about whether the portfolio truly provides better risk-adjusted returns compared to simply picking one of the best-performing stocks in the industry.\nAs final step, your job is to prove whether the portfolio offers superior risk-adjusted returns by computing the Sharpe Ratio for both the portfolio and its individual stocks in 2024. If the portfolio has a higher Sharpe ratio, it means that diversification helps maximize returns while controlling for risk — an essential argument when managing institutional funds.\nIn order to do that, your task is to provide a visualization of the Sharpe Ratio of the equally-weighted portfolio you’ve just created and compare that to those of the individual stocks. I have already created the portfolio results for you, so you can copy-paste that to your session:\n\nportfolio_sharpe=data.frame(symbol='Portfolio',\n                            yearly_return=0.03705213,\n                            yearly_volatility=0.1223983)\n\n\n\n\n\n\n\nHint\n\n\n\n\nCreate the portfolio_sharpe in your session using the code chunk above.\nUsing the yearly_returns object you’ve created in Exercise 2, filter by year(date)==2024 and left_join() with the yearly_volatility object you have created in Exercise 3, assigning the result to a new object\nFinally, bind portfolio_sharpe to the resulting data.frame in a rowwise manner using rbind(dataframe1,dataframe2)\nFinally, call ggplot() and adjust the aesthetics to show the relationship between risk (x-axis) and return (y-axis) for all individual stocks and the portfolio.\n\n\n\n\n\n\nNow that you have analyzed the Sharpe ratios of both individual pharmaceutical stocks and the equally weighted portfolio, take a step back and summarize your insights. Did the portfolio offer a better risk-adjusted return compared to individual stocks? If so, why? If not, what might explain the results?\nBased on your findings, what would you recommend to the fund manager? Would you suggest investing in the diversified portfolio, or do certain individual stocks offer superior risk-adjusted returns? Would you propose an alternative weighting scheme, such as a market cap-weighted portfolio, to further improve performance?\nWrite a short conclusion summarizing your key takeaways and justify your investment recommendation using data-driven insights."
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1.html#case-outline",
    "href": "quant-fin/datacases/data-case-1/data-case-1.html#case-outline",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "The pharmaceutical industry is a critical sector in financial markets, influenced by regulatory approvals, drug developments, and global health events. In this first Data Case, you will analyze stock performance for a set of 10 pharmaceutical companies over time, applying the tidyverse and tidyquant packages to extract and interpret insights from the data.\nYou are a junior analyst at Atlas Capital, a leading buyside investment firm specializing in sector-focused strategies. The firm is considering increasing its exposure to the pharmaceutical industry, given its long-term growth potential and resilience in volatile markets. In the latest investment committee meeting, your fund manager raised an important question: “How has the pharmaceutical industry performed over time? We need to identify whether now is the right time to increase our position.”\nYour team has been tasked with conducting an in-depth financial analysis of the pharmaceutical sector. The goal is to assess industry-wide trends, identify risks and opportunities, and ultimately recommend an investment stance. More specifically, your task will involve:\n\nCollecting stock price data and compute returns\nVisualizing key trends in returns and volatility\nInterpreting findings and suggest investment insights\n\nTo streamline our research, you will focus on the 10 largest publicly traded pharmaceutical companies in the U.S, analyze their performance, risks, and potential catalysts that could drive returns in the near future. As of February 2025, the 10 largest pharmaceutical companies traded in the U.S., along with their ticker symbols, are:\n\nEli Lilly and Co. (LLY): A leading pharmaceutical company known for its innovative treatments in diabetes and oncology.\nNovo Nordisk A/S (NVO): Specializing in diabetes care, Novo Nordisk has a significant presence in the U.S. market.\nJohnson & Johnson (JNJ): A diversified healthcare company with a strong pharmaceutical division.\nAbbVie Inc. (ABBV): Known for its immunology and oncology products, AbbVie is a major player in the pharmaceutical industry.\nMerck & Co., Inc. (MRK): Merck offers a wide range of prescription medicines, vaccines, and therapies.\nPfizer Inc. (PFE): A global pharmaceutical corporation recognized for its vaccines and therapeutics.\nBristol-Myers Squibb Company (BMY): Focused on oncology, cardiovascular, and immunology, Bristol-Myers Squibb is a key industry player.\nAstraZeneca PLC (AZN): A biopharmaceutical company with a strong portfolio in oncology and respiratory diseases.\nAmgen Inc. (AMGN): Specializing in biotechnology, Amgen develops therapies for serious illnesses.\nGilead Sciences, Inc. (GILD): Known for its antiviral drugs, Gilead has a significant market presence.\n\nNow, it’s up to you and your team to dive into the data, extract key insights, and present your data-driven investment thesis. Good luck—your next career milestone at Atlas Capital depends on it. 🚀\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®."
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1.html#tech-setup",
    "href": "quant-fin/datacases/data-case-1/data-case-1.html#tech-setup",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\",\"ggcorrplot\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)"
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-1",
    "href": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-1",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Use the tq_get() function from the tidyquant package to retrieve historical adjusted closing prices for the 10 largest publicly traded pharmaceutical companies in the U.S. from Yahoo! Finance. Your dataset should cover the period from January 1, 2020, to December 31, 2024. Using the functions from the tidyverse, ensure that your data includes only the timestamp column, as well as the column that contains the daily adjusted stock price information. Store this into an object called financial_data (or something similar). Store this data set for all the subsequent analysis - make sure not to override this dataset as you move along the data case to make sure you are always referring to the raw data pull!"
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-2",
    "href": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-2",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Using the tidyquant package, use the object you’ve just created with the tq_transmute function to compute the yearly returns for each stock over the analysis period. More specifically, pass the yearlyReturn function to adjusted column using the tq_transmute, labeling this new variable as yearly_return. Arrange your dataset by year and in descending order of yearly_return (highest-to-lowest). Store this into a new object called, for example, yearly_returns. Which stock had the highest return in 2024, and which one had the lowest? Prompt the results in your session."
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-3",
    "href": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-3",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "With your data.frame containing the yearly returns over time for each stock, use ggplot to create a line chart of the historical cumulative returns for each stock during the study period. Which stock had the highest cumulative return up-to-date? Recall that cumulative returns can be calculated from period returns as:\n\\[\n\\text{Cumulative Return}= (1+R_1)\\times(1+R_2)\\times ... \\times(1+R_T)-1\\equiv \\prod (1+R_t)-1\n\\]\nYour chart should map date to the x-axis, the yearly return variable to the y axis, and group the results by symbol. To make sure that you are plotting a line chart, use the geom_line() function after you have mapped your data. In addition to these two layers, add any customizations that you believe that are beneficial to convey the message - see the Data Visualization\n\n\n\n\n\n\nHint\n\n\n\n\nWith the data.frame you created to store yearly returns, group by symbol, and use tq_transmute() to apply the Return.cumulative function to the data.\nNow, your resulting data.frame contains the cumulative returns for all stocks. You can adjust the column names with the setNames() function and pipe that into a ggplot call, mapping the symbol to the x-axis, the cumulative return column to the y-axis, and the geom_col() function to create a bar chart. Add as many customizations you think are worth the effort."
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-4",
    "href": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-4",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "After reviewing your initial analysis, your fund manager at Atlas Capital liked the idea of examining yearly returns to get a broader perspective on performance. However, they pointed out that pharmaceutical companies vary significantly in terms of risk exposure, so it’s crucial to account for volatility as well. To complement the analysis, use the same rationale from the previous exercise to calculate the yearly volatility for each stock. How do the risk levels differ between firms? Store your results in a new object and prompt it in your session.\n\n\n\n\n\n\nHint\n\n\n\nAs opposed to the yearlyReturn function, the tidyquant package does not have a pre-built dailyStdev function. Instead, what you can do is to use a combination of functions to get the expected result:\n\nFirst, use tq_transmute() to calculate daily returns passing the dailyReturn function\nNow, your resulting data.frame contains daily returns for all stocks. It is now in a convenient format to chain this object again, in another tq_transmute() function, applying the StdDev.annualized function and assign to a new object, like yearly_volatility. Note, however, that if you simply use StdDev.annualized, it will calculate an annualized metric for each stock for the whole period, which is not what you want.\n\nTo make sure that you have calculating the annualized standard deviation for each year, you can do a composition of apply.yearly, which applies a given function at yearly intervals, and StdDev.annualized, using the following syntax:\n\nyour_daily_return_object%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=StdDev.annualized,\n               col_rename = 'yearly_volatility')\n\nHere, tq_transmute() will apply the function defined in FUN over each interval."
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-5",
    "href": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-5",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Building on your previous findings, since some companies exhibit higher returns but also greater risk, it might be a good idea to add a risk-adjusted performance metric to the analysis. The Sharpe ratio for stock \\(i\\) in period \\(t\\) measures the risk-adjusted return of an asset and is calculated as:\n\\[\n\\text{Sharpe Ratio}_{i,t}=\\dfrac{R_{i,t}-R_{f,t}}{\\sigma_{i,t}},\n\\]\nwhere \\(R_{i,t}\\) is the return of a given stock \\(i\\) in period \\(t\\), \\(R_{f,t}\\) is the risk-free return for the same period, and \\(\\sigma_{i,t}\\) is the volatility for stock \\(i\\) in period \\(t\\).\nYour task is to calculate the Sharpe Ratio for each pharmaceutical stock using yearly returns and yearly volatility. To simplify your calculations, assume a risk-free rate of \\(0\\%\\) per year (i.e, no risk-free premium). Compare the Sharpe ratios across companies. Do the highest-return stocks also have the best risk-adjusted performance? Are there any stocks that stand out as particularly efficient in generating returns relative to their risk? Are there companies that deliver strong returns but with disproportionately high volatility?\n\n\n\n\n\n\nHint\n\n\n\nThere are two ways you can use to create the Sharpe Ratio:\n\nUsing the previously created yearly_returns and yearly_volatility objects, use the left_join() function to merge them based on a common set of identifiers (in this case, date and symbol). After that, manipulate the resulting data.frame with mutate to generate the Sharpe Ratio.\nUsing tq_transmute in a very similar fashion to what you have done to calculate the yearly volatility, but now passing the the SharpeRatio.annualized function with arguments Rf=0 and scale=252.\n\nAlthough both approaches should yield similar results, potential differences might stem from rounding."
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-6",
    "href": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-6",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Way to go! As you delve deeper into your investment analysis, your fund manager emphasizes the importance of understanding how different pharmaceutical stocks interact with one another over time. To gain insights into the relationships between these companies, your next task is to calculate the correlation of daily stock returns for the selected pharmaceutical companies for the analysis period.\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the daily returns for each stock. You can use the tq_transmute function into your dataset and apply the dailyReturn function.\nAfter that, you need to pivot your data in such a way that each column is a specific ticker with information on daily returns. You can do that by calling pivot_wider(names_from='symbol',values_from='daily_return'), assuming that your daily return variable is called daily_return.\nWith that, you’ll achieve a data frame that now has \\(11\\) columns, namely, the date and the \\(10\\) individual ticker columns with daily return information.\nTo make sure that you are calculating the correlation using a \\(10\\times10\\) matrix, use select(-date) to get rid of the date column and pipe that into cor(), which calculates the correlation across all pairs of variables within a data.frame, and outputs a correlation matrix.\n\nIf you want, you can pipe the result into ggcorplot(), a function from the ggcorplot package that provides meaningful visualizations of correlation matrices."
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-7",
    "href": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-7",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Based on your analysis of the correlation between each stock, it seems that these pharmaceutical firms are relatively trending together. Notwithstanding, there might be gains from diversification if instead of choosing a specific firm, we decide to hold a portfolio of pharmaceutical stocks.\nInvesting in a single stock exposes an investor to company-specific (idiosyncratic) risk, such as lawsuits, failed drug trials, or regulatory changes. However, constructing a diversified portfolio of multiple stocks within the same industry can help smooth out these risks while still capturing the overall sector trends. For instance, while one pharmaceutical company may experience a stock price drop due to a failed drug trial, another might gain due to a successful FDA approval. By equally weighting multiple stocks, investors can reduce the impact of any single company’s negative performance while still benefiting from the broader industry’s growth.\nYour manager liked your idea and wanted to test it out by creating an equally-weighted portfolio of all pharmaceutical companies over time. Using the tq_transmute() function, create an object, portfolio_returns, that contains the yearly returns of a portfolio that assigns equal weights - in this case, 10% - on each stock, and compare that to the yearly returns of the S&P 500 Index. Would the fund manager be better-off by investing in the portfolio relative to the S&P500?\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the yearly returns for each stock using the tq_transmute() function as before, grouping the data by symbol and creating a new variable, yearly_return.\nKnowing that you have an equally-weighted portfolio, group your data my date and pipe the result into a summarize() function to create a new variable, portfolio_return as the average across all stocks. Assign this result to an object called portfolio_returns\nFetch S&P 500 data using a similar call to tq_get() like you did in the beginning of the exercise, but now collecting data for ^GSPC. Calculate the yearly returns and assign to a new variable, index_return. Store the result in another data.frame, index_returns.\nMerge both datasets using left_join()."
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-8",
    "href": "quant-fin/datacases/data-case-1/data-case-1.html#exercise-8",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "After analyzing the equally weighted pharmaceutical portfolio, the fund manager was impressed with the performance results. However, they remain skeptical about whether the portfolio truly provides better risk-adjusted returns compared to simply picking one of the best-performing stocks in the industry.\nAs final step, your job is to prove whether the portfolio offers superior risk-adjusted returns by computing the Sharpe Ratio for both the portfolio and its individual stocks in 2024. If the portfolio has a higher Sharpe ratio, it means that diversification helps maximize returns while controlling for risk — an essential argument when managing institutional funds.\nIn order to do that, your task is to provide a visualization of the Sharpe Ratio of the equally-weighted portfolio you’ve just created and compare that to those of the individual stocks. I have already created the portfolio results for you, so you can copy-paste that to your session:\n\nportfolio_sharpe=data.frame(symbol='Portfolio',\n                            yearly_return=0.03705213,\n                            yearly_volatility=0.1223983)\n\n\n\n\n\n\n\nHint\n\n\n\n\nCreate the portfolio_sharpe in your session using the code chunk above.\nUsing the yearly_returns object you’ve created in Exercise 2, filter by year(date)==2024 and left_join() with the yearly_volatility object you have created in Exercise 3, assigning the result to a new object\nFinally, bind portfolio_sharpe to the resulting data.frame in a rowwise manner using rbind(dataframe1,dataframe2)\nFinally, call ggplot() and adjust the aesthetics to show the relationship between risk (x-axis) and return (y-axis) for all individual stocks and the portfolio."
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1.html#wrapping-up-your-analysis",
    "href": "quant-fin/datacases/data-case-1/data-case-1.html#wrapping-up-your-analysis",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Now that you have analyzed the Sharpe ratios of both individual pharmaceutical stocks and the equally weighted portfolio, take a step back and summarize your insights. Did the portfolio offer a better risk-adjusted return compared to individual stocks? If so, why? If not, what might explain the results?\nBased on your findings, what would you recommend to the fund manager? Would you suggest investing in the diversified portfolio, or do certain individual stocks offer superior risk-adjusted returns? Would you propose an alternative weighting scheme, such as a market cap-weighted portfolio, to further improve performance?\nWrite a short conclusion summarizing your key takeaways and justify your investment recommendation using data-driven insights."
  },
  {
    "objectID": "quant-fin/replications/L3/L3-Replication.html",
    "href": "quant-fin/replications/L3/L3-Replication.html",
    "title": "Manipulating Time Series",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nIn this lecture, we will be working with daily stock price data from several stocks included in the S&P 500 index. Instead of loading the data from a .csv file, we will pull the data directly from R using the tidyquant package. Before you start, make sure to follow the instructions from our previous replication to set up your working directory correctly."
  },
  {
    "objectID": "quant-fin/replications/L3/L3-Replication.html#about-this-document",
    "href": "quant-fin/replications/L3/L3-Replication.html#about-this-document",
    "title": "Manipulating Time Series",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nIn this lecture, we will be working with daily stock price data from several stocks included in the S&P 500 index. Instead of loading the data from a .csv file, we will pull the data directly from R using the tidyquant package. Before you start, make sure to follow the instructions from our previous replication to set up your working directory correctly."
  },
  {
    "objectID": "quant-fin/replications/L3/L3-Replication.html#loading-packages",
    "href": "quant-fin/replications/L3/L3-Replication.html#loading-packages",
    "title": "Manipulating Time Series",
    "section": "Loading packages",
    "text": "Loading packages\nAs we get started, we will be loading all packages referred in our official website.\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\", \"glue\",\"scales\",\"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nNote that you could easily get around this by installing and loading all necessary packages using a more simple syntax:\n\n#Install if not already available - I have commented these lines so that R does not attempt to install it everytime\n  #install.packages('tidyverse')\n  #install.packages('tidyquant')\n  #install.packages('glue')\n  #install.packages('scales')\n  #install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)"
  },
  {
    "objectID": "quant-fin/replications/L3/L3-Replication.html#working-with-time-series",
    "href": "quant-fin/replications/L3/L3-Replication.html#working-with-time-series",
    "title": "Manipulating Time Series",
    "section": "Working with Time Series",
    "text": "Working with Time Series\nIn the previous lectures, you worked your way through the exercises by using the amazing dplyr functionalities on data.frames. In some cases, however, you had to do some workarounds with drop_na(), slice_tail() and lag() simply because you were manipulating time series data.\nThe xts, which stantds for eXtensible Time Series is an R package that is is widely used for handling and manipulating time series data. It extends the functionality of the zoo package by providing a structured framework for managing time-indexed data efficiently. Such package provides a matrix-like structure where each row is indexed by a date-time value, allowing for efficient subsetting, merging, and manipulation of time series data. It is especially useful in financial applications where time-stamped data is common.\n\nlibrary(xts) # Note that this is loaded together with the tidyquant package\n\n# Create a vector of random values\ndata_values &lt;- rnorm(10)\n\n# Create a sequence of dates\ndates &lt;- as.Date(\"2024-01-01\") + 0:9\n\n# Convert to xts\nxts_data &lt;- xts(data_values, order.by = dates)\n\n# Print the xts object\nprint(xts_data)\n\n                 [,1]\n2024-01-01 -0.8782227\n2024-01-02 -0.4597098\n2024-01-03 -0.3964338\n2024-01-04 -1.0016150\n2024-01-05 -1.5862849\n2024-01-06 -0.1767326\n2024-01-07 -0.9713712\n2024-01-08  0.3229833\n2024-01-09 -1.7203924\n2024-01-10 -0.7158226\n\n\nThe output shows an interesting feature of an xts format in R:\n\nThe first column contains the values\nThe row names are timestamps\nThe xts object retains an efficient internal structure"
  },
  {
    "objectID": "quant-fin/replications/L3/L3-Replication.html#core-features-of-xts",
    "href": "quant-fin/replications/L3/L3-Replication.html#core-features-of-xts",
    "title": "Manipulating Time Series",
    "section": "Core features of xts",
    "text": "Core features of xts\n\nTime-Based Indexing & Subsetting: you can subset an xts object using time-based indexing in a variety of ways. If you were to do this in a data.frame, R wouldn’t be able to retrieve the data, as a data.frame does not carry any time series properties that are needed for the job:\n\n\n# Subset a specific date\nxts_data[\"2024-01-03\"]\n\n                 [,1]\n2024-01-03 -0.3964338\n\n# Subset a range\nxts_data[\"2024-01-03/2024-01-06\"]\n\n                 [,1]\n2024-01-03 -0.3964338\n2024-01-04 -1.0016150\n2024-01-05 -1.5862849\n2024-01-06 -0.1767326\n\n# Subset a custom Y-M-D definition\nxts_data[\"2024\"]       # Returns all data from 2024\n\n                 [,1]\n2024-01-01 -0.8782227\n2024-01-02 -0.4597098\n2024-01-03 -0.3964338\n2024-01-04 -1.0016150\n2024-01-05 -1.5862849\n2024-01-06 -0.1767326\n2024-01-07 -0.9713712\n2024-01-08  0.3229833\n2024-01-09 -1.7203924\n2024-01-10 -0.7158226\n\nxts_data[\"2024-01\"]    # Returns all data from January 2024\n\n                 [,1]\n2024-01-01 -0.8782227\n2024-01-02 -0.4597098\n2024-01-03 -0.3964338\n2024-01-04 -1.0016150\n2024-01-05 -1.5862849\n2024-01-06 -0.1767326\n2024-01-07 -0.9713712\n2024-01-08  0.3229833\n2024-01-09 -1.7203924\n2024-01-10 -0.7158226\n\n\n\nMerging and Combining Time Series: you can merge two xts objects using by the timestamp component that is embedded in the xts structure:\n\n\ndata1 &lt;- xts(rnorm(5), order.by = as.Date(\"2024-01-01\") + 0:4)\ndata2 &lt;- xts(rnorm(5), order.by = as.Date(\"2024-01-01\") + 2:6)\n\nmerged_data &lt;- merge(data1, data2, join = \"outer\")\nprint(merged_data)\n\n                data1       data2\n2024-01-01 -0.3247350          NA\n2024-01-02 -0.4150009          NA\n2024-01-03  1.4498303 -1.47773093\n2024-01-04  1.6398024  0.41617149\n2024-01-05 -0.9977372 -0.06436564\n2024-01-06         NA  0.81567706\n2024-01-07         NA  1.17189255\n\n\n\njoin = \"outer\" ensures all time points are included\nIf a time point is missing in one dataset, it is filled with NA\nMerging and Combining Time Series: using functions that retrieve leads and lags of a given variable are a key component of xts objects. Due to its timestamp component, one can shift variables backwards (using the lag() function) as well as forward (using the lag()) function:\n\n\nlagged_data &lt;- lag(data1)  # Shift values by 1 day\nprint(lagged_data)\n\n                 [,1]\n2024-01-01         NA\n2024-01-02 -0.3247350\n2024-01-03 -0.4150009\n2024-01-04  1.4498303\n2024-01-05  1.6398024\n\nmerged_data &lt;- merge(lagged_data, data1, join = \"outer\")\nprint(merged_data)\n\n           lagged_data      data1\n2024-01-01          NA -0.3247350\n2024-01-02  -0.3247350 -0.4150009\n2024-01-03  -0.4150009  1.4498303\n2024-01-04   1.4498303  1.6398024\n2024-01-05   1.6398024 -0.9977372\n\n\nAll in all, when it comes to working with financial data, xts objects are particularly useful for:\n\nHandling stock price data from tidyquant\nCalculating log returns for portfolio management\nAligning time series data (e.g., joining different financial datasets)\nAggregating financial data (e.g., monthly returns)\nSubsetting data by years/months/days\nCalculating rolling functions (e.g, yearly averages)\nAggregating data at different intervals (e.g, convert daily to weekly prices)\n\nUnfortunately, there is an issue: the tidyverse is not fully designed to work with time series classes, such as xts. Since xts is optimized for time series operations, some functions that would work well when managing time series are not easily translated using the packages from the tidyverse:\n\ntidyverse: Designed for tabular (data frame-like) structures, emphasizing “tidy” data (each row is an observation, each column is a variable)\nxts: Designed for time series, optimizing time-based indexing and calculations but less compatible with tidyverse workflows"
  },
  {
    "objectID": "quant-fin/replications/L3/L3-Replication.html#introducing-the-tidyquant-package",
    "href": "quant-fin/replications/L3/L3-Replication.html#introducing-the-tidyquant-package",
    "title": "Manipulating Time Series",
    "section": "Introducing the tidyquant package",
    "text": "Introducing the tidyquant package\nThe tidyquant package (see official documentation here) helps integrate both paradigms, making financial analysis more intuitive in R. It integrates several financial packages, like zoo, xts, quantmod, TTR, and PerformanceAnalytics, with the tidy data infrastructure of the tidyverse, allowing for seamless interaction between tidyverse data manipulation and financial functions.\nThere are mainly three functions in the tidyquant package that we will be using throughout this lecture: tq_get(), tq_mutate(), and tq_transmute()."
  },
  {
    "objectID": "quant-fin/replications/L3/L3-Replication.html#the-tq_get-function",
    "href": "quant-fin/replications/L3/L3-Replication.html#the-tq_get-function",
    "title": "Manipulating Time Series",
    "section": "The tq_get() function",
    "text": "The tq_get() function\n\n\n\n\n\n\nDefinition\n\n\n\nThe tq_transmute() returns only newly created columns and is typically used when periodicity changes. Its syntax is the following:\nThe tq_get() function is a powerful tool for retrieving financial data in a tidy format. It provides an easy way to import stock prices, economic data, and key financial metrics from multiple sources, including Yahoo Finance, FRED, Alpha Vantage, and Quandl.\n\ntq_get(x, get = \"stock.prices\", from = NULL, to = NULL, ...)\n\n\nx: a character string or vector specifying the stock symbol(s) or identifier(s)\nget: the type of data to retrieve. Defaults to \"stock.prices\"\nfrom: start date, in YYYY-MM-DD format. Defaults to NULL (gets all available data)\nto: end date, in YYYY-MM-DD format. Defaults to NULL (gets all available data)\n...: additional arguments specific to the data source\n\n\n\nOne of the most common uses of tq_get() is fetching stock price data. Say, for example, that you want to fetch data from Apple between January and February for 2024. It is easy to use tq_get() to retrieve such information:\n\n# Assuming you have the tidyquant loaded in your session\n\n# Fetch Apple (AAPL) stock data from 2024-01-01 to 2024-02-01\naapl_data &lt;- tq_get(\"AAPL\", from = \"2024-01-01\", to = \"2024-02-01\")\n\n# Print first few rows\nhead(aapl_data)\n\n# A tibble: 6 × 8\n  symbol date        open  high   low close   volume adjusted\n  &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 AAPL   2024-01-02  187.  188.  184.  186. 82488700     185.\n2 AAPL   2024-01-03  184.  186.  183.  184. 58414500     183.\n3 AAPL   2024-01-04  182.  183.  181.  182. 71983600     181.\n4 AAPL   2024-01-05  182.  183.  180.  181. 62303300     180.\n5 AAPL   2024-01-08  182.  186.  182.  186. 59144500     184.\n6 AAPL   2024-01-09  184.  185.  183.  185. 42841800     184.\n\n\n\nThe result is a tidy tibble, unlike quantmod’s xts format."
  },
  {
    "objectID": "quant-fin/replications/L3/L3-Replication.html#the-tq_mutate-function",
    "href": "quant-fin/replications/L3/L3-Replication.html#the-tq_mutate-function",
    "title": "Manipulating Time Series",
    "section": "The tq_mutate() function",
    "text": "The tq_mutate() function\n\n\n\n\n\n\nDefinition\n\n\n\nThe tq_mutate() function adds adds new variables to an existing tibble:\n\ntq_mutate(.data, #The object you are performing the calculations \n       selected_variables, #The columns to send to the mutation function\n       mutate_fun, #The mutation function from either the xts, quantmod, or TTR package.\n       col_rename #A string or character vector containing names that can be used to quickly rename columns\n       ) \n\n\n\n\nThe main advantage is the results are returned as a tibble and the function can be used with the tidyverse\nIt is used when you expected additional columns to be added to the resulting data frame\nYou can use several time series related functions from other R packages - call tq_mutate_fun_options() to see the list of available options\nAll in all, it is similar in spirit to mutate()"
  },
  {
    "objectID": "quant-fin/replications/L3/L3-Replication.html#the-tq_transmute-function",
    "href": "quant-fin/replications/L3/L3-Replication.html#the-tq_transmute-function",
    "title": "Manipulating Time Series",
    "section": "The tq_transmute() function",
    "text": "The tq_transmute() function\n\n\n\n\n\n\nDefinition\n\n\n\nThe tq_transmute() returns only newly created columns and is typically used when periodicity changes. Its syntax is the following:\n\ntq_mutate(.data, #The object you are performing the calculations \n       selected_variables, #The columns to send to the mutation function\n       mutate_fun, #The mutation function from either the xts, quantmod, or TTR package.\n       col_rename #A string or character vector containing names that can be used to quickly rename columns\n       )\n\n\n\n\ntq_transmute() works exactly like tq_mutate() except it only returns the newly created columns\nThis is helpful when changing periodicity where the new columns would not have the same number of rows as the original tibble\nAll in all, it is similar in spirit to summarize()"
  },
  {
    "objectID": "quant-fin/replications/L3/L3-Replication.html#working-with-time-series-objects",
    "href": "quant-fin/replications/L3/L3-Replication.html#working-with-time-series-objects",
    "title": "Manipulating Time Series",
    "section": "Working with time series objects",
    "text": "Working with time series objects\nAn immediate useful example of using a time series specific functionality with a tidyverse logic relates to filtering. Sometimes, we may be interested in getting only a subset of the data (for example, only GOOG information). Furthermore, we may be interested in subsetting only a specific time frame for our analysis\nIt is relatively straightforward to do it with tidyquant:\n\nUse filter() to select only rows where symbol=='GOOG'\nIn the same call, filter for date&gt;= min_date and date&lt;=max_date\n\n\n#Assuming you have the tidyverse and the tidyquant packages loadded\n\n#Set up the list of assets\nassets=c('AMZN','GOOG','META','GME')\n\n#Filter out\nassets%&gt;%\n  tq_get()%&gt;%\n  filter(symbol=='GOOG',\n         date&gt;='2020-01-01',\n         date&lt;='2024-12-31')\n\n# A tibble: 1,258 × 8\n   symbol date        open  high   low close   volume adjusted\n   &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 GOOG   2020-01-02  67.1  68.4  67.1  68.4 28132000     68.1\n 2 GOOG   2020-01-03  67.4  68.6  67.3  68.0 23728000     67.8\n 3 GOOG   2020-01-06  67.5  69.8  67.5  69.7 34646000     69.5\n 4 GOOG   2020-01-07  69.9  70.1  69.5  69.7 30054000     69.4\n 5 GOOG   2020-01-08  69.6  70.6  69.5  70.2 30560000     70.0\n 6 GOOG   2020-01-09  71.0  71.4  70.5  71.0 30018000     70.7\n 7 GOOG   2020-01-10  71.4  71.7  70.9  71.5 36414000     71.2\n 8 GOOG   2020-01-13  71.8  72.0  71.3  72.0 33046000     71.7\n 9 GOOG   2020-01-14  72.0  72.1  71.4  71.5 31178000     71.3\n10 GOOG   2020-01-15  71.5  72.1  71.5  72.0 25654000     71.7\n# ℹ 1,248 more rows\n\n\nAnother example of using a time series specific functionality is working with leads and lags: sometimes, we need to shift our variables by a specific interval, like getting the previous day’s price. Say, for example, that you want to understand how S&P returns levels relate to NFLX returns one-week ahead. It is relatively straightforward to do it with tidyquant:\n\nDownload S&P 500 and NFLX data using the tq_get() function\nUse tq_transmute() to compute the weekly returns for each security based on daily data\nUse tq_mutate() to generate a lagged series of S&P 500 returns\n\n\n#Assuming you have the tidyverse and the tidyquant packages loadded\n\n#Netflix Data\nNFLX=tq_get('NFLX')%&gt;%\n  #Select only the necessary columns\n  select(date,symbol,adjusted)%&gt;%\n  #Apply the weeklyReturn function and call the new column 'NFLX'\n  tq_transmute(mutate_fun = weeklyReturn,\n               col_rename = 'NFLX')\n\n#S&P Data\nSP500=tq_get('^GSPC')%&gt;%\n  #Select only the necessary columns\n  select(date,symbol,adjusted)%&gt;%\n  #Apply the weeklyReturn function and call the new column 'SP500'\n  tq_transmute(mutate_fun = weeklyReturn,\n               col_rename = 'SP500')%&gt;%\n  #Apply the lag function for n=1 week and call the new column 'SP500'\n  tq_transmute(mutate_fun = lag.xts,\n            n=1,\n            col_rename = 'SP500')%&gt;%\n  #Drop all rows with NA information (row 1, in this case)\n  drop_na()\n\n#Merge Data \ninner_join(NFLX,SP500)\n\nJoining with `by = join_by(date)`\n\n\n# A tibble: 530 × 3\n   date           NFLX    SP500\n   &lt;date&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 2015-01-09 -0.0563   0      \n 2 2015-01-16  0.0244  -0.00651\n 3 2015-01-23  0.297   -0.0124 \n 4 2015-01-30  0.00992  0.0160 \n 5 2015-02-06  0.00579 -0.0277 \n 6 2015-02-13  0.0489   0.0303 \n 7 2015-02-20  0.0260   0.0202 \n 8 2015-02-27 -0.00688  0.00635\n 9 2015-03-06 -0.0438  -0.00275\n10 2015-03-13 -0.0346  -0.0158 \n# ℹ 520 more rows"
  },
  {
    "objectID": "quant-fin/replications/L3/L3-Replication.html#rolling-functions",
    "href": "quant-fin/replications/L3/L3-Replication.html#rolling-functions",
    "title": "Manipulating Time Series",
    "section": "Rolling functions",
    "text": "Rolling functions\nFinance practitioners are often asked to perform analysis on a rolling basis: we may want to calculate a given signal on day \\(t\\) based on past \\(x\\) periods of information. Say, for example, that you want to calculate a simple and exponential moving average of adjusted prices from 5 days back for a given stock. It is relatively straightforward to do it with tidyquant:\n\nDownload stock data using the tq_get() function\nUse tq_mutate() twice along with the SMA() and EMA() functions setting n=5\n\n\n#Assuming you have the tidyverse and the tidyquant packages loadded\n\n#Set up the list of assets\nassets=c('AMZN')\n\nassets%&gt;%\n  tq_get()%&gt;%\n  select(date,symbol,adjusted)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_mutate(adjusted, mutate_fun = SMA, n = 5)%&gt;%\n  tq_mutate(adjusted, mutate_fun = EMA, n = 5)\n\n# A tibble: 2,551 × 5\n# Groups:   symbol [1]\n   symbol date       adjusted   SMA   EMA\n   &lt;chr&gt;  &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 AMZN   2015-01-02     15.4  NA    NA  \n 2 AMZN   2015-01-05     15.1  NA    NA  \n 3 AMZN   2015-01-06     14.8  NA    NA  \n 4 AMZN   2015-01-07     14.9  NA    NA  \n 5 AMZN   2015-01-08     15.0  15.0  15.0\n 6 AMZN   2015-01-09     14.8  14.9  15.0\n 7 AMZN   2015-01-12     14.6  14.8  14.8\n 8 AMZN   2015-01-13     14.7  14.8  14.8\n 9 AMZN   2015-01-14     14.7  14.8  14.8\n10 AMZN   2015-01-15     14.3  14.6  14.6\n# ℹ 2,541 more rows\n\n\nLastly, financial analysts often cover a collection of securities on a rolling basis. For example, a buy-side analyst will monitor stocks from a given industry so as to understand which ones are overvalued, and which ones are undervalued. Say, for example, that you want to focus on a subset of 4 stocks, and you need to compare the cumulative return up to the latest closing price.\nIt is easy to integrate the tidyquant functions along with the group_by() function you’ve learned when working with dplyr:\n\nGet the information using tq_get()\nGroup the data by symbol\nApply the tq_mutate and tq_transmute functions to pass time series functions to the data - in this case, the dailyReturn() and the Return.cumulative() function\n\n\n#Assuming you have the tidyverse and the tidyquant packages loadded\n\n#Set up the list of assets\nassets=c('AMZN','GOOG','META','GME')\n\nassets%&gt;%\n  tq_get()%&gt;%\n  select(date,symbol,adjusted)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_mutate(adjusted, mutate_fun = dailyReturn,col_rename = 'daily_return')%&gt;%\n  tq_transmute(daily_return,mutate_fun = Return.cumulative)%&gt;%\n  mutate(across(where(is.numeric),percent,big.mark='.'))%&gt;%\n  setNames(c('Ticker','Cumulative Return up-to-date'))\n\nWarning: There were 5 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(where(is.numeric), percent, big.mark = \".\")`.\nℹ In group 1: `symbol = \"AMZN\"`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\nℹ Run `dplyr::last_dplyr_warnings()` to see the 4 remaining warnings.\n\n\n# A tibble: 4 × 2\n# Groups:   Ticker [4]\n  Ticker `Cumulative Return up-to-date`\n  &lt;chr&gt;  &lt;chr&gt;                         \n1 AMZN   1.279%                        \n2 GOOG   595%                          \n3 META   755%                          \n4 GME    298%"
  },
  {
    "objectID": "quant-fin/replications/L3/L3-Replication.html#hands-on-exercise",
    "href": "quant-fin/replications/L3/L3-Replication.html#hands-on-exercise",
    "title": "Manipulating Time Series",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\nYour manager (who did not lift any weights past the last 5 years) wanted to replicate the returns of the Deadlift ETF from 2020 to 2024. You job is to create a simple table of yearly returns comparing the Deadlift ETF vis-a-vis the S&P 500 Index. Follow the instructions and answer to the following questions:\n\nWhen looking at the yearly results from both the Deadlift ETF and S&P 500, which one did perform better?\nWhat are the potential explanations for the result you have found?\n\nTo answer to these questions, you will be using the a combination of dplyr and tidyquant functions you have learned so far. The expected result is a data.frame object that shows both the Deadlift ETF as well as the S&P 500 returns (columns) on a yearly basis (rows).\n\n\n\n\n\n\nInstructions\n\n\n\nBefore you start, make sure to have the tidyverse and the tidyquant packages loaded in your session. Following the instructions from the previous lectures, you can either make a direct call to each package, library(tidyverse) and library(tidyquant), or copy-paste the script from the course’s official website.\n\nUse tq_get() to load information from the S&P Index and the Deadlift ETF constituents in two separate objects. You can use the code ^GSPC to retrieve information for the index, and you can pass a vector c('ticker1','ticker2',...,'ticker_n') to get information on the Deadlift ETF constituents\nFilter for observations starting between 2020 (beginning of) and 2024 (end of) using the from and to arguments of the tq_get() function\nGroup the Deadlift ETF data by symbol using the group_by() function\nFor both data sets, create a yearly_ret variable that calculates the yearly return of a given security. You can use the tq_transmute() function, passing the yearlyReturn() function along the chain\nFor the Deadlift data set, regroup the data by date and calculate the Deadlift returns using a mutate() function (Hint: it is an equally weighted portfolio)\nMerge both datasets using inner_join()"
  },
  {
    "objectID": "quant-fin/replications/L3/L3-Replication.html#solution-walkthrough",
    "href": "quant-fin/replications/L3/L3-Replication.html#solution-walkthrough",
    "title": "Manipulating Time Series",
    "section": "Solution walkthrough",
    "text": "Solution walkthrough\n\n#Assuming you have the tidyverse and the tidyquant packages loadded\n\n# Set up the list of assets\ndeadlift=c('META','AMZN','GS','UBER','MSFT','AAPL','BLK','NVDA')\n\n#Set up the starting date\nstart='2020-01-01'\nend='2024-12-31'\n\n#Step 1: Read the Deadlift data using tidyquant\nDeadlift_Performance=deadlift%&gt;%\n  tq_get(from=start,to=end)%&gt;%\n  #Select only the columns of interest\n  select(symbol,date,adjusted)%&gt;%\n  #Group by symbol and date\n  group_by(symbol)%&gt;%\n  #Use tq_transmute to aggregate and calculate weekly returns\n  tq_transmute(selected=adjusted,\n               mutate_fun=yearlyReturn,\n               col_rename = 'Deadlift')%&gt;%\n  #Group by date\n  group_by(date)%&gt;%\n  #Summarize average return (since it is an equally-weighted portfolio)\n  summarize(Deadlift=mean(Deadlift,na.rm=TRUE))\n\n#Step 2: Read the S&P 500 data using tidyquant\nSP500_Performance=tq_get('^GSPC',from=start,to=end)%&gt;%\n  #Select only the columns of interest\n  select(symbol,date,adjusted)%&gt;%\n  #Group by symbol and date\n  group_by(symbol)%&gt;%\n  #Use tq_transmute to aggregate and calculate weekly returns\n  tq_transmute(selected=adjusted,\n               mutate_fun=yearlyReturn,\n               col_rename = 'SP500')%&gt;%\n  ungroup()%&gt;%\n  select(-symbol)\n    \n#Merge\nSP500_Performance%&gt;%\n  inner_join(Deadlift_Performance)%&gt;%\n  mutate(across(where(is.numeric),percent))%&gt;%\n  mutate(date=year(date))%&gt;%\n  setNames(c('Year','S&P 500','DeadLift ETF'))\n\nJoining with `by = join_by(date)`\n\n\n# A tibble: 5 × 3\n   Year `S&P 500` `DeadLift ETF`\n  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;         \n1  2020 15.29%    57.9%         \n2  2021 26.89%    37.2%         \n3  2022 -19.44%   -36.0%        \n4  2023 24.23%    100.5%        \n5  2024 23.84%    52.1%         \n\n\nThis solution uses tidyquant and tidyverse to analyze the yearly returns of a custom portfolio (i.e., the “Deadlift ETF”) consisting of eight stocks and compares it with the S&P 500.\n\nDefine Assets and Time Range. first, we dedfine a custom portfolio (deadlift) containing eight stocks, and set the start and end dates for data collection.\nFetch & Process the Deadlift Portfolio Returns. Starting with the Deadlift ETF, we first fetch historical stock prices using tq_get() for each asset contained in the ETF. After that, we only keep the relevant columns - namely, symbol, date, and adjusted. Using group_by() to group by symbol, we use the tq_transmute() function to apply the yearlyReturns function to the adjusted column, renaming it as Deadlift.\nCalculating portfolio returns. Since this ETF is an equally-weighted portfolio, and using the fact that a portfolio return is a weighted average of the individual securities, you can safely use the mean() function to calculate the return of such portfolio. To make sure that the calculation is performed for each year, use group_by() again, grouping by date."
  },
  {
    "objectID": "quant-fin/replications/L3/L3-Replication.html#try-doing-some-edits-on-your-own",
    "href": "quant-fin/replications/L3/L3-Replication.html#try-doing-some-edits-on-your-own",
    "title": "Manipulating Time Series",
    "section": "Try doing some edits on your own!",
    "text": "Try doing some edits on your own!\nTry thinking about changes you could do to either improve code readability of the analysis. A couple of edits that can be made include, but are not limited, to:\n\nAdding more time periods to the analysis\nContrasting the DeadLift ETF with the S&P 500 in terms of variance and Sharpe Ratio\nDoing the comparison using value-weighted returns (i.e, weighting the securities inside the portfolio according to its market capitalization) or inverse volatility (i.e, riskier assets have lower weights)\n\nPlay around with these concepts to get familiar with all the data manipulation tools that come with the tidyquant package!"
  },
  {
    "objectID": "quant-fin/replications/L1/L1-Replication.html",
    "href": "quant-fin/replications/L1/L1-Replication.html",
    "title": "Bridging Data with Programming - Replication",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nThis is our first lecture, and I do not expect you to fully understand the underlying code. The goal is to showcase the most important R packages, how they work, and how you can make sure to have them available in our session. As we progress through the lectures, we will have a deep-dive on some of the most important aspects of these packages, and you will have some hands-on exercises to practice your coding skills."
  },
  {
    "objectID": "quant-fin/replications/L1/L1-Replication.html#about-this-document",
    "href": "quant-fin/replications/L1/L1-Replication.html#about-this-document",
    "title": "Bridging Data with Programming - Replication",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nThis is our first lecture, and I do not expect you to fully understand the underlying code. The goal is to showcase the most important R packages, how they work, and how you can make sure to have them available in our session. As we progress through the lectures, we will have a deep-dive on some of the most important aspects of these packages, and you will have some hands-on exercises to practice your coding skills."
  },
  {
    "objectID": "quant-fin/replications/L1/L1-Replication.html#introduction-to-install.packages-and-library-in-r",
    "href": "quant-fin/replications/L1/L1-Replication.html#introduction-to-install.packages-and-library-in-r",
    "title": "Bridging Data with Programming - Replication",
    "section": "1. Introduction to install.packages() and library() in R",
    "text": "1. Introduction to install.packages() and library() in R\nWhen you’re starting with R, you’ll frequently need to install and load packages to access additional functions beyond the base R functionalities. Two essential functions for this are:\n\ninstall.packages() – Downloads and installs a package from CRAN (Comprehensive R Archive Network).\nlibrary() – Loads an installed package so that you can use its functions\n\nFor installing new packages, the syntax is:\n\ninstall.packages(\"package_name\")\n\nNote that:\n\nYou only need to install a package once (unless you update or reinstall R).\nPackages are stored in a library (a folder on your computer).\nIf a package is not available, check your internet connection or make sure CRAN is accessible.\n\nOnce a package is installed, you need to load it every time you start a new R session. The function library() makes the functions from the referred package available for use. For example, after installing ggplot2 using install.packages(\"ggplot2\"), load it by calling:\n\nlibrary(ggplot2)\n\nIf you forget to load a package and try to use its functions, R will throw an error:\n\nError in ggplot(): could not find function \"ggplot\"\n\nIn this course, we will be mostly using a handful of packages for data collection, data wrangling, and visualization. Namely, we will be mostly working with the following packages:\n\ntidyverse: for data manipulation and visualization, including packages such as dplyr,gpplot2, and tidyr\ntidyquant: for retrieving and working with financial data series\ntidymodels: a set of routines for running statistical models\nglue: a simple and efficient way to interpolate strings\nscales: provides functions that are particularly useful for visualization, helping scaling axes, formatting labels, and applying transformations like logarithms or percentages.\nggthemes: a set of powerful themes for data visualization\n\nFor installing these packages, you could simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)\n\nAlternatively, the code below searches for a given set of packages in our computer, installing only the packages that are not found. After installing all missing packages (if any) it loads all packages together:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\", \"glue\",\"scales\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nMake sure to install all packages and load them, either by calling library() individually, or running the aforementioned code.\n\n\n\n\n\n\nNote\n\n\n\nFor the remainder of the sessions, you will use financial data from multiple sources, either that being from a local .csv file or a pull from Yahoo! Finance. Whenever you are working with local files, it is always important to make sure that your R is able to locate it. To check the working directory of your session, simply type getwd(), and it will prompt your current directory. If you want to change your directory, simply type setwd('C:/path/to/your/folder') with the specific path to your desired folder. To make sure that you switched directories, you can type getwd() to confirm the new directory.\nMost of the issues regarding not being able to load a specific file, like .csv and .xlsx spreadsheets can be easily solved by placing your R file (either a plain script, like myscript.R, or a quarto document, myquartodoc.qmd) in the same folder as of your data. When you open your R script or Quarto document, it will automatically set that folder (which coincides with the data folder) as the working directory. To confirm which files are available to you, you can simply type list.files() to get the list of all files that R can find in the working directory.\nIf you prefer, whenever you are calling a function that requires a path to your computer, you can always provide the full path of the file: for example, using \"C:/Users/Lucas/Documents/GitHub/Folder/test.csv' would find the test.csv even if Folder is not your working directory."
  },
  {
    "objectID": "quant-fin/replications/L1/L1-Replication.html#using-dplyr-the-data-manipulation-package-in-the-tidyverse",
    "href": "quant-fin/replications/L1/L1-Replication.html#using-dplyr-the-data-manipulation-package-in-the-tidyverse",
    "title": "Bridging Data with Programming - Replication",
    "section": "2. Using dplyr, the data manipulation package in the tidyverse",
    "text": "2. Using dplyr, the data manipulation package in the tidyverse\nThe dplyr package is one of the core packages in the tidyverse and is designed for efficient and readable data manipulation. It provides a set of functions (also called “verbs”) that make working with data frames (or tibbles) intuitive and expressive. Key Features:\n\nFilter rows: filter()\nSelect columns: select()\nMutate (create new columns): mutate()\nSummarize data: summarize()\nGroup operations: group_by()\nJoin tables: left_join(), right_join(), inner_join(), full_join()\n\nFor this section, you will be using a .csv file that contains Nvidia (ticker: NVDA) prices collected directly from Yahoo! Finance. You can download the data using directly on eClass® and place it in the same folder of your R/Quarto report. In my case, I have created a folder, called Assets, inside my working directory.\n\nread.csv('Assets/NVDA.csv')%&gt;%\n  select(Timestamp,Adjusted)%&gt;%\n  mutate(Date=as.Date(Timestamp),\n         Year=year(Date))%&gt;%\n  filter(Year!='2025')%&gt;%\n  arrange(Date)%&gt;%\n  mutate(Return = Adjusted/lag(Adjusted,1)-1)%&gt;%\n  group_by(Year)%&gt;%\n  summarize(\n    `Average Daily Return` = percent(mean(Return,na.rm=TRUE),accuracy = 0.01),\n    `Annualized Return`= percent(prod(1+Return,na.rm=TRUE)-1,accuracy = 0.01))\n\n# A tibble: 18 × 3\n    Year `Average Daily Return` `Annualized Return`\n   &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt;              \n 1  2007 0.18%                  41.44%             \n 2  2008 -0.42%                 -76.28%            \n 3  2009 0.40%                  131.47%            \n 4  2010 -0.04%                 -17.56%            \n 5  2011 0.02%                  -10.00%            \n 6  2012 -0.03%                 -10.97%            \n 7  2013 0.12%                  33.52%             \n 8  2014 0.11%                  27.40%             \n 9  2015 0.23%                  67.12%             \n10  2016 0.51%                  226.96%            \n11  2017 0.27%                  81.99%             \n12  2018 -0.10%                 -30.82%            \n13  2019 0.26%                  76.94%             \n14  2020 0.38%                  122.30%            \n15  2021 0.36%                  125.48%            \n16  2022 -0.20%                 -50.26%            \n17  2023 0.53%                  239.02%            \n18  2024 0.45%                  171.25%            \n\n\nLet’s break this code down. First, the function read.csv() reads an NVDA.csv file inside the Assets subfolder. This function returns a data.frame object that is an input to the rest of the code. All other functions are from dplyr, and are intended to facilitate data wrangling. We’ll cover all these functions in our upcoming lecture."
  },
  {
    "objectID": "quant-fin/replications/L1/L1-Replication.html#using-ggplot2-for-data-visualization",
    "href": "quant-fin/replications/L1/L1-Replication.html#using-ggplot2-for-data-visualization",
    "title": "Bridging Data with Programming - Replication",
    "section": "3. Using ggplot2 for data visualization",
    "text": "3. Using ggplot2 for data visualization\nThe ggplot2 package is the most powerful and widely used data visualization package in R. It is part of the tidyverse and follows the Grammar of Graphics, a systematic approach to creating complex graphics by layering components.\nUnlike base R plotting functions, like plot(), ggplot2 provides:\n\nHighly customizable plots. Its modular approach makes it easy to modify and extend plots.\nElegant default themes. Default themes are aesthetically pleasing and can be customized.\nLayered structure for complex graphics. Users can combine multiple layers, change colors, themes, scales, and annotations effortlessly.\nSeamless integration with the tidyverse. It integrates seamlessly with dplyr, tidyr and tidyquant, allowing smooth data manipulation and visualization.\n\nThe code below shows how you can use ggplot2 to load a file. It reads a .csv file containing Nvidia stock prices, processes the data, and visualizes the stock price trends for 2023 and 2024 using ggplot2. Note that I have not called library(tidyverse) again since my session already had it loaded for the previous code chunks!\n\nread.csv('Assets/NVDA.csv')%&gt;%\n  select(Timestamp,Adjusted)%&gt;%\n  mutate(Date=as.Date(Timestamp))%&gt;%\n  arrange(Date)%&gt;%\n  filter(year(Date) %in% c(2023,2024))%&gt;%\n  ggplot(aes(x=Date,y=Adjusted))+\n  geom_line()+\n  theme_light()+\n  labs(title = 'Nvidia stock prices between 2023 and 2024',\n       subtitle = 'Source: Yahoo! Finance',\n       x='',\n       y='Adjusted Close (in $)')+\n  scale_y_continuous(labels = dollar)"
  },
  {
    "objectID": "quant-fin/replications/L1/L1-Replication.html#using-tidyr-to-reshape-and-simplify-data",
    "href": "quant-fin/replications/L1/L1-Replication.html#using-tidyr-to-reshape-and-simplify-data",
    "title": "Bridging Data with Programming - Replication",
    "section": "4. Using tidyr to reshape and simplify data",
    "text": "4. Using tidyr to reshape and simplify data\nThe tidyr package in R is designed for data cleaning and reshaping, making datasets “tidy” for easier analysis. It provides functions to pivot, separate, unite, and fill missing data efficiently. For this section, you will be using a .csv file that contains information from multiple tickers collected directly (Multiple_Assets.csv). You can download the data using directly on eClass® and place it in the same folder of your R/Quarto report. In my case, I have created a folder, called Assets, inside my working directory.\nOpening this file promptly shows that the .csv file is in a very messy format, with different columns that refer to the same variable, like closing prices. Using tidyr facilitates the transitions towards a tidy format that will be key for data manipulation:\n\nread.csv('Assets/Multiple_Assets.csv')%&gt;%\n  pivot_longer(cols=matches('Open|High|Low|Close|Volume|Adjusted'),\n               names_to = c('Asset','Metric'),\n               names_sep = '\\\\.',\n               values_to = 'Value')%&gt;%\n  mutate(Date=as.Date(Timestamp),\n         Year=year(Date))%&gt;%\n  group_by(Asset,Metric,Year)%&gt;%\n  summarize(Value=mean(Value,na.rm=TRUE))%&gt;%\n  pivot_wider(names_from = c('Metric'),values_from = 'Value')\n\n# A tibble: 57 × 8\n# Groups:   Asset [3]\n   Asset  Year Adjusted Close  High   Low  Open      Volume\n   &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1 AAPL   2007     3.86  4.58  4.65  4.51  4.59  984047751.\n 2 AAPL   2008     4.28  5.07  5.18  4.96  5.08 1130360498.\n 3 AAPL   2009     4.42  5.24  5.30  5.18  5.24  568467011.\n 4 AAPL   2010     7.83  9.28  9.37  9.17  9.28  599305267.\n 5 AAPL   2011    11.0  13.0  13.1  12.9  13.0   492298967.\n 6 AAPL   2012    17.4  20.6  20.8  20.4  20.6   527856818.\n 7 AAPL   2013    14.6  16.9  17.1  16.7  16.9   406434800 \n 8 AAPL   2014    20.4  23.1  23.3  22.9  23.1   252610922.\n 9 AAPL   2015    27.0  30.0  30.3  29.7  30.0   207397617.\n10 AAPL   2016    24.0  26.2  26.4  25.9  26.1   153690124.\n# ℹ 47 more rows"
  },
  {
    "objectID": "quant-fin/replications/L1/L1-Replication.html#collecting-and-exporting-data",
    "href": "quant-fin/replications/L1/L1-Replication.html#collecting-and-exporting-data",
    "title": "Bridging Data with Programming - Replication",
    "section": "5. Collecting and exporting data",
    "text": "5. Collecting and exporting data\nEverybody who has experience working with data is also familiar with storing and reading data in formats like .csv, .xls, .xlsx or other delimited value storage. However, if your goal is to replicate a common task at a predefined time interval, like charting weekly stock prices for a selected bundle of stocks every end-of-week, it might be overwhelming to manually perform these tasks every week. Our slides covered a handful of data sources that are widely used among finance practictioners, such as stock-level data, macroeconomic data, among others.\nWhen it comes to stock-level data, the tidyquant is a powerful package in R that simplifies financial data retrieval and analysis. One of its key features is the ability to fetch stock market data from Yahoo! Finance using the tq_get() function.\nTo get historical stock prices from Yahoo! Finance, use the tq_get() function:\n\n# Fetch historical stock prices for FANG (Facebook, Amazon, Netflix, Google) stocks\nFANG_data &lt;- tq_get(c(\"META\",\"AMZN\",\"NFLX\",\"GOOG\"), from = \"2020-01-01\", to = \"2024-01-01\")"
  },
  {
    "objectID": "quant-fin/replications/L1/L1-Replication.html#using-write.csv-to-export-a-file",
    "href": "quant-fin/replications/L1/L1-Replication.html#using-write.csv-to-export-a-file",
    "title": "Bridging Data with Programming - Replication",
    "section": "6. Using write.csv() to export a file",
    "text": "6. Using write.csv() to export a file\nThe write.csv() function in R allows users to export data frames to .csv (Comma-Separated Values) files. These files are commonly used for storing tabular data and can be opened in Excel, Google Sheets, or other data analysis tools. It works by defining the following arguments:\n\nwrite.csv(x,file)\n\n\nx: The data frame to export.\nfile: The file path where the CSV will be saved\n\nIt is now easy to use tq_get() in conjunction with write.csv() to retrieve data from Yahoo! Finance and export it to a .csv file:\n\nwrite.csv(FANG_data,'FANG_prices.csv')\n\nThe code above assumes that you have created the FANG_data object in your R session."
  },
  {
    "objectID": "quant-fin/replications/L1/L1-Replication.html#try-doing-some-edits-on-your-own",
    "href": "quant-fin/replications/L1/L1-Replication.html#try-doing-some-edits-on-your-own",
    "title": "Bridging Data with Programming - Replication",
    "section": "Try doing some edits on your own!",
    "text": "Try doing some edits on your own!\nThe code below downloads data for all stocks contained in the assets vector using the tq_get() function and the arguments from and to, and exports a .csv file with your newly retrieved data. Try changing this code to download data for a list of stocks of your choice and a specific timeframe. Make sure to write the date ranges in YYYY-MM-DD format.\n\n#Define the list of assets\nassets = c('MMM','GOOGL','NFLX','WEGE3.SA')\n\n#Define the time ranges\nstart=\"2020-01-01\"\nend=Sys.Date() #Today\n\n# Fetch historical stock prices for selected assets\nfinancial_data &lt;- tq_get(assets, from = start, to=end)\n\n# Export it\nwrite.csv(financial_data,'my_first_export.csv')"
  },
  {
    "objectID": "quant-fin/datacases/data-case-prep/data-case-prep-solutions.html",
    "href": "quant-fin/datacases/data-case-prep/data-case-prep-solutions.html",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise list is a preparation for students of the Practical Applications in Quantitative Finance course held at FGV-EAESP. This document serves as a refresher on key R programming concepts covered in previous lectures, reinforcing the essential skills needed for quantitative finance applications. By revisiting these foundations, you’ll ensure a solid grasp of the tools and techniques required to analyze financial data effectively. As we progress, you’ll apply these concepts to real-world finance examples, including portfolio analysis, risk assessment, and asset pricing models. Mastering these skills now will prepare you to fully engage with the practical applications we’ll explore throughout the course.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®.\n\n\n\n\n\n\nDeliverable\n\n\n\nEach student is expected to deliver his/her assignment individually, although you can freely work in groups for solving the questions. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both your code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details.\n\n\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\n\n\n\nThis exercise aims to gauge your skill in fetching data in R through different sources. You are asked to work on the following questions:\n\nGo to our eClass® and download the Ibovespa_Sample.txt file inside the Data Cases folder. This file contains stock price information regarding 10 publicly traded companies that are part of the Brazilian market index, Ibovespa.\nRead the file into your session using the read_delim() function, and assign to an object called FILE_SAMPLE. Make sure you have the tidyverse package loaded. Depending on how your computer reads delimited data, you may need to set the argument sep to sep=';' or sep=' '.\n\n\nFILE_SAMPLE=read_delim('Ibovespa_Sample.txt',delim=' ')\n\n\nInspect the first 10 lines of the object you have just created using the head() function, setting n=10 as an argument.\n\n\nFILE_SAMPLE%&gt;%head(n=10)\n\n# A tibble: 10 × 8\n   symbol   date        open  high   low close   volume adjusted\n   &lt;chr&gt;    &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 PETR3.SA 2020-01-02  32.3  32.8  32.1  32.8  6660800     10.6\n 2 PETR3.SA 2020-01-03  33    33.2  32.0  32.0 20340400     10.4\n 3 PETR3.SA 2020-01-06  32    33.1  31.8  33.0 17549700     10.7\n 4 PETR3.SA 2020-01-07  33.0  33.0  32.4  32.6  5480400     10.6\n 5 PETR3.SA 2020-01-08  32.7  32.8  31.8  32.0 10030200     10.4\n 6 PETR3.SA 2020-01-09  32.1  32.4  31.8  32.2 15411600     10.4\n 7 PETR3.SA 2020-01-10  32.3  32.3  32.0  32.1  3867200     10.4\n 8 PETR3.SA 2020-01-13  32.2  32.3  31.9  32.1  6666500     10.4\n 9 PETR3.SA 2020-01-14  32.0  32.1  31.5  31.8  6752000     10.3\n10 PETR3.SA 2020-01-15  31.6  31.7  31.1  31.1  7113300     10.1\n\n\n\nHow many rows and columns does your object have? You can use the function dim(yourdata) to find the exact dimensions.\n\nThis object has 7464 rows and 8 columns.\n\nAlternatively, fetch the exact same stock level information directly from Yahoo! Finance using the tq_get() function. Collect the data beginning in 2020 (January \\(1^{st}\\) until the end of 2024 (December \\(31^{st}\\)) using the from and to arguments of the tq_get() function. Make sure you have the tidyquant package loaded. Assign this data to a new object in your R session called YAHOO_SAMPLE.\n\n\nassets = unique(FILE_SAMPLE$symbol)\n\nYAHOO_SAMPLE=assets%&gt;%tq_get(from='2020-01-01',to='2024-12-31')\n\n\nHow many rows and columns does this new object have?\n\nThis object has 7464 rows and 8 columns.\n\n\n\nThis exercise aims to gauge your skill on manipulating data using the tidyverse and the tidyquant. After loading these packages, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE you created before, create a new column, Year, that is defined as the year of a given timestamp found in the date column. You can use year(date) along with a mutate function to create this new variable. Update your YAHOO_SAMPLE object to make sure you have saved this variable.\n\n\nYAHOO_SAMPLE=YAHOO_SAMPLE%&gt;%\n  mutate(Year=year(date))\n\n\nUsing the updated YAHOO_SAMPLE object, use the filter function to filter for observations that occur in \\(2024\\), use group_by() to group the data by symbol, and use the summarize the function to create a new variable, avg_price, defined as the average adjusted prices for each year-symbol combination. Sort your dataset by avg_price (highest-to-lowest), storing this result in an object called 2023_2024_Summary and call it in your terminal. Which company had the highest price levels, and which one had the lowest?\n\n\nSummary_2023_2024=YAHOO_SAMPLE%&gt;%\n  filter(Year==2024)%&gt;%\n  group_by(symbol)%&gt;%\n  summarize(avg_price=mean(adjusted,na.rm=TRUE))%&gt;%\n  arrange(avg_price)\n\nSummary_2023_2024\n\n# A tibble: 6 × 2\n  symbol   avg_price\n  &lt;chr&gt;        &lt;dbl&gt;\n1 BEEF3.SA      6.48\n2 BBDC3.SA     11.8 \n3 BRFS3.SA     19.9 \n4 ITUB3.SA     26.9 \n5 PETR3.SA     36.1 \n6 WEGE3.SA     44.8 \n\n\n\nUsing the YAHOO_SAMPLE dataset, group your data by symbol and create a summary of yearly returns. To do that, first pipe the grouped data into into the tq_transmute function applying the yearlyReturn, assigning it to a new variable,yearly_return. Arrange your result by date and yearly_return (highest-to-lowest). Which stock had the best performance in 2020, and which stock had the worst?\n\n\nYAHOO_SAMPLE%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select='adjusted',\n            mutate_fun = yearlyReturn,\n            col_rename = 'yearly_return')%&gt;%\n  arrange(date,desc(yearly_return))\n\n# A tibble: 30 × 3\n# Groups:   symbol [6]\n   symbol   date       yearly_return\n   &lt;chr&gt;    &lt;date&gt;             &lt;dbl&gt;\n 1 WEGE3.SA 2020-12-30        1.17  \n 2 PETR3.SA 2020-12-30       -0.111 \n 3 ITUB3.SA 2020-12-30       -0.119 \n 4 BEEF3.SA 2020-12-30       -0.155 \n 5 BBDC3.SA 2020-12-30       -0.228 \n 6 BRFS3.SA 2020-12-30       -0.386 \n 7 PETR3.SA 2021-12-30        0.304 \n 8 BEEF3.SA 2021-12-30        0.161 \n 9 BRFS3.SA 2021-12-30        0.0218\n10 WEGE3.SA 2021-12-30       -0.119 \n# ℹ 20 more rows\n\n\nWEGE3.SA has the highest return in 2020, whereas BRFS3.SA had the worst.\n\n\n\nThis exercise aims to gauge your skill on data visualization using the ggplot2 package. After loading this package, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE object, create a ggplot object with the follow aes: the x component should refer to the date column, the y axis should refer to the adjusted column, and the group argument should refer to the symbol column.\n\n\nYAHOO_SAMPLE%&gt;%ggplot(aes(x=date,y=adjusted,group=symbol))\n\n\n\n\n\n\n\n\n\nAdd a geometry layer to your chart using the geom_line() function and call it in your terminal.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()\n\n\n\n\n\n\n\n\n\nChange the titles of your x and y axis, as well as chart title and subtitle using the labs() function.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')\n\n\n\n\n\n\n\n\n\nFacet your data by each symbol using the facet_wrap(symbol~.) function. Set the arguments of this function in such a way that you have a 2x3 orientation (i.e, nrow=2 and ncol=3).\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')+\n  facet_wrap(symbol~.,nrow=3,ncol=2)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nApply a predefined theme using the theme_minimal() function.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')+\n  facet_wrap(symbol~.,nrow=3,ncol=2)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSave your plot using the ggsave() function. To use it, call ggsave('myplot.jpg',width=10,height = 6) to save the last plot that has been prompted to your terminal using a 10x6 resolution (in units).\n\n\nggsave('myplot.jpg',width=10,height = 6)"
  },
  {
    "objectID": "quant-fin/datacases/data-case-prep/data-case-prep-solutions.html#tech-setup",
    "href": "quant-fin/datacases/data-case-prep/data-case-prep-solutions.html#tech-setup",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))"
  },
  {
    "objectID": "quant-fin/datacases/data-case-prep/data-case-prep-solutions.html#exercise-1",
    "href": "quant-fin/datacases/data-case-prep/data-case-prep-solutions.html#exercise-1",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise aims to gauge your skill in fetching data in R through different sources. You are asked to work on the following questions:\n\nGo to our eClass® and download the Ibovespa_Sample.txt file inside the Data Cases folder. This file contains stock price information regarding 10 publicly traded companies that are part of the Brazilian market index, Ibovespa.\nRead the file into your session using the read_delim() function, and assign to an object called FILE_SAMPLE. Make sure you have the tidyverse package loaded. Depending on how your computer reads delimited data, you may need to set the argument sep to sep=';' or sep=' '.\n\n\nFILE_SAMPLE=read_delim('Ibovespa_Sample.txt',delim=' ')\n\n\nInspect the first 10 lines of the object you have just created using the head() function, setting n=10 as an argument.\n\n\nFILE_SAMPLE%&gt;%head(n=10)\n\n# A tibble: 10 × 8\n   symbol   date        open  high   low close   volume adjusted\n   &lt;chr&gt;    &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 PETR3.SA 2020-01-02  32.3  32.8  32.1  32.8  6660800     10.6\n 2 PETR3.SA 2020-01-03  33    33.2  32.0  32.0 20340400     10.4\n 3 PETR3.SA 2020-01-06  32    33.1  31.8  33.0 17549700     10.7\n 4 PETR3.SA 2020-01-07  33.0  33.0  32.4  32.6  5480400     10.6\n 5 PETR3.SA 2020-01-08  32.7  32.8  31.8  32.0 10030200     10.4\n 6 PETR3.SA 2020-01-09  32.1  32.4  31.8  32.2 15411600     10.4\n 7 PETR3.SA 2020-01-10  32.3  32.3  32.0  32.1  3867200     10.4\n 8 PETR3.SA 2020-01-13  32.2  32.3  31.9  32.1  6666500     10.4\n 9 PETR3.SA 2020-01-14  32.0  32.1  31.5  31.8  6752000     10.3\n10 PETR3.SA 2020-01-15  31.6  31.7  31.1  31.1  7113300     10.1\n\n\n\nHow many rows and columns does your object have? You can use the function dim(yourdata) to find the exact dimensions.\n\nThis object has 7464 rows and 8 columns.\n\nAlternatively, fetch the exact same stock level information directly from Yahoo! Finance using the tq_get() function. Collect the data beginning in 2020 (January \\(1^{st}\\) until the end of 2024 (December \\(31^{st}\\)) using the from and to arguments of the tq_get() function. Make sure you have the tidyquant package loaded. Assign this data to a new object in your R session called YAHOO_SAMPLE.\n\n\nassets = unique(FILE_SAMPLE$symbol)\n\nYAHOO_SAMPLE=assets%&gt;%tq_get(from='2020-01-01',to='2024-12-31')\n\n\nHow many rows and columns does this new object have?\n\nThis object has 7464 rows and 8 columns."
  },
  {
    "objectID": "quant-fin/datacases/data-case-prep/data-case-prep-solutions.html#exercise-2",
    "href": "quant-fin/datacases/data-case-prep/data-case-prep-solutions.html#exercise-2",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise aims to gauge your skill on manipulating data using the tidyverse and the tidyquant. After loading these packages, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE you created before, create a new column, Year, that is defined as the year of a given timestamp found in the date column. You can use year(date) along with a mutate function to create this new variable. Update your YAHOO_SAMPLE object to make sure you have saved this variable.\n\n\nYAHOO_SAMPLE=YAHOO_SAMPLE%&gt;%\n  mutate(Year=year(date))\n\n\nUsing the updated YAHOO_SAMPLE object, use the filter function to filter for observations that occur in \\(2024\\), use group_by() to group the data by symbol, and use the summarize the function to create a new variable, avg_price, defined as the average adjusted prices for each year-symbol combination. Sort your dataset by avg_price (highest-to-lowest), storing this result in an object called 2023_2024_Summary and call it in your terminal. Which company had the highest price levels, and which one had the lowest?\n\n\nSummary_2023_2024=YAHOO_SAMPLE%&gt;%\n  filter(Year==2024)%&gt;%\n  group_by(symbol)%&gt;%\n  summarize(avg_price=mean(adjusted,na.rm=TRUE))%&gt;%\n  arrange(avg_price)\n\nSummary_2023_2024\n\n# A tibble: 6 × 2\n  symbol   avg_price\n  &lt;chr&gt;        &lt;dbl&gt;\n1 BEEF3.SA      6.48\n2 BBDC3.SA     11.8 \n3 BRFS3.SA     19.9 \n4 ITUB3.SA     26.9 \n5 PETR3.SA     36.1 \n6 WEGE3.SA     44.8 \n\n\n\nUsing the YAHOO_SAMPLE dataset, group your data by symbol and create a summary of yearly returns. To do that, first pipe the grouped data into into the tq_transmute function applying the yearlyReturn, assigning it to a new variable,yearly_return. Arrange your result by date and yearly_return (highest-to-lowest). Which stock had the best performance in 2020, and which stock had the worst?\n\n\nYAHOO_SAMPLE%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select='adjusted',\n            mutate_fun = yearlyReturn,\n            col_rename = 'yearly_return')%&gt;%\n  arrange(date,desc(yearly_return))\n\n# A tibble: 30 × 3\n# Groups:   symbol [6]\n   symbol   date       yearly_return\n   &lt;chr&gt;    &lt;date&gt;             &lt;dbl&gt;\n 1 WEGE3.SA 2020-12-30        1.17  \n 2 PETR3.SA 2020-12-30       -0.111 \n 3 ITUB3.SA 2020-12-30       -0.119 \n 4 BEEF3.SA 2020-12-30       -0.155 \n 5 BBDC3.SA 2020-12-30       -0.228 \n 6 BRFS3.SA 2020-12-30       -0.386 \n 7 PETR3.SA 2021-12-30        0.304 \n 8 BEEF3.SA 2021-12-30        0.161 \n 9 BRFS3.SA 2021-12-30        0.0218\n10 WEGE3.SA 2021-12-30       -0.119 \n# ℹ 20 more rows\n\n\nWEGE3.SA has the highest return in 2020, whereas BRFS3.SA had the worst."
  },
  {
    "objectID": "quant-fin/datacases/data-case-prep/data-case-prep-solutions.html#exercise-3",
    "href": "quant-fin/datacases/data-case-prep/data-case-prep-solutions.html#exercise-3",
    "title": "Data Case Prep - Solutions",
    "section": "",
    "text": "This exercise aims to gauge your skill on data visualization using the ggplot2 package. After loading this package, provide your answers to the following questions:\n\nUsing the YAHOO_SAMPLE object, create a ggplot object with the follow aes: the x component should refer to the date column, the y axis should refer to the adjusted column, and the group argument should refer to the symbol column.\n\n\nYAHOO_SAMPLE%&gt;%ggplot(aes(x=date,y=adjusted,group=symbol))\n\n\n\n\n\n\n\n\n\nAdd a geometry layer to your chart using the geom_line() function and call it in your terminal.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()\n\n\n\n\n\n\n\n\n\nChange the titles of your x and y axis, as well as chart title and subtitle using the labs() function.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')\n\n\n\n\n\n\n\n\n\nFacet your data by each symbol using the facet_wrap(symbol~.) function. Set the arguments of this function in such a way that you have a 2x3 orientation (i.e, nrow=2 and ncol=3).\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')+\n  facet_wrap(symbol~.,nrow=3,ncol=2)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nApply a predefined theme using the theme_minimal() function.\n\n\nYAHOO_SAMPLE%&gt;%\n  ggplot(aes(x=date,y=adjusted,group=symbol))+\n  geom_line()+\n  labs(title='Stock prices over time',\n       subtitle='Source: Yahoo! Finance',\n       x='Date',\n       y='Adjusted Prices')+\n  facet_wrap(symbol~.,nrow=3,ncol=2)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSave your plot using the ggsave() function. To use it, call ggsave('myplot.jpg',width=10,height = 6) to save the last plot that has been prompted to your terminal using a 10x6 resolution (in units).\n\n\nggsave('myplot.jpg',width=10,height = 6)"
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1-solutions.html",
    "href": "quant-fin/datacases/data-case-1/data-case-1-solutions.html",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "This Data Case is part of the Practical Applications in Quantitative Finance course, held at FGV-EAESP’s undergraduate course in business. Carefully follow the instructions contained in the data case as well as eClass® before you make your submission.\n\n\nThe pharmaceutical industry is a critical sector in financial markets, influenced by regulatory approvals, drug developments, and global health events. In this first Data Case, you will analyze historical stock performance for a set of 10 pharmaceutical companies, applying the tidyverse and tidyquant packages to extract and interpret insights from the data, and create meaningful visualizations using ggplot2.\nYou are a junior analyst at Atlas Capital, a leading buyside investment firm specializing in sector-focused strategies. The firm is considering increasing its exposure to the pharmaceutical industry, given its long-term growth potential and resilience in volatile markets. In the latest investment committee meeting, your fund manager raised an important question: “How has the pharmaceutical industry performed over time? We need to identify whether now is the right time to increase our position.”\nYour team has been tasked with conducting an in-depth financial analysis of the pharmaceutical sector. The goal is to assess industry-wide trends, identify risks and opportunities, and ultimately recommend an investment stance. More specifically, your task will involve:\n\nCollecting stock price data and compute returns\nVisualizing key trends in returns and volatility\nInterpreting findings and suggest investment insights\n\nTo streamline our research, you will focus on the 10 largest publicly traded pharmaceutical companies in the U.S, analyze their performance, risks, and potential catalysts that could drive returns in the near future. As of February 2025, the 10 largest pharmaceutical companies traded in the U.S., along with their ticker symbols, are:\n\nEli Lilly and Co. (LLY): A leading pharmaceutical company known for its innovative treatments in diabetes and oncology.\nNovo Nordisk A/S (NVO): Specializing in diabetes care, Novo Nordisk has a significant presence in the U.S. market.\nJohnson & Johnson (JNJ): A diversified healthcare company with a strong pharmaceutical division.\nAbbVie Inc. (ABBV): Known for its immunology and oncology products, AbbVie is a major player in the pharmaceutical industry.\nMerck & Co., Inc. (MRK): Merck offers a wide range of prescription medicines, vaccines, and therapies.\nPfizer Inc. (PFE): A global pharmaceutical corporation recognized for its vaccines and therapeutics.\nBristol-Myers Squibb Company (BMY): Focused on oncology, cardiovascular, and immunology, Bristol-Myers Squibb is a key industry player.\nAstraZeneca PLC (AZN): A biopharmaceutical company with a strong portfolio in oncology and respiratory diseases.\nAmgen Inc. (AMGN): Specializing in biotechnology, Amgen develops therapies for serious illnesses.\nGilead Sciences, Inc. (GILD): Known for its antiviral drugs, Gilead has a significant market presence.\n\nNow, it’s up to you and your team to dive into the data, extract key insights, and present your data-driven investment thesis. Good luck—your next career milestone at Atlas Capital depends on it. 🚀\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®.\n\n\n\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\",\"ggcorrplot\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n  install.packages('ggcorrplot')\n  install.packages('ggrepel')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)\n  library(ggcorrplot)\n  library(ggrepel)\n\n\n\n\nUse the tq_get() function from the tidyquant package to retrieve historical adjusted closing prices for the 10 largest publicly traded pharmaceutical companies in the U.S. from Yahoo! Finance. Your dataset should cover the period from January 1, 2020, to December 31, 2024. Using the functions from the tidyverse, ensure that your data includes only the timestamp column, as well as the column that contains the daily adjusted stock price information. Store this into an object called financial_data (or something similar). Store this data set for all the subsequent analysis - make sure not to override this dataset as you move along the data case to make sure you are always referring to the raw data pull!\n\n#Define the list of assets\nassets &lt;- c('LLY','NVO','JNJ','ABBV','MRK','PFE','BMY','AZN','AMGN','GILD')\nstart_date &lt;- '2020-01-01'\nend_date &lt;- '2024-12-31'\n\nfinancial_data=assets%&gt;%\n  tq_get(from=start_date,\n         to= end_date)%&gt;%\n  select(date,symbol,adjusted)\n\n\n\n\nUsing the tidyquant package, use the object you’ve just created with the tq_transmute function to compute the yearly returns for each stock over the analysis period. More specifically, pass the yearlyReturn function to adjusted column using the tq_transmute, labeling this new variable as yearly_return. Arrange your dataset by year and in descending order of yearly_return (highest-to-lowest). Store this into a new object called, for example, yearly_returns. Which stock had the highest return in 2024, and which one had the lowest? Prompt the results in your session.\n\nyearly_returns=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = yearlyReturn,\n               col_rename = 'yearly_return')%&gt;%\n  arrange(year(date),desc(yearly_return))\n\n#Full analysis\nyearly_returns\n\n# A tibble: 50 × 3\n# Groups:   symbol [10]\n   symbol date       yearly_return\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;\n 1 LLY    2020-12-31        0.303 \n 2 ABBV   2020-12-31        0.263 \n 3 NVO    2020-12-31        0.206 \n 4 JNJ    2020-12-31        0.107 \n 5 PFE    2020-12-31        0.0317\n 6 AZN    2020-12-31        0.0204\n 7 BMY    2020-12-31        0.0104\n 8 AMGN   2020-12-31       -0.0159\n 9 GILD   2020-12-31       -0.0706\n10 MRK    2020-12-31       -0.0830\n# ℹ 40 more rows\n\n#Best and worst performance\nyearly_returns%&gt;%\n  #Ungroup the data to make sure calculations are done rowwise\n  ungroup()%&gt;%\n  #Select only the last year\n  filter(year(date)==2024)%&gt;%\n  #Select only the 1st and 10th \n  filter(rank(yearly_return) %in% c(1,10))\n\n# A tibble: 2 × 3\n  symbol date       yearly_return\n  &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;\n1 LLY    2024-12-30         0.336\n2 NVO    2024-12-30        -0.162\n\n\n\n\n\nWith your data.frame containing the yearly returns over time for each stock, use the ggplot function to create a line chart of the historical cumulative returns for each stock during the study period. Which stock had the highest cumulative return up-to-date? Recall that cumulative returns can be calculated from period returns as:\n\\[\n\\text{Cumulative Return}= (1+R_1)\\times(1+R_2)\\times ... \\times(1+R_T)-1\\equiv \\prod (1+R_t)-1\n\\]\nYour chart should map date to the x axis, the yearly return variable to the y axis, and group the results by symbol. To make sure that you are plotting a line chart, use the geom_line() function after you have mapped your data. In addition to these two layers, add any customizations that you believe that are beneficial to convey the message - see the Data Visualization lecture.\n\n\n\n\n\n\nHint\n\n\n\n\nWith the data.frame you created to store yearly returns, group by symbol, arrange the results by symbol and date, and use mutate() function to apply the cumprod function to the data. Note that you need to calculate it as cumprod(1+yearly_return)-1, assuming that yearly_return is the variable that contains the previously calculated yearly returns.\nNow, your resulting data.frame contains the cumulative returns for all stocks. You can pipe that into a ggplot call, mapping the symbol to the x-axis, the cumulative return column to the y-axis, and the geom_col() function to create a bar chart. Add as many customizations you think are worth the effort.\n\n\n\n\nyearly_returns%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = yearly_return,\n               mutate_fun = Return.cumulative)%&gt;%\n  setNames(c('symbol','cum_returns'))%&gt;%\n  ggplot(aes(x=reorder(symbol,desc(cum_returns)),y=cum_returns,fill=symbol))+\n  geom_col()+\n  geom_text(aes(label=percent(cum_returns),vjust=-1))+\n  theme_minimal()+\n  labs(x='Stocks',\n       y='Cumulative Return',\n       title='A comparison of individual cumulative returns from selected stocks',\n       subtitle = 'Source: Yahoo! Finance')+\n  scale_y_continuous(labels=percent,limits = c(-0.5,6))+\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\nyearly_returns%&gt;%\n  group_by(symbol)%&gt;%\n  arrange(symbol,date)%&gt;%\n  mutate(cum_returns=cumprod(1+yearly_return)-1)%&gt;%\n  ggplot(aes(x=date,y=cum_returns,group=symbol,col=symbol))+\n  geom_line()+\n  theme_minimal()+\n  labs(x='Stocks',\n       y='Cumulative Return',\n       title='A comparison of individual cumulative returns from selected stocks',\n       subtitle = 'Source: Yahoo! Finance')+\n  scale_y_continuous(labels=percent,limits = c(-0.5,6))+\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\n\nAfter reviewing your initial analysis, your fund manager at Atlas Capital liked the idea of examining yearly returns to get a broader perspective on performance. However, they pointed out that pharmaceutical companies vary significantly in terms of risk exposure, so it’s crucial to account for volatility as well. To complement the analysis, use the same rationale from the previous exercise to calculate the yearly volatility for each stock. How do the risk levels differ between firms? Store your results in a new object and prompt it in your session.\n\n\n\n\n\n\nHint\n\n\n\nAs opposed to the yearlyReturn function, the tidyquant package does not have a pre-built dailyStdev function. Instead, what you can do is to use a combination of functions to get the expected result:\n\nFirst, use tq_transmute() to calculate daily returns passing the dailyReturn function.\nNow, your resulting data.frame contains daily returns for all stocks. It is now in a convenient format to chain this object again, in another tq_transmute() function, applying the StdDev.annualized function and assign to a new object, like yearly_volatility.\nNote, however, that if you simply use StdDev.annualized, it will calculate an annualized metric for each stock for the whole period, which is not what you want. To make sure that you have calculating the annualized standard deviation for each year, you can do a composition of apply.yearly, which applies a given function at yearly intervals, and StdDev.annualized, using the following syntax:\n\n\nyour_daily_return_object%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=StdDev.annualized,\n               col_rename = 'yearly_volatility')\n\nHere, tq_transmute() will apply the function defined in FUN over each interval.\n\n\n\nyearly_volatility=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=StdDev.annualized,\n               col_rename = 'yearly_volatility')\n\n#Full analysis\nyearly_volatility\n\n# A tibble: 50 × 3\n# Groups:   symbol [10]\n   symbol date       yearly_volatility\n   &lt;chr&gt;  &lt;date&gt;                 &lt;dbl&gt;\n 1 LLY    2020-12-31             0.426\n 2 LLY    2021-12-31             0.317\n 3 LLY    2022-12-30             0.273\n 4 LLY    2023-12-29             0.288\n 5 LLY    2024-12-30             0.303\n 6 NVO    2020-12-31             0.315\n 7 NVO    2021-12-31             0.244\n 8 NVO    2022-12-30             0.325\n 9 NVO    2023-12-29             0.306\n10 NVO    2024-12-30             0.357\n# ℹ 40 more rows\n\n#Best and worst performance\nyearly_volatility%&gt;%\n  #Ungroup the data to make sure calculations are done rowwise\n  ungroup()%&gt;%\n  #Select only the last year\n  filter(year(date)==2024)%&gt;%\n  #Select only the 1st and 10th \n  filter(rank(yearly_volatility) %in% c(1,10))\n\n# A tibble: 2 × 3\n  symbol date       yearly_volatility\n  &lt;chr&gt;  &lt;date&gt;                 &lt;dbl&gt;\n1 NVO    2024-12-30             0.357\n2 JNJ    2024-12-30             0.151\n\n\n\n\n\nBuilding on your previous findings, since some companies exhibit higher returns but also greater risk, it might be a good idea to add a risk-adjusted performance metric to the analysis. The Sharpe ratio for stock \\(i\\) in period \\(t\\) measures the risk-adjusted return of an asset and is calculated as:\n\\[\n\\text{Sharpe Ratio}_{i,t}=\\dfrac{R_{i,t}-R_{f,t}}{\\sigma_{i,t}},\n\\]\nwhere \\(R_{i,t}\\) is the return of a given stock \\(i\\) in period \\(t\\), \\(R_{f,t}\\) is the risk-free return for the same period, and \\(\\sigma_{i,t}\\) is the volatility for stock \\(i\\) in period \\(t\\).\nYour task is to calculate the Sharpe Ratio for each pharmaceutical stock using yearly returns and yearly volatility. To simplify your calculations, assume a risk-free rate of \\(0\\%\\) per year (i.e, no risk-free premium). Compare the Sharpe ratios across companies. Do the highest-return stocks also have the best risk-adjusted performance? Are there any stocks that stand out as particularly efficient in generating returns relative to their risk? Are there companies that deliver strong returns but with disproportionately high volatility?\n\n\n\n\n\n\nHint\n\n\n\nThere are two ways you can use to create the Sharpe Ratio:\n\nUsing the previously created yearly_returns and yearly_volatility objects, use the left_join() function to merge them based on a common set of identifiers (in this case, date and symbol). After that, manipulate the resulting data.frame with mutate to generate the Sharpe Ratio.\nUsing tq_transmute in a very similar fashion to what you have done to calculate the yearly volatility. First, use tq_transmute to create a series of daily returns, and after that, pipe the resulting dataset into tq_transmute again, but now passing the the SharpeRatio.annualized function with arguments Rf=0 and scale=252.\n\nAlthough both approaches should yield similar results, potential differences might stem from rounding.\n\n\n\n#Option 1\nyearly_sharpe_1=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=SharpeRatio.annualized,\n               Rf=0,\n               scale=252,\n               col_rename = 'yearly_sharpe')\n\n#Option 2\nyearly_sharpe_2=yearly_returns%&gt;%\n  left_join(yearly_volatility)%&gt;%\n  group_by(symbol,date)%&gt;%\n  mutate(yearly_sharpe=yearly_return/yearly_volatility)\n\nyearly_sharpe_1\n\n# A tibble: 50 × 3\n# Groups:   symbol [10]\n   symbol date       yearly_sharpe\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;\n 1 LLY    2020-12-31         0.708\n 2 LLY    2021-12-31         2.09 \n 3 LLY    2022-12-30         1.26 \n 4 LLY    2023-12-29         2.13 \n 5 LLY    2024-12-30         1.11 \n 6 NVO    2020-12-31         0.652\n 7 NVO    2021-12-31         2.60 \n 8 NVO    2022-12-30         0.701\n 9 NVO    2023-12-29         1.81 \n10 NVO    2024-12-30        -0.456\n# ℹ 40 more rows\n\nyearly_sharpe_2\n\n# A tibble: 50 × 5\n# Groups:   symbol, date [50]\n   symbol date       yearly_return yearly_volatility yearly_sharpe\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n 1 LLY    2020-12-31        0.303              0.426        0.711 \n 2 ABBV   2020-12-31        0.263              0.349        0.754 \n 3 NVO    2020-12-31        0.206              0.315        0.654 \n 4 JNJ    2020-12-31        0.107              0.303        0.355 \n 5 PFE    2020-12-31        0.0317             0.357        0.0889\n 6 AZN    2020-12-31        0.0204             0.360        0.0566\n 7 BMY    2020-12-31        0.0104             0.296        0.0352\n 8 AMGN   2020-12-31       -0.0159             0.383       -0.0416\n 9 GILD   2020-12-31       -0.0706             0.375       -0.188 \n10 MRK    2020-12-31       -0.0830             0.318       -0.261 \n# ℹ 40 more rows\n\n\n\n\n\nWay to go! As you delve deeper into your investment analysis, your fund manager emphasizes the importance of understanding how different pharmaceutical stocks interact with one another over time. To gain insights into the relationships between these companies, your next task is to calculate the correlation of daily stock returns for the selected pharmaceutical companies for the analysis period.\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the daily returns for each stock. You can use the tq_transmute function into your dataset and apply the dailyReturn function.\nAfter that, you need to pivot your data in such a way that each column is a specific ticker with information on daily returns. You can do that by calling pivot_wider(names_from='symbol',values_from='daily_return'), assuming that your daily return variable is called daily_return.\nWith that, you’ll achieve a data frame that now has \\(11\\) columns, namely, the date and the \\(10\\) individual ticker columns with daily return information.\nTo make sure that you are calculating the correlation using a \\(10\\times10\\) matrix, use select(-date) to get rid of the date column and pipe that into cor(), which calculates the correlation across all pairs of variables within a data.frame, and outputs a correlation matrix.\n\nIf you want, you can pipe the result into ggcorplot(), a function from the ggcorplot package that provides meaningful visualizations of correlation matrices.\n\n\n\ncorr_returns=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  pivot_wider(names_from='symbol',values_from = 'daily_return')%&gt;%\n  select(-date)%&gt;%\n  cor()\n\ncorr_returns%&gt;%ggcorrplot(hc.order = TRUE, type = \"lower\",lab = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nBased on your analysis of the correlation between each stock, it seems that these pharmaceutical firms are relatively trending together. Notwithstanding, there might be gains from diversification if instead of choosing a specific firm, we decide to hold a portfolio of pharmaceutical stocks.\nInvesting in a single stock exposes an investor to company-specific (idiosyncratic) risk, such as lawsuits, failed drug trials, or regulatory changes. However, constructing a diversified portfolio of multiple stocks within the same industry can help smooth out these risks while still capturing the overall sector trends. For instance, while one pharmaceutical company may experience a stock price drop due to a failed drug trial, another might gain due to a successful FDA approval. By equally weighting multiple stocks, investors can reduce the impact of any single company’s negative performance while still benefiting from the broader industry’s growth.\nYour manager liked your idea and wanted to test it out by creating an equally-weighted portfolio of all pharmaceutical companies over time. Using the tq_transmute() function, create an object, portfolio_returns, that contains the yearly returns of a portfolio that assigns equal weights - in this case, 10% - on each stock, and compare that to the yearly returns of the S&P 500 Index. Would the fund manager be better-off by investing in the portfolio relative to the S&P500?\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the yearly returns for each stock using the tq_transmute() function as before, grouping the data by symbol and creating a new variable, yearly_return.\nKnowing that you have an equally-weighted portfolio, group your data my date and pipe the result into a summarize() function to create a new variable, portfolio_return as the average across all stocks. Assign this result to an object called portfolio_returns\nFetch S&P 500 data using a similar call to tq_get() like you did in the beginning of the exercise, but now collecting data for ^GSPC. Calculate the yearly returns and assign to a new variable, index_return. Store the result in another data.frame, index_returns.\nMerge both datasets using left_join().\n\n\n\n\nportfolio_returns=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = yearlyReturn,\n               col_rename = 'yearly_return')%&gt;%\n  group_by(date)%&gt;%\n  summarize(portfolio_return=mean(yearly_return,na.rm=TRUE))\n\nindex_returns=tq_get('^GSPC',from=start_date,to=end_date)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = yearlyReturn,\n               col_rename = 'index_return')\n\nleft_join(portfolio_returns,index_returns)\n\nJoining with `by = join_by(date)`\n\n\n# A tibble: 5 × 3\n  date       portfolio_return index_return\n  &lt;date&gt;                &lt;dbl&gt;        &lt;dbl&gt;\n1 2020-12-31           0.0772        0.153\n2 2021-12-31           0.295         0.269\n3 2022-12-30           0.208        -0.194\n4 2023-12-29           0.0534        0.242\n5 2024-12-30           0.0449        0.238\n\n\n\n\n\nAfter analyzing the equally weighted pharmaceutical portfolio, the fund manager was impressed with the performance results. However, they remain skeptical about whether the portfolio truly provides better risk-adjusted returns compared to simply picking one of the best-performing stocks in the industry.\nAs final step, your job is to show whether the portfolio offers superior risk-adjusted returns by computing the Sharpe Ratio for both the portfolio and its individual stocks in 2024. If the portfolio has a higher Sharpe Ratio, it means that diversification helps maximize returns while controlling for risk — an essential argument when managing institutional funds.\nIn order to do that, your task is to provide a visualization of the Sharpe Ratio of the equally-weighted portfolio you’ve just created and compare that to those of the individual stocks. I have already created the portfolio results for you, so you can copy-paste that to your session:\n\nportfolio_sharpe=data.frame(symbol='Portfolio',\n                            yearly_return=0.03705213,\n                            yearly_volatility=0.1223983)\n\n\n\n\n\n\n\nHint\n\n\n\n\nCreate the portfolio_sharpe in your session using the code chunk above.\nUsing the yearly_returns object you’ve created in Exercise 2, filter by year(date)==2024 and left_join() with the yearly_volatility object you have created in Exercise 3, assigning the result to a new object. Assign this to another object.\nBind portfolio_sharpe to the resulting data.frame from the previous step in a rowwise manner using rbind(dataframe1,dataframe2). The resulting dataset should contain both the portfolio andthe individual stocks annualized returns and annualized volatility during 2024.\nFinally, call ggplot() and adjust the aesthetics to show the relationship between risk (x-axis) and return (x-axis) for all individual stocks and the portfolio.\n\n\n\n\n#Individual Stocks\nindividual_sharpe_2024=yearly_returns%&gt;%\n  filter(year(date)==2024)%&gt;%\n  left_join(yearly_volatility)\n\n#Portfolio \nportfolio_returns_2024=financial_data%&gt;%\n  filter(year(date)==2024)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  group_by(date)%&gt;%\n  summarize(daily_return=mean(daily_return,na.rm=TRUE))%&gt;%as.xts()\n\nportfolio_sharpe_2024=data.frame(symbol='Portfolio',\n                                 yearly_return=as.numeric(Return.annualized(portfolio_returns_2024)),\n                                 yearly_volatility=as.numeric(StdDev.annualized(portfolio_returns_2024)))\n\n\nlibrary(ggrepel)\n\nindividual_sharpe_2024%&gt;%\n  rbind(portfolio_sharpe_2024)%&gt;%\n  mutate(sharpe_ratio=yearly_return/yearly_volatility,\n         color=ifelse(symbol=='Portfolio','Portfolio','Individual Stocks'))%&gt;%\n  ggplot(aes(y=yearly_return,x=yearly_volatility))+\n  geom_abline(slope = 0,intercept = 0,linetype='dashed')+\n  geom_point(aes(size=sharpe_ratio,color=color))+\n  geom_text_repel(aes(label=glue('{symbol}: {round(sharpe_ratio,2)}'),vjust=3),size=3)+\n  theme_minimal()+\n  labs(x='Yearly Volatility',\n       y='Yearly Return',\n       title='A comparison of individual returns and volatilities vis-a-vis portfolio results',\n       subtitle = 'Source: Yahoo! Finance')+\n  scale_y_continuous(labels=percent,limits = c(-0.5,0.5))+\n  scale_x_continuous(labels=percent)+\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n\n\n\nNow that you have analyzed the Sharpe ratios of both individual pharmaceutical stocks and the equally weighted portfolio, take a step back and summarize your insights. Did the portfolio offer a better risk-adjusted return compared to individual stocks? If so, why? If not, what might explain the results?\nBased on your findings, what would you recommend to the fund manager? Would you suggest investing in the diversified portfolio, or do certain individual stocks offer superior risk-adjusted returns? Would you propose an alternative weighting scheme, such as a market cap-weighted portfolio, to further improve performance?\nWrite a short conclusion summarizing your key takeaways and justify your investment recommendation using data-driven insights.\nA: as expected, an equally weighted portfolio of assets within a given industry can reduce the portfolio volatility through eliminating firm-specific (or idiosyncratic) risk. Notwithstanding, the correlation analysis shows that these assets are, in general, reasonably correlated - as expected, given that they all refer to the same industry, and as a consequence, share common industry-wise risks. In this sense, there are limits to gains through diversification: you can get rid of firm-specific risk, but since you are fully investing in the pharmaceutical industry, industry and macroeconomic risks that affect all firms are not easily eliminated through diversification.\nAs such, an explanation for the relatively low Sharpe Ratio of the portfolio can be due to the fact that diversification gains are not enough to provide better risk-adjusted returns. Fund managers should keep that in mind when picking the optimal choice of stocks for investing in a single industry, like in this case."
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#case-outline",
    "href": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#case-outline",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "The pharmaceutical industry is a critical sector in financial markets, influenced by regulatory approvals, drug developments, and global health events. In this first Data Case, you will analyze historical stock performance for a set of 10 pharmaceutical companies, applying the tidyverse and tidyquant packages to extract and interpret insights from the data, and create meaningful visualizations using ggplot2.\nYou are a junior analyst at Atlas Capital, a leading buyside investment firm specializing in sector-focused strategies. The firm is considering increasing its exposure to the pharmaceutical industry, given its long-term growth potential and resilience in volatile markets. In the latest investment committee meeting, your fund manager raised an important question: “How has the pharmaceutical industry performed over time? We need to identify whether now is the right time to increase our position.”\nYour team has been tasked with conducting an in-depth financial analysis of the pharmaceutical sector. The goal is to assess industry-wide trends, identify risks and opportunities, and ultimately recommend an investment stance. More specifically, your task will involve:\n\nCollecting stock price data and compute returns\nVisualizing key trends in returns and volatility\nInterpreting findings and suggest investment insights\n\nTo streamline our research, you will focus on the 10 largest publicly traded pharmaceutical companies in the U.S, analyze their performance, risks, and potential catalysts that could drive returns in the near future. As of February 2025, the 10 largest pharmaceutical companies traded in the U.S., along with their ticker symbols, are:\n\nEli Lilly and Co. (LLY): A leading pharmaceutical company known for its innovative treatments in diabetes and oncology.\nNovo Nordisk A/S (NVO): Specializing in diabetes care, Novo Nordisk has a significant presence in the U.S. market.\nJohnson & Johnson (JNJ): A diversified healthcare company with a strong pharmaceutical division.\nAbbVie Inc. (ABBV): Known for its immunology and oncology products, AbbVie is a major player in the pharmaceutical industry.\nMerck & Co., Inc. (MRK): Merck offers a wide range of prescription medicines, vaccines, and therapies.\nPfizer Inc. (PFE): A global pharmaceutical corporation recognized for its vaccines and therapeutics.\nBristol-Myers Squibb Company (BMY): Focused on oncology, cardiovascular, and immunology, Bristol-Myers Squibb is a key industry player.\nAstraZeneca PLC (AZN): A biopharmaceutical company with a strong portfolio in oncology and respiratory diseases.\nAmgen Inc. (AMGN): Specializing in biotechnology, Amgen develops therapies for serious illnesses.\nGilead Sciences, Inc. (GILD): Known for its antiviral drugs, Gilead has a significant market presence.\n\nNow, it’s up to you and your team to dive into the data, extract key insights, and present your data-driven investment thesis. Good luck—your next career milestone at Atlas Capital depends on it. 🚀\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®."
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#tech-setup",
    "href": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#tech-setup",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\",\"ggcorrplot\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n  install.packages('ggcorrplot')\n  install.packages('ggrepel')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)\n  library(ggcorrplot)\n  library(ggrepel)"
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-1",
    "href": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-1",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Use the tq_get() function from the tidyquant package to retrieve historical adjusted closing prices for the 10 largest publicly traded pharmaceutical companies in the U.S. from Yahoo! Finance. Your dataset should cover the period from January 1, 2020, to December 31, 2024. Using the functions from the tidyverse, ensure that your data includes only the timestamp column, as well as the column that contains the daily adjusted stock price information. Store this into an object called financial_data (or something similar). Store this data set for all the subsequent analysis - make sure not to override this dataset as you move along the data case to make sure you are always referring to the raw data pull!\n\n#Define the list of assets\nassets &lt;- c('LLY','NVO','JNJ','ABBV','MRK','PFE','BMY','AZN','AMGN','GILD')\nstart_date &lt;- '2020-01-01'\nend_date &lt;- '2024-12-31'\n\nfinancial_data=assets%&gt;%\n  tq_get(from=start_date,\n         to= end_date)%&gt;%\n  select(date,symbol,adjusted)"
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-2",
    "href": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-2",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Using the tidyquant package, use the object you’ve just created with the tq_transmute function to compute the yearly returns for each stock over the analysis period. More specifically, pass the yearlyReturn function to adjusted column using the tq_transmute, labeling this new variable as yearly_return. Arrange your dataset by year and in descending order of yearly_return (highest-to-lowest). Store this into a new object called, for example, yearly_returns. Which stock had the highest return in 2024, and which one had the lowest? Prompt the results in your session.\n\nyearly_returns=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = yearlyReturn,\n               col_rename = 'yearly_return')%&gt;%\n  arrange(year(date),desc(yearly_return))\n\n#Full analysis\nyearly_returns\n\n# A tibble: 50 × 3\n# Groups:   symbol [10]\n   symbol date       yearly_return\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;\n 1 LLY    2020-12-31        0.303 \n 2 ABBV   2020-12-31        0.263 \n 3 NVO    2020-12-31        0.206 \n 4 JNJ    2020-12-31        0.107 \n 5 PFE    2020-12-31        0.0317\n 6 AZN    2020-12-31        0.0204\n 7 BMY    2020-12-31        0.0104\n 8 AMGN   2020-12-31       -0.0159\n 9 GILD   2020-12-31       -0.0706\n10 MRK    2020-12-31       -0.0830\n# ℹ 40 more rows\n\n#Best and worst performance\nyearly_returns%&gt;%\n  #Ungroup the data to make sure calculations are done rowwise\n  ungroup()%&gt;%\n  #Select only the last year\n  filter(year(date)==2024)%&gt;%\n  #Select only the 1st and 10th \n  filter(rank(yearly_return) %in% c(1,10))\n\n# A tibble: 2 × 3\n  symbol date       yearly_return\n  &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;\n1 LLY    2024-12-30         0.336\n2 NVO    2024-12-30        -0.162"
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-3",
    "href": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-3",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "With your data.frame containing the yearly returns over time for each stock, use the ggplot function to create a line chart of the historical cumulative returns for each stock during the study period. Which stock had the highest cumulative return up-to-date? Recall that cumulative returns can be calculated from period returns as:\n\\[\n\\text{Cumulative Return}= (1+R_1)\\times(1+R_2)\\times ... \\times(1+R_T)-1\\equiv \\prod (1+R_t)-1\n\\]\nYour chart should map date to the x axis, the yearly return variable to the y axis, and group the results by symbol. To make sure that you are plotting a line chart, use the geom_line() function after you have mapped your data. In addition to these two layers, add any customizations that you believe that are beneficial to convey the message - see the Data Visualization lecture.\n\n\n\n\n\n\nHint\n\n\n\n\nWith the data.frame you created to store yearly returns, group by symbol, arrange the results by symbol and date, and use mutate() function to apply the cumprod function to the data. Note that you need to calculate it as cumprod(1+yearly_return)-1, assuming that yearly_return is the variable that contains the previously calculated yearly returns.\nNow, your resulting data.frame contains the cumulative returns for all stocks. You can pipe that into a ggplot call, mapping the symbol to the x-axis, the cumulative return column to the y-axis, and the geom_col() function to create a bar chart. Add as many customizations you think are worth the effort.\n\n\n\n\nyearly_returns%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = yearly_return,\n               mutate_fun = Return.cumulative)%&gt;%\n  setNames(c('symbol','cum_returns'))%&gt;%\n  ggplot(aes(x=reorder(symbol,desc(cum_returns)),y=cum_returns,fill=symbol))+\n  geom_col()+\n  geom_text(aes(label=percent(cum_returns),vjust=-1))+\n  theme_minimal()+\n  labs(x='Stocks',\n       y='Cumulative Return',\n       title='A comparison of individual cumulative returns from selected stocks',\n       subtitle = 'Source: Yahoo! Finance')+\n  scale_y_continuous(labels=percent,limits = c(-0.5,6))+\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\nyearly_returns%&gt;%\n  group_by(symbol)%&gt;%\n  arrange(symbol,date)%&gt;%\n  mutate(cum_returns=cumprod(1+yearly_return)-1)%&gt;%\n  ggplot(aes(x=date,y=cum_returns,group=symbol,col=symbol))+\n  geom_line()+\n  theme_minimal()+\n  labs(x='Stocks',\n       y='Cumulative Return',\n       title='A comparison of individual cumulative returns from selected stocks',\n       subtitle = 'Source: Yahoo! Finance')+\n  scale_y_continuous(labels=percent,limits = c(-0.5,6))+\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-4",
    "href": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-4",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "After reviewing your initial analysis, your fund manager at Atlas Capital liked the idea of examining yearly returns to get a broader perspective on performance. However, they pointed out that pharmaceutical companies vary significantly in terms of risk exposure, so it’s crucial to account for volatility as well. To complement the analysis, use the same rationale from the previous exercise to calculate the yearly volatility for each stock. How do the risk levels differ between firms? Store your results in a new object and prompt it in your session.\n\n\n\n\n\n\nHint\n\n\n\nAs opposed to the yearlyReturn function, the tidyquant package does not have a pre-built dailyStdev function. Instead, what you can do is to use a combination of functions to get the expected result:\n\nFirst, use tq_transmute() to calculate daily returns passing the dailyReturn function.\nNow, your resulting data.frame contains daily returns for all stocks. It is now in a convenient format to chain this object again, in another tq_transmute() function, applying the StdDev.annualized function and assign to a new object, like yearly_volatility.\nNote, however, that if you simply use StdDev.annualized, it will calculate an annualized metric for each stock for the whole period, which is not what you want. To make sure that you have calculating the annualized standard deviation for each year, you can do a composition of apply.yearly, which applies a given function at yearly intervals, and StdDev.annualized, using the following syntax:\n\n\nyour_daily_return_object%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=StdDev.annualized,\n               col_rename = 'yearly_volatility')\n\nHere, tq_transmute() will apply the function defined in FUN over each interval.\n\n\n\nyearly_volatility=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=StdDev.annualized,\n               col_rename = 'yearly_volatility')\n\n#Full analysis\nyearly_volatility\n\n# A tibble: 50 × 3\n# Groups:   symbol [10]\n   symbol date       yearly_volatility\n   &lt;chr&gt;  &lt;date&gt;                 &lt;dbl&gt;\n 1 LLY    2020-12-31             0.426\n 2 LLY    2021-12-31             0.317\n 3 LLY    2022-12-30             0.273\n 4 LLY    2023-12-29             0.288\n 5 LLY    2024-12-30             0.303\n 6 NVO    2020-12-31             0.315\n 7 NVO    2021-12-31             0.244\n 8 NVO    2022-12-30             0.325\n 9 NVO    2023-12-29             0.306\n10 NVO    2024-12-30             0.357\n# ℹ 40 more rows\n\n#Best and worst performance\nyearly_volatility%&gt;%\n  #Ungroup the data to make sure calculations are done rowwise\n  ungroup()%&gt;%\n  #Select only the last year\n  filter(year(date)==2024)%&gt;%\n  #Select only the 1st and 10th \n  filter(rank(yearly_volatility) %in% c(1,10))\n\n# A tibble: 2 × 3\n  symbol date       yearly_volatility\n  &lt;chr&gt;  &lt;date&gt;                 &lt;dbl&gt;\n1 NVO    2024-12-30             0.357\n2 JNJ    2024-12-30             0.151"
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-5",
    "href": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-5",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Building on your previous findings, since some companies exhibit higher returns but also greater risk, it might be a good idea to add a risk-adjusted performance metric to the analysis. The Sharpe ratio for stock \\(i\\) in period \\(t\\) measures the risk-adjusted return of an asset and is calculated as:\n\\[\n\\text{Sharpe Ratio}_{i,t}=\\dfrac{R_{i,t}-R_{f,t}}{\\sigma_{i,t}},\n\\]\nwhere \\(R_{i,t}\\) is the return of a given stock \\(i\\) in period \\(t\\), \\(R_{f,t}\\) is the risk-free return for the same period, and \\(\\sigma_{i,t}\\) is the volatility for stock \\(i\\) in period \\(t\\).\nYour task is to calculate the Sharpe Ratio for each pharmaceutical stock using yearly returns and yearly volatility. To simplify your calculations, assume a risk-free rate of \\(0\\%\\) per year (i.e, no risk-free premium). Compare the Sharpe ratios across companies. Do the highest-return stocks also have the best risk-adjusted performance? Are there any stocks that stand out as particularly efficient in generating returns relative to their risk? Are there companies that deliver strong returns but with disproportionately high volatility?\n\n\n\n\n\n\nHint\n\n\n\nThere are two ways you can use to create the Sharpe Ratio:\n\nUsing the previously created yearly_returns and yearly_volatility objects, use the left_join() function to merge them based on a common set of identifiers (in this case, date and symbol). After that, manipulate the resulting data.frame with mutate to generate the Sharpe Ratio.\nUsing tq_transmute in a very similar fashion to what you have done to calculate the yearly volatility. First, use tq_transmute to create a series of daily returns, and after that, pipe the resulting dataset into tq_transmute again, but now passing the the SharpeRatio.annualized function with arguments Rf=0 and scale=252.\n\nAlthough both approaches should yield similar results, potential differences might stem from rounding.\n\n\n\n#Option 1\nyearly_sharpe_1=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  tq_transmute(select = daily_return,\n               mutate_fun = apply.yearly,\n               FUN=SharpeRatio.annualized,\n               Rf=0,\n               scale=252,\n               col_rename = 'yearly_sharpe')\n\n#Option 2\nyearly_sharpe_2=yearly_returns%&gt;%\n  left_join(yearly_volatility)%&gt;%\n  group_by(symbol,date)%&gt;%\n  mutate(yearly_sharpe=yearly_return/yearly_volatility)\n\nyearly_sharpe_1\n\n# A tibble: 50 × 3\n# Groups:   symbol [10]\n   symbol date       yearly_sharpe\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;\n 1 LLY    2020-12-31         0.708\n 2 LLY    2021-12-31         2.09 \n 3 LLY    2022-12-30         1.26 \n 4 LLY    2023-12-29         2.13 \n 5 LLY    2024-12-30         1.11 \n 6 NVO    2020-12-31         0.652\n 7 NVO    2021-12-31         2.60 \n 8 NVO    2022-12-30         0.701\n 9 NVO    2023-12-29         1.81 \n10 NVO    2024-12-30        -0.456\n# ℹ 40 more rows\n\nyearly_sharpe_2\n\n# A tibble: 50 × 5\n# Groups:   symbol, date [50]\n   symbol date       yearly_return yearly_volatility yearly_sharpe\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n 1 LLY    2020-12-31        0.303              0.426        0.711 \n 2 ABBV   2020-12-31        0.263              0.349        0.754 \n 3 NVO    2020-12-31        0.206              0.315        0.654 \n 4 JNJ    2020-12-31        0.107              0.303        0.355 \n 5 PFE    2020-12-31        0.0317             0.357        0.0889\n 6 AZN    2020-12-31        0.0204             0.360        0.0566\n 7 BMY    2020-12-31        0.0104             0.296        0.0352\n 8 AMGN   2020-12-31       -0.0159             0.383       -0.0416\n 9 GILD   2020-12-31       -0.0706             0.375       -0.188 \n10 MRK    2020-12-31       -0.0830             0.318       -0.261 \n# ℹ 40 more rows"
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-6",
    "href": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-6",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Way to go! As you delve deeper into your investment analysis, your fund manager emphasizes the importance of understanding how different pharmaceutical stocks interact with one another over time. To gain insights into the relationships between these companies, your next task is to calculate the correlation of daily stock returns for the selected pharmaceutical companies for the analysis period.\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the daily returns for each stock. You can use the tq_transmute function into your dataset and apply the dailyReturn function.\nAfter that, you need to pivot your data in such a way that each column is a specific ticker with information on daily returns. You can do that by calling pivot_wider(names_from='symbol',values_from='daily_return'), assuming that your daily return variable is called daily_return.\nWith that, you’ll achieve a data frame that now has \\(11\\) columns, namely, the date and the \\(10\\) individual ticker columns with daily return information.\nTo make sure that you are calculating the correlation using a \\(10\\times10\\) matrix, use select(-date) to get rid of the date column and pipe that into cor(), which calculates the correlation across all pairs of variables within a data.frame, and outputs a correlation matrix.\n\nIf you want, you can pipe the result into ggcorplot(), a function from the ggcorplot package that provides meaningful visualizations of correlation matrices.\n\n\n\ncorr_returns=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  pivot_wider(names_from='symbol',values_from = 'daily_return')%&gt;%\n  select(-date)%&gt;%\n  cor()\n\ncorr_returns%&gt;%ggcorrplot(hc.order = TRUE, type = \"lower\",lab = TRUE)"
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-7",
    "href": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-7",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Based on your analysis of the correlation between each stock, it seems that these pharmaceutical firms are relatively trending together. Notwithstanding, there might be gains from diversification if instead of choosing a specific firm, we decide to hold a portfolio of pharmaceutical stocks.\nInvesting in a single stock exposes an investor to company-specific (idiosyncratic) risk, such as lawsuits, failed drug trials, or regulatory changes. However, constructing a diversified portfolio of multiple stocks within the same industry can help smooth out these risks while still capturing the overall sector trends. For instance, while one pharmaceutical company may experience a stock price drop due to a failed drug trial, another might gain due to a successful FDA approval. By equally weighting multiple stocks, investors can reduce the impact of any single company’s negative performance while still benefiting from the broader industry’s growth.\nYour manager liked your idea and wanted to test it out by creating an equally-weighted portfolio of all pharmaceutical companies over time. Using the tq_transmute() function, create an object, portfolio_returns, that contains the yearly returns of a portfolio that assigns equal weights - in this case, 10% - on each stock, and compare that to the yearly returns of the S&P 500 Index. Would the fund manager be better-off by investing in the portfolio relative to the S&P500?\n\n\n\n\n\n\nHint\n\n\n\n\nFirst, start by calculating the yearly returns for each stock using the tq_transmute() function as before, grouping the data by symbol and creating a new variable, yearly_return.\nKnowing that you have an equally-weighted portfolio, group your data my date and pipe the result into a summarize() function to create a new variable, portfolio_return as the average across all stocks. Assign this result to an object called portfolio_returns\nFetch S&P 500 data using a similar call to tq_get() like you did in the beginning of the exercise, but now collecting data for ^GSPC. Calculate the yearly returns and assign to a new variable, index_return. Store the result in another data.frame, index_returns.\nMerge both datasets using left_join().\n\n\n\n\nportfolio_returns=financial_data%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = yearlyReturn,\n               col_rename = 'yearly_return')%&gt;%\n  group_by(date)%&gt;%\n  summarize(portfolio_return=mean(yearly_return,na.rm=TRUE))\n\nindex_returns=tq_get('^GSPC',from=start_date,to=end_date)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = yearlyReturn,\n               col_rename = 'index_return')\n\nleft_join(portfolio_returns,index_returns)\n\nJoining with `by = join_by(date)`\n\n\n# A tibble: 5 × 3\n  date       portfolio_return index_return\n  &lt;date&gt;                &lt;dbl&gt;        &lt;dbl&gt;\n1 2020-12-31           0.0772        0.153\n2 2021-12-31           0.295         0.269\n3 2022-12-30           0.208        -0.194\n4 2023-12-29           0.0534        0.242\n5 2024-12-30           0.0449        0.238"
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-8",
    "href": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#exercise-8",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "After analyzing the equally weighted pharmaceutical portfolio, the fund manager was impressed with the performance results. However, they remain skeptical about whether the portfolio truly provides better risk-adjusted returns compared to simply picking one of the best-performing stocks in the industry.\nAs final step, your job is to show whether the portfolio offers superior risk-adjusted returns by computing the Sharpe Ratio for both the portfolio and its individual stocks in 2024. If the portfolio has a higher Sharpe Ratio, it means that diversification helps maximize returns while controlling for risk — an essential argument when managing institutional funds.\nIn order to do that, your task is to provide a visualization of the Sharpe Ratio of the equally-weighted portfolio you’ve just created and compare that to those of the individual stocks. I have already created the portfolio results for you, so you can copy-paste that to your session:\n\nportfolio_sharpe=data.frame(symbol='Portfolio',\n                            yearly_return=0.03705213,\n                            yearly_volatility=0.1223983)\n\n\n\n\n\n\n\nHint\n\n\n\n\nCreate the portfolio_sharpe in your session using the code chunk above.\nUsing the yearly_returns object you’ve created in Exercise 2, filter by year(date)==2024 and left_join() with the yearly_volatility object you have created in Exercise 3, assigning the result to a new object. Assign this to another object.\nBind portfolio_sharpe to the resulting data.frame from the previous step in a rowwise manner using rbind(dataframe1,dataframe2). The resulting dataset should contain both the portfolio andthe individual stocks annualized returns and annualized volatility during 2024.\nFinally, call ggplot() and adjust the aesthetics to show the relationship between risk (x-axis) and return (x-axis) for all individual stocks and the portfolio.\n\n\n\n\n#Individual Stocks\nindividual_sharpe_2024=yearly_returns%&gt;%\n  filter(year(date)==2024)%&gt;%\n  left_join(yearly_volatility)\n\n#Portfolio \nportfolio_returns_2024=financial_data%&gt;%\n  filter(year(date)==2024)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = dailyReturn,\n               col_rename = 'daily_return')%&gt;%\n  group_by(date)%&gt;%\n  summarize(daily_return=mean(daily_return,na.rm=TRUE))%&gt;%as.xts()\n\nportfolio_sharpe_2024=data.frame(symbol='Portfolio',\n                                 yearly_return=as.numeric(Return.annualized(portfolio_returns_2024)),\n                                 yearly_volatility=as.numeric(StdDev.annualized(portfolio_returns_2024)))\n\n\nlibrary(ggrepel)\n\nindividual_sharpe_2024%&gt;%\n  rbind(portfolio_sharpe_2024)%&gt;%\n  mutate(sharpe_ratio=yearly_return/yearly_volatility,\n         color=ifelse(symbol=='Portfolio','Portfolio','Individual Stocks'))%&gt;%\n  ggplot(aes(y=yearly_return,x=yearly_volatility))+\n  geom_abline(slope = 0,intercept = 0,linetype='dashed')+\n  geom_point(aes(size=sharpe_ratio,color=color))+\n  geom_text_repel(aes(label=glue('{symbol}: {round(sharpe_ratio,2)}'),vjust=3),size=3)+\n  theme_minimal()+\n  labs(x='Yearly Volatility',\n       y='Yearly Return',\n       title='A comparison of individual returns and volatilities vis-a-vis portfolio results',\n       subtitle = 'Source: Yahoo! Finance')+\n  scale_y_continuous(labels=percent,limits = c(-0.5,0.5))+\n  scale_x_continuous(labels=percent)+\n  theme(legend.position = 'none')"
  },
  {
    "objectID": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#wrapping-up-your-analysis",
    "href": "quant-fin/datacases/data-case-1/data-case-1-solutions.html#wrapping-up-your-analysis",
    "title": "Data Case I - Investing in the Pharmaceutical Industry",
    "section": "",
    "text": "Now that you have analyzed the Sharpe ratios of both individual pharmaceutical stocks and the equally weighted portfolio, take a step back and summarize your insights. Did the portfolio offer a better risk-adjusted return compared to individual stocks? If so, why? If not, what might explain the results?\nBased on your findings, what would you recommend to the fund manager? Would you suggest investing in the diversified portfolio, or do certain individual stocks offer superior risk-adjusted returns? Would you propose an alternative weighting scheme, such as a market cap-weighted portfolio, to further improve performance?\nWrite a short conclusion summarizing your key takeaways and justify your investment recommendation using data-driven insights.\nA: as expected, an equally weighted portfolio of assets within a given industry can reduce the portfolio volatility through eliminating firm-specific (or idiosyncratic) risk. Notwithstanding, the correlation analysis shows that these assets are, in general, reasonably correlated - as expected, given that they all refer to the same industry, and as a consequence, share common industry-wise risks. In this sense, there are limits to gains through diversification: you can get rid of firm-specific risk, but since you are fully investing in the pharmaceutical industry, industry and macroeconomic risks that affect all firms are not easily eliminated through diversification.\nAs such, an explanation for the relatively low Sharpe Ratio of the portfolio can be due to the fact that diversification gains are not enough to provide better risk-adjusted returns. Fund managers should keep that in mind when picking the optimal choice of stocks for investing in a single industry, like in this case."
  },
  {
    "objectID": "quant-fin/datacases/quarto-mock-template.html",
    "href": "quant-fin/datacases/quarto-mock-template.html",
    "title": "Data Case Prep - Exercises",
    "section": "",
    "text": "To help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. You can include your code inside the R code chunks. To run a specific code chunk, select all lines that apply and hit Ctrl+Enter (alternatively, click on the Run Current Chunk button at the top-right corner of the code chunk. Alternatively, to render the Quarto altogether, ckick on the Render button (shortcut: Ctrl+Shift+K).\nYou can copy-paste the code chunk below each time you need to manipulate data. To provide your explanation and interpretation of the results for each question, you can write outside of the R coding chunk.\n\n### Enter your code here\n\nEnter your analysis here.\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)"
  },
  {
    "objectID": "quant-fin/datacases/quarto-mock-template.html#tech-setup",
    "href": "quant-fin/datacases/quarto-mock-template.html#tech-setup",
    "title": "Data Case Prep - Exercises",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages by running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\", \"ggthemes\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)"
  },
  {
    "objectID": "quant-fin/replications/L2/L2-Replication.html",
    "href": "quant-fin/replications/L2/L2-Replication.html",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nIn this lecture, we will be working with daily stock price data from the Magnificent Seven (AAPL, GOOG, MSFT, NVDA, TSLA, AMZN, and META). I have already downloaded the data for you using the tidyquant package, which allows us to pull stock price data from multiple securities in a convenient format. You can hit the Download button to get a grasp on how the data looks like or download it directly on eClass® - file name: M7.csv. Before you start, make sure to follow the instructions from our previous replication to set up your working directory correctly."
  },
  {
    "objectID": "quant-fin/replications/L2/L2-Replication.html#about-this-document",
    "href": "quant-fin/replications/L2/L2-Replication.html#about-this-document",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nIn this lecture, we will be working with daily stock price data from the Magnificent Seven (AAPL, GOOG, MSFT, NVDA, TSLA, AMZN, and META). I have already downloaded the data for you using the tidyquant package, which allows us to pull stock price data from multiple securities in a convenient format. You can hit the Download button to get a grasp on how the data looks like or download it directly on eClass® - file name: M7.csv. Before you start, make sure to follow the instructions from our previous replication to set up your working directory correctly."
  },
  {
    "objectID": "quant-fin/replications/L2/L2-Replication.html#loading-packages",
    "href": "quant-fin/replications/L2/L2-Replication.html#loading-packages",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "Loading packages",
    "text": "Loading packages\nAs we get started, we will be loading all packages referred in our official website.\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\", \"glue\",\"scales\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nNote that you could easily get around this by installing and loading all necessary packages using a more simple syntax:\n\n#Install if not already available - I have commented these lines so that R does not attempt to install it everytime\n  #install.packages('tidyverse')\n  #install.packages('tidyquant')\n  #install.packages('glue')\n  #install.packages('scales')\n  #install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)"
  },
  {
    "objectID": "quant-fin/replications/L2/L2-Replication.html#using-dplyr-the-data-manipulation-package-in-the-tidyverse",
    "href": "quant-fin/replications/L2/L2-Replication.html#using-dplyr-the-data-manipulation-package-in-the-tidyverse",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "Using dplyr, the data manipulation package in the tidyverse",
    "text": "Using dplyr, the data manipulation package in the tidyverse\nThe dplyr package is one of the core packages in the tidyverse and is designed for efficient and readable data manipulation. It provides a set of functions (also called “verbs”) that make working with data frames (or tibbles) intuitive and expressive. Key Features:\n\nFilter rows: filter()\nSelect columns: select()\nMutate (create new columns): mutate()\nSummarize data: summarize()\nGroup operations: group_by()\nJoin tables: left_join(), right_join(), inner_join(), full_join()\n\nTo get started with our exercises, we will refer to the M7.csv file that has been provided. After setting the current directory of the file, we load the data using the read.csv() function:\n\n#Apply function to the data\nM7=read.csv('M7.csv')\n\n#Show the first 10 observations\nhead(M7)\n\n  symbol       date    open    high     low   close    volume adjusted\n1   AAPL 2020-01-02 74.0600 75.1500 73.7975 75.0875 135480400 72.79604\n2   AAPL 2020-01-03 74.2875 75.1450 74.1250 74.3575 146322800 72.08828\n3   AAPL 2020-01-06 73.4475 74.9900 73.1875 74.9500 118387200 72.66272\n4   AAPL 2020-01-07 74.9600 75.2250 74.3700 74.5975 108872000 72.32098\n5   AAPL 2020-01-08 74.2900 76.1100 74.2900 75.7975 132079200 73.48434\n6   AAPL 2020-01-09 76.8100 77.6075 76.5500 77.4075 170108400 75.04523"
  },
  {
    "objectID": "quant-fin/replications/L2/L2-Replication.html#the-mutate-function",
    "href": "quant-fin/replications/L2/L2-Replication.html#the-mutate-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "1. The mutate() function",
    "text": "1. The mutate() function\nThe mutate() function adds new variables that are functions of existing variables:\n\nmutate(.data, #The object you are performing the calculations \n       new_variable_1 = var1 * 2, #Can use basic operations...\n       new_variable_2 = median(var2), #Or predefined functions)\n       variable_3 = as.character(var3) #And can be used to modify existing variables)\n       ) \n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nIt sequentially creates the columns you asked for and place them to the right of your data.frame (or tibble)\nYou can use any function, predefined or custom, and apply it to mutate()\nIt can also modify any columns you want (if the name is the same as an existing column)\n\nExercise: use columns high and low and create a new column, mid, defined as the average between daily high and low prices.\n\n#Apply function to the data\nM7=mutate(M7, mid= (high+low)/2)\n\n#Show the first 10 observations\nhead(M7)\n\n  symbol       date    open    high     low   close    volume adjusted      mid\n1   AAPL 2020-01-02 74.0600 75.1500 73.7975 75.0875 135480400 72.79604 74.47375\n2   AAPL 2020-01-03 74.2875 75.1450 74.1250 74.3575 146322800 72.08828 74.63500\n3   AAPL 2020-01-06 73.4475 74.9900 73.1875 74.9500 118387200 72.66272 74.08875\n4   AAPL 2020-01-07 74.9600 75.2250 74.3700 74.5975 108872000 72.32098 74.79750\n5   AAPL 2020-01-08 74.2900 76.1100 74.2900 75.7975 132079200 73.48434 75.20000\n6   AAPL 2020-01-09 76.8100 77.6075 76.5500 77.4075 170108400 75.04523 77.07875"
  },
  {
    "objectID": "quant-fin/replications/L2/L2-Replication.html#the-select-function",
    "href": "quant-fin/replications/L2/L2-Replication.html#the-select-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "2. The select() function",
    "text": "2. The select() function\nThe select() function select (and optionally rename) variables in a data frame, using a concise mini-language that makes it easy to refer to variables based on their name (e.g. a:f selects all columns from a on the left to f on the right) or type (e.g. where(is.numeric) selects all numeric columns):\n\nselect(.data, #The object which you are performing the operations \n       variable_3, #Can reorder columns\n       variable_1, \n       variable_2:variable_4, #Matches position patterns \n       where(is.numeric) #Can select all columns that match a given pattern\n       ) \n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd select only the columns you’ve asked for\nYou can also use select(.data,-variable) to remove a variable\nIt keeps the structure of the data.frame intact - no rows are affected\n\nThe select() function also comes with a handy companion of selectors, which are functions that help you cherry pick columns in a concise way, rather than hardcoding them altogether:\n\n: for selecting a range of consecutive variables.\nstarts_with() starts with a string\nends_with() ends with a string\ncontains() contains a string\nmatches()matches a regular expression.\nwhere()a function to all variables and selects those for which the function returns TRUE\n\nExercise: Select only the symbol, date, volume, and adjusted, in that order.\n\n#Apply function to the data\nM7=select(M7,symbol,date,volume,adjusted)\n\n#Show the first 10 observations\nhead(M7)\n\n  symbol       date    volume adjusted\n1   AAPL 2020-01-02 135480400 72.79604\n2   AAPL 2020-01-03 146322800 72.08828\n3   AAPL 2020-01-06 118387200 72.66272\n4   AAPL 2020-01-07 108872000 72.32098\n5   AAPL 2020-01-08 132079200 73.48434\n6   AAPL 2020-01-09 170108400 75.04523"
  },
  {
    "objectID": "quant-fin/replications/L2/L2-Replication.html#the-filter-function",
    "href": "quant-fin/replications/L2/L2-Replication.html#the-filter-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "3. The filter() function",
    "text": "3. The filter() function\nThe filter() function is used to subset a data frame, retaining all rows that satisfy your conditions. To be retained, the row must produce a value of TRUE for all conditions:\n\nfilter(.data, #The object which you are performing the operations\n       variable_1 &gt;10, #Simple arithmetic operators\n       variable_2 %in% c('AAPL','MSFT','FORD'), #Pattern search\n       !(variable_3 %in% c('Boston','Mass','Silicon Valley')), #Negate pattern search\n       variable_4 &gt;=10 & variable_3&lt;= 4 | is.na(variable_4) #IF and OR conditions\n       ) \n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd filter the rows based on the conditions outlined\nYou can use any function, predefined or custom, and apply it to filter()\nIt returns a subset of the whole object, keeping the columns and the data structure intact\n\nExercise: filter for observations that occurred in 2025, only. You can use the year() function with the date variable to retrieve the year.\n\n#Apply function to the data\nM7=filter(M7,year(date)==2025)\n\n#Show the first 10 observations\nhead(M7)\n\n  symbol       date   volume adjusted\n1   AAPL 2025-01-02 55740700   243.85\n2   AAPL 2025-01-03 40244100   243.36\n3   AAPL 2025-01-06 45045600   245.00\n4   AAPL 2025-01-07 40856000   242.21\n5   AAPL 2025-01-08 37628900   242.70\n6   AAPL 2025-01-10 61710900   236.85"
  },
  {
    "objectID": "quant-fin/replications/L2/L2-Replication.html#the-arrange-function",
    "href": "quant-fin/replications/L2/L2-Replication.html#the-arrange-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "3. The arrange() function",
    "text": "3. The arrange() function\nThe arrange() function reorders the rows of a data frame by the values of selected columns:\n\n#Some Options, always in the following format: the object you are rearranging + the reordering scheme\narrange(.data, variable1) #Ascending by variable_1\narrange(.data, variable1, variable_2) #Ascending by variable_1 and then variable_2\narrange(.data, variable2, variable_1) #Ascending by variable_2 and then variable 1\narrange(.data, variable1, desc(variable_2)) #Ascending by variable_1, and then descending by variable_2\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd reorders the rows of your data.frame (or tibble)\nThis can be useful for visualization, but also for applying position-dependent functions, like lag(), lead(), head(), and tail()\n\nExercise: arrange the dataset by descending date (newest to oldest) and symbol..\n\n#Apply function to the data\nM7=arrange(M7,desc(date),symbol)\n\n#Show the first 10 observations\nhead(M7)\n\n  symbol       date    volume adjusted\n1   AAPL 2025-01-29  45486100   239.36\n2   AMZN 2025-01-29  26091700   237.07\n3   GOOG 2025-01-29  12287800   197.18\n4   META 2025-01-29  21377800   676.49\n5   MSFT 2025-01-29  23581400   442.33\n6   NVDA 2025-01-29 467120600   123.70"
  },
  {
    "objectID": "quant-fin/replications/L2/L2-Replication.html#the-summarize-function",
    "href": "quant-fin/replications/L2/L2-Replication.html#the-summarize-function",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "4. The summarize() function",
    "text": "4. The summarize() function\nThe summarise() - or summarize() - function creates a new data frame. It returns one row for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarising all observations in the input. It will contain one column for each grouping variable and one column for each of the summary statistics that you have specified.\n\nsummarize(.data, #The object which you are performing the operations \n       new_variable_1 = mean(var1,na.rm=TRUE), #Average of var1, removing NA values\n       new_variable_2 = median(var2,na.rm=TRUE), #Median of var1, removing, NA values\n       new_variable_3 = n_distinct(var2) #Number of unique values of var2\n       ) \n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd reshapes the data.frame (or tibble) by the aggregation functions\nAs the name suggests, it is used to summarize a table\n\nExercise: summarize the dataset by creating an average column, defined as the average adjusted prices. You can use the mean() function to get the average. Use the option na.rm=TRUE inside the mean function to make sure that NA values are disregarded.\n\n#Apply function to the data\nSummary=summarize(M7,average=mean(adjusted,na.rm=TRUE))\n\n#Show the first observations\nhead(Summary)\n\n   average\n1 322.1803"
  },
  {
    "objectID": "quant-fin/replications/L2/L2-Replication.html#slice-and-dice-through-group_by",
    "href": "quant-fin/replications/L2/L2-Replication.html#slice-and-dice-through-group_by",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "5. Slice and dice through group_by",
    "text": "5. Slice and dice through group_by\nThe group_by() function takes an existing table and converts it into a grouped table where operations are performed “by group”. Using ungroup() removes grouping.\n\nData=group_by(Data,v1,v2,v3)\nData=summarize(avg=mean(x,na.rm=TRUE))\n\n\nThis function takes the .data argument you provided (in your example, the Data object)…\nAnd creates the avg variable taking the average of x within each tuple defined by the grouping variables (in this case, v1,v2, and v3 )\nIt returns a grouped dataframe, with the results of avg displayed for each unique combination of v1,v2, and v3\n\nThe group_by() function in R is part of the dplyr package and is used to create grouped data frames. It is commonly used in combination with summarize(), mutate(), and other dplyr functions to perform operations within groups.\n\nImportant\nAfter grouping, it’s often necessary to ungroup the data to prevent unintended behavior in subsequent operations:\n\nData=group_by(Data,v1,v2,v3)\nData=summarize(avg=mean(x,na.rm=TRUE))\nData=ungroup(Data)\n\n\nLet’s try the latest summarize() call again, but now grouping the data by symbol first:\n\n#Apply function to the data\nM7=group_by(M7,symbol)\nSummary=summarize(M7,average=mean(adjusted,na.rm=TRUE))\n\n#Show the first 10 observations\nhead(Summary,10)\n\n# A tibble: 7 × 2\n  symbol average\n  &lt;chr&gt;    &lt;dbl&gt;\n1 AAPL      234.\n2 AMZN      227.\n3 GOOG      196.\n4 META      625.\n5 MSFT      430.\n6 NVDA      137.\n7 TSLA      405."
  },
  {
    "objectID": "quant-fin/replications/L2/L2-Replication.html#pipe-your-way-through-the-code",
    "href": "quant-fin/replications/L2/L2-Replication.html#pipe-your-way-through-the-code",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "6. Pipe your way through the code %>%",
    "text": "6. Pipe your way through the code %&gt;%\nThe dplyr verbs, in isolation, are a great tool for data analysts, but what really makes them to shine is what glues them together. The pipe operator (%&gt;% or |&gt;) is a key feature of the magrittr package (included in the tidyverse) and is widely used in R, especially together with dplyr, for improving code readability and structuring data transformation workflows. Key benefits include:\n✅ Improved Readability – The sequence of transformations is clear.\n✅ No Need for Temporary Variables – Each step directly passes its result to the next function.\n✅ Avoids Nesting – No deeply nested function calls.\nThe pipe operator allows you to pass the result of one function as the first argument to the next function, making code more readable and eliminating the need for nested function calls. To show its functionality in action, in the code chunk below, both parts of the code produce the exact same result, but the latter, using the pipe operator, is much simpler to read:\n\n#Instead of \nData = read.csv('Data.csv') #Start with the data\nData = mutate(Data, new_var_1=var_1*10)#Mutate\nData = select(Data, var_1,var_2,new_var_1,where(is.numeric))#Select\nData = filter(Data, new_var_1&gt;5)#Filter\nData = arrange(Data, new_var_1,desc(var2))#Arrange\nData = summarize(Data, new_var=mean(new_var_1,na.rm=TRUE))#Summarize\n\n#Do\nData = read.csv('Data.csv')%&gt;% #Start with the data\n        mutate(new_var_1=var_1*10)%&gt;% #Mutate\n        select(var_1,var_2,new_var_1,where(is.numeric))%&gt;% #Select\n        filter(new_var_1&gt;5)%&gt;% #Filter\n        arrange(new_var_1,desc(var2))%&gt;% #Arrange\n        summarize(new_var=mean(new_var_1,na.rm=TRUE))#Summarize"
  },
  {
    "objectID": "quant-fin/replications/L2/L2-Replication.html#hands-on-exercise",
    "href": "quant-fin/replications/L2/L2-Replication.html#hands-on-exercise",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\nOn January \\(25^{th}\\), chinese startup DeepSeek disrupted the tech stock market as investors reassessed the likely future investment in Artificial Intelligence hardware. As part of your work as a buy-side analyst, you were asked to analyze how the Magnificent 7 performed after the DeepSeek. To this point, follow the instructions and answer to the following question: which stock suffered the most during January 2025?\n\nTo answer this question, you will be using all dplyr verbs you’ve practiced so far\nFurthermore, you will be also using some common base R and ther dplyr functions, like lag(), prod(), as.Date() and drop_na()\n\nThe expected result is a data.frame object that shows, for each symbol, the monthly return on January, 2025, ordered from lowest-to-highest.\n\n\n\n\n\n\nInstructions\n\n\n\nThe data, stored in M7.csv, can be loaded using read.csv('M7.csv'). You can download it using the link shown in Slide 4.\n\nSelect only the symbol, date, and adjusted columns, and arrange the dataset from oldest to newest\nMutate your date variable, making sure to read it as a Date object using as.Date()\nCreate a Year variable and filter only on observations happening in 2025. You can use the year() function to retrieve the year of a given Date column.\nGroup data by symbol\nCreate, for each different symbol, a Return variable that is defined as \\(P_{t+1}/P_{t}\\), where \\(t\\) refers to a date. You can use the lag() function for this\nYou will see that lag produces an NA whenever you try to lag the first observation. To make sure your data does not contain any NA, call drop_na()\nCreate, for each different symbol, a Cum_Return variable that is defined as the cumulative return. Compounded returns over time can be written as \\(\\small \\prod(1+R_t)=(1+R_1)\\times(1+R_2)\\times...\\times(1+R_t)\\). For this, you can use the prod() function.\nPick the latest observation from each symbol and arrange the table from lowest-to-highest return. The function slice_tail(n=x) retrieves the bottom x observations, whereas slice_head(n=y) retrieves the top y."
  },
  {
    "objectID": "quant-fin/replications/L2/L2-Replication.html#solution-walkthrough",
    "href": "quant-fin/replications/L2/L2-Replication.html#solution-walkthrough",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "Solution walkthrough",
    "text": "Solution walkthrough\n\n#Read the Data\nM7%&gt;%\n#Select only the columns of interest\nselect(symbol,date,adjusted)%&gt;%\n#Make sure date is read as a Date object\nmutate(date=as.Date(date))%&gt;%\n#Filter for observations happening in 2025\nfilter(year(date)==2025)%&gt;%\n#Arrange from chronological order\narrange(date)%&gt;%\n#Group by Symbol to perform the calculations\ngroup_by(symbol)%&gt;%\n#Create the return\nmutate(Return = adjusted/lag(adjusted,default = NA))%&gt;%\n#Remove NAs before doing the cumulative product\ndrop_na()%&gt;%\nmutate(Cum_Return = cumprod(Return)-1)%&gt;%\n#Select the latest observation from each symbol\nslice_tail(n=1)%&gt;%\n#Select symbol, date, and cumulative return\nselect(symbol,date,Cum_Return)%&gt;%\n#Arrange from lowest-to-highest\narrange(Cum_Return)\n\n# A tibble: 7 × 3\n# Groups:   symbol [7]\n  symbol date       Cum_Return\n  &lt;chr&gt;  &lt;date&gt;          &lt;dbl&gt;\n1 NVDA   2025-01-29    -0.106 \n2 AAPL   2025-01-29    -0.0184\n3 TSLA   2025-01-29     0.0259\n4 GOOG   2025-01-29     0.0344\n5 MSFT   2025-01-29     0.0567\n6 AMZN   2025-01-29     0.0765\n7 META   2025-01-29     0.129 \n\n\nThis code processes stock price data from M7 using the dplyr package. It calculates the cumulative return for each stock (symbol) in the year 2025, then selects the latest available observation per stock and sorts them from lowest to highest cumulative return.\n\nRead the Data. M7 is assumed to be a data frame or tibble containing stock data ready in your session. You can use read.csv() and store it in an R object. The pipe operator %&gt;%) is used to chain functions together.\nSelect Relevant Columns. Keeps only the relevant columns for the analysis:\n\n\nsymbol → The stock ticker\ndate → The trading date\nadjusted → The adjusted closing price (used for return calculations)\n\nMaking sure the select function is applied as one of the first adjustments can facilitate data wrangling as it shrinks the dataset for the upcoming operations.\n\nEnsure date is a Date object in your session. The code converts the date column to a Date object to enable time-based filtering and calculations, like year().\nFilter Data for 2025. The code uses year(date) == 2025 (from the lubridate package, loaded together with the tidyverse) to keep only data from 2025.\nSort Data in Chronological Order. The code ensures that stock prices are arranged earliest to latest for correct return calculations.\nGroups the dataset by stock (symbol). Using group_by() ensures that return calculations are performed for each stock separately\nCalculate Daily Returns. After the dataset is grouped, we use the mutate() function to create our return metric:\n\n\\[\nReturn=\\dfrac{P_{t}}{P_{t-1}}\n\\]\n\nUses lag(adjusted) to get the previous day’s adjusted price.\nThe first row in each group will have NA (because there’s no previous price)\n\nBecause of that, we also need a call to drop_na() to make sure that whenever we are multiplying these indices, we are not including NA values.\n\n\n\n\n\n\nImportant\n\n\n\nThe function cumprod(), which calculates the cumulative product of a series, multiplies values sequentially. However, if there are missing values (NA) in the sequence, cumprod() propagates NA to all subsequent values. This can corrupt the entire computation.\nFor example, ommitting the drop_na() step in the solution code would produce NA all over Cum_Return:\n\n#Read the Data\nM7%&gt;%\n#Select only the columns of interest\nselect(symbol,date,adjusted)%&gt;%\n#Make sure date is read as a Date object\nmutate(date=as.Date(date))%&gt;%\n#Filter for observations happening in 2025\nfilter(year(date)==2025)%&gt;%\n#Arrange from chronological order\narrange(date)%&gt;%\n#Group by Symbol to perform the calculations\ngroup_by(symbol)%&gt;%\n#Create the return\nmutate(Return = adjusted/lag(adjusted,default = NA))%&gt;%\n#Remove NAs before doing the cumulative product\nmutate(Cum_Return = cumprod(Return)-1)\n\n# A tibble: 126 × 5\n# Groups:   symbol [7]\n   symbol date       adjusted Return Cum_Return\n   &lt;chr&gt;  &lt;date&gt;        &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n 1 AAPL   2025-01-02     244. NA             NA\n 2 AMZN   2025-01-02     220. NA             NA\n 3 GOOG   2025-01-02     191. NA             NA\n 4 META   2025-01-02     599. NA             NA\n 5 MSFT   2025-01-02     419. NA             NA\n 6 NVDA   2025-01-02     138. NA             NA\n 7 TSLA   2025-01-02     379. NA             NA\n 8 AAPL   2025-01-03     243.  0.998         NA\n 9 AMZN   2025-01-03     224.  1.02          NA\n10 GOOG   2025-01-03     193.  1.01          NA\n# ℹ 116 more rows\n\n\n\n\n\nCalculate Cumulative Returns. With the series of daily returns in place cumulative return over time can be retrieved by compounding each individual return over time:\n\n\\[\n\\text{Cumulative Return}_{t=1\\rightarrow T}= (1+R_1)\\times(1+R_2)\\times(1+R_3)\\times...\\times(1+R_t)\\equiv\\prod_{t=1}^{T}(1+R_t)\n\\] To perform such calculations, the code uses cumprod(Return), which multiplies returns over time. In the end, we also need to subtract 1 to express it as a percentage return.\n\nSelect the Latest Observation Per Stock. The slice_tail() function keeps only the last row (i.e., the most recent date) for each stock. Note that this behavior is only possible because our data has been grouped by symbol in the subsequent steps. In an ungrouped case, slice_tail() would retrieve the latest observation considering the data as a whole - in this case, META cumulative returns.\nKeep Only Key Columns and Rearrange. After we’re done creating the relevant variables, we can use the select() function to keep only the columns that are of interest: symbol,date, and Cum_Return, and use the arrange() function to sort observations by ascending order of cumulative returns (i.e, lowest-to-highest)."
  },
  {
    "objectID": "quant-fin/replications/L2/L2-Replication.html#try-doing-some-edits-on-your-own",
    "href": "quant-fin/replications/L2/L2-Replication.html#try-doing-some-edits-on-your-own",
    "title": "Collecting, Organizing, and Manipulating Financial Data - Replication",
    "section": "Try doing some edits on your own!",
    "text": "Try doing some edits on your own!\nTry thinking about changes you could do to either improve code readability of the analysis. A couple of edits that can be made include, but are not limited, to:\n\nAdding more time periods to the analysis\nIncreasing the set of assets to include more tech firms other than the magnificent seven\nCalculate volatility metrics using var() or stdev() functions\n\nPlay around with these concepts to get familiar with all the data manipulation tools that come with dplyr!"
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html",
    "href": "quant-fin/replications/L4/L4-Replication.html",
    "title": "Data Visualization",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nIn this lecture, we will be using the FANG dataset, which contains basic stock information from popular U.S. techonology firms: Facebook (Meta), Amazon, Netflix, and Google (Alphabet). Instead of loading the data from a .csv file, we will be loading data from a .txt file using the read_delim() function from readr, a package that is included in the tidyverse. Before you start, make sure to follow the instructions from our previous replication to set up your working directory correctly."
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#about-this-document",
    "href": "quant-fin/replications/L4/L4-Replication.html#about-this-document",
    "title": "Data Visualization",
    "section": "",
    "text": "This file replicates the codes that have been discussed in the live-session lectures of the Practical Applications in Quantitative Finance course. To ensure you can run the codes without issues, please install and load all required packages beforehand. It is always a good practice to replicate this Quarto document and experiment by making edits to the parameters. At the end of this report, you will find a suggestion on how to tweak this report — try doing some changes on your own!\n\n\n\n\n\n\nAttention\n\n\n\nIn this lecture, we will be using the FANG dataset, which contains basic stock information from popular U.S. techonology firms: Facebook (Meta), Amazon, Netflix, and Google (Alphabet). Instead of loading the data from a .csv file, we will be loading data from a .txt file using the read_delim() function from readr, a package that is included in the tidyverse. Before you start, make sure to follow the instructions from our previous replication to set up your working directory correctly."
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#loading-packages",
    "href": "quant-fin/replications/L4/L4-Replication.html#loading-packages",
    "title": "Data Visualization",
    "section": "Loading packages",
    "text": "Loading packages\nAs we get started, we will be loading all packages referred in our official website.\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"glue\",\"scales\", \"ggthemes\",\"highcharter\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\nlapply(packages, library, character.only = TRUE)\n\nNote that you could easily get around this by installing and loading all necessary packages using a more simple syntax:\n\n#Install if not already available - I have commented these lines so that R does not attempt to install it everytime\n  #install.packages('tidyverse')\n  #install.packages('tidyquant')\n  #install.packages('glue')\n  #install.packages('scales')\n  #install.packages('ggthemes')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(ggthemes)\n  library(highcharter)"
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#data-visualization-in-r",
    "href": "quant-fin/replications/L4/L4-Replication.html#data-visualization-in-r",
    "title": "Data Visualization",
    "section": "Data Visualization in R",
    "text": "Data Visualization in R\nThe ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. The Grammar of Graphics, developed by Leland Wilkinson, is a structured approach to visualization where:\n\nData is mapped to aesthetic attributes (e.g., color, shape, size)\nA geometric object (geom) represents data visually (e.g., points, lines, bars)\nStatistical transformations (stats) summarize data\nScales control how data is mapped to visual properties\nFacets split data into panels for comparison\n\nKey Highlights\n\nIt is, by and large, the richest and most widely used plotting ecosystem in the  language\nggplot2 has a rich ecosystem of extensions - ranging from annotations and interactive visualizations to specialized genomics - click here a community maintained list"
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#the-ggplot2-foundations",
    "href": "quant-fin/replications/L4/L4-Replication.html#the-ggplot2-foundations",
    "title": "Data Visualization",
    "section": "The ggplot2 foundations",
    "text": "The ggplot2 foundations\nWe will illustrate the use of ggplot2 to replicate the Grammar of Graphics foundations using the FANG dataset. To load it into your R session, hit the download button and load it using read_delim('FANG.txt') or download the FANG.txt file directly on eClass®. To get ggplot2 in your session, either load tidyverse altogether of directly load the library:\n\n#Load the tidyquant package\nlibrary(tidyquant)\n\n#Option 1: load the tidyverse, which includes ggplot2\nlibrary(tidyverse)\n\n#Option 2: load ggplot2 directly\nlibrary(ggplot2)\n\n\nFANG=read_delim('FANG.txt')\n\n\n\n Download Raw data"
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#step-1-the-data",
    "href": "quant-fin/replications/L4/L4-Replication.html#step-1-the-data",
    "title": "Data Visualization",
    "section": "Step 1: the data",
    "text": "Step 1: the data\nFirst and foremost, in our call to ggplot, we need to make sure that it knows where the data is located. We will be using the FANG dataset, which contains basic stock information from popular U.S. techonology firms: Facebook (Meta), Amazon, Netflix, and Google (Alphabet). The first step in using ggplot2 is to call your data dataframe and supply the aesthetic mapping, which we’ll refer to as aes\n\nggplot(data=your_data, aes(x= variable_1, y=variable_2, ...))\n\n\nThe data argument refers to the dataset used\nThe aes argument contains all the aesthetic mappings that will be used\n\nTogether, these constitute the backbone of your visualization: they tell ggplot2 what the raw information to be used and where it should be mapped! For example, we can create another object, META, filtering for observations from FANG where symbol=='META' and chaining this the newly created dataset onto ggplot, mapping the date variable in the x axis, adjusted variable in the y axis, and symbol in the group aesthetic:\n\n#Read the data\nFANG=read_delim('FANG.txt')\n\n#Let's use Apple (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol))"
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#step-2-adding-your-geom",
    "href": "quant-fin/replications/L4/L4-Replication.html#step-2-adding-your-geom",
    "title": "Data Visualization",
    "section": "Step 2: adding your geom",
    "text": "Step 2: adding your geom\nYou probably thought you did something wrong when you saw an empty chart with the named axis, right? However, I can assure: you did great! It is all about the philosophy embedded in the Grammar of Graphics: you first provide the data and the aes(thetic) mapping to your data. Now, ggplot knows exactly which information to select and where to place it. However, it is still agnostic about how to display it. We will now add a geometry layer - in short, a geom:\n\n\n\n\n\n\nAdding layers on top of a ggplot object\n\n\n\n\nYou can add layers on top of ggplot object addition symbol (+)\nThere are many types of potential geometries, to name a few: geom_point(), geom_col(), geom_line() - click here for a complete list\n\n\n\nA layer combines data, aesthetic mapping, a geom (geometric object), a stat (statistical transformation), and a position adjustment. Typically, you will create layers using a geom_{} function, overriding the default position and stat if needed. With your ggplot call, use the + operator to add a geometry layer on top of the actual empty ggplot chart - in this case, we will be using the geom_line() geometry:\n\n#Use ggplot2 to map the aesthetics to the plot and add a geom_line()\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()"
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#step-3-be-creative-with-additional-layers",
    "href": "quant-fin/replications/L4/L4-Replication.html#step-3-be-creative-with-additional-layers",
    "title": "Data Visualization",
    "section": "Step 3: be creative with additional layers",
    "text": "Step 3: be creative with additional layers\nYour main chart is now all set: it contains the data and the necessary aes(thetic) mappings to the chart, and it also contains a shape, or geom(metry), that was selected to display the data. What’s next? The philosophy behind the Grammar of Graphics is now to add layers of information on top of the base chart using the + operator, like before.\nWe will proceed by including several layers of information that will either add or modify the behavior of the chart, making it more appealing to our audience:\n\nAdding trend lines using geom_smooth()\nAdding annotations and labels using annotation and labs\nModifying the behavior of the scales using scale_y and scale_x\n\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()+\n#Adding a trend\ngeom_smooth(method='loess')+\n#Adding Annotations\nlabs(title='META adjusted prices',\n     subtitle = 'Source: Yahoo! Finance',\n     x = 'Year',\n     y = 'Adjusted Prices')"
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#more-annotations",
    "href": "quant-fin/replications/L4/L4-Replication.html#more-annotations",
    "title": "Data Visualization",
    "section": "More annotations",
    "text": "More annotations\nApart from simply changing the labels of your axis, titles and subtitles, you can also use ggplot2 to customize the appearance of your axis: 1. The family of functions scale_x_{} apply a given structure to the x-axis - e.g, scale_x_date(),scale_x_continuous() 2. The family of functions scale_y_{} apply a given structure to the y-axis - e.g, scale_y_continuous() etc\nWith that, you can, for example:\n\nForce the x-axis to be formatted as a date, adjusting how it is being displayed\nForce the y-axis to be formatted in terms of dollar amounts\n\nIn this way, you can impose meaningful structures in your chart depending on the type of data you are considering in your mapping to x and y axis! Say, for example, that you want to format the x-axis to show breaks at the year level, and the y-axis in such a way that it goes from \\(\\small\\$0\\) to \\(\\small\\$1,000\\) by increments of \\(\\small\\$50\\). You can do so by adding the following syntax to your ggplot object:\n\n  #Your previous ggplot call up to now\n  {your_previous_ggplot} +\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar, breaks = seq(from=0,to=1000,by=50))\n\n\nClick here to see comprehensive list of all customizations that can be done across both x-axis and y-axis for continuous scales (scale_x_continuous() and scale_y_continuous())\nClick here to see comprehensive list of all customizations that can be done across both x-axis and y-axis for date scales (scale_x_date() and scale_y_date())\n\n\n\n\n\n\n\nFormatting scales\n\n\n\nTo properly format the appearance of your axis, make sure to have the scales package properly installed and loaded. You can do so by calling install.packages('scales') and library(scales).\n\n\n\n#Let's use Meta (META) adjusted prices\nMETA=FANG%&gt;%filter(symbol=='META')\n\n#Use ggplot2 to map the aesthetics to the plot\nggplot(META, aes(x=date,y=adjusted,group=symbol)) +\ngeom_line()+\n#Adding a trend\ngeom_smooth(method='loess')+\n#Adding Annotations\nlabs(title='META adjusted prices',\n     subtitle = 'Source: Yahoo! Finance',\n     x = 'Year',\n     y = 'Adjusted Prices')+\n#Changing the behavior of scales\nscale_x_date(date_breaks = '1 year',labels = year) +\nscale_y_continuous(labels = dollar, breaks = seq(from=0,to=1000,by=50))"
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#adding-multiple-data-points",
    "href": "quant-fin/replications/L4/L4-Replication.html#adding-multiple-data-points",
    "title": "Data Visualization",
    "section": "Adding multiple data points",
    "text": "Adding multiple data points\nWhat if we wanted to add more data? In our first example, we set filter(symbol)=='META' to select only information from Meta to your chart. However, one might be interested in understanding how did Meta perform relative to its FANG peers.m It is easy to do it with ggplot:\n\nBecause you have set group=symbol, ggplot already knows that it needs to group by each different string contained in the ticker column\nIn such a way, all you need to do is to add a new aes mapping, colour=symbol, so that ggplot knows that each symbol needs to have a different color!\n\nIn what follows, we will be charting all four FANG stocks in the same chart, adjusting the layers to try keeping aesthetics as good as possible.\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar, breaks = seq(0,1000,50))"
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#facet-it-until-you-make-it",
    "href": "quant-fin/replications/L4/L4-Replication.html#facet-it-until-you-make-it",
    "title": "Data Visualization",
    "section": "Facet it until you make it",
    "text": "Facet it until you make it\nWe have included all FANG stocks into the same chart. Easy peasy, lemon squeezy!. As far as we could go on adjusting the layers, it seems that the chart conveys too much information:\n\nBecause of the different scales, you can hardly tell the different between AMZN AND GOOG during 2015-2018\nFurthermore, trend lines are, in some cases, effectively hiding the data undernearth\n\nAlthough you could easily remove the trend lines, ggplot2 also comes with a variety of alternatives when it comes to charting multiple data that may come in handy:\n\nYou can facet your chart using facet_wrap, controlling the axis as well as the number of rows and columns\nYou can grid your chart, making the comparison easier with fixed axes\n\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting scales: free_y or fixed\n  facet_wrap(facets=symbol~.,ncol=2,nrow=2,scales='free_y')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting vertical orientation (.~symbol) and horizontal orientation (symbol~.)\n  facet_grid(rows=.~symbol,scales='fixed')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar, breaks = seq(0,1000,250))"
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#adding-themes-youre-in-full-control-of-your-message",
    "href": "quant-fin/replications/L4/L4-Replication.html#adding-themes-youre-in-full-control-of-your-message",
    "title": "Data Visualization",
    "section": "Adding themes: you’re in full control of your message!",
    "text": "Adding themes: you’re in full control of your message!\nBy now, you are already looking like a data manipulation wizard in your firm:\n\nYou have created a fully automated data ingestion process using tq_get() to get live FANG prices.\nSet up ggplot to automatically update the chart;\nFinally, you have adjusted all aesthetics to make it more much more professional\n\nA lot of the ggplot adoption throughout the R usiverse relates to themes: complete configurations which control all non-data display: first, there are a lot of available themes that you can pass to your ggplot, like theme_minimal(), theme_bw(). Alternatively, you can pass theme() if you just need to tweak the display of an existing theme.\nFor example, the code below adds theme_minimal(), a predefined theme that is loaded together with ggplot2, to further customize the appearance of the chart:\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting scales: free_y or fixed\n  facet_wrap(facets=symbol~.,ncol=2,nrow=2,scales='free_y')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar)+\n  #Using the theme_minimal() theme configuration that comes with ggplot2\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere are endless customizations that you could think of that could be applied to a theme. In special, the package ggthemes provides extra themes, geoms, and scales for ggplot2 that replicate the look of famous aesthetics that you have often looked and said: “how could I replicate that?”\nTo get access to these additional graphical resources in your R session, install and load the package using:\n\ninstall.packages('ggthemes') #Install if not available\nlibrary(ggthemes) #Load\n\nyour_previous_ggplot_object + theme_{insertyourtheme}\n\nTo check all available themes, check the ggthemes library here website. Below, you can find the same visualization using distinct themes coming from the ggthemes library:\n\nWSJThe EconomistExcelFiveThirtyEightGoogle Docs\n\n\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting scales: free_y or fixed\n  facet_wrap(facets=symbol~.,ncol=2,nrow=2,scales='free_y')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar)+\n  #Try out all available themes\n  theme_wsj()\n\n\n\n\n\n\n\n\n\n\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting scales: free_y or fixed\n  facet_wrap(facets=symbol~.,ncol=2,nrow=2,scales='free_y')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar)+\n  #Try out all available themes\n  theme_economist()\n\n\n\n\n\n\n\n\n\n\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting scales: free_y or fixed\n  facet_wrap(facets=symbol~.,ncol=2,nrow=2,scales='free_y')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar)+\n  #Try out all available themes\n  theme_excel()\n\n\n\n\n\n\n\n\n\n\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting scales: free_y or fixed\n  facet_wrap(facets=symbol~.,ncol=2,nrow=2,scales='free_y')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar)+\n  #Try out all available themes\n  theme_fivethirtyeight()\n\n\n\n\n\n\n\n\n\n\n\n#Make sure that date is read as a Date object\nFANG=FANG%&gt;%mutate(date=as.Date(date))\n\n#Use ggplot2 to map the aesthetics to the plot using the full FANG data\nggplot(FANG, aes(x=date,y=adjusted,group=symbol,colour=symbol)) +\n  #Basic layer - aesthetic mapping\n  geom_line()+\n  #Adding a trend\n  geom_smooth(method='loess')+\n  #Facet the data: try experimenting scales: free_y or fixed\n  facet_wrap(facets=symbol~.,ncol=2,nrow=2,scales='free_y')+\n  #Adding Annotations\n  labs(title='FANG adjusted prices',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Year',\n       y = 'Adjusted Prices')+\n  #Changing the behavior of scales\n  scale_x_date(date_breaks = '1 year',labels = year) +\n  scale_y_continuous(labels = dollar)+\n  #Try out all available themes\n  theme_gdocs()"
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#integrating-tidyquant",
    "href": "quant-fin/replications/L4/L4-Replication.html#integrating-tidyquant",
    "title": "Data Visualization",
    "section": "Integrating tidyquant",
    "text": "Integrating tidyquant\nLike in our previous lecture, the tidyquant added very important functionalities for those who work in finance to easily manage financial time series using the well-established foundations of the tidyverse. When it comes to data visualization, tidyquant also provides a handful of integrations that can be inserted into your ggplot call:\n\nPossibility of using geom_barchart and geom_candlestick\nMoving average visualizations and Bollinger Bands available using geom_ma and geom_bbands\nA new theme, theme_tq, available\n\nThe code below shows an example of how tidyquant objects can be chained on a ggplot call to generate meaningful visualization of financial time series. Say, for example, that you wanted to understand how each FANG stock behaved during the DeepSeek announcement. You could use the geom_candlestick() and geom_ma() functions with its appropriate arguments to a purely financial visualization:\n\n#Set up start and end dates\nend=Sys.Date()\nstart=end-weeks(5)\n\nFANG%&gt;%\n  #Make sure that date is read as a Date object\n  mutate(date=as.Date(date))%&gt;%\n  #Filter\n  filter(date &gt;= start, date&lt;=end)%&gt;%\n  #Basic layer - aesthetic mapping including fill\n  ggplot(aes(x=date,y=close,group=symbol))+\n  #Charting data - you could use geom_line(), geom_col(), geom_point(), and others\n  geom_candlestick(aes(open = open, high = high, low = low, close = close))+\n  geom_ma(ma_fun = SMA, n = 5, color = \"black\", size = 0.25)+\n  #Facetting\n  facet_wrap(symbol~.,scales='free_y')+\n  #DeepSeek date\n  geom_vline(xintercept=as.Date('2025-01-24'),linetype='dashed')+\n  #Annotations\n  labs(title='FANG adjusted prices before/after DeepSeek announcement',\n       subtitle = 'Source: Yahoo! Finance',\n       x = 'Date',\n       y = 'Adjusted Prices')+\n  #Scales\n  scale_x_date(date_breaks = '3 days') +\n  scale_y_continuous(labels = dollar) +\n  #Custom 'The Economist' theme\n  theme_economist()+\n  #Adding further customizations\n  theme(legend.position='none',\n        axis.title.y = element_text(vjust=+4,face='bold'),\n        axis.title.x = element_text(vjust=-3,face='bold'),\n        plot.subtitle = element_text(size=8,vjust=-2,hjust=0,margin = margin(b=15)),\n        axis.text.y = element_text(size=8),\n        axis.text.x = element_text(angle=90,size=8))\n\n\n\n\n\n\n\n\nFor a thorough discussion, see a detailed discussion on tidyquant’s charting capabilities here."
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#alternatives-to-ggplot2",
    "href": "quant-fin/replications/L4/L4-Replication.html#alternatives-to-ggplot2",
    "title": "Data Visualization",
    "section": "Alternatives to ggplot2",
    "text": "Alternatives to ggplot2\nThe ggplot2 package is, by and large, the richest and most widely used plotting ecosystem in the  language. However, there are also other interesting options, especially when it comes to interactive data visualization\n\nThe plotly ecosystem provides interactive charts for R, Python, Julia, Java, among others - you can install the R package using install.packages('plotly')\nThe Highcharts is another option whenever there is a need for interactive data visualization - you can install the R package using install.packages('highcharter')\n\nIn special, the highcharter package works seamlessly with time series data, especially those retrieved by the tidyquant’s tq_get() function.\n\n#Install the highcharter package (if not installed yet)\n#install.packages('highcharter')\n\n#Load the highcharter package (if not loaded yet)\nlibrary(highcharter)\n\n#Select the Google Stock with OHLC information and transform to an xts object\nGOOG=tq_get('GOOG')%&gt;%select(-symbol)%&gt;%as.xts()\n\n  #Initialize an empty highchart\n  highchart(type='stock')%&gt;%\n  #Add the Google Series\n  hc_add_series(GOOG,name='Google')%&gt;%\n  #Add title and subtitle\n  hc_title(text='A Dynamic Visualization of Google Stock Prices Over Time')%&gt;%\n  hc_subtitle(text='Source: Yahoo! Finance')%&gt;%\n  #Customize the tooltip\n  hc_tooltip(valueDecimals=2,valuePrefix='$')%&gt;%\n  #Convert it to a 'The Economist' theme\n  hc_add_theme(hc_theme_economist())"
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#hands-on-exercise",
    "href": "quant-fin/replications/L4/L4-Replication.html#hands-on-exercise",
    "title": "Data Visualization",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\n\nIn late January 2021, Reddit traders took on the short-sellers by forcing them to liquidate their short positions using GameStop stocks. This coordinated behavior had significant repercussions for various investment funds, such as Melvin Capital - see here and here\n\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse tq_get() to load information for GameStop (ticker: GME) and store it in a data.frame. Using the arguments from and to from tq_get(), filter for observations between occurring in between December 2020 (beginning of) and March 2021 (end of)\nUse ggplot(aes(x=date,group=symbol)), along with geom_candlestick() and its appropriate arguments, to chart the historical OHLC prices\nCreate a vertical line annotation using geom_vline, setting the xintercept argument to the date of the Reddit frenzy (as.Date('2021-01-25'))\nUse the theme from The Economist calling theme_economist(). Make sure to have the ggthemes package installed and loaded\nFinally, call theme() and labs() to adjust the aesthetics of your theme and labels as you think it would best convey your message. For example, you can use the scales package to format the appearance of your x and y labels (for example, displaying a dollar sign in front of adjusted prices)"
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#solution-walkthrough",
    "href": "quant-fin/replications/L4/L4-Replication.html#solution-walkthrough",
    "title": "Data Visualization",
    "section": "Solution walkthrough",
    "text": "Solution walkthrough\n\n#Libraries\nlibrary(tidyquant)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(scales)\n\n#Setting start/end dates + reddit date\nstart='2020-12-01'\nend='2021-03-31'\nreddit_date=as.Date('2021-01-25')\n\n#Get the data\ntq_get('GME',from=start,to=end)%&gt;%\n  #Mapping\n  ggplot(aes(x=date,group=symbol))+\n  #Geom\n  geom_candlestick(aes(open = open, high = high, low = low, close = close))+\n  #Labels\n  labs(x='',\n       y='Adjusted Prices',\n       title='GameStop (ticker: GME) prices during the reddit (Wall St. Bets) frenzy',\n       subtitle='Source: Yahoo! Finance')+\n  #Annotation\n  geom_vline(xintercept=reddit_date,linetype='dashed')+\n  annotate(geom='text',x=reddit_date-5,y=75,label='Reddit Frenzy Starts',angle=90)+\n  #Scales\n  scale_x_date(date_breaks = '2 weeks') +\n  scale_y_continuous(labels = dollar) +\n  #Custom 'The Economist' theme\n  theme_economist()+\n  #Adding further customizations\n  theme(legend.position='none',\n        axis.title.y = element_text(vjust=+4,face='bold'),\n        axis.title.x = element_text(vjust=-3,face='bold'),\n        plot.title = element_text(size=10),\n        plot.subtitle = element_text(size=8,vjust=-2,hjust=0,margin = margin(b=15)),\n        axis.text.y = element_text(size=8),\n        axis.text.x = element_text(angle=45,size=8,vjust=0.75))\n\n\n\n\n\n\n\n\nThis code visualizes GameStop (GME) stock prices during the Reddit (Wall Street Bets) frenzy in early 2021 using a candlestick chart. It retrieves stock data from Yahoo! Finance, applies ggplot2 for visualization, and customizes the plot using ggthemes.\n\nDefine Date Ranges. These dates specify the period over which GME stock data will be retrieved. More specifically, start and end are used to filter the tq_get() function, whereas reddit_date marks the key event when WallStreetBets (WSB) discussions fueled the GME rally, and will be used in the ggplot call to annotate the exact period where the frenzy happened.\nRetrieve Stock Data. The tq_get() fetches stock price data for GameStop (GME) from Yahoo! Finance. It returns a data frame with the following columns: date, open, high, low, close, adjusted, volume.\nCreate Candlestick Chart. While the ggplot(aes(x = date, group = symbol)) creates a basic ggplot chart, the geom_candlestick(aes(open = open, high = high, low = low, close = close)) maps the specific variables onto the OHLC information.\nAdd Labels. Using the labs() function, it is possible to customize several aspects of the chart, such as the x and y labels, as well as the title and subtitle.\nAnnotate the Reddit Frenzy date. The geom_vline() adds a dashed vertical line at reddit_date (Jan 25, 2021) to highlight the Reddit-driven rally. More specifically, the function places a text label “Reddit Frenzy Starts” near the line, x = reddit_date - 5 shifts text 5 days to the left for better visibility, y = 75 positions it at $75, and angle = 90 rotates the text vertically.\nCustomize Axes. Thescale_x_date(date_breaks = '2 weeks') ensures the x-axis shows date breaks every 2 weeks, while the scale_y_continuous(labels = dollar) formats y-axis values as dollar amounts.\nApply and customize predefined themes. The theme_economist() function applies a professional, clean theme from ggthemes, inspired by The Economist famous financial charts. On top of that, the theme() function edits various aspects of the predefined theme, such as bold titles, font sizes, and more, to better convey the message."
  },
  {
    "objectID": "quant-fin/replications/L4/L4-Replication.html#exploring-ggplot2-beyond-this-lecture",
    "href": "quant-fin/replications/L4/L4-Replication.html#exploring-ggplot2-beyond-this-lecture",
    "title": "Data Visualization",
    "section": "Exploring ggplot2 beyond this lecture",
    "text": "Exploring ggplot2 beyond this lecture\nThe ggplot2 package is an incredibly vast and flexible data visualization package. While this lecture covers the core concepts and essential functions, it is impossible to cover every aspect of ggplot2 in a single session. The package includes a wide range of geometric objects (geoms), themes, and customization options, along with an extensive ecosystem of extensions that add even more functionality.\nFor further exploration, students are encouraged to refer to the following resources:\n\nComplete List of Geoms (geometric objects): learn about all available geom functions, such as geom_violin(), geom_ribbon(), and more - click here\nThemes in ggplot2: explore built-in themes like theme_minimal(), theme_classic(), and specialized options such as theme_void() - click here\nTheme Customization: customize every visual element of a ggplot, including fonts, margins, grid lines, and legend positions - click here\nExtensions: discover additional packages that enhance ggplot2 with interactive features, advanced annotations, and more - click here\n\nBy exploring these resources, you can unlock the full potential of ggplot2 and create even more powerful and visually compelling data visualizations."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#outline",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#outline",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\nTidy Finance (Scheuch, Voigt, and Weiss 2023)\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\n\n\n\n\n\n\n\n\nCoding Replications\n\n\nFor coding replications, whenever applicable, please follow this page or hover on the specific slides with containing coding chunks.\n\nEnsure that you have your  session properly set-up according to the instructions outlined in the course webpage\nIn the webpage, you can also find a detailed discussion of the examples covered in this lecture"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#disclaimer",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#disclaimer",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\n\n\n\n\nDisclaimer\n\n\nThe information presented in this lecture is for educational and informational purposes only and should not be construed as investment advice. Nothing discussed constitutes a recommendation to buy, sell, or hold any financial instrument or security. Investment decisions should be made based on individual research and consultation with a qualified financial professional. The presenter assumes no responsibility for any financial decisions made based on this content.\nAll code used in this lecture is publicly available and is also shared on my GitHub page. Participants are encouraged to review, modify, and use the code for their own learning and research purposes. However, no guarantees are made regarding the accuracy, completeness, or suitability of the code for any specific application.\nFor any questions or concerns, please feel free to reach out via email at lucas.macoris@fgv.br"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#moving-beyond-capm",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#moving-beyond-capm",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Moving Beyond CAPM",
    "text": "Moving Beyond CAPM\n\nCAPM assumes a single risk factor (market risk)\nEmpirical evidence shows CAPM inadequacies\nAdditional factors can explain anomalies"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#market-anomalies-size-and-value",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#market-anomalies-size-and-value",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Market Anomalies: Size and Value",
    "text": "Market Anomalies: Size and Value\n\nSmall-cap stocks tend to outperform large-cap stocks (Size effect)\nValue stocks (high book-to-market) outperform growth stocks (Value effect)\nThese anomalies suggest additional risk factors"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#multi-factor-models-of-risk",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#multi-factor-models-of-risk",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Multi-Factor Models of Risk",
    "text": "Multi-Factor Models of Risk\n\nExtending CAPM to include more risk factors\nSystematic vs. idiosyncratic risk\nArbitrage Pricing Theory (APT) framework"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#the-factor-zoo",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#the-factor-zoo",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "The Factor Zoo",
    "text": "The Factor Zoo\n\nGrowing number of proposed factors in asset pricing literature\n(Cochrane 2011) refers to the proliferation as a “factor zoo”\nMany factors lack theoretical justification or robustness\n(Harvey, Liu, and Zhu 2016) highlight issues with data mining and p-hacking in factor discovery\nImportance of replication and out-of-sample validation in factor research\nExtending CAPM to include more risk factors\nSystematic vs. idiosyncratic risk\nArbitrage Pricing Theory (APT) framework\nRecent literature:\n\n(Hou, Xue, and Zhang 2015) propose the q-factor model, adding investment and profitability factors\n(Fama and French 2015) extend their model to five factors, incorporating profitability and investment\nRecent studies assess factor model performance across global markets"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#the-fama-french-model",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#the-fama-french-model",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "The Fama-French Model",
    "text": "The Fama-French Model\n\nThree factors: Market, Size, and Value\nEquation: \\[ R_i - R_f = \\alpha + \\beta_m (R_m - R_f) + \\beta_s SMB + \\beta_v HML + \\epsilon \\] where:\n\n\\(R_i\\): Stock return\n\\(R_f\\): Risk-free rate\n\\(R_m\\): Market return\n\\(SMB\\): Small-minus-big factor (Size)\n\\(HML\\): High-minus-low factor (Value)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#empirical-evidence",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#empirical-evidence",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Empirical Evidence",
    "text": "Empirical Evidence\n\n(Fama and French 1993) study\nHistorical data supports the three-factor model\nStill widely used in academia and practice\nRecent academic literature:\n\n(Novy-Marx 2013) identifies the profitability factor as a strong predictor of returns\n(Asness, Frazzini, and Pedersen 2013) discuss the role of quality and momentum in asset pricing\nRecent empirical studies validate factor investing strategies across asset classes\n\nFama & French (1993) study\nHistorical data supports the three-factor model\nStill widely used in academia and practice"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#practical-applications-hedge-fund-performance-evaluation",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#practical-applications-hedge-fund-performance-evaluation",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Practical Applications: Hedge Fund Performance Evaluation",
    "text": "Practical Applications: Hedge Fund Performance Evaluation\n\nOne of the most widely known examples of applications using the Fama-French model is to assess a hedge fund’s true skill in generating excess returns:\n\nMany hedge-fund managers claim to generate excess returns (\\(\\alpha\\))…\n…but after adjusting for Fama-French factors, true skill is revealed!\n\nInvestors can use this model to determine whether returns come from skill (true alpha) or simply exposure to common risk factors\n\nIf a fund shows positive \\(\\alpha\\) after accounting for the Fama-French factors, it suggests manager skill\nIf returns are fully explained by factors, performance comes solely from risk exposure, not skill\n\nFurthermore, investors can understand how exposed a given strategy is to the most common risk factors, and understand the determinants of the fund’s returns over time"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#practical-applications-mutual-funds",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#practical-applications-mutual-funds",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Practical Applications: Mutual Funds",
    "text": "Practical Applications: Mutual Funds\n\nMutual fund performance can also be analyzed using Fama-French\nHelps identify whether a fund’s returns are due to factor exposures or true alpha\nUsed in performance attribution analysis"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#implementing-fama-french-model-in-r-step-by-step",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#implementing-fama-french-model-in-r-step-by-step",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Implementing Fama-French Model in R: Step-by-Step",
    "text": "Implementing Fama-French Model in R: Step-by-Step\nStep 1: Load Required Libraries\nStep 2: Load Fama-French Dataset\nStep 3: Visualizing Factor Returns"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#interpreting-results",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#interpreting-results",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Interpreting Results",
    "text": "Interpreting Results"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#conclusion",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#conclusion",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Conclusion",
    "text": "Conclusion\n\nFama-French extends CAPM with size and value factors\nWidely used for asset pricing and performance evaluation\nPractical applications in hedge fund and mutual fund analysis"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#references",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#references",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nAsness, Clifford S., Andrea Frazzini, and Lasse Heje Pedersen. 2013. “Quality Minus Junk.” AQR Capital Management Working Paper.\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ.\n\n\nCarhart, Mark M. 1997. “On Persistence in Mutual Fund Performance.” The Journal of Finance 52 (1): 57–82. https://doi.org/10.1111/j.1540-6261.1997.tb03808.x.\n\n\nCochrane, John H. 2011. “Presidential Address: Discount Rates.” The Journal of Finance 66 (4): 1047–1108.\n\n\nFama, Eugene F., and Kenneth R. French. 1993. “Common Risk Factors in the Returns on Stocks and Bonds.” Journal of Financial Economics 33 (1): 3–56.\n\n\n———. 2015. “A Five-Factor Asset Pricing Model.” Journal of Financial Economics 116 (1): 1–22.\n\n\nHarvey, Campbell R., Yan Liu, and Heqing Zhu. 2016“...and the Cross-Section of Expected Returns.” The Review of Financial Studies 29 (1): 5–68.\n\n\nHou, Kewei, Chen Xue, and Lu Zhang. 2015. “Digesting Anomalies: An Investment Approach.” The Review of Financial Studies 28 (3): 650–705.\n\n\nNovy-Marx, Robert. 2013. “The Other Side of Value: The Gross Profitability Premium.” Journal of Financial Economics 108 (1): 1–28.\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#extensions",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#extensions",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Extensions",
    "text": "Extensions\n\nExtending CAPM to include more risk factors\nSystematic vs. idiosyncratic risk\nArbitrage Pricing Theory (APT) framework"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#capm-and-efficient-markets",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#capm-and-efficient-markets",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "CAPM and Efficient Markets",
    "text": "CAPM and Efficient Markets\n\nThe CAPM model relates to the Efficient Market Hypothesis (Fama 1970) by providing a framework to determine expected returns based on a single factor: the systematic risk\nHence, if the Efficient Market Hypothesis holds, CAPM should correctly price all assets:\n\nExpected returns depend only on beta, the sensitivity to systematic risk\nNo other risk factors should systematically predict returns\n\n\n\nQuestion: if the CAPM does not hold in practice, does that mean that markets are inefficient?\n\n\n\nIt is tempting to argue that if the CAPM fails to account for some market anomalies, it must be that the markets are not efficient as the model would predict\nNote, however, that this discussion is more nuanced than simply rejecting the Efficient Market Hypothesis altogether"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#capm-and-efficient-markets-continued",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#capm-and-efficient-markets-continued",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "CAPM and Efficient Markets, continued",
    "text": "CAPM and Efficient Markets, continued\n\nIt is tempting to argue that if the CAPM fails to account for some market anomalies, it must be that the markets are not efficient as the model would predict\nNote, however, that this discussion is more nuanced than simply rejecting the Efficient Market Hypothesis altogether\n\n\nAll in all, testing the CAPM is inherently a joint test of:\n\nMarket Efficiency - if the model is correctly specified, there should be no systematic \\(\\alpha&gt;0\\)\nModel Specification - there is an omitted factor other than the systematic risk that explains excess returns\n\nTherefore, if the CAPM fails empirical tests, it is unclear whether it is because I)markets are inefficient; or II) the model is simply misspecified\nAlternative models, such as the Fama-French Three-Factor Model (Fama and French 1993), attempt to address these limitations"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#the-dynamic-nature-of-beta",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#the-dynamic-nature-of-beta",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "The Dynamic Nature of \\(\\beta\\)",
    "text": "The Dynamic Nature of \\(\\beta\\)\n\nIn our previous example, \\(\\beta\\) was estimated using a simple OLS regression of asset excess returns on market excess returns\nOur estimate of \\(\\beta\\) was then used to build the required return for RYAM. Note, however, that the sensitivity of RYAM to systematic risk changes over time:\n\nShifts in business models (e.g., firms diversifying revenue streams)\nMacroeconomic conditions (e.g., monetary policy, recessions)\nMarket structure changes (e.g., sector rotations, liquidity shifts)\nLeverage variations (e.g., debt levels affecting risk exposure)\n\nIn what follows, we’ll look how the \\(\\beta\\) estimate for RYAM changes over time considering an estimation window of \\(24\\) weeks"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#the-dynamic-nature-of-beta-practice",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#the-dynamic-nature-of-beta-practice",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "The Dynamic Nature of \\(\\beta\\), practice",
    "text": "The Dynamic Nature of \\(\\beta\\), practice\n\nCodeOutput\n\n\n\n#Pipe the rolling regression object into ggplot\nrolling_regs%&gt;%\n  ggplot(aes(x=date,y=beta))+\n  geom_line()+\n  geom_smooth()+\n  #Annotations\n  labs(title='Rolling Beta regression (RYAM)',\n       subtitle = 'Source: Yahoo! Finance. Using the latest 24 observations of weekly returns.',\n       x = 'Week',\n       y = 'Estimated Beta')+\n  #Theme\n  theme_minimal()+\n  #Scale x\n  scale_x_date(date_breaks = '4 weeks')+  \n  #Adding further customizations\n  theme(legend.position='none',\n        axis.title.y = element_text(vjust=+4,face='bold'),\n        axis.title.x = element_text(vjust=-3,face='bold'),\n        axis.text = element_text(size=8))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#the-dynamic-nature-of-beta-practice-1",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#the-dynamic-nature-of-beta-practice-1",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "The Dynamic Nature of \\(\\beta\\), practice",
    "text": "The Dynamic Nature of \\(\\beta\\), practice\n\nCodeOutput\n\n\n\n#Pipe the rolling regression object into ggplot\nrolling_regs%&gt;%\n  ggplot(aes(x=date,y=beta))+\n  geom_line()+\n  geom_smooth()+\n  #Annotations\n  labs(title='Rolling Beta regression (DOW)',\n       subtitle = 'Source: Yahoo! Finance. Using the latest 12 observations of weekly returns.',\n       x = 'Week',\n       y = 'Estimated Beta')+\n  #Theme\n  theme_minimal()+\n  #Scale x\n  scale_x_date(date_breaks = '4 weeks')+  \n  #Adding further customizations\n  theme(legend.position='none',\n        axis.title.y = element_text(vjust=+4,face='bold'),\n        axis.title.x = element_text(vjust=-3,face='bold'),\n        axis.text = element_text(size=8))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#output-4",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#output-4",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Output",
    "text": "Output\n\n#Pipe the rolling regression object into ggplot\nrolling_regs%&gt;%\n  ggplot(aes(x=date,y=beta))+\n  geom_line()+\n  geom_smooth()+\n  #Annotations\n  labs(title='Rolling Beta regression (DOW)',\n       subtitle = 'Source: Yahoo! Finance. Using the latest 12 observations of weekly returns.',\n       x = 'Week',\n       y = 'Estimated Beta')+\n  #Theme\n  theme_minimal()+\n  #Scale x\n  scale_x_date(date_breaks = '4 weeks')+  \n  #Adding further customizations\n  theme(legend.position='none',\n        axis.title.y = element_text(vjust=+4,face='bold'),\n        axis.title.x = element_text(vjust=-3,face='bold'),\n        axis.text = element_text(size=8))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#rolling-betas",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#rolling-betas",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Rolling Betas",
    "text": "Rolling Betas"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#rolling-alphas",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#rolling-alphas",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Rolling Alphas",
    "text": "Rolling Alphas\n\nCodeOutput\n\n\n\nFull_Data%&gt;%\n  mutate(excess_return=weekly_return-Rf)%&gt;%\n  select(date,symbol,excess_return,MRP)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(\n      mutate_fun = rollapply,\n      by.column=FALSE,\n      width=24, #Use latest 24 weeks\n      FUN = custom_OLS,\n      col_rename = c('alpha','beta'))%&gt;%\n  filter(!is.na(alpha))%&gt;%\n  ggplot(aes(x=date,y=alpha))+\n  geom_line()+\n  geom_smooth()+\n  facet_wrap(symbol~.,nrow=2,ncol=5,scales='free_y')+\n  #Annotations\n  labs(title='Rolling Alpha Regression',\n       subtitle = 'Source: Yahoo! Finance. Using the latest 24 observations of weekly returns.',\n       x = '',\n       y = 'Estimated Alpha')+\n  #Theme\n  theme_minimal()+\n  #Scale x\n  scale_x_date(date_breaks = '4 weeks')+\n  scale_y_continuous(labels = percent)+\n  #Adding further customizations\n  theme(legend.position='none',\n        axis.title.y = element_text(face='bold',size=15),\n        axis.text.x = element_text(size=10,angle = 90),\n        axis.text.y = element_text(size=10),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15,face='bold'))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#the-dynamic-nature-of-beta-continued",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#the-dynamic-nature-of-beta-continued",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "The Dynamic Nature of \\(\\beta\\), continued",
    "text": "The Dynamic Nature of \\(\\beta\\), continued\n\nCodeOutput\n\n\n\n#Manipulate data\nRYAM=Full_Data%&gt;%\n  filter(symbol=='RYAM')%&gt;%\n  mutate(excess_return=weekly_return-Rf)%&gt;%\n  select(date,symbol,excess_return,MRP)\n\n#Create a custom OLS function that extracts all coefficients to pass it to tq_transmute \ncustom_OLS &lt;- function(data) {\n    coef(lm(excess_return ~ MRP, data = data))\n}\n\n#Apply the custom function to tq_transmute in rolling format\nrolling_regs=RYAM%&gt;%\n  tq_transmute(\n      mutate_fun = rollapply,\n      by.column=FALSE,\n      width=24, #Use latest 24 weeks\n      FUN = custom_OLS,\n      col_rename = c('alpha','beta'))%&gt;%\n  filter(!is.na(alpha))\n\n\n\n\n\n# A tibble: 30 × 4\n# Groups:   symbol [1]\n   symbol date         alpha     beta\n   &lt;chr&gt;  &lt;date&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1 RYAM   2024-06-14 0.0122   0.274  \n 2 RYAM   2024-06-21 0.0110   0.311  \n 3 RYAM   2024-06-28 0.0106   0.510  \n 4 RYAM   2024-07-05 0.00960  0.387  \n 5 RYAM   2024-07-12 0.00884  0.358  \n 6 RYAM   2024-07-19 0.0122   0.00874\n 7 RYAM   2024-07-26 0.0151  -0.186  \n 8 RYAM   2024-08-02 0.0117   0.254  \n 9 RYAM   2024-08-09 0.0222   0.334  \n10 RYAM   2024-08-16 0.0288   0.615  \n# ℹ 20 more rows"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-3-estimating-alpha-and-beta-ryam-only",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#step-3-estimating-alpha-and-beta-ryam-only",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Step 3: estimating \\(\\alpha\\) and \\(\\beta\\) (RYAM only)",
    "text": "Step 3: estimating \\(\\alpha\\) and \\(\\beta\\) (RYAM only)\n\nCodeOutput\n\n\n\n#Manipulate data\nRYAM=Full_Data%&gt;%\n  filter(symbol=='RYAM')%&gt;%\n  mutate(excess_return=weekly_return-Rf)%&gt;%\n  select(symbol,excess_return,MRP)\n\n#Run the OLS regression\nOLS=lm(excess_return~MRP,data=RYAM)\n\n#Inspect the results using summary()\nsummary(OLS)\n\n\n\n\n\n\nCall:\nlm(formula = excess_return ~ MRP, data = RYAM)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.153351 -0.036530 -0.005546  0.026486  0.185118 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 0.008144   0.009698   0.840   0.4050  \nMRP         1.237611   0.529959   2.335   0.0236 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06842 on 50 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.09835,   Adjusted R-squared:  0.08031 \nF-statistic: 5.454 on 1 and 50 DF,  p-value: 0.02358"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#size-effect",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#size-effect",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Size Effect",
    "text": "Size Effect\n\nIdea: small market capitalization stocks have historically earned higher average returns than the market portfolio, even after accounting for their higher betas\nA way to replicate this thesis is to split stocks each year into 10 portfolios by ranking them based on their market capitalizations:\n\nThe first portfolio had the 10% smallest stocks in terms of market capitalization\nThe second portfolio had the 20% smallest stocks; and so on, until…\nThe tenth portfolio had the 10% biggest stocks in terms of market capitalization\n\n\n\nCalculating the monthly excess returns and the beta of each decile portfolio, we see that:\n\nPortfolios with higher betas yield higher future returns (as expected)\nMost portfolios were above the security market (\\(\\small \\alpha&gt;0\\))\nThe smallest deciles - i.e, small-cap firms - exhibit the most extreme effect"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#book-to-market-ratio",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#book-to-market-ratio",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Book-to-Market Ratio",
    "text": "Book-to-Market Ratio\n\nAs with Size, a similar rationale could be applied to stocks that have higher levels of Market Value of Equity vis-a-vis their historical values (Book Value of Equity)\nIdea: small market capitalization stocks have historically earned higher average returns than the market portfolio, even after accounting for their higher betas\n\nHigh book-to-market stocks have historically earned higher average returns than low book-to-market stocks\nStocks with high book-to-market ratios are value stocks, and those with low book-to-market ratios are growth stocks\n\n\n\nCalculating the monthly excess returns and the beta of each decile portfolio, we see that:\n\nIn this case, value stocks - i.e, the stocks in the highest deciles - tend to present higher \\(\\alpha\\)\nAs such, a strategy that goes long on value stocks and short on growth stocks tends to present positive \\(\\alpha\\)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#momentum",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#momentum",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Momentum",
    "text": "Momentum\n\nDo past returns explain future performance?\nIdea: rank stocks each month by their realized returns over the prior 6-12 months\n\n\nCalculating the monthly excess returns and the beta of each decile portfolio, we see that:\n\nThey found that the best-performing stocks had positive alphas over the next 3-12 months\nAs an investor, you could buy stocks that have had past high returns and (short) sell stocks that have had past low returns\n\n\n\n\\(\\rightarrow\\) All in all, these three factors (Size, Book-to-Market, and Momentum) are widely known as the three Fama-French factors"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#what-if-alpha-is-consistently-different-from-zero",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#what-if-alpha-is-consistently-different-from-zero",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "What if \\(\\alpha\\) is consistently different from zero?",
    "text": "What if \\(\\alpha\\) is consistently different from zero?\n\nLet’s go back to our \\(\\alpha\\) definition for a given stock \\(i\\):\n\n\\[\n\\alpha_i = E[R_i] - R_i\n\\]\n\nAs we discussed, if you assume that CAPM is the correct model to explain expected returns, competition in financial markets should make \\(\\alpha \\rightarrow 0\\) in equilibrium:\n\nStocks above the SML are cheap, so the prices should rise (positive alpha).\nStocks below the SML are expensive, so the prices should drop (negative alpha).\n\nHowever, over the years since the discovery of the CAPM, it has become increasingly clear that forming portfolios based on market capitalization, book-to-market ratios, and past returns, investors can construct trading strategies that have a \\(\\small alpha&gt;0\\)\nWhy? There can be two reasons why positive-alpha strategies exist in a persistent way"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#why-alpha-is-consistently-different-from-zero",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#why-alpha-is-consistently-different-from-zero",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Why \\(\\alpha\\) is consistently different from zero?",
    "text": "Why \\(\\alpha\\) is consistently different from zero?\n\nReason #1: Investors are systematically ignoring positive-NPV investment opportunities:\n\nThe CAPM correctly computes required risk premiums, but investors are ignoring opportunities to earn extra returns without bearing any extra risk\nThat could happen either because they are unaware of them or because the costs to implement the strategies are larger than the NPV of undertaking them\n\n\n\n\\(\\rightarrow\\) This explanation goes straight to the hypotheses outlined by the CAPM!\n\nThe only way a positive-NPV opportunity can persist in a market is if some barrier to entry restricts competition. Nowadays, this hypothesis seems unlikely:\n\nInformation required to form the portfolios is readily available;\nTrading costs are decreasing"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#why-alpha-is-consistently-different-from-zero-continued",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#why-alpha-is-consistently-different-from-zero-continued",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Why \\(\\alpha\\) is consistently different from zero? Continued",
    "text": "Why \\(\\alpha\\) is consistently different from zero? Continued\nReason #2: The positive-alpha trading strategies contain risk that investors are unwilling to bear but the CAPM does not capture:\n\nA stock’s beta with the market portfolio does not adequately measure a stock’s systematic risk\nBecause of that, the CAPM does not correctly compute the risk premium as it leaves out important risk factors that investors care about other than the market sensitivity!\n\n\n\nIn other words, the positive alphas from the trading strategy are really returns for bearing risk that investors are averse to but the model does not capture:\n\n\nWe assumed that investor would always seek for the best risk \\(\\times\\) return combination\nHowever, investors may stick with inefficient portfolios because they care about risk characteristics other than the volatility of their traded portfolio. For instance, they prefer to not be exposed to the sector they work in or to specific industries (i.e., ESG-based decisions)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#potential-explanations",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#potential-explanations",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Potential explanations",
    "text": "Potential explanations\n\nSome reasons of why positive-alpha strategies can persist can be inherently tied to the assumptions tied out to the CAPM definition:\n\nProxy error: we might be not using a good proxy for the market portfolio\nBehavioral biases: we have made assumptions on investor behavior, but it might be the case that non-sophisticated investors find hard do approximate their portfolio to the market portfolio\nAlternative Risk Preferences and Non-Tradable Wealth:: we assumed that investor would always seek for the best risk \\(\\times\\) return combination. However, investors may stick with inefficient portfolios because they care about risk characteristics other than the volatility of their traded portfolio. For instance, they prefer to not be exposed to the sector they work in or to specific industries (i.e., ESG-based decisions)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#multifactor-models-of-risk-1",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#multifactor-models-of-risk-1",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Multifactor Models of Risk",
    "text": "Multifactor Models of Risk\n\nWhen we introduced the CAPM, we implicitly assumed that there was a single portfolio (or “factor”) that represented the efficient portfolio: the market (a “single factor” portfolio)\nHowever, it is not actually necessary to identify the efficient portfolio itself, as long as you identify a collection of portfolios from which the efficient portfolio can be constructed\nA Multi-Factor Model is a pricing model that uses more than one portfolio (“factors”) to approximate the efficient portfolio:\n\n\n\\[\n\\small E[R_i] = R_f + \\beta_i^{\\text{F1}} \\times \\underbrace{(E[R_{\\text{F1}} - R_f])}_{\\text{Excess return for Factor 1}}+ \\beta_i^{\\text{F2}} \\times \\underbrace{(E[R_{\\text{F2}} - R_f])}_{\\text{Excess return for Factor 2}}+...+\\beta_i^{\\text{Fn}} \\times \\underbrace{(E[R_{\\text{Fn}} - R_f])}_{\\text{Excess return for Factor n}}\n\\]\n\nEach \\(\\beta_i^{n}\\) here is called a factor beta: like the CAPM, it is the expected % change in the excess return of a security for a 1% change in the excess return of that factor portfolio, holding everything else constant"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#multifactor-models-of-risk-2",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#multifactor-models-of-risk-2",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Multifactor Models of Risk",
    "text": "Multifactor Models of Risk\n\nWhen we introduced the CAPM, we implicitly assumed that there was a single portfolio (or “factor”) that represented the efficient portfolio: the market (a “single factor” portfolio)\nHowever, it is not actually necessary to identify the efficient portfolio itself, as long as you identify a collection of portfolios from which the efficient portfolio can be constructed\nA Multi-Factor Model is a pricing model that uses more than one portfolio (“factors”) to approximate the efficient portfolio:\n\n\n\\[\n\\small E[R_i] = R_f + \\beta_i^{\\text{F1}} \\times \\underbrace{(E[R_{\\text{F1}} - R_f])}_{\\text{Excess return for Factor 1}}+ \\beta_i^{\\text{F2}} \\times \\underbrace{(E[R_{\\text{F2}} - R_f])}_{\\text{Excess return for Factor 2}}+...+\\beta_i^{\\text{Fn}} \\times \\underbrace{(E[R_{\\text{Fn}} - R_f])}_{\\text{Excess return for Factor n}}\n\\]\n\nEach \\(\\beta_i^{n}\\) here is called a factor beta: like the CAPM, it is the expected % change in the excess return of a security for a 1% change in the excess return of that factor portfolio, holding everything else constant"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#multifactor-models-of-risk-continued",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#multifactor-models-of-risk-continued",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Multifactor Models of Risk, continued",
    "text": "Multifactor Models of Risk, continued\n\nThe previous equation showed that that we can write the risk premium of any marketable security as the sum of the risk premium of each factor multiplied by the sensitivity of the stock with that factor:\n\nSingle-factor: We use an presumably efficient portfolio, it will alone capture all systematic risk (for example, the CAPM)\nMultifactor: If we use multiple portfolios as factors, then together these factors will capture all systematic risk - this is also known as the Arbitrage Pricing Theory (APT)\n\nMultifactor models allow investors to break the risk premium down into different factors:\n\nAs they might not be equally averse to the different factors, multifactor models allows investors to tailor their risk exposure\nThis idea of tailoring risk exposures based on common risk factors has become increasingly known amongst practitioners as a smart beta strategy - click here for an extensive list of factor ETFs from Fidelity"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#which-factors-portfolios-to-use",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#which-factors-portfolios-to-use",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Which factors (portfolios) to use?",
    "text": "Which factors (portfolios) to use?\n\nIf investors can tailor their risk exposure to specific risk factors, then the next question is: which risk factors an investor should be exposed to?\nSome important risk factors found in the previous literature include, but not limited to:\n\nMarket Strategy: the most straightforward example is to expose to the market itself, like the CAPM did. Even if the market portfolio is not efficient, it still captures many components of systematic risk\nMarket Capitalization Strategy: a trading strategy that each year buys portfolio S (small stocks) and finances this position by short selling portfolio B (big stocks) has produced positive risk-adjusted returns historically. This is called a small-minus-big (SMB) portfolio\nBook-to-Market Strategy: a trading strategy that each year buys a portfolio of growth stocks and finances it by selling value stocks. This is called a high-minus-low (HML) portfolio"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#example-fama-french-carhart-ffc",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#example-fama-french-carhart-ffc",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Example: Fama-French-Carhart (FFC):",
    "text": "Example: Fama-French-Carhart (FFC):\n\nA direct application of the previous slide is the Fama-French-Carhart (FFC) portfolio, which aggregates all the risk factors discussed before:\n\n\n\\[\\small E[R_i] = R_f + \\beta_s^m \\times \\underbrace{(E[R_m]− R_f)}_{\\text{Market}}  + \\beta_s^{SMB} \\times \\underbrace{E[R_{SMB}]}_{\\text{Size}} + \\beta_s^{HML} \\times \\underbrace{E[R_{HML}]}_{\\text{Market Cap.}} + \\beta_s^{Mon} \\times \\underbrace{E[R_{Mom}]}_{\\text{Past Returns}} \\]\n\nNote that we can price the required returns for a given security \\(i\\) according to its exposure (the \\(\\beta\\)’s) to each of the factor portfolios\nBefore, we claimed using the CAPM that only the first factor should drive required returns (i.e., the market)\nNow, our measure for the efficient portfolio is to say that investors also care about other risk factors, and because of that, the exposure of a given security needs to take that into account when estimating the required returns!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#fama-french-carhart-portfolio-returns",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#fama-french-carhart-portfolio-returns",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Fama-French-Carhart portfolio returns",
    "text": "Fama-French-Carhart portfolio returns\n\n\n\n\n\n\nThese are the returns for investing in each portfolio\nIf the investment that you are trying to estimate the required returns is exposed to these portfolios, we then use these estimates with the estimates \\(\\beta\\)’s to get to the expected return"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#multifactor-models-of-risk-example",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#multifactor-models-of-risk-example",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Multifactor Models of Risk, example",
    "text": "Multifactor Models of Risk, example\n\nYou are considering making an investment in a project in the fast food industry. You determine that the project has the same level of non-diversifiable risk as investing in McDonald’s stock. Determine the cost of capital by using the FFC factor specification assuming a monthly risk-free rate of 0.20% and the factor returns from the previous slide.\n\n\n\n\n\nFactor\nBeta\n\n\n\n\nMarket\n0.72\n\n\nSMB\n-0.6\n\n\nHML\n0.14\n\n\nPR1YR\n0.09"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#multifactor-models-of-risk-example-1",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#multifactor-models-of-risk-example-1",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Multifactor Models of Risk, example",
    "text": "Multifactor Models of Risk, example\n\nUsing the FFC specification:\n\n\n\\[\n\\small \\underbrace{0.2}_{\\text{Risk-free}}\\%+\\underbrace{(0.68\\%-0.2\\%)}_{\\text{Market Excess Return}}\\times{0.72}+\\underbrace{(0.2\\%)}_{SMB}\\times{-0.6}+\\underbrace{(0.35\\%)}_{HML}\\times{0.14}+\\underbrace{(0.64\\%)}_{PR1YR}\\times{0.09}=0.68\\%\n\\]\n\nIn annual terms (no compoundind), this is approximately \\(\\small 0.68\\% \\times 12=8.16\\%\\)\nAs a comparison, a standard CAPM regression over the same time period leads to an estimated market beta of \\(\\small 0.58\\) for McDonald’s —the market \\(\\beta\\) differs from the estimate of 0.72 above because we are using only a single factor in the CAPM regression\nUsing the CAPM would have given us an estimated annual required return of \\(\\small (0.2\\%+0.58\\times0.68\\%)\\times12=7.1\\%\\)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#multifactor-models-of-risk",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#multifactor-models-of-risk",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Multifactor Models of Risk",
    "text": "Multifactor Models of Risk\n\nWhen we first introduced the CAPM, we implicitly assumed that there was a single portfolio (or “factor”) that represented the efficient portfolio: the market (a “single factor” portfolio)\nHowever, it is not actually necessary to identify the efficient portfolio itself, as long as you identify a collection of portfolios from which the efficient portfolio can be constructed\nA Multi-Factor Model is a pricing model that uses more than one portfolio (“factors”) to approximate the efficient portfolio:\n\n\n\\[\n\\small E[R_i] = R_f + \\beta_i^{\\text{F1}} \\times \\underbrace{(E[R_{\\text{F1}} - R_f])}_{\\text{Excess return for Factor 1}}+ \\beta_i^{\\text{F2}} \\times \\underbrace{(E[R_{\\text{F2}} - R_f])}_{\\text{Excess return for Factor 2}}+...+\\beta_i^{\\text{Fn}} \\times \\underbrace{(E[R_{\\text{Fn}} - R_f])}_{\\text{Excess return for Factor n}}\n\\]\n\nEach \\(\\beta_i^{n}\\) here is called a factor beta: like the CAPM, it is the expected % change in the excess return of a security for a 1% change in the excess return of that factor portfolio, holding everything else constant"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#example-the-fama-french-model",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#example-the-fama-french-model",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Example: the Fama-French model",
    "text": "Example: the Fama-French model\n\nA direct application of the previous slide is the Fama-French portfolio (Fama and French 1993), which considers the Market, the Market Capitalization Strategy (Size), and the Book-to-Market (Value) strategy:\n\n\\[\\small E[R_i] = R_f + \\beta_s^m \\times \\underbrace{(E[R_m]− R_f)}_{\\text{Market}}  + \\beta_s^{SMB} \\times \\underbrace{E[R_{SMB}]}_{\\text{Size}} + \\beta_s^{HML} \\times \\underbrace{E[R_{HML}]}_{\\text{Market Cap.}}\\]\n\nNote that we can price the required returns for a given security \\(i\\) according to its exposure (the \\(\\beta\\)’s) to each of the factor portfolios\nBefore, we claimed using the CAPM that only the first factor should drive required returns (i.e., the market)\nIf investors also care about other risk factors, the exposure of a given security needs to take that into account when estimating the required returns!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#extensions-to-risk-factors---the-factor-zoo",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#extensions-to-risk-factors---the-factor-zoo",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Extensions to risk factors - the Factor Zoo",
    "text": "Extensions to risk factors - the Factor Zoo\n\nAll in all, what is the gain in performance when using the three-factor Fama-French model relative to the CAPM?\n\nWhen applied to historical data, results support the three-factor model proposed in (Fama and French 1993)\nFama-French explains over \\(\\small90\\%\\) of the diversified portfolios returns, compared with the average \\(\\small70\\%\\) given by the CAPM (in-sample performance)\n\nImportantly, there is a growing number of proposed factors in the Asset Pricing literature - a non-exhaustive list includes:\n\n(Carhart 1997) extend the Fama-French 3-factor model by adding a momentum factor\n(Fama and French 2015) extend the Fama-French 3-factor model by incorporating profitability and investment\n(Asness, Frazzini, and Pedersen 2013) discuss the role of quality in asset returns"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#extensions-to-risk-factors---the-factor-zoo-continued",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#extensions-to-risk-factors---the-factor-zoo-continued",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Extensions to risk factors - the Factor Zoo, continued",
    "text": "Extensions to risk factors - the Factor Zoo, continued\n\nAll in all, there is a growing number of risk factors that have been documented in the Asset Pricing literature. Such proliferation of risk factors in the literature has been widely known as the “Factor Zoo” (Cochrane 2011)\nMany factors lack theoretical justification or robustness, highlighting the role of replication and out-of-sample validation in factor research:\n\n(Novy-Marx 2013) show that controlling for gross profitability explains most market anomalies and a wide range of seemingly unrelated profitable trading strategies\n(Hou, Xue, and Zhang 2015) shows that a model using market, size, investment, and profitability factors argely summarizes the cross section of average stock returns. A comprehensive examination of nearly 80 anomalies reveals that about one-half of the anomalies are insignificant in the broad cross section\n(Harvey, Liu, and Zhu 2016) highlight issues with data mining and p-hacking"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#moving-beyond-the-capm",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#moving-beyond-the-capm",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Moving Beyond the CAPM",
    "text": "Moving Beyond the CAPM\n\nWe previously defined that the required return for any given security \\(i\\) should follow:\n\n\\[\nE[R_i] = R_f + \\beta_i^P \\times (E[R_P - R_f])\n\\]\n\nAt first, we were agnostic on what \\(P\\), the portfolio returns, should stand for\nWhen we introduced the CAPM, we claimed that \\(P=M\\) - i.e, the efficient portfolio is the market portfolio\n\n\nHowever, real-world frictions points us to an uncomfortable outcome:\n\nThe CAPM assumes a single risk factor (market risk), but…\nThere is empirical evidence that shows additional factors can explain anomalies\n\n\n\\(\\rightarrow\\) When the market portfolio is not efficient, we have to find a method to identify an efficient portfolio before we can use the above equation!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#application-hedge-fund-performance-evaluation",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#application-hedge-fund-performance-evaluation",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Application: Hedge Fund Performance Evaluation",
    "text": "Application: Hedge Fund Performance Evaluation\n\nOne of the most widely known examples of applications using the Fama-French model is to assess a hedge fund’s true skill in generating excess returns:\n\nMany hedge-fund managers claim to generate excess returns (\\(\\alpha\\))…\n…but after adjusting for Fama-French factors, true skill is revealed!\n\nInvestors can use this model to determine whether returns come from skill (true alpha) or simply exposure to common risk factors\n\nIf a fund shows positive \\(\\alpha\\) after accounting for the Fama-French factors, it suggests manager skill\nIf returns are fully explained by factors, performance comes from risk exposure, not manager’s skill\n\nFurthermore, investors can understand how exposed a given strategy is to the most common risk factors, and understand the determinants of the fund’s returns over time"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#hands-on-exercise",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#hands-on-exercise",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\nYou work as a quantitative analyst at Axe Capital. You have been given the task of analyzing a couple of Hedge Fund strategies and assess whether they have generated true excess returns that could have been attributed to their manager’s skill:\n\nWhat is the historical performance of each strategy over time?\nWhich strategies, according to the CAPM model, have generated \\(\\alpha&gt;0\\)?\nWhich strategies, according to the Fama-French model, have generated \\(\\alpha&gt;0\\)?\n\n\n\n\n\n\n\n\nSpecific Instructions\n\n\n\nYou will be using some of the data contained in the edhec dataset - click here for a detailed explanation on the dataset.\nModel estimation should be done at the monthly level"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#about-the-dataset",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#about-the-dataset",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "About the dataset",
    "text": "About the dataset\n\nThe edhec dataset, from the EDHEC Risk and Asset Management Research Center, is a dataset that covers monthly Hedge Fund returns starting from 1997\nEach series of returns represents a Hedge Fund strategy that seeks to exploit a given type of market anomaly:\n\nConvertible Arbitrage - click here for details\nEvent Driven - click here for details\nMerger Arbitrage - click here for details\nRelative Value - click here for details\nDistressed Securities - click here for details\nCTA Global - click here for details"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-1-loading-the-data",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-1-loading-the-data",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 1: Loading the data",
    "text": "Step 1: Loading the data\n\nThe first step is to load the data on historical returns on hedge fund strategies. For that, the edhec dataset - provided in the PerformanceAnalytics package, contains the historical monthly returns for a handful of alternative global strategies\nI have already prepped the data for you in an .rds file that can be downloaded using the Download Data button below or directly through eClass®. An .rds file is an R object that can be loaded directly into your R session\nTo load an .rds file, you can either double-click and open using RStudio, or run the following command:\n\n\n#Assuming that the file is in the correct folder\nhf_data=readRDS('hf_data.rds')\n\n\nNote that the object that has been loaded is an xts object, which inherits several useful properties for working with time series data!\n\n\n\n Download Raw data"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-2-assessing-historical-performance",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-2-assessing-historical-performance",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 2: Assessing historical performance",
    "text": "Step 2: Assessing historical performance"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-analyzing-the-determinants-of-historical-performance",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-analyzing-the-determinants-of-historical-performance",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 3: Analyzing the determinants of historical performance",
    "text": "Step 3: Analyzing the determinants of historical performance"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-the-capm-model",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-the-capm-model",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 3: The CAPM Model",
    "text": "Step 3: The CAPM Model\n\nWe will start by analyzing of a given strategy \\(i\\) can be done estimating a CAPM model\nIn this model, the only risk factor is the market:\n\n\\[\n(R_{i,t}-R_{f,t})=\\alpha + \\beta \\times (R_{m,t}-R_{f,t})+\\varepsilon_{i,t}\n\\] where:\n\n\\(R_{i,t}\\) is the return of a given manager \\(i\\) in month \\(t\\)\n\\(R_{m,t}\\) is the market return in month \\(t\\)\n\\(R_{f,t}\\) is the risk-free return in month \\(t\\)\n\n\nIn what follows, we will be estimating and interpreting the CAPM results for each strategy"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-the-capm-model-1",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-the-capm-model-1",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 3: The CAPM Model",
    "text": "Step 3: The CAPM Model"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-fama-french-model",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-fama-french-model",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 4: The Fama-French Model",
    "text": "Step 4: The Fama-French Model\n\nUsing the CAPM as our model for expected returns, 5 out of 6 strategies did deliver positive alphas in the study period\nWe can now extend the same rationale to adopt the Fama-French three-factor model, which considers the Market, the Market Capitalization Strategy (Size), and the Book-to-Market (Value) strategy:\n\n\\[\n\\small E[R_i] = R_f + \\beta_s^m \\times \\underbrace{(E[R_m]− R_f)}_{\\text{Market}}  + \\beta_s^{SMB} \\times \\underbrace{E[R_{SMB}]}_{\\text{Size}} + \\beta_s^{HML} \\times \\underbrace{E[R_{HML}]}_{\\text{Market Cap.}}\n\\] - As before, we will start analyzing the output for one individual strategy individually and then apply the same rationale to replicate the result across all strategies"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-fama-french-model-1",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-fama-french-model-1",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 4: The Fama-French Model",
    "text": "Step 4: The Fama-French Model"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#discussion",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#discussion",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-analyzing-the-determinants-of-historical-performance-1",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-analyzing-the-determinants-of-historical-performance-1",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 3: Analyzing the determinants of historical performance",
    "text": "Step 3: Analyzing the determinants of historical performance"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-analyzing-performance-determinants",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-analyzing-performance-determinants",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 3: Analyzing performance determinants",
    "text": "Step 3: Analyzing performance determinants\n\nAssessing the performance of a given strategy \\(i\\) can be done by estimating:\n\n\\[\nR_{i,t}=\\alpha + \\sum\\beta_z R_{z,t}+\\varepsilon_{i,t}\n\\] where:\n\n\\(R_{i,t}\\) is the return of a given manager \\(i\\) in month \\(t\\)\n\\(R_{i,t}\\) is the return of a given factor portfolio \\(z\\) in month \\(t\\)\n\n\nAn OLS regression as described above yields two important components:\n\nThe estimated \\(\\beta_z\\)’s are the factor returns, and represent exposure of \\(i\\) to a given risk factor\nThe estimated \\(\\alpha\\) is the return that could not be attributed to any other risk factor, and it is thus understood as the manager’s true skill return"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-2-historical-performance",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-2-historical-performance",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 2: Historical Performance",
    "text": "Step 2: Historical Performance\n\nIt is very easy to work with time series using the base R capabilities. For example, you can pass call cumprod(1+x) in your dataset, and R understands that you want to do these operations column-wise\nAlternatively, you can use the steps from the previous lectures to get the data into a proper format for using ggplot2\n\n\nCodeOutput\n\n\n\n#Apply the cumprod function to all columns\ncumprod(1+hf_data)-1%&gt;%head()\n\n\n\n\n\n           Convertible.Arbitrage CTA.Global Distressed.Securities\n1997-01-01            0.01190000 0.03930000            0.01780000\n1997-02-01            0.02434637 0.07027114            0.03021716\n1997-03-01            0.03233627 0.06802357            0.02898090\n1997-04-01            0.04121436 0.04986717            0.03206784\n1997-05-01            0.05745731 0.04829237            0.05611502\n1997-06-01            0.07987540 0.05720285            0.07903272\n1997-07-01            0.10071700 0.11968354            0.10428208\n1997-08-01            0.11546661 0.06672251            0.12051503\n1997-09-01            0.12907530 0.08784362            0.15973306\n1997-10-01            0.14036605 0.07718275            0.15231077\n       ...                                                       \n2020-08-01            3.68582245 1.94567988            4.80813414\n2020-09-01            3.71065731 1.88941739            4.84821027\n2020-10-01            3.74127659 1.87872655            4.83300492\n2020-11-01            3.87972186 1.91471063            5.08149093\n2020-12-01            3.98561183 2.04645555            5.26819270\n2021-01-01            4.10875644 2.03670689            5.41800251\n2021-02-01            4.20531194 2.13236316            5.62081139\n2021-03-01            4.18032644 2.14645879            5.72873061\n2021-04-01            4.17980841 2.22512026            5.87407120\n2021-05-01            4.20881533 2.27801223            5.98955559\n           Emerging.Markets Equity.Market.Neutral Event.Driven\n1997-01-01        0.0791000            0.01890000   0.02130000\n1997-02-01        0.1357527            0.02919089   0.02987892\n1997-03-01        0.1221237            0.03083760   0.02751020\n1997-04-01        0.1354770            0.04310456   0.02699644\n1997-05-01        0.1712445            0.06281924   0.06253052\n1997-06-01        0.2392938            0.08035576   0.08994381\n1997-07-01        0.3086943            0.10704054   0.12340508\n1997-08-01        0.3000569            0.10892251   0.13138126\n1997-09-01        0.3298282            0.13132275   0.16860370\n1997-10-01        0.2537620            0.14207031   0.17573218\n       ...                                                    \n2020-08-01        4.2056980            2.33237019   4.34054821\n2020-09-01        4.1296948            2.32570545   4.34161632\n2020-10-01        4.1163576            2.31073977   4.35443620\n2020-11-01        4.4069667            2.32166521   4.71104165\n2020-12-01        4.6524430            2.37149019   4.96860963\n2021-01-01        4.7683181            2.37250164   5.06888227\n2021-02-01        4.8617648            2.41937941   5.29464469\n2021-03-01        4.8142846            2.45425708   5.39535901\n2021-04-01        4.9637117            2.50503466   5.57187092\n2021-05-01        5.0883532            2.51730228   5.65401930"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-2-historical-performance-1",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-2-historical-performance-1",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 2: Historical Performance",
    "text": "Step 2: Historical Performance\n\nCodeOutput\n\n\n\n(cumprod(1+hf_data)-1)%&gt;%\n  as.data.frame()%&gt;%\n  rownames_to_column('date')%&gt;%\n  mutate(date=as.Date(date))%&gt;%\n  pivot_longer(names_to='strategy',values_to = 'cum_return',cols=2:6)%&gt;%\n  ggplot(aes(x=date,y=cum_return,group=strategy,col=strategy))+\n  geom_line()+\n  scale_x_date(date_breaks = 'years',date_labels='%Y')+\n  scale_y_continuous(labels = percent)+\n  labs(title='Comparison of hedge fund global strategies over time',\n       subtitle='Considering EDHEC dataset of monthly hedge fund returns.',\n       col='Strategy',\n       x='',\n       y='Cumulative Return')+\n  theme_minimal()+\n  theme(legend.position = 'bottom',\n        axis.text.x = element_text(angle=90),\n        axis.title = element_text(face='bold',size=12),\n        plot.title = element_text(face='bold',size=15),\n        plot.subtitle  = element_text(size=12))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-analyzing-performance-determinants-1",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-analyzing-performance-determinants-1",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 3: Analyzing performance determinants",
    "text": "Step 3: Analyzing performance determinants\n\nOur hf_data contains information regarding all Hedge Fund Returns. Where to find information regarding factor portfolio returns?\n\nLuckily, you don’t need to manually create them - there are a couple ways by which you can download historical data on factor portfolio returns\nIn Kenneth French’s website, you will find historical data on factor portfolio returns for a variety of economies including the U.S., Developed Economies, Emerging Markets, and Global\nIn special, we will be collect monthly factor portfolio returns regarding the three-factor model as in (Fama and French 1993)\n\n\n\n#This is the URl where the Factor Portfolio Returns are stored (US)\nFF_url &lt;- \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors.CSV\"\n\n#Download the file, adjust names and Date, and input missing information\nFF_data &lt;- read.csv(FF_url, skip = 3) %&gt;%\n  setNames(c('date','mkt_minus_rf','SMB','HML','Rf'))%&gt;%\n  mutate(date = ymd(paste0(substr(date, 1, 4), \"-\", substr(date, 5, 6), \"-01\"))) %&gt;%\n  na.locf()"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-analyzing-performance-determinants-continued",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-analyzing-performance-determinants-continued",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 3: Analyzing performance determinants, continued",
    "text": "Step 3: Analyzing performance determinants, continued\n\nOur hf_data contains information regarding all Hedge Fund Returns. Where to find information regarding factor portfolio returns?\n\nLuckily, you don’t need to manually create them - there are a couple ways by which you can download historical data on factor portfolio returns\nIn Kenneth French’s website, you will find historical factor portfolio returns for a variety of economies including the U.S., Developed Economies, Emerging Markets, and Global\nIn special, we will using monthly factor portfolio returns regarding the three-factor model as in (Fama and French 1993), which you can download below or direclty through eClass®\n\n\n\n\n Download Raw data"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-analyzing-performance-determinants-continued-1",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-analyzing-performance-determinants-continued-1",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 3: Analyzing performance determinants, continued",
    "text": "Step 3: Analyzing performance determinants, continued\n\nWe will now manipulate the data in order to merge both hf_data and the ff3_data\n\n\nCodeOutput\n\n\n\n#Convert the hf_data to a data.frame object and adjust columns\n hf_data = hf_data%&gt;%\n  as.data.frame()%&gt;%\n  rownames_to_column('date')%&gt;%\n  mutate(date=as.Date(date))\n\n#Merge both datasets by date\nmerged_df &lt;- hf_data%&gt;%\n  #Merge\n  left_join(ff3_data, by = \"date\")%&gt;%\n  #Pivot the data for each strategy\n  pivot_longer(cols = -c(date, MKT_MINUS_RF, SMB, HML, RF), names_to = \"strategy\", values_to = \"return\") %&gt;%\n  mutate(excess_return = return - RF)\n\n\n\n\n\n# A tibble: 1,908 × 8\n   date       MKT_MINUS_RF     SMB     HML     RF strategy  return excess_return\n   &lt;date&gt;            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n 1 1997-01-01       0.0499 -0.0195 -0.0142 0.0045 Converti… 0.0119        0.0074\n 2 1997-01-01       0.0499 -0.0195 -0.0142 0.0045 CTA Glob… 0.0393        0.0348\n 3 1997-01-01       0.0499 -0.0195 -0.0142 0.0045 Distress… 0.0178        0.0133\n 4 1997-01-01       0.0499 -0.0195 -0.0142 0.0045 Emerging… 0.0791        0.0746\n 5 1997-01-01       0.0499 -0.0195 -0.0142 0.0045 Equity M… 0.0189        0.0144\n 6 1997-01-01       0.0499 -0.0195 -0.0142 0.0045 Event Dr… 0.0213        0.0168\n 7 1997-02-01      -0.0049 -0.0322  0.0567 0.0039 Converti… 0.0123        0.0084\n 8 1997-02-01      -0.0049 -0.0322  0.0567 0.0039 CTA Glob… 0.0298        0.0259\n 9 1997-02-01      -0.0049 -0.0322  0.0567 0.0039 Distress… 0.0122        0.0083\n10 1997-02-01      -0.0049 -0.0322  0.0567 0.0039 Emerging… 0.0525        0.0486\n# ℹ 1,898 more rows"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-2-historical-performance-continued",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-2-historical-performance-continued",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 2: Historical Performance, continued",
    "text": "Step 2: Historical Performance, continued\n\nCodeOutput\n\n\n\n#Apply the cumprod function to all columns\ncumprod(1+hf_data)%&gt;%head()\n\n\n\n\n\n           Convertible Arbitrage CTA Global Distressed Securities\n1997-01-01              1.011900   1.039300              1.017800\n1997-02-01              1.024346   1.070271              1.030217\n1997-03-01              1.032336   1.068024              1.028981\n1997-04-01              1.041214   1.049867              1.032068\n1997-05-01              1.057457   1.048292              1.056115\n1997-06-01              1.079875   1.057203              1.079033\n           Emerging Markets Equity Market Neutral Event Driven\n1997-01-01         1.079100              1.018900     1.021300\n1997-02-01         1.135753              1.029191     1.029879\n1997-03-01         1.122124              1.030838     1.027510\n1997-04-01         1.135477              1.043105     1.026996\n1997-05-01         1.171245              1.062819     1.062531\n1997-06-01         1.239294              1.080356     1.089944"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-capm-model-one-strategy-output",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-capm-model-one-strategy-output",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 4: The CAPM model, one strategy output",
    "text": "Step 4: The CAPM model, one strategy output\n\nCodeOutput\n\n\n\nstrategy_name = 'Convertible Arbitrage'\n#Get the filtered data\nfiltered_data = merged_df%&gt;%filter(strategy==strategy_name)\n#Estimate the Model\nmodel=lm(excess_return ~ MKT_MINUS_RF, data = filtered_data)\n#Visualize\nsummary(model)\n\n\n\n\n\n\nCall:\nlm(formula = excess_return ~ MKT_MINUS_RF, data = filtered_data)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.113929 -0.004806  0.000581  0.006432  0.052478 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.0030327  0.0009526   3.184   0.0016 ** \nMKT_MINUS_RF 0.0789553  0.0137361   5.748 2.15e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01659 on 312 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.09576,   Adjusted R-squared:  0.09286 \nF-statistic: 33.04 on 1 and 312 DF,  p-value: 2.149e-08"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-capm-model-all-strategies",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-capm-model-all-strategies",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 4: The CAPM model, all strategies",
    "text": "Step 4: The CAPM model, all strategies\n\nFor-loopFunctional ProgrammingOutput\n\n\n\n#Initially an empy data frame\nCAPM_results = data.frame()\n#Get all possible strategies\nstrategy_names = names(hf_data)[2:7]\n\n#For each i in strategy names:\nfor(i in strategy_names){\n  #Get the filtered data\n  filtered_data = merged_df%&gt;%filter(strategy==i)\n  #Estimate the Model\n  model=lm(excess_return ~ MKT_MINUS_RF, data = filtered_data)\n  #Extract Coefficients applying the tidy() function\n  model_tidy=model%&gt;%tidy()%&gt;%mutate(strategy=i)\n  #Append\n  CAPM_results=CAPM_results%&gt;%rbind(model_tidy)\n}\n\n\n\n\nCAPM_results &lt;- merged_df%&gt;%\n  group_by(strategy)%&gt;%\n  nest()%&gt;%\n  mutate(model = map(data, ~ lm(excess_return ~ MKT_MINUS_RF, data = .)),\n         results = map(model, tidy)) %&gt;%\n  unnest(results)%&gt;%\n  select(strategy, term, estimate, std.error, p.value)\n\n\n\n\n\n# A tibble: 12 × 5\n# Groups:   strategy [6]\n   strategy              term         estimate std.error  p.value\n   &lt;chr&gt;                 &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 Convertible Arbitrage (Intercept)   0.00303  0.000953 1.60e- 3\n 2 Convertible Arbitrage MKT_MINUS_RF  0.0790   0.0137   2.15e- 8\n 3 CTA Global            (Intercept)   0.00256  0.00135  5.77e- 2\n 4 CTA Global            MKT_MINUS_RF  0.0394   0.0194   4.30e- 2\n 5 Distressed Securities (Intercept)   0.00352  0.00104  8.09e- 4\n 6 Distressed Securities MKT_MINUS_RF  0.119    0.0150   3.93e-14\n 7 Emerging Markets      (Intercept)   0.00261  0.00169  1.25e- 1\n 8 Emerging Markets      MKT_MINUS_RF  0.237    0.0244   1.30e-19\n 9 Equity Market Neutral (Intercept)   0.00159  0.000501 1.65e- 3\n10 Equity Market Neutral MKT_MINUS_RF  0.0453   0.00723  1.25e- 9\n11 Event Driven          (Intercept)   0.00354  0.00111  1.55e- 3\n12 Event Driven          MKT_MINUS_RF  0.154    0.0160   2.06e-19"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-fama-french-model-continued",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-fama-french-model-continued",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 4: The Fama-French Model, continued",
    "text": "Step 4: The Fama-French Model, continued"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-the-capm-model-one-strategy-output",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-the-capm-model-one-strategy-output",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 3: The CAPM model, one strategy output",
    "text": "Step 3: The CAPM model, one strategy output\n\nCodeOutput\n\n\n\nstrategy_name = 'Convertible Arbitrage'\n#Get the filtered data\nfiltered_data = merged_df%&gt;%filter(strategy==strategy_name)\n#Estimate the Model\nmodel=lm(excess_return ~ MKT_MINUS_RF, data = filtered_data)\n#Visualize\nsummary(model)\n\n\n\n\n\n\nCall:\nlm(formula = excess_return ~ MKT_MINUS_RF, data = filtered_data)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.113929 -0.004806  0.000581  0.006432  0.052478 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.0030327  0.0009526   3.184   0.0016 ** \nMKT_MINUS_RF 0.0789553  0.0137361   5.748 2.15e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01659 on 312 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.09576,   Adjusted R-squared:  0.09286 \nF-statistic: 33.04 on 1 and 312 DF,  p-value: 2.149e-08"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-the-capm-model-all-strategies",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-the-capm-model-all-strategies",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 3: The CAPM model, all strategies",
    "text": "Step 3: The CAPM model, all strategies\n\nFor-loopFunctional ProgrammingOutput\n\n\n\n#Initially an empy data frame\nCAPM_results = data.frame()\n#Get all possible strategies\nstrategy_names = names(hf_data)[2:7]\n\n#For each i in strategy names:\nfor(i in strategy_names){\n  #Get the filtered data\n  filtered_data = merged_df%&gt;%filter(strategy==i)\n  #Estimate the Model\n  model=lm(excess_return ~ MKT_MINUS_RF, data = filtered_data)\n  #Extract Coefficients applying the tidy() function\n  model_tidy=model%&gt;%tidy()%&gt;%mutate(strategy=i)\n  #Append\n  CAPM_results=CAPM_results%&gt;%rbind(model_tidy)\n}\n\n\n\n\nCAPM_results &lt;- merged_df%&gt;%\n  #Group by strategy\n  group_by(strategy)%&gt;%\n  #Nest the data\n  nest()%&gt;%\n  #For each nest, map the lm() function and the tidy function\n  mutate(model = map(data, ~ lm(excess_return ~ MKT_MINUS_RF, data = .)),\n         results = map(model, tidy)) %&gt;%\n  #Unnest the results\n  unnest(results)%&gt;%\n  #Select the desired columns\n  select(strategy, term, estimate, std.error, p.value)\n\n\n\n\n\n# A tibble: 12 × 5\n# Groups:   strategy [6]\n   strategy              term         estimate std.error  p.value\n   &lt;chr&gt;                 &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 Convertible Arbitrage (Intercept)   0.00303  0.000953 1.60e- 3\n 2 Convertible Arbitrage MKT_MINUS_RF  0.0790   0.0137   2.15e- 8\n 3 CTA Global            (Intercept)   0.00256  0.00135  5.77e- 2\n 4 CTA Global            MKT_MINUS_RF  0.0394   0.0194   4.30e- 2\n 5 Distressed Securities (Intercept)   0.00352  0.00104  8.09e- 4\n 6 Distressed Securities MKT_MINUS_RF  0.119    0.0150   3.93e-14\n 7 Emerging Markets      (Intercept)   0.00261  0.00169  1.25e- 1\n 8 Emerging Markets      MKT_MINUS_RF  0.237    0.0244   1.30e-19\n 9 Equity Market Neutral (Intercept)   0.00159  0.000501 1.65e- 3\n10 Equity Market Neutral MKT_MINUS_RF  0.0453   0.00723  1.25e- 9\n11 Event Driven          (Intercept)   0.00354  0.00111  1.55e- 3\n12 Event Driven          MKT_MINUS_RF  0.154    0.0160   2.06e-19"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-which-strategies-generate-positive-alpha",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-which-strategies-generate-positive-alpha",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 3: which strategies generate positive alpha?",
    "text": "Step 3: which strategies generate positive alpha?\n\nCodeOutput\n\n\n\nCAPM_results %&gt;%\n  filter(term=='(Intercept)')%&gt;%\n  mutate(stat_sig=ifelse(p.value&lt;0.10,'Statistically sig. at 10%','Not statistically sig. at 10%'))%&gt;%\n  ggplot(aes(x=reorder(strategy,desc(estimate)),y=estimate,fill=stat_sig))+\n  geom_col(size=3)+\n  geom_text(aes(label = percent(estimate,accuracy=0.01),vjust=-1))+\n  #Annotations\n  labs(title='Which strategies did generate positive and statistically significant alphas?',\n       subtitle = 'Using the CAPM model with monthly return data.',\n       x = 'Strategy',\n       y = 'Alpha (%)',\n       fill = 'Stat. Sig')+\n  #Scales\n  scale_y_continuous(labels = percent)+\n  scale_fill_manual(values=c('darkred','darkgreen'))+\n  #Custom theme minimal\n  theme_minimal()+\n  #Adding further customizations\n  theme(legend.position='bottom',\n        axis.title = element_text(face='bold',size=15),\n        axis.text = element_text(size=10),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-fama-french-model-one-strategy-output",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-fama-french-model-one-strategy-output",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 4: The Fama-French model, one strategy output",
    "text": "Step 4: The Fama-French model, one strategy output\n\nCodeOutput\n\n\n\nstrategy_name = 'Convertible Arbitrage'\n#Get the filtered data\nfiltered_data = merged_df%&gt;%filter(strategy==strategy_name)\n#Estimate the Model\nmodel=lm(excess_return ~ MKT_MINUS_RF + HML + SMB, data = filtered_data)\n#Visualize\nsummary(model)\n\n\n\n\n\n\nCall:\nlm(formula = excess_return ~ MKT_MINUS_RF + HML + SMB, data = filtered_data)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.115871 -0.004508  0.001188  0.006993  0.053934 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.0028446  0.0009793   2.905  0.00394 ** \nMKT_MINUS_RF 0.0601943  0.0139721   4.308  2.2e-05 ***\nHML          0.0248444  0.0181225   1.371  0.17138    \nSMB          0.0203995  0.0236709   0.862  0.38946    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01712 on 314 degrees of freedom\nMultiple R-squared:  0.06444,   Adjusted R-squared:  0.0555 \nF-statistic: 7.209 on 3 and 314 DF,  p-value: 0.000108"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-the-capm-model-all-strategies-1",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-the-capm-model-all-strategies-1",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 3: The CAPM model, all strategies",
    "text": "Step 3: The CAPM model, all strategies\n\nFor-loopFunctional ProgrammingOutput\n\n\n\n#Initially an empy data frame\nFF_results = data.frame()\n#Get all possible strategies\nstrategy_names = names(hf_data)[2:7]\n\n#For each i in strategy names:\nfor(i in strategy_names){\n  #Get the filtered data\n  filtered_data = merged_df%&gt;%filter(strategy==i)\n  #Estimate the Model\n  model=lm(excess_return ~ MKT_MINUS_RF + HML + SMB, data = filtered_data)\n  #Extract Coefficients applying the tidy() function\n  model_tidy=model%&gt;%tidy()%&gt;%mutate(strategy=i)\n  #Append\n  FF_results=FF_results%&gt;%rbind(model_tidy)\n}\n\n\n\n\nFF_results &lt;- merged_df%&gt;%\n  #Group by strategy\n  group_by(strategy)%&gt;%\n  #Nest the data\n  nest()%&gt;%\n  #For each nest, map the lm() function and the tidy function\n  mutate(model = map(data, ~ lm(excess_return ~ MKT_MINUS_RF + HML + SMB, data = .)),\n         results = map(model, tidy)) %&gt;%\n  #Unnest the results\n  unnest(results)%&gt;%\n  #Select the desired columns\n  select(strategy, term, estimate, std.error, p.value)\n\n\n\n\n\n# A tibble: 12 × 5\n# Groups:   strategy [6]\n   strategy              term         estimate std.error  p.value\n   &lt;chr&gt;                 &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 Convertible Arbitrage (Intercept)   0.00303  0.000953 1.60e- 3\n 2 Convertible Arbitrage MKT_MINUS_RF  0.0790   0.0137   2.15e- 8\n 3 CTA Global            (Intercept)   0.00256  0.00135  5.77e- 2\n 4 CTA Global            MKT_MINUS_RF  0.0394   0.0194   4.30e- 2\n 5 Distressed Securities (Intercept)   0.00352  0.00104  8.09e- 4\n 6 Distressed Securities MKT_MINUS_RF  0.119    0.0150   3.93e-14\n 7 Emerging Markets      (Intercept)   0.00261  0.00169  1.25e- 1\n 8 Emerging Markets      MKT_MINUS_RF  0.237    0.0244   1.30e-19\n 9 Equity Market Neutral (Intercept)   0.00159  0.000501 1.65e- 3\n10 Equity Market Neutral MKT_MINUS_RF  0.0453   0.00723  1.25e- 9\n11 Event Driven          (Intercept)   0.00354  0.00111  1.55e- 3\n12 Event Driven          MKT_MINUS_RF  0.154    0.0160   2.06e-19"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-which-strategies-generate-positive-alpha-1",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-which-strategies-generate-positive-alpha-1",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 3: which strategies generate positive alpha?",
    "text": "Step 3: which strategies generate positive alpha?\n\nCodeOutput\n\n\n\nFF_results %&gt;%\n  filter(term=='(Intercept)')%&gt;%\n  mutate(stat_sig=ifelse(p.value&lt;0.10,'Statistically sig. at 10%','Not statistically sig. at 10%'))%&gt;%\n  ggplot(aes(x=reorder(strategy,desc(estimate)),y=estimate,fill=stat_sig))+\n  geom_col(size=3)+\n  geom_text(aes(label = percent(estimate,accuracy=0.01),vjust=-1))+\n  #Annotations\n  labs(title='Which strategies did generate positive and statistically significant alphas?',\n       subtitle = 'Using the Fama-French three-factor model with monthly return data.',\n       x = 'Strategy',\n       y = 'Alpha (%)',\n       fill = 'Stat. Sig')+\n  #Scales\n  scale_y_continuous(labels = percent)+\n  scale_fill_manual(values=c('darkred','darkgreen'))+\n  #Custom theme minimal\n  theme_minimal()+\n  #Adding further customizations\n  theme(legend.position='bottom',\n        axis.title = element_text(face='bold',size=15),\n        axis.text = element_text(size=10),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-fama-french-model-all-strategies",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-fama-french-model-all-strategies",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 4: The Fama-French model, all strategies",
    "text": "Step 4: The Fama-French model, all strategies\n\nFor-loopFunctional ProgrammingOutput\n\n\n\n#Initially an empy data frame\nFF_results = data.frame()\n#Get all possible strategies\nstrategy_names = names(hf_data)[2:7]\n\n#For each i in strategy names:\nfor(i in strategy_names){\n  #Get the filtered data\n  filtered_data = merged_df%&gt;%filter(strategy==i)\n  #Estimate the Model\n  model=lm(excess_return ~ MKT_MINUS_RF + HML + SMB, data = filtered_data)\n  #Extract Coefficients applying the tidy() function\n  model_tidy=model%&gt;%tidy()%&gt;%mutate(strategy=i)\n  #Append\n  FF_results=FF_results%&gt;%rbind(model_tidy)\n}\n\n\n\n\nFF_results &lt;- merged_df%&gt;%\n  #Group by strategy\n  group_by(strategy)%&gt;%\n  #Nest the data\n  nest()%&gt;%\n  #For each nest, map the lm() function and the tidy function\n  mutate(model = map(data, ~ lm(excess_return ~ MKT_MINUS_RF + HML + SMB, data = .)),\n         results = map(model, tidy)) %&gt;%\n  #Unnest the results\n  unnest(results)%&gt;%\n  #Select the desired columns\n  select(strategy, term, estimate, std.error, p.value)\n\n\n\n\n\n# A tibble: 24 × 5\n# Groups:   strategy [6]\n   strategy              term         estimate std.error  p.value\n   &lt;chr&gt;                 &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 Convertible Arbitrage (Intercept)   0.00284  0.000979 3.94e- 3\n 2 Convertible Arbitrage MKT_MINUS_RF  0.0602   0.0140   2.20e- 5\n 3 Convertible Arbitrage HML           0.0248   0.0181   1.71e- 1\n 4 Convertible Arbitrage SMB           0.0204   0.0237   3.89e- 1\n 5 CTA Global            (Intercept)   0.00274  0.00134  4.13e- 2\n 6 CTA Global            MKT_MINUS_RF  0.0326   0.0191   8.88e- 2\n 7 CTA Global            HML          -0.00785  0.0248   7.52e- 1\n 8 CTA Global            SMB          -0.0503   0.0324   1.21e- 1\n 9 Distressed Securities (Intercept)   0.00337  0.00106  1.65e- 3\n10 Distressed Securities MKT_MINUS_RF  0.0993   0.0151   2.31e-10\n# ℹ 14 more rows"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-which-strategies-generate-positive-alpha",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-which-strategies-generate-positive-alpha",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 4: which strategies generate positive alpha?",
    "text": "Step 4: which strategies generate positive alpha?\n\nCodeOutput\n\n\n\nFF_results %&gt;%\n  filter(term=='(Intercept)')%&gt;%\n  mutate(stat_sig=ifelse(p.value&lt;0.01,'Statistically sig. at 1%','Not statistically sig. at 10%'))%&gt;%\n  ggplot(aes(x=reorder(strategy,desc(estimate)),y=estimate,fill=stat_sig))+\n  geom_col(size=3)+\n  geom_text(aes(label = percent(estimate,accuracy=0.01),vjust=-1))+\n  #Annotations\n  labs(title='Which strategies did generate positive and statistically significant alphas?',\n       subtitle = 'Using the Fama-French three-factor model with monthly return data.',\n       x = 'Strategy',\n       y = 'Alpha (%)',\n       fill = 'Stat. Sig')+\n  #Scales\n  scale_y_continuous(labels = percent)+\n  #Custom theme minimal\n  theme_minimal()+\n  #Adding further customizations\n  theme(legend.position='bottom',\n        axis.title = element_text(face='bold',size=15),\n        axis.text = element_text(size=10),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-fama-french-model-attribution",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-4-the-fama-french-model-attribution",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 4: The Fama-French Model Attribution",
    "text": "Step 4: The Fama-French Model Attribution"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#performance-of-fund-managers",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#performance-of-fund-managers",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Performance of fund managers",
    "text": "Performance of fund managers\n\nSome piece of evidence from (Berk and DeMarzo 2023):\n\nThe average mutual fund manager can generate value (before computing trading costs and fees, i.e., “gross alpha”)\nThe median mutual fund manager, on the other hand, destroys value\nOnly a small portion of managers are skilled enough to add value, according to this reference, in terms of net alpha\n\nBecause individual investors pay fees to fund managers, the net alpha is negative - you should be better-off by putting your money on a passively-managed fund!\nThat is, on average, fund managers (“active” strategies) do not provide value after fees, comparing to index funds (“passive strategies”)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#performance-of-fund-managers-continued",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#performance-of-fund-managers-continued",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Performance of fund managers, continued",
    "text": "Performance of fund managers, continued\n\nIf fund managers are high-skilled investors, why they have a hard time adding value?\nOne reason why it might be difficult to add value is because there is a trap of liquidity:\n\nIf a manager is perceived as skilled, the deposits will grow, making harder to find above-average investment opportunities - that is why you see a lot of closed-end funds\nPerformance would converge to the mean, at best\n\nAt the end of the day, the market is competitive and people profit following the theoretical predictions\n\nSkilled managers are recompensated for their skills. They capture the economic rents associated with their skills\nInvestors are not recompensated for the skills of the managers they select - in the end, they derive little benefit, because this superior performance is captured by the manager in the form of fees"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#alpha-estimation-over-time",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#alpha-estimation-over-time",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Alpha estimation over time",
    "text": "Alpha estimation over time\n\nCodeOutput\n\n\n\nFull_Data%&gt;%\n  mutate(excess_return=weekly_return-Rf)%&gt;%\n  select(date,symbol,excess_return,MRP)%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(\n      mutate_fun = rollapply,\n      by.column=FALSE,\n      width=24, #Use latest 24 weeks\n      FUN = custom_OLS,\n      col_rename = c('alpha','beta'))%&gt;%\n  filter(!is.na(alpha))%&gt;%\n  ggplot(aes(x=date,y=alpha))+\n  geom_line()+\n  geom_smooth()+\n  facet_wrap(symbol~.,nrow=2,ncol=5,scales='free_y')+\n  #Annotations\n  labs(title='Rolling Alpha Regression',\n       subtitle = 'Source: Yahoo! Finance. Using the latest 24 observations of weekly returns.',\n       x = '',\n       y = 'Estimated Alpha')+\n  #Theme\n  theme_minimal()+\n  #Scale x\n  scale_x_date(date_breaks = '4 weeks')+\n  scale_y_continuous(labels = percent)+\n  #Adding further customizations\n  theme(legend.position='none',\n        axis.title.y = element_text(face='bold',size=15),\n        axis.text.x = element_text(size=10,angle = 90),\n        axis.text.y = element_text(size=10),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15,face='bold'))"
  },
  {
    "objectID": "quant-fin/capstone/guidelines.html",
    "href": "quant-fin/capstone/guidelines.html",
    "title": "Capstone Project - Practical Applications in Quantitative Finance",
    "section": "",
    "text": "As the final evaluation for the Practical Applications in Quantitative Finance course, students will develop a Capstone Project demonstrating their ability to apply quantitative finance concepts using R. The project should showcase at least one of the topics covered in the course or another finance-related topic, leveraging the tools and techniques introduced throughout the semester.\nPlease take some time to review the specific guidelines that should be the basis of your project."
  },
  {
    "objectID": "quant-fin/capstone/guidelines.html#objective",
    "href": "quant-fin/capstone/guidelines.html#objective",
    "title": "Capstone Project - Practical Applications in Quantitative Finance",
    "section": "",
    "text": "As the final evaluation for the Practical Applications in Quantitative Finance course, students will develop a Capstone Project demonstrating their ability to apply quantitative finance concepts using R. The project should showcase at least one of the topics covered in the course or another finance-related topic, leveraging the tools and techniques introduced throughout the semester.\nPlease take some time to review the specific guidelines that should be the basis of your project."
  },
  {
    "objectID": "quant-fin/capstone/guidelines.html#group-formation",
    "href": "quant-fin/capstone/guidelines.html#group-formation",
    "title": "Capstone Project - Practical Applications in Quantitative Finance",
    "section": "Group Formation",
    "text": "Group Formation\nStudents must work in teams of up to five members, maximum. Each team is responsible for defining roles and organizing their workflow effectively to ensure timely completion."
  },
  {
    "objectID": "quant-fin/capstone/guidelines.html#project-scope",
    "href": "quant-fin/capstone/guidelines.html#project-scope",
    "title": "Capstone Project - Practical Applications in Quantitative Finance",
    "section": "Project Scope",
    "text": "Project Scope\nTeams are expected to generate an analysis using one or more finance concepts studied in this course or other finance-related concepts that align with the course’s focus. The analysis should be conducted using the R and incorporate the tools and techniques covered, such as the tidyverse, tidyquant, ggplot2, among others.\nMore specifically, the expected output must be structured as one of the following deliverables:\n\nAn R script, well-documented and reproducible\nA Quarto document containing a dynamic report with code, analysis, and visualizations; or\nA Shiny application file\n\nIn addition to your code, groups are expected to create a 10-minutes presentation (in PowerPoint, Quarto, or even through a Shiny app) outlining the main points from their capstone project. All students are expected to present, and should be prepared for technical questions regarding the underlying code.\n\n\n\n\n\n\nUsing Shiny\n\n\n\nFor groups that are willing to showcase their work using Shiny, the professor will provide full support for hosting the application online, ensuring accessibility for external review and future professional showcasing.\nFor a reference of what can be considered a capstone project for the course, access this link to check a web application that replicates a momentum strategy in the U.S. financial markets. Kindly note that capstone projects are expected to be much simpler than this example, as the goal is to provide students with a first hands-on finance experience on tools for data analysis and reporting in finance."
  },
  {
    "objectID": "quant-fin/capstone/guidelines.html#final-submission-hosting",
    "href": "quant-fin/capstone/guidelines.html#final-submission-hosting",
    "title": "Capstone Project - Practical Applications in Quantitative Finance",
    "section": "Final Submission & Hosting",
    "text": "Final Submission & Hosting\n\nAll applications must be hosted online, ensuring accessibility for external review and future professional showcasing.\nDetailed submission guidelines, including hosting platforms and file formats, will be provided."
  },
  {
    "objectID": "quant-fin/capstone/guidelines.html#evaluation-criteria",
    "href": "quant-fin/capstone/guidelines.html#evaluation-criteria",
    "title": "Capstone Project - Practical Applications in Quantitative Finance",
    "section": "Evaluation Criteria",
    "text": "Evaluation Criteria\nThe evaluation criteria will be mainly take two aspects into consideration: a written report (\\(40\\%\\)) and a project showcase (\\(15\\%\\)). First, the written report, which should be done using one out of the three aforementioned output formats, will be evaluated on the basis of:\n\nThe quality and adherence of the deliverable relative to the expected goals outlined in the guidelines\nThe group’s ability to interpret data using finance theory and draw insights from their findings\nCode organization, providing clean, concise, and reproducible results\nOriginality, focusing on real-world problems that are not trivial\n\nFurthermore, at the end of the semester, each group is also expected to present their project to the course audience. All individuals from the group are expected to attend and present. During this showcase, which should last no longer than 10 (ten) minutes, students will be evaluated on the basis of:\n\nAbility to Answer Practical and Technical Questions\n\nDemonstrate a deep understanding of their analysis, methods, and results\nPresenters should be prepared to respond to questions regarding data sources, methodology, and financial interpretation\n\nOrganization\n\nThe project should be well-structured, with a logical flow from problem statement to conclusion\nCode and documentation should be clear, well-commented, and reproducible\n\nClear Communication\n\nPresentations should be concise, engaging, and accessible to the audience\nVisualizations and explanations should effectively convey key insights"
  },
  {
    "objectID": "quant-fin/capstone/guidelines.html#expectations-best-practices",
    "href": "quant-fin/capstone/guidelines.html#expectations-best-practices",
    "title": "Capstone Project - Practical Applications in Quantitative Finance",
    "section": "Expectations & Best Practices",
    "text": "Expectations & Best Practices\nAll in all, the Capstone Project is an opportunity for students not only to showcase their work not only for the course audience, but also for external audiences, like HR managers, recruiters, among others. At the same time you can expect to receive feedback from your colleagues, be prepared to also provide useful feedback to other groups regarding their work.\nTo make sure that you are set up for success, try to follow these best practices to best of our ability:\n\nStart early: effective time management is crucial for success. Make sure to devote enough time for somewhat cumbersome tasks, such as data collection and manipulation\nCollaborate efficiently: distribute tasks within your team to leverage individual strengths. Test code snippets and make sure that the whole analysis is reproducible\nMaintain clarity: ensure that your analysis and conclusions are well-documented and easy to follow\nReview and iterate: test your code, validate results, and refine your presentation"
  },
  {
    "objectID": "quant-fin/capstone/guidelines.html#deadlines",
    "href": "quant-fin/capstone/guidelines.html#deadlines",
    "title": "Capstone Project - Practical Applications in Quantitative Finance",
    "section": "Deadlines",
    "text": "Deadlines\nThe deadline for the submission of the Capstone Project is May, 23rd, at 11:59 PM, Brazilian time. Submissions made after this date will not be accepted. Make sure that one (and only one) member of your group posts the file on eClass®."
  },
  {
    "objectID": "quant-fin/capstone/guidelines.html#final-thoughts",
    "href": "quant-fin/capstone/guidelines.html#final-thoughts",
    "title": "Capstone Project - Practical Applications in Quantitative Finance",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nStudents are highly encouraged to share their codes so that everyone can replicate and contribute to their analyses. This capstone project represents an opportunity to apply your knowledge in a meaningful way, demonstrating both technical and analytical skills. Make the most of it!\nIn case of any questions, feel free to reach out to lucas.macoris@fgv.br"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#managing-payables-trade-credit",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#managing-payables-trade-credit",
    "title": "Working Capital Management",
    "section": "#1 Managing Payables: Trade Credit",
    "text": "#1 Managing Payables: Trade Credit\n\nThe credit that the firm is extending to its customer is known as trade credit\n\nA firm would, of course, prefer to be paid in cash at the time of purchase\nHowever, a “cash-only” policy may cause it to lose its customers to competition\n\nTrade credit is, in essence, a loan from the selling firm to its customer\n\nThe price discount represents an interest rate\nFirms offer favorable interest rates on trade credit as a price discount to their customers\n\nUnderstanding the terminology: what is “2/10, Net 30”? It means that the buying firm will receive a \\(\\small 2\\%\\) discount if it pays for the goods within \\(\\small 10\\) days; otherwise, the full amount is due in \\(\\small 30\\) days\nAs a Financial Manager, it is important to understand that Trade Credit has a cost (or a discount): by taking an offer to pay later, what is the additional cost that the firm needs to incur?"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#managing-receivables-credit-policy",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#managing-receivables-credit-policy",
    "title": "Working Capital Management",
    "section": "#2 Managing Receivables: Credit Policy",
    "text": "#2 Managing Receivables: Credit Policy\n\nA firm can better manage its receivables by establishing a clear Credit Policy:\n\n\nEstablishing Credit Standards:\n\nWhat is the criteria to extend credit?\nInternal evaluation versus the use of credit rating agencies\n\nEstablishing Credit Terms:\n\nAfter creating the criteria, what will be the credit terms?\nThis is also highly influenced by competition and industry standards\n\nEstablishing a Collection Policy:\n\nIn case of default, what should the firm do?\nPolicies range from seizing inventory, interest expenses, legal actions\nDebt Collection business (collection agencies)"
  },
  {
    "objectID": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#monitoring-payable---accounts-payable-outstanding",
    "href": "fin-mgmt/Lecture 4 - Working Capital Managament/index.html#monitoring-payable---accounts-payable-outstanding",
    "title": "Working Capital Management",
    "section": "Monitoring Payable - Accounts Payable Outstanding",
    "text": "Monitoring Payable - Accounts Payable Outstanding\n\nSimilar to the situation with its accounts receivable, a firm should monitor its accounts payable to ensure that it is making its payments at an optimal time\nOne way is to the accounts payable days outstanding and compare it to the credit terms\nSuppose that a firm has an average accounts payable balance of \\(\\small\\$250,000\\). Its average daily cost of goods sold is \\(\\small \\$14,000\\), and it receives terms of 2/15, Net 40, from its suppliers. The firm chooses to forgo the discount. Is the firm managing its accounts payable well?\n\n\n\nA: the account payable days is \\(\\small 250,000/14,000= 17.9\\) days. There are two cases:\n\nIf the firm made payment three days earlier, it could take advantage of the \\(\\small2\\%\\) discount\nIf for some reason it chooses to forgo the discount, it should not be paying the full amount until the fortieth day\n\nImportant Note: these are average terms and do not refer to specific payments."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#it-is-now-your-turn",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#it-is-now-your-turn",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "It is now your turn…",
    "text": "It is now your turn…\n\nIn our Manipulating Time Series Data lecture, we recreated the at the Deadlift ETF and compared it to the returns from the S&P 500 index\nNow, assuming that the CAPM correctly prices all assets, you are equipped to use the model and evaluate whether such strategy yielded true skill returns, or \\(\\small \\alpha\\)\n\n\n\n\n\n\n\nHands-on Exercise\n\n\n\nAdapt the code we have used in the Manipulating Time Series Data to generate the weekly returns during 2024 for the Deadlift ETF\nMerge the dataset with the Fama French dataset you have just used. You can use the left_join function with the by argument to join two dataframes with different column names\nUse mutate to create a column that calculates the weekly excess return of the Deadlift ETF relative to the risk-free rate\nEstimate an OLS regression of the form:\n\n\\[\nR_{i,t}=\\alpha+\\beta \\times(R_{m,t}-R_{f,t})+\\varepsilon_{i,t}\n\\]\nHow do you interpret these findings? Does investing Deadlift ETF provide true skill returns? Explain your rationale."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#code-snippet-from-previous-classes",
    "href": "quant-fin/coursework/Lecture 5 - CAPM in Practice/index.html#code-snippet-from-previous-classes",
    "title": "The Capital Asset Pricing Model (CAPM) in practice",
    "section": "Code snippet from previous classes",
    "text": "Code snippet from previous classes\n\n# Set up the list of assets\ndeadlift=c('META','AMZN','GS','UBER','MSFT','AAPL','BLK','NVDA')\n\n#Set up the starting date\nstart='2020-01-01'\nend='2024-12-31'\n\n#Step 1: Read the Deadlift data using tidyquant\nDeadlift_Performance=deadlift%&gt;%\n  tq_get(from=start,to=end)%&gt;%\n  #Select only the columns of interest\n  select(symbol,date,adjusted)%&gt;%\n  #Group by symbol and date\n  group_by(symbol)%&gt;%\n  #Use tq_transmute to aggregate and calculate weekly returns\n  tq_transmute(selected=adjusted,\n               mutate_fun=yearlyReturn,\n               col_rename = 'Deadlift')%&gt;%\n  #Group by date\n  group_by(date)%&gt;%\n  #Summarize average return (since it is an equally-weighted portfolio)\n  summarize(Deadlift=mean(Deadlift,na.rm=TRUE))\n\n#Step 2: Read the S&P 500 data using tidyquant\nSP500_Performance=tq_get('^GSPC',from=start,to=end)%&gt;%\n  #Select only the columns of interest\n  select(symbol,date,adjusted)%&gt;%\n  #Group by symbol and date\n  group_by(symbol)%&gt;%\n  #Use tq_transmute to aggregate and calculate weekly returns\n  tq_transmute(selected=adjusted,\n               mutate_fun=yearlyReturn,\n               col_rename = 'SP500')%&gt;%\n  ungroup()%&gt;%\n  select(-symbol)\n    \n#Merge\nSP500_Performance%&gt;%\n  inner_join(Deadlift_Performance)%&gt;%\n  mutate(across(where(is.numeric),percent))%&gt;%\n  mutate(date=year(date))%&gt;%\n  setNames(c('Year','S&P 500','DeadLift ETF'))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-the-fama-french-model",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-3-the-fama-french-model",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 3: The Fama-French Model",
    "text": "Step 3: The Fama-French Model\n\nWe can now extend the same rationale to adopt the Fama-French three-factor model, which considers the Market, the Market Capitalization Strategy (Size), and the Book-to-Market (Value) strategy:\n\n\\[\n\\small E[R_i] = R_f + \\beta_s^m \\times \\underbrace{(E[R_m]− R_f)}_{\\text{Market}}  + \\beta_s^{SMB} \\times \\underbrace{E[R_{SMB}]}_{\\text{Size}} + \\beta_s^{HML} \\times \\underbrace{E[R_{HML}]}_{\\text{Book-to-Market}}\n\\]\n\nAs before, we will start analyzing the output for one individual strategy individually and then apply the same rationale to replicate the result across all strategies"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-5-the-fama-french-model-attribution",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#step-5-the-fama-french-model-attribution",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "Step 5: The Fama-French Model Attribution",
    "text": "Step 5: The Fama-French Model Attribution\n\nCodeOutput\n\n\n\nFF_results %&gt;%\n  filter(term != \"(Intercept)\")%&gt;%\n  group_by(strategy)%&gt;%\n  ggplot(aes(x = reorder(term,desc(estimate)), y = estimate, fill = term)) +\n  geom_col(position = position_dodge())+\n  geom_label(aes(label = round(estimate,2)),position=position_stack(vjust=0.25),col='black',fill='white')+\n  theme_minimal()+\n  facet_wrap(strategy~.,ncol=3,nrow=2)+\n  #Annotations\n  labs(title = \"Fama-French Factor Loadings by Hedge Fund Strategy\",\n       x = \"Hedge Fund Strategy\",\n       y = \"Factor Loading\",\n       fill = 'Risk Factor')+\n  #Scales+\n  scale_fill_manual(values=c('darkred','darkgreen','black'),\n                    labels=c('High-minus-Low','Market Excess','Small-minus-Big'))+\n  #Custom theme minimal\n  theme_minimal()+\n  #Adding further customizations\n  theme(legend.position='bottom',\n        axis.title = element_text(face='bold',size=15),\n        axis.text = element_blank(),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#the-performance-of-fund-managers",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#the-performance-of-fund-managers",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "The Performance of Fund Managers",
    "text": "The Performance of Fund Managers\n\nSome piece of evidence from (Berk and DeMarzo 2023):\n\nThe average mutual fund manager can generate value (before computing trading costs and fees, i.e., “gross alpha”)\nThe median mutual fund manager, on the other hand, destroys value\nOnly a small portion of managers are skilled enough to add value, according to this reference, in terms of net alpha\n\nBecause individual investors pay fees to fund managers, the net alpha is negative - you should be better-off by putting your money on a passively-managed fund!\nThat is, on average, fund managers (“active” strategies) do not provide value after fees, comparing to index funds (“passive strategies”)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#the-performance-of-fund-managers-continued",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#the-performance-of-fund-managers-continued",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "The Performance of Fund Managers, continued",
    "text": "The Performance of Fund Managers, continued\nIf fund managers are high-skilled investors, why they have a hard time adding value?\n\n\nOne reason why it might be difficult to add value is because there is a trap of liquidity:\n\nIf a manager is perceived as skilled, the deposits will grow, making harder to find above-average investment opportunities - that is why you see a lot of closed-end funds\nPerformance would converge to the mean, at best\n\n\n\n\n\nAt the end of the day, the market is competitive and people profit following the theoretical predictions\n\nSkilled managers are recompensated for their skills. They capture the economic rents associated with their skills\nInvestors are not recompensated for the skills of the managers they select - in the end, they derive little benefit, because this superior performance is captured by the manager in the form of fees"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#it-is-now-your-turn",
    "href": "quant-fin/coursework/Lecture 6 - Fama French and Multifactor Models of Risk/index.html#it-is-now-your-turn",
    "title": "Fama French and Multi-Factor Models of Risk",
    "section": "It is now your turn…",
    "text": "It is now your turn…\n\nUsing the Hedge Fund monthly returns from the database you have just used to perform the analysis, you will now add two additional factors: Investment and Profitability. The resulting dataset comprises the famous Fama-French Five Factor model. Proceed as follows:\n\n\n\n\n\n\n\nHands-on Exercise\n\n\n\nDownload the Fama-French 5 Factor portfolios using the download link below (or directly through eClass®)\nMerge the dataset with the Fama French dataset you have just used. You can use the left_join function to join two dataframes\nRepeat the procedures we have used before to estimate and chart the output of an OLS regression of the form for all investment strategies:\n\n\\[\n\\small E[R_i] = R_f + \\beta_s^M \\times \\underbrace{(E[R_m]− R_f)}_{\\text{Market}}  + \\beta_s^{SMB} \\times \\underbrace{E[R_{SMB}]}_{\\text{Size}} + \\beta_s^{HML} \\times \\underbrace{E[R_{HML}]}_{\\text{Book-to-Market.}} + \\beta_s^{RMW} \\times \\underbrace{E[R_{RMW}]}_{\\text{Profitability}} + \\beta_s^{CMA} \\times \\underbrace{E[R_{CMA}]}_{\\text{Investment}}\n\\]\nHow do you interpret your new findings? Does your conclusion hold after including the two additional risk factors?\n\n\n\n\n\n Download Raw data"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#outline",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#outline",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\nTidy Finance (Scheuch, Voigt, and Weiss 2023)\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\n\n\n\n\n\n\n\n\nCoding Replications\n\n\nFor coding replications, whenever applicable, please follow this page or hover on the specific slides with containing coding chunks.\n\nEnsure that you have your  session properly set-up according to the instructions outlined in the course webpage\nIn the webpage, you can also find a detailed discussion of the examples covered in this lecture"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#disclaimer",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#disclaimer",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\n\n\n\n\nDisclaimer\n\n\nThe information presented in this lecture is for educational and informational purposes only and should not be construed as investment advice. Nothing discussed constitutes a recommendation to buy, sell, or hold any financial instrument or security. Investment decisions should be made based on individual research and consultation with a qualified financial professional. The presenter assumes no responsibility for any financial decisions made based on this content.\nAll code used in this lecture is publicly available and is also shared on my GitHub page. Participants are encouraged to review, modify, and use the code for their own learning and research purposes. However, no guarantees are made regarding the accuracy, completeness, or suitability of the code for any specific application.\nFor any questions or concerns, please feel free to reach out via email at lucas.macoris@fgv.br"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#hands-on-exercise",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#hands-on-exercise",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\nYou are a quantitative analyst at SeekingAlpha, a quantitative Hedge Fund specialized in automated strategies. Your goal is to quantity how a momentum-based strategy in the Brazilian stock market performed from 2018 to 2024\n\n\n\n\n\n\n\nInstructions\n\n\n\nWe will be using data from a set of \\(\\small 20\\) selected Brazilian stocks that have been traded over the study period\nOur portfolio will be rebalanced monthly using a lookback period of 90 days, selecting the top \\(\\small5\\) stocks based on the adjusted prices\nThe strategy will assign equal weights to all stocks and will consist of a long-biased strategy - i.e, you will only buy stocks\n\nThe selected stocks are: RAIZ4, ITUB3, IRBR3, BBDC4, ABEV3, YDUQ3, BBAS3, B3SA3, WEGE3, RADL3, LREN3, BRFS3, CSAN3, HAPV3, SUZB3, GRND3, MGLU3, BEEF3, EGIE3, and HYPE3\n\n\n\nWould you recommend investing in a fund that replicates this strategy?"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#about-the-dataset",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#about-the-dataset",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "About the dataset",
    "text": "About the dataset\n\nThe edhec dataset, from the EDHEC Risk and Asset Management Research Center, is a dataset that covers monthly Hedge Fund returns starting from 1997\nEach series of returns represents a Hedge Fund strategy that seeks to exploit a given type of market anomaly:\n\nConvertible Arbitrage - click here for details\nEvent Driven - click here for details\nMerger Arbitrage - click here for details\nRelative Value - click here for details\nDistressed Securities - click here for details\nCTA Global - click here for details"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#it-is-now-your-turn",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#it-is-now-your-turn",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "It is now your turn…",
    "text": "It is now your turn…\n\nUsing the Hedge Fund monthly returns from the database you have just used to perform the analysis, you will now add two additional factors: Investment and Profitability. The resulting dataset comprises the famous Fama-French Five Factor model. Proceed as follows:\n\n\n\n\n\n\n\nHands-on Exercise\n\n\n\nDownload the Fama-French 5 Factor portfolios using the download link below (or directly through eClass®)\nMerge the dataset with the Fama French dataset you have just used. You can use the left_join function to join two dataframes\nRepeat the procedures we have used before to estimate and chart the output of an OLS regression of the form for all investment strategies:\n\nHow do you interpret your new findings? Does your conclusion hold after including the two additional risk factors?"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-expected-return-of-a-two-stock-portfolio",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-expected-return-of-a-two-stock-portfolio",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "The Expected Return of a two-stock portfolio",
    "text": "The Expected Return of a two-stock portfolio\n\nPreviously, we looked at returns from individual assets, but investors are always looking for ways to invest in multiple assets at the same time. What if you hold a portfolio of \\(n\\) assets?\nLet’s keep it simple for now. Suppose you have a two-stock portfolio of assets that hipotetically consists of:\n\nAmazon (AMZN) 40% of the portfolio, 10% return and standard deviation (volatility) of 25%\nFerrari (RACE): 60% of the portfolio, 15% return and stardard deviation (volatility) of 30%\n\nIf your portfolio is 40% Amazon and 60% Ferrari, then the return of your portfolio, \\(R_p\\) is:\n\n\n\\[\\small R_p= \\sum_{i=1}^{2}\\big(x_i\\times R_i\\big)=(0.4 \\times 10\\%) +(0.6 \\times 15\\%) = 13\\%\\]\n\n\n\\(\\rightarrow\\) In other words, the return of a portfolio is the weighted average of the individual asset returns!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-expected-return-of-a-two-stock-portfolio-continued",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-expected-return-of-a-two-stock-portfolio-continued",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "The Expected Return of a two-stock portfolio, continued",
    "text": "The Expected Return of a two-stock portfolio, continued\n\n\n\n\n\n\nImportant\n\n\nThe weights are selected by the investors and may change over time if the prices change (unless the investor actively rebalances it to match the same proportion by buying/selling). Without trading, the weights increase for those stocks whose returns exceed the portfolio’s return.\n\n\n\n\nTo see that, assume that your initial holdings are $100,000:\n\nIf Amazon is 40% of the portfolio with a 10% return \\(\\small \\rightarrow 40,000\\times (1+10\\%)=44,000\\)\nIf Ferrari is 60% of the portfolio with a 15% return \\(\\small \\rightarrow 60,000\\times (1+15\\%)=69,000\\)\n\nYour holdings are now worth \\(\\small 44,000 + 69,000= 113,000\\), which yields the 13% return, but now the weights from each stock are different:\n\nAmazon holdings are \\(\\small \\frac{44}{113}\\approx 38.9\\%\\) of the portfolio\nFerrari holdings are \\(\\small \\frac{69}{113}\\approx 61.1\\%\\) of the portfolio"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-two-stock-portfolio",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-two-stock-portfolio",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Volatility of a two-stock portfolio",
    "text": "Volatility of a two-stock portfolio\n\nAs we now have a portfolio of two different assets, what should be the volatility? Recall that:\n\nAmazon (AMZN)’s standard deviation (volatility) was hipothetically 25%\nFerrari (RACE)’s stardard deviation (volatility) was hipothetically 30%\n\nAs we’ll see in the next slides, to compute the standard deviation of a portfolio, we cannot rely on the weighted average anymore!\nIn order to see that, let’s look at the historical prices from both stocks and compare:\n\nThe price trend from Amazon\nThe price trend from Ferrari\nThe dollar holdings for a portfolio of $100 that holds 40% Amazon and 60% Ferrari\n\nIn what follows, we’ll see how these dynamics shed light on the covariance of the portfolio"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-two-stock-portfolio-historical-trends",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-two-stock-portfolio-historical-trends",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Volatility of a two-stock portfolio, historical trends",
    "text": "Volatility of a two-stock portfolio, historical trends"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-two-stock-portfolio-summary-statistics",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-two-stock-portfolio-summary-statistics",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Volatility of a two-stock portfolio, summary statistics",
    "text": "Volatility of a two-stock portfolio, summary statistics\n\nIf we now look at the annualized statistics, we see that..\n\n\n\n\n\n\n\n\n\nThe weighted average between the individual volatilies is around 25.9%\nBut our portfolio volatility is lower than the individual volatilities. Why? Diversification!\n\n\nIntuitively, the returns from both stocks do not move in lockstep: whenever Ferrari prices goes down, Amazon prices move in a different fashion\nIn fact, the correlation between the returns of these stocks is around 0.39. In other words, these assets do not move in the same direction all the time!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#an-interlude-variance-covariance-and-correlation",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#an-interlude-variance-covariance-and-correlation",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "An interlude: Variance, Covariance, and Correlation",
    "text": "An interlude: Variance, Covariance, and Correlation\n\nYou can find the proofs for both definitions in the Appendix\n\n\n\n\n\n\n\n\nDefinition\n\n\n\n\nThe Variance (\\(\\sigma^2\\)) is the squared deviations of the returns from their means: \\(\\small E[X-E(X)]^2\\)\nCovariance (\\(Cov\\)) is the expected product of the deviations of two returns from their means: \\(\\small [X-E(X)][Y-Y(X)]\\)\n\n\n\n\n\n\n\n\n\n\n\nTheorem I: the variance of the sum of two random variables equals the sum of the variances of those random variables, plus two times their covariance:\n\n\n\\[\n  \\sigma^2(A+B) = \\sigma^2_A + \\sigma^2_B + 2\\times Cov(A,B)\n  \\]\n\n\n\n\n\n\n\n\n\nTheorem II: The variance scales upon multiplication with a constant:\n\n\n\\[\n  \\sigma^2(\\beta \\times A ) = \\beta^2\\times \\sigma^2_A\n  \\]"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-two-stock-portfolio-continued",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-two-stock-portfolio-continued",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Volatility of a two-stock portfolio, continued",
    "text": "Volatility of a two-stock portfolio, continued\n\nLet’s now apply this to understand our portfolio’s volatility using daily data. Define:\n\n\\(\\sigma^2_1\\) = variance of Asset 1 (Amazon).\n\\(\\sigma^2_2\\) = variance of Asset 2 (Ferrari)\n\\(\\sigma_{1,2}\\) = covariance between both assets\n\\(w_1\\) and \\(w_2\\) are the weights for both assets\n\nUsing the definitions highlighted before, that the variance of our portfolio returns, \\(R_p\\), is:\n\n\n\\[\n\\small \\sigma^2(R_p)=\\sigma^2(w_1\\times R_1+w_2\\times R_2)=w_1^2\\times \\sigma^2_1 + w_2^2 \\times \\sigma^2_2 + 2\\times w_1\\times w_2\\times \\sigma_{1,2}\n\\]"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#variance-in-terms-of-correlation",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#variance-in-terms-of-correlation",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Variance in terms of correlation",
    "text": "Variance in terms of correlation\n\nWe can further simplify this using the fact that the Covariance is the product of the assets’ standard deviations and their correlation:\n\n\n\\[\n\\small \\sigma^2(R_p)=w_1^2\\times \\sigma^2_1 + w_2^2 \\times \\sigma^2_1 + 2\\times w_1\\times w_2\\times \\underbrace{Cov(R_1,R_2)}_{\\sigma_1 \\times \\sigma_2 \\times Corr_{12}}\\\\\n\\small = \\underbrace{w_1^2\\times \\sigma^2_1}_{1} + \\underbrace{w_2^2 \\times \\sigma^2_1}_{2} + \\underbrace{2\\times w_1\\times w_2\\times \\sigma_1 \\times \\sigma_2 \\times Corr_{12}}_{3}\n\\]\n\n\n\nThe first term relates to the first asset (in our case, Amazon) variance and its proportion in the portfolio\nThe second term relates to the second asset (in our case, Ferrari) variance and its proportion in the portfolio\nThe third term relates to the relationship between both assets"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#a-note-on-sample-analogues",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#a-note-on-sample-analogues",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "A note on sample analogues",
    "text": "A note on sample analogues\n\nRecall that we cannot rely on expectations to future outcomes and/or probabilities to calculate the expected returns of the portfolio and its volatility\nBecause of that, as we did before, we replace the expectation estimator, \\(E(\\cdot)\\), with our sample analogue, which is the sample average, and it is always backward-looking:\n\nFix a period for calculation (for example, the latest 30 days)\nCalculate the risk and return using sample averages\n\nTo facilitate the notation, we’ll always refer to it using a bar: \\(\\overline{R}\\). For example the covariance between the returns from stocks \\(i\\) and \\(j\\) is:\n\n\n\\[Cov(R_i,R_j) = \\frac{\\sum_{1}^{T}(R_i-\\overline{R_i} ) \\times (R_j-\\overline{R_j})}{T-1}\\]"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#getting-back-to-our-portfolio",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#getting-back-to-our-portfolio",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Getting back to our portfolio",
    "text": "Getting back to our portfolio\n\n\n\n\n\n\n\nUsing our formula for the variance of the portfolio:\n\n\n\\[\n\\small \\sigma^2_p= w_1^2\\times \\sigma^2_1 + w_2^2 \\times \\sigma^2_1 + 2\\times w_1\\times w_2\\times \\sigma_1 \\times \\sigma_2 \\times Corr_{12}\\\\\n\\small \\underbrace{\\small 0.4^2\\times 0.3290^2+0.6^2\\times 0.2124^2}_{I} + \\underbrace{2\\times 0.4\\times0.6\\times 0.3290\\times 0.2124\\times 0.3886}_{II} = 0.04659402\n\\]\n\nTherefore, \\(\\sigma_{p}\\) is simply \\(\\small \\sqrt{0.04659402}=0.2159\\) or 21.59%. You could also calculate the daily portfolio variance and annualize it at the end multiplying it by \\(\\sqrt{n}\\)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#variance-of-a-2-stock-portfolio-continued",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#variance-of-a-2-stock-portfolio-continued",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Variance of a 2-stock portfolio, continued",
    "text": "Variance of a 2-stock portfolio, continued\n\nLooking at our formula, we have:\n\n\\[\n\\small \\sigma^2_p= w_1^2\\times \\sigma^2_1 + w_2^2 \\times \\sigma^2_1 + 2\\times w_1\\times w_2\\times \\sigma_1 \\times \\sigma_2 \\times Corr_{12}\\\\\n\\small \\underbrace{\\small 0.4^2\\times 0.3290^2+0.6^2\\times 0.2124^2}_{I} + \\underbrace{2\\times 0.4\\times0.6\\times 0.3290\\times 0.2124\\times 0.3886}_{II} = 0.04659402\n\\]\n\nThe first term (I) captures the individual variances contribution\nThe second term (II) captures the relationship between assets (note that 0.3312 is the correlation between both individual returns)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#wrapping-up---2-stock-portfolio-volatility",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#wrapping-up---2-stock-portfolio-volatility",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Wrapping up - 2-stock portfolio volatility",
    "text": "Wrapping up - 2-stock portfolio volatility\n\nThese assets have roughly similar historical return and volatility, but they ‘move’ very differently:\n\n\nFor example, when Amazon performed well, Ferrari did not move in lockstep\nAs a matter of fact, their correlation is about 0.39, meaning that their prices do not trend in the same direction most of the time!\n\n\nAs you now see, the return of the portfolio is equal to the weighted average of the individual returns…\n…However, the volatility of is much lower than the volatility of the two individual stocks, as the assets are offseting each other and making the return “smoother”!\nHow much risk is eliminated when creating a portfolio? It depends on the degree to which the stocks face common risks and their prices move together (in mathematical terms, this will be captured by the correlation parameter, \\(\\small Corr_{12}\\)!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-large-portfolio",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-large-portfolio",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Volatility of a large portfolio",
    "text": "Volatility of a large portfolio\n\nWhat if we added a third asset to our portfolio? Let’s say that you’re really into discretionary goods and decide to place a bet on Victoria’s Secret (ticker: VSCO), which had 30% of return in the previous last\nYou decided to keep the weight on Amazon and split Ferrari and Victoria’s Secret evenly, with 30% each\nHow does that play a role in your portfolio? As before, your portfolio Return is simply a weighted average of the individual returns:\n\n\n\\[\nR_{p}=\\sum_{i=1}^{3}w_i\\times R_i = \\underbrace{(0.4 \\times 10\\%)}_{\\text{Amazon}} +\\underbrace{(0.3 \\times 15\\%)}_{\\text{Ferrari}}+ \\underbrace{(0.3 \\times 25\\%)}_{\\text{VSCO}}\\approx 16\\%\n\\]\n\nLet’s see how this strategy performed from 2023 up to now!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#ouch-youve-seen-to-be-worse-off-now",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#ouch-youve-seen-to-be-worse-off-now",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Ouch! You’ve seen to be worse off now!",
    "text": "Ouch! You’ve seen to be worse off now!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#your-vsco-bet-did-not-play-well-unfortunately",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#your-vsco-bet-did-not-play-well-unfortunately",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Your VSCO bet did not play well, unfortunately",
    "text": "Your VSCO bet did not play well, unfortunately\n\n\n\n\n\n\n\nAdding VSCO to the portfolio severely hit the overall return, \\(\\small R_p\\)\nVSCO was also very volatile: its standard deviation was twice as high as Amazon. However, the portfolio volatility is substantially lower\nOne way to understand this is to look at the correlation matrix between the assets"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#vsco-is-not-strongly-correlated-with-the-other-assets",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#vsco-is-not-strongly-correlated-with-the-other-assets",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "VSCO is not strongly correlated with the other assets",
    "text": "VSCO is not strongly correlated with the other assets\n\n\n\n\n\n\n\nLooking at the correlation matrix, we can see that:\n\n\\(\\small Corr_{\\text{AMZN,VSCO}}=0.16\\)\n\\(\\small Corr_{\\text{RACE,VSCO}}=0.14\\)\n\nAs we can see, because the correlation coefficient is very small, the contribution of VSCO to the overall portfolio volatility is limited"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-variance-of-a-3-stock-portfolio",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-variance-of-a-3-stock-portfolio",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "The variance of a 3-stock portfolio",
    "text": "The variance of a 3-stock portfolio\n\nHow can we extend your variance calculation for three assets?\nThe general formulation of the variance formula shows us that:\n\n\n\\[\n\\small  Var(X)=Var(A+B+C) = E[((A+B+C)-E(A+B+C))]^2 =\\\\\n\\small E[(\\underbrace{[A-E(A)]}_{\\text{First Term}}+[\\underbrace{B-E(B)}_{\\text{Second Term}}]+[\\underbrace{C-E(C)}_{\\text{Third Term}}])^2] \\\\\n\\]\n\nThis is a quadratic form, and we can decompose it as:\n\n\n\n\\[\n\\small (A+B+C)^2= A^2+B^2+C^2 + 2AB + 2AC +2BC\n\\]"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-variance-of-a-3-stock-portfolio-continued",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-variance-of-a-3-stock-portfolio-continued",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "The variance of a 3-stock portfolio, continued",
    "text": "The variance of a 3-stock portfolio, continued\n\nIn portfolio terms:\n\n\n\\[\n\\small \\sigma^2_p= \\underbrace{w_1^2\\times \\sigma^2_1}_{I} + \\underbrace{w_2^2 \\times \\sigma^2_1}_{II}\n\\small + \\underbrace{w_3^2 \\times \\sigma^3_1}_{III} \\\\\n\\small + \\underbrace{2\\times w_1\\times w_2\\times \\sigma_1\\times\\sigma_2\\times Corr_{12}}_{IV}\n\\small + \\underbrace{2\\times w_1\\times w_3\\times \\sigma_1\\times\\sigma_3\\times Corr_{13}}_{V}\n\\small + \\underbrace{2\\times w_2\\times w_3\\times \\sigma_2\\times\\sigma_3\\times Corr_{23}}_{VI}\n\\]\n\nI, II, and III capture the individual stock variance contribution\nIV captures the relationship between Amazon and Ferrari (as before)\nV captures the relationship between Amazon and VSCO (new!)\nVI captures the relationship between Ferrari and VSCO (new!)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-variance-of-a-3-stock-portfolio-continued-1",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-variance-of-a-3-stock-portfolio-continued-1",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "The variance of a 3-stock portfolio, continued",
    "text": "The variance of a 3-stock portfolio, continued\n\nPlugging in the numbers from our exercise:\n\n\n\\[\n\\small \\underbrace{\\small (0.4^2\\times 0.3290^2)}_{I} + \\underbrace{(0.3^2\\times 0.2124^2)}_{II} + \\underbrace{(0.3^2\\times 0.5861^2)}_{III} \\\\\n\\small + \\underbrace{(2\\times0.4\\times0.3\\times0.3290\\times0.2124\\times0.39)}_{IV} + \\underbrace{(2\\times0.4\\times0.3\\times0.3290\\times 0.5861\\times0.16)}_{V} \\\\\n+\\small \\underbrace{(2\\times0.4\\times0.3\\times0.2124\\times 0.5861\\times0.14)}_{VI}=26.38\\%\n\\]\n\nAlthough VSCO had incurred in significant losses, its returns were minimally correlated to the the stocks"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-variance-of-a-large-portfolio",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-variance-of-a-large-portfolio",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "The variance of a large portfolio",
    "text": "The variance of a large portfolio\n\nWhat if you had a significantly high number \\(N\\) of assets in a portfolio? You can generalize the previous formulas using the definition below:\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe variance of a portfolio \\(P\\) with \\(N\\) stocks with weights \\(w_{1,2,...,N}\\) is:\n\\[\n\\sigma^2_p=\\sum_{i=1}^{N}w_i^2\\sigma_i^2 + 2\\times\\sum_{i=1}^{N}\\sum_{j\\neq i} w_i w_j \\sigma_{ij}\n\\]\n\nWhere:\n\n\\(\\sum_{i=1}^{N}w_i\\sigma_i^2\\) is the individual stock volatility contribution terms; and\n\\(2\\times\\sum_{i=1}^{N}\\sum_{j\\neq i} w_i w_j \\sigma_{ij}\\) represents all the covariance terms from every pairwise combination of stocks in the portfolio"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-large-portfolio---matrix-form",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-large-portfolio---matrix-form",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Volatility of a large portfolio - matrix form",
    "text": "Volatility of a large portfolio - matrix form\n\nFrom our previous slide, we saw that the variance of a portfolio was defined as: \\[\n\\sigma^2_p=\\sum_{i=1}^{N}w_i^2\\sigma_i^2 + 2\\times\\sum_{i=1}^{N}\\sum_{j\\neq i} w_i w_j \\sigma_{ij}\n\\]\nDefine \\(\\mathbf{w}\\) as the vector of stock’s weights, \\([w_1,w_2,w_3,...,w_n]\\) and \\(\\Sigma\\) as the covariance matrix of returns. Then, you can rewrite \\(\\sigma^2_p\\) as:\n\n\n\\[\n\\sigma^2_p = \\mathbf{w}'\\Sigma\\mathbf{w}\n\\]\n\nIn practice, writing the variance of a portfolio in matrix terms simplifies a lot of the calculations if you want to perform calculations using Excel with \\(n&gt;2\\)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-large-portfolio---a-graphical-representation",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-large-portfolio---a-graphical-representation",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Volatility of a large portfolio - a graphical representation",
    "text": "Volatility of a large portfolio - a graphical representation\n\n\n\n\n\nDiagonal cells contain variance terms, and the off-diagonal cells contain the covariance terms\nAs \\(N\\) assets increases, the number of terms outside the main diagonal increases more than the main diagonal.\nTherefore, the variance of a well-diversified portfolio is mostly determined by the covariances, and not the individual stock variance terms!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#limits-to-diversification",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#limits-to-diversification",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Limits to diversification",
    "text": "Limits to diversification\n\nHow much of the variance of a portfolio we can eliminate through diversification? To see that, assume that we hold a portfolio with equal weights \\(\\frac{1}{N}\\) to all assets\n\n\nIn the diagonal, we have \\(N\\) boxes with \\(\\dfrac{1}{N^2}\\times \\overline{Var}\\), where \\(\\overline{Var}\\) is the average variance\nOff the diagonal, we have \\(N^2-N\\) boxes with \\(\\dfrac{1}{N^2}\\times \\overline{Cov}\\), where \\(\\overline{Cov}\\) is the average covariance\n\n\nTherefere, you can rearrange terms to get:\n\n\n\\[\n\\small Var(R_p) = \\bigg(N\\times\\dfrac{1}{N^2}\\times \\overline{Var}\\bigg)+ \\bigg[(N^2-N)\\dfrac{1}{N^2}\\times \\overline{Cov}\\bigg]\\\\\n\\small = \\bigg(\\frac{1}{N} \\times \\overline{Var}\\bigg) + \\bigg[\\bigg(1-\\frac{1}{N}\\bigg) \\times \\overline{Cov}\\bigg]\n\\]"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#limits-to-diversification-graphical-interpretation",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#limits-to-diversification-graphical-interpretation",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Limits to diversification, graphical interpretation",
    "text": "Limits to diversification, graphical interpretation"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-portfolios",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-portfolios",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Choosing Portfolios",
    "text": "Choosing Portfolios\n\nNow that we understood how to analyze the performance of single stocks, let’s turn our attention to determine how an investor can analyze a portfolio of assets!\nLet’s start of with the simplest case: create a portfolio with two stocks, Amazon and Ferrari. Previously, we’ve shown that, for the analysis period, we had the following results in terms of risk and return:\n\n\n\n\n\n\n\n\nLet’s create \\(5\\) different portfolios using \\(\\pm20\\%\\) of allocation weights in each asset"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-an-efficient-portfolio-continued",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-an-efficient-portfolio-continued",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Choosing an Efficient Portfolio, continued",
    "text": "Choosing an Efficient Portfolio, continued\n\n\n\n\n\n\n\nWe can plot this in a figure to show all possible risk \\(\\times\\) return combinations"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-an-efficient-portfolio-6-portfolios",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-an-efficient-portfolio-6-portfolios",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Choosing an Efficient Portfolio, 6 portfolios",
    "text": "Choosing an Efficient Portfolio, 6 portfolios"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-an-efficient-portfolio-11-portfolios",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-an-efficient-portfolio-11-portfolios",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Choosing an Efficient Portfolio, 11 portfolios",
    "text": "Choosing an Efficient Portfolio, 11 portfolios"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-an-efficient-portfolio-100-portfolios",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-an-efficient-portfolio-100-portfolios",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Choosing an Efficient Portfolio, >100 portfolios",
    "text": "Choosing an Efficient Portfolio, &gt;100 portfolios"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-an-efficient-portfolio-1",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-an-efficient-portfolio-1",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Choosing an Efficient Portfolio",
    "text": "Choosing an Efficient Portfolio\n\nWhat happens when you continuously increase the number of assets?\n\nIf you add stocks, you improve the frontier - i.e, you are able to create portfolios that span better options in terms of risk and return\nIf you continue adding assets, you will have what is called Efficient Frontier\n\nThe Efficient Frontier is the set of portfolios where:\n\nFor a given level of volatility, you have the highest possible return among all portfolios with the same volalitity level\nFor a given level of return, you have the lowest possible volatility among all portfolios with the same return level\n\nBased on this, is there a single portfolio in which all investors should hold? No! In practice, investors will choose among portfolios based on their specific preferences for risk and return"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-efficient-frontier",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-efficient-frontier",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "The Efficient Frontier",
    "text": "The Efficient Frontier\n\nIn our example, we used only two assets. What happens when we increase the number of potential assets?\nLet’s replicate the same rationale by now investing our money in three possible stocks: Amazon,Ferrari, and VSCO"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#which-of-these-portfolios-are-efficient",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#which-of-these-portfolios-are-efficient",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Which of these portfolios are efficient?",
    "text": "Which of these portfolios are efficient?"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#which-of-these-portfolios-are-efficient-1",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#which-of-these-portfolios-are-efficient-1",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Which of these portfolios are efficient?",
    "text": "Which of these portfolios are efficient?"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-an-efficient-portfolio-2",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-an-efficient-portfolio-2",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Choosing an Efficient Portfolio",
    "text": "Choosing an Efficient Portfolio\n\nWhat happens when you continuously increase the number of assets?\n\nIf you add stocks, you improve the frontier - i.e, you are able to create portfolios that span better options in terms of risk and return\nIf you continue adding assets, you will have what is called Efficient Frontier\n\nThe Efficient Frontier is the set of portfolios where:\n\nFor a given level of volatility, you have the highest possible return among all portfolios with the same volalitity level\nFor a given level of return, you have the lowest possible volatility among all portfolios with the same return level\n\nBased on this, is there a single portfolio in which all investors should hold? No! In practice, investors will choose among portfolios based on their specific preferences for risk and return"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-efficient-frontier-1",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-efficient-frontier-1",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "The Efficient Frontier",
    "text": "The Efficient Frontier"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-free-saving-and-borrowing-1",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-free-saving-and-borrowing-1",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Risk-Free Saving and Borrowing",
    "text": "Risk-Free Saving and Borrowing\n\nThus far, we have considered the risk and return possibilities that result from combining risky investments into portfolios\nBy including all risky investments in the construction of the efficient frontier, we achieve the maximum diversification possible with risky assets\nIn practice, however, we have the existence of risk-free assets, such as Treasury Bills, Treasury Bonds, Tesouro Direto etc\nUp to this point, we assumed that all the available assets had a minimum level of risk - for example, stocks\nIn what follows, we will investigate what happens when we add the possibility of investing in a risk-free asset\nAs you’ll see, when you add the risk-free asset, the implication is that there should only be only one efficient portfolio!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-free-saving-and-borrowing-2",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-free-saving-and-borrowing-2",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Risk-Free Saving and Borrowing",
    "text": "Risk-Free Saving and Borrowing\n\nAssume that you have a risk-free asset, such as a Treasury Bill, where the return is \\(R_f\\) and, by definition, has no risk\nYou decided to invest a portion \\(x\\) of your holdings in the risky portfolio, and the remaining \\(1-x\\) in the risk-free asset\nThe expected return from your new portfolio, which we’ll call \\(R_{C}\\), is:\n\n\n\\[E[R_{C}] =  x \\times E[R_p] + (1-x) \\times R_f \\]\n\nRearranging terms, we have that:\n\n\n\n\\[\nE[R_{C}] =  x \\times E[R_p] + R_f - x \\times R_f\\rightarrow R_f + x \\times ( E[R_p] - R_f )\n\\]"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-free-saving-and-borrowing-continued",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-free-saving-and-borrowing-continued",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Risk-Free Saving and Borrowing, continued",
    "text": "Risk-Free Saving and Borrowing, continued\n\nTherefore, the expected return of a portfolio that contains risky assets and a risk-free asset is defined as:\n\n\n\\[\nE[R_{C}] =  R_f + x \\times ( E[R_p] - R_f )\n\\]\n\nThis equation shows that the expected return of your portfolio is the sum of:\n\nThe risk-free rate; and\nA fraction of the portfolio’s risk premium, \\(E[R_p] - R_f\\), based on the fraction \\(x\\) that we invest in it\nA risk-premium is nothing more than the reward for bearing additional risk\n\nIn other words, if you increase \\(x\\), your expected gains should increase because you’re more exposed to risk!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-free-saving-and-borrowing-continued-1",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-free-saving-and-borrowing-continued-1",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Risk-Free Saving and Borrowing, continued",
    "text": "Risk-Free Saving and Borrowing, continued\n\nWhat happens to the volatility of a portfolio when you add the possibility of a risk-free asset?\nRemember that the risk free rate is assumed to have no risk, and therefore its variance is zero. As a consequence, the standard deviation of your portfolio that contains risky and risk-free assets is:\n\n\n\\[\\small \\sigma_{R_{C}} = \\sqrt{(1-x)^2 \\times \\sigma^2_{R_f} + x^2 \\times \\sigma^2_{R_p}  + 2 \\times(1-x) \\times x \\times Cov(R_f, R_p)}\\]\n\nBecause \\(\\small \\sigma^2_{R_f}=0\\), this simplifies to:\n\n\n\n\\[\\sigma_{R_{C}} = \\sqrt{x^2 \\times \\sigma^2_{R_p}}\\rightarrow x \\times \\sigma_{R_p}\\]\n\nThat is, the volatility is only a fraction of the volatility of the portfolio, based on the amount we invest in it"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-free-combinations-in-practice",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-free-combinations-in-practice",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Risk-free combinations in practice",
    "text": "Risk-free combinations in practice\n\nIn order to make this point clear, let’s go back to the efficient frontier built on top of the \\(&gt;100\\) portfolios created using AMZN and RACE\nTo facilitate the comparison, we’ll convert risk and return to annual terms\nIf that is true, then the minimum-variance portfolio has:\n\nAnnual Return \\(\\approx 3.82\\%\\)\nAnnual Volatility \\(\\approx 3.24\\%\\)\n\nSuppose you have an risk-free investment opportunity, \\(R_f\\), that generates a risk-free return of 2%\nIf that is true, we can think about all linear combinations of \\([R_f,R_p]\\) in which the returns are a weighted average of \\(R_f\\) and \\(R_p\\) and the volatility is only a fraction of \\(R_p\\)’s volatility"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-free-combinations-graphical-interpretation",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-free-combinations-graphical-interpretation",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Risk-free combinations, graphical interpretation",
    "text": "Risk-free combinations, graphical interpretation"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-free-saving-and-borrowing-3",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-free-saving-and-borrowing-3",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Risk-Free Saving and Borrowing",
    "text": "Risk-Free Saving and Borrowing\n\nRemember that you can have short positions (“buying stocks on margin”) by:\n\nBorrowing at \\(R_f\\) to invest in a portfolio of risky assets\nInvesting all your funds + the money you borrow in the portolio \\(P\\)\n\nIf that is true, then you will have a negative weight in one asset (a short position) and a positive and &gt;1 weight in the other asset\nIf, as we’d imagine, \\(R_p&gt;R_f\\) (i.e, the return of the minimum-variance portfolio is higher than \\(R_f\\), we could proceed by:\n\nBorrowing money at \\(R_f\\)\nUsing this money to buy the minimum-variance portfolio, yielding a return of \\(R_p\\)\n\nIn practice, because your return is a weighted average of \\(R_f\\) and \\(R_p\\) and \\(R_p&gt;R_f\\), the linear combination line “extends” to the right as you are placing a higher weight on the asset with higher return"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#buying-stocks-on-margin",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#buying-stocks-on-margin",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Buying stocks on margin",
    "text": "Buying stocks on margin"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#selecting-the-portfolio",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#selecting-the-portfolio",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Selecting the portfolio",
    "text": "Selecting the portfolio\n\nAs before, is there a single portfolio in which all investors should hold? No! In practice, investors will choose among portfolios based on their specific preferences for risk and return\nBut you may have noticed something strange…the linear combination of \\(R_f\\) and \\(R_p\\) yields a sub-optimal portolio:\n\nThere are many portfolios that yield higher returns than the minimum-variance portfolio (and are also riskier)\nHowever, if we want to have a lower risk, we can simply combine them with the risk-free asset, \\(R_f\\), and the volatility of the portfolio will be only a fraction of \\(\\sigma_P\\)\n\nIn other words, we could be better off if we had done a linear combination using a portfolio that yields a better risk \\(\\times\\) return relationship!\nHow can we find such a portfolio? We need to think about the point where \\(\\small \\partial R_c/\\partial\\sigma_p\\) is the highest - in graphical terms, the portfolio with the highest slope!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#buying-stocks-on-margin-1",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#buying-stocks-on-margin-1",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Buying stocks on margin",
    "text": "Buying stocks on margin"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#sharpe-ratio-and-the-tangent-portfolio",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#sharpe-ratio-and-the-tangent-portfolio",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Sharpe-ratio and the tangent portfolio",
    "text": "Sharpe-ratio and the tangent portfolio\n\nThe ideal portfolio that we want to combine with the risk-free asset is called the tangent portfolio:\n\nIt is tangent to the efficient frontier\nBecause of that, it has the highest risk \\(\\times\\) return combination\n\nTo identify the tangent portfolio, we compute the Sharpe Ratio:\n\n\n\\[\\text{Sharpe Ratio} = \\frac{E[R_p]-R_f}{\\sigma_{R_p}}\\]\n\nIn the Sharpe Ratio measures the ratio of reward-to-volatility provided by a portfolio:\n\n\nIt indicates the amount of excess returns a portfolio provides a “risk-adjusted” format\nHigher Sharpe Ratio \\(\\rightarrow\\) better risk \\(\\times\\) return relationship"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-use-of-the-sharpe-ratio",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-use-of-the-sharpe-ratio",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "The use of the Sharpe Ratio",
    "text": "The use of the Sharpe Ratio\n\nWhat is the optimal portfolio to combine with the risk-free asset? The one with the highest Sharpe Ratio! This portfolio is the one where the line with the risk-free investment is tangent to the efficient frontier of risky investments. There are two important facts about it:\n\n\nThe tangent portfolio is efficient.\nOnce we include the risk-free investment, all efficient portfolios are combinations of the risk-free investment and the tangent portfolio.\n\n\nTherefore, all investors should have the tangent portfolio. All investors should combine the tangent portfolio with the risk free asset to adjust the level of risk.\nIf you ignore the risk free asset, you have several efficient portfolios (efficient frontier). But once you combine with the risk free rate, there is only one!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-efficient-portfolio-and-required-returns-1",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-efficient-portfolio-and-required-returns-1",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "The Efficient Portfolio and Required Returns",
    "text": "The Efficient Portfolio and Required Returns\n\nSuppose you hold a portfolio \\(P\\). How do you decide whether to include a new asset? In sum, you should include a new asset \\(i\\) in a portfolio if it increases the Sharpe Ratio of the resulting portfolio! Note that the new asset has the following properties:\n\nThe excess return that this asset \\(i\\) brings is \\(E[R_i] - R_f\\)\nOn the other hand, the risk that this asset \\(i\\) brings to your portfolio is \\(\\sigma_i \\times corr(R_i,R_p)\\) - see the Appendix for a detailed explanation\n\nTherefore, is the gain in excess return from investing in \\(i\\) sufficient to make up for the increase in risk? Think about this as your average grade in the university:\n\nIf you take a test and obtains a score that is higher than your average grade \\(\\rightarrow\\) your average grade will go up\nIf, on the other hand, your score is lower than your average \\(\\rightarrow\\) your average will go down\nFinally, if your score is exactly your average grade, your average grade remains unchanged"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#efficient-portfolio-and-required-returns-continued",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#efficient-portfolio-and-required-returns-continued",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Efficient Portfolio and Required Returns, continued",
    "text": "Efficient Portfolio and Required Returns, continued\n\nGoing back to our portfolio: if you compute the Sharpe Ratio of \\(i\\) and \\(P\\), including \\(i\\) is beneficial if and only if:\n\n\n\\[\\underbrace{\\frac{E[R_i] - R_f}{\\sigma_{R_i} \\times corr(R_i,R_p)}}_{\\text{Sharpe Ratio of } i} &gt; \\underbrace{\\frac{E[R_p] - R_f}{\\sigma_{R_p}}}_{\\text{Sharpe Ratio of } P}\\]\n\nMoving the denominator to the right-hand side, we have:\n\n\n\n\\[E[R_i] - R_f &gt;\\sigma_{R_i} \\times corr(R_i,R_p) \\times \\frac{E[R_p] - R_f}{\\sigma_{R_p}}\\]"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#efficient-portfolios-and-required-returns-continued",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#efficient-portfolios-and-required-returns-continued",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Efficient Portfolios and Required Returns, continued",
    "text": "Efficient Portfolios and Required Returns, continued\n\\[E[R_i] - R_f &gt; \\underbrace{\\frac{\\sigma_{R_i} \\times corr(R_i,R_p)}{\\sigma_{R_p}}}_{\\beta^P_i}  \\times (E[R_p] - R_f)\\]\n\nUsing a \\(\\beta\\) notation and moving \\(R_f\\) to the right-hand-side:\n\n\n\\[\nE[R_i]  &gt;  R_f + \\beta_i^P  \\times (E[R_p] - R_f)\n\\]"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#efficient-portfolio-and-required-returns-continued-1",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#efficient-portfolio-and-required-returns-continued-1",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Efficient Portfolio and Required Returns, continued",
    "text": "Efficient Portfolio and Required Returns, continued\n\nTo conclude, increasing the amount invested in \\(i\\) will increase the Sharpe Ratio of portfolio \\(P\\) if its expected return \\(E[R_i]\\) exceeds its required return given portfolio \\(P\\), defined as:\n\n\n\\[\nE[R_i]  &gt;  R_f + \\beta_i^P  \\times (E[R_p] - R_f)\n\\]\n\nThe required return is the expected return that is necessary to compensate for the risk investment \\(i\\) will contribute to the portfolio.\n\n\nIt is is equal to the risk-free interest rate…\n…plus the risk premium of the current portfolio, P…\n… scaled by \\(i\\)’s sensitivity to \\(P\\), denoted by \\(\\beta_i^P\\)\n\n\nIf \\(i\\) expected return exceeds this required return, then adding more of it will improve the performance of the portfolio!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#efficient-port.-and-required-returns-conclusion",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#efficient-port.-and-required-returns-conclusion",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Efficient Port. and Required Returns, conclusion",
    "text": "Efficient Port. and Required Returns, conclusion\n\nWe saw what an efficient portfolio is and how to find it in terms of the Sharpe Ratio\nBased on this notion, this equation establishes the relation between an investment’s risk and its expected return:\n\n\n\\[R_i =  R_f + \\beta_i^P  \\times (E[R_p] - R_f)\\]\n\nIt states that we can determine the appropriate risk premium for an investment based on its \\(\\beta\\) with the efficient portfolio\nIn such a way, it enables us to “price” the required returns for investing in any asset based on the amount of required returns that are needed to improve the performance of an efficient portfolio"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#practice-2",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#practice-2",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Practice",
    "text": "Practice\n\n\n\n\n\n\n\nImportant\n\n\nPractice using the following links:\n\nMultiple-choice Questions\nNumeric Questions"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#proof---variance-of-a-sum-of-two-random-variables",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#proof---variance-of-a-sum-of-two-random-variables",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Proof - Variance of a sum of two random variables",
    "text": "Proof - Variance of a sum of two random variables\n\n\n\n\n\n\nTheorem I: the variance of the sum of two random variables equals the sum of the variances of those random variables, plus two times their covariance:\n\n\n\\[\n\\small\n\\sigma^2(A+B) = \\sigma^2_A + \\sigma^2_B + 2\\times Cov(A,B)\n\\] Proof: variance is defined as:\n\\[\n\\small Var(X)=E[(X-E(X))]^2\n\\] Therefore, if \\(X=A+B\\), with \\(A\\) and \\(B\\) being two random variables:\n\\[\n\\small  Var(X)=Var(A+B) = E[((A+B)-E(A+B))]^2 = E[(\\underbrace{[A-E(A)]}_{\\text{First Term}}+[\\underbrace{B-E(B)}_{\\text{Second Term}}])^2] \\\\\n\\] Which is now in the form \\(\\small (A+B)^2=A^2+B^2+2AB\\). Using the fact that \\(E(\\cdot)\\) is a linear operator, we can apply it to each of the terms:\n\\[\n\\small\n= \\underbrace{E([A-E(A)]^2}_{\\sigma^2_A}+\\underbrace{E[B-E(B)]^2}_{\\sigma^2_B}+2\\times\\underbrace{E[A-E(A)][B-E(B)]}_{Cov(A,B)} = \\sigma^2_A + \\sigma^2_B + 2\\times Cov(A,B)\\\\\n\\]"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#proof---variance-multiplied-by-a-scalar",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#proof---variance-multiplied-by-a-scalar",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Proof - Variance multiplied by a scalar",
    "text": "Proof - Variance multiplied by a scalar\n\n\n\n\n\n\nTheorem II: The variance scales upon multiplication with a constant:\n\n\n\\[\n  \\sigma^2(\\beta \\times A ) = \\beta^2\\times \\sigma^2_A\n  \\] Proof: define the variance in terms of \\(\\beta\\) and \\(A\\):\n\\[\n\\sigma^2(\\beta\\times A)=E[\\beta\\times A-E(\\beta\\times A)]^2 \\\\\n\\] Since \\(\\beta\\) is a constant, \\(E[\\beta]=\\beta\\) and we can write:\n\\[\nE[\\beta\\times A - \\beta\\times E(A)]^2= E[\\beta\\times (A-E[A])]^2=\\beta^2\\times \\underbrace{E(A-E[A])]^2}_{\\sigma^2_A}\n\\] Therefore, whenever there is a scale constant multiplying a random variable, it scales the variance: \\(\\beta^2 \\times\\sigma^2_A\\)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#limits-to-diversification-1",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#limits-to-diversification-1",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Limits to diversification",
    "text": "Limits to diversification\n\nHow much of the variance of a portfolio we can eliminate through diversification? To see that, assume that we hold a portfolio with equal weights \\(\\frac{1}{N}\\) to all assets\n\n\nIn the diagonal, we have \\(N\\) boxes with \\(\\dfrac{1}{N^2}\\times \\overline{Var}\\), where \\(\\overline{Var}\\) is the average variance\nOff the diagonal, we have \\(N^2-N\\) boxes with \\(\\dfrac{1}{N^2}\\times \\overline{Cov}\\), where \\(\\overline{Cov}\\) is the average covariance\n\n\nTherefere, you can rearrange terms to get:\n\n\n\\[\n\\small Var(R_p) = \\bigg(N\\times\\dfrac{1}{N^2}\\times \\overline{Var}\\bigg)+ \\bigg[(N^2-N)\\dfrac{1}{N^2}\\times \\overline{Cov}\\bigg]\\\\\n\\small = \\bigg(\\frac{1}{N} \\times \\overline{Var}\\bigg) + \\bigg[\\bigg(1-\\frac{1}{N}\\bigg) \\times \\overline{Cov}\\bigg]\n\\]"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#limits-to-diversification-continued",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#limits-to-diversification-continued",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Limits to diversification, continued",
    "text": "Limits to diversification, continued\n\nWhat happens if \\(N\\rightarrow\\infty\\)? If you look at our equation, you’ll see that only the covariance terms remain:\n\n\n\\[\n\\small \\underset{N\\rightarrow\\infty}{\\small Var(R_p)} = \\bigg(\\frac{1}{\\infty} \\times \\overline{Var}\\bigg) + \\bigg[\\bigg(1-\\frac{1}{\\infty}\\bigg) \\times \\overline{Cov}\\bigg]\\rightarrow \\overline{Cov}\n\\]"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-large-portfolio-with-arbitrary-weights",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-large-portfolio-with-arbitrary-weights",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Volatility of a large portfolio with arbitrary weights",
    "text": "Volatility of a large portfolio with arbitrary weights\n\nWhat if we don’t have an equally-weighted portfolio? As \\(N\\) increases, you can rewrite the variance of the portfolio1 as:\n\n\n\\[\\small \\sigma^2_{R_p} = \\sum_i w_i \\times Cov(R_i,R_p)\\]\n\nThe variance of a portfolio approaches the weighted average covariance of each stock with the portfolio!\nBecause \\(\\small Cov(R_i,R_P) = \\sigma_i \\times \\sigma_p \\times Corr_{i,p}\\) you can also write it as:\n\n\n\n\\[\\sigma^2_{R_p} = \\sum_i x_i \\times \\sigma_{i} \\times \\sigma_{p} \\times Corr_{i,p}\\]\n\nSee Berk and DeMarzo (2023), pg. 409 for a detailed explanation."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-large-portfolio-with-arbitrary-weights-1",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#volatility-of-a-large-portfolio-with-arbitrary-weights-1",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Volatility of a large portfolio with arbitrary weights",
    "text": "Volatility of a large portfolio with arbitrary weights\n\nIf we divide both sides of this equation by \\(\\sigma_{p}\\), we find:\n\n\n\\[\\sigma_{p} = \\sum_i x_i \\times \\sigma_{i}  \\times Corr(R_i,R_p)\\]\n\nWhy this equation is important?\n\nIt shows the amount of risk that each security brings to portfolio\nEach asset \\(i\\) contributes to the portfolio’s volatility according to its standard deviation (\\(\\sigma_i\\)) scaled by its correlation with the portfolio"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#references",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#references",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nJegadeesh, Narasimhan, and Sheridan Titman. 1993. “Returns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency.” The Journal of Finance 48 (1): 65–91. https://doi.org/10.1111/j.1540-6261.1993.tb04702.x.\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#practice",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#practice",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Practice",
    "text": "Practice\n\n\n\n\n\n\n\nImportant\n\n\nPractice using the following links:\n\nMultiple-choice Questions\nNumeric Questions"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-an-efficient-portfolio",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-an-efficient-portfolio",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Choosing an Efficient Portfolio",
    "text": "Choosing an Efficient Portfolio\n\nAs a financial manager, one crucial job you have is to find portfolios that are not sub-optimal\n\nFor a given level of volatility, they deliver the highest possible return\nAlternatively, for a given level of return, they deliver the lowest possible volatility\n\nAn easy what to look at this is to identify the minimum variance portfolio (MVP)\n\nThis portfolio is, among all combinations, the one with the lowest volatility\nFrom there, if a given portfolio is riskier than the MVP, it needs to deliver higher returns!\nOn the other hand, if a portfolio is riskier than the MVP and deliver the same/lower returns, it can be considered inefficient\n\n\n\n\\(\\rightarrow\\) In other words: investors should look only for efficient portfolios and will choose based on his specific preferences for risk!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#analyzing-a-stocks-performance",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#analyzing-a-stocks-performance",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Analyzing a stock’s performance",
    "text": "Analyzing a stock’s performance\n\nFrom our previous lectures, we have already been able to:\n\nEmploy methods for analyzing the performance of individual stocks over time\nCalculate the historical performance of simple combinations of assets, such as equally-weighted portfolios\n\nIn reality, however, portfolio construction is way more dynamic and complex. What if we wanted to analyze the performance of a trading strategy that has a dynamic allocation rule?\nIn this lecture, we will be working with the PerformanceAnalytics and PortfolioAnalytics packages to:\n\n\nQuantify the performance of a momentum strategy in the Brazilian financial markets\nLook at ways for optimizing portfolios based on Markowitz Mean-Variance optimization and its variations"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#momentum-in-the-stock-market",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#momentum-in-the-stock-market",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Momentum in the stock market",
    "text": "Momentum in the stock market\nDo past returns explain future performance? Ideally, that shouldn’t be the case, but…\n\n\n\n\n\n\nDefinition\n\n\nMomentum strategies capitalize on the tendency of assets that have performed well in the past to continue performing well, and vice-versa, based on the idea that market trends persist due to behavioral biases (e.g., herding, overreaction, among others).\n\n\n\n\nPreviously documented in academic research (Jegadeesh and Titman 1993), such rationale can be applied to stocks, bonds, commodities, and currencies, etc\n\nWorks across different asset classes and time periods but subject to periodic crashes (e.g., 2009 momentum crash)\nIt is often used alongside factor models like the Fama-French-Carhart Four-Factor Model, which adds a momentum factor to the Fama-French three-factor model"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#momentum-in-the-stock-market-continue",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#momentum-in-the-stock-market-continue",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Momentum in the stock market, continue",
    "text": "Momentum in the stock market, continue\n\nHow can we implement a momentum strategy? In short, there are some key steps in defining the strategy parameters that needed to be taken into consideration:\n\nDefine the lookback period (e.g., \\(3\\), \\(6\\), or \\(12\\) months) to measure past returns.\nRank assets based on past performance\nConstruct a portfolio that goes long on the top-performing assets and eventually short on the worst performers\nRebalance periodically (e.g., monthly or quarterly) based on a weighting criteria\nCalculate the performance metrics\n\nIn what follows, we will be looking at a step-by-step guide for implementing such strategy using Brazilian stocks through a hands-on exercise"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-performanceanalytics-package",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-performanceanalytics-package",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "The PerformanceAnalytics package",
    "text": "The PerformanceAnalytics package"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#summary-statistics-3-assets",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#summary-statistics-3-assets",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Summary Statistics, 3 assets",
    "text": "Summary Statistics, 3 assets\n\nCodeOutput\n\n\n\nportfolio%&gt;%as.xts()%&gt;%charts.RollingPerformance()"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#chart-sharpe-over-time-3-assets",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#chart-sharpe-over-time-3-assets",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Chart Sharpe over Time, 3 assets",
    "text": "Chart Sharpe over Time, 3 assets"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-measures-var-and-expected-shortfall-1-asset",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#risk-measures-var-and-expected-shortfall-1-asset",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Risk Measures: VaR and Expected Shortfall, 1 asset",
    "text": "Risk Measures: VaR and Expected Shortfall, 1 asset"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#maximum-drawdown-3-assets",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#maximum-drawdown-3-assets",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Maximum Drawdown, 3 assets",
    "text": "Maximum Drawdown, 3 assets"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#return-distribution-1-asset",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#return-distribution-1-asset",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Return Distribution, 1 asset",
    "text": "Return Distribution, 1 asset"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#challenge-creating-reports",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#challenge-creating-reports",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Challenge: creating reports",
    "text": "Challenge: creating reports\n\nYou work as a quantitative analyst at Axe Capital. You have been given the task of analyzing a couple of Hedge Fund strategies and assess whether they have generated true excess returns that could have been attributed to their manager’s skill:\n\nWhat is the historical performance of each strategy over time?\nWhich strategies, according to the CAPM model, have generated \\(\\alpha&gt;0\\)?\nWhich strategies, according to the Fama-French model, have generated \\(\\alpha&gt;0\\)?\n\n\n\n\n\n\n\n\nSpecific Instructions\n\n\n\nYou will be using some of the data contained in the edhec dataset - click here for a detailed explanation on the dataset.\nModel estimation should be done at the monthly level"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-1-collecting-data",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-1-collecting-data",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Step 1: Collecting Data",
    "text": "Step 1: Collecting Data\n\nCodeOutput\n\n\n\nYou can use the previous tq_get() function from the tidyquant package to collect and retrieve the data from all tickers during the study period, already in tidy format for you to manipulate:\n\n\n# Define a list of Brazilian stocks (tickers)\nbr_stocks &lt;- c(\"RAIZ4.SA\", \"ITUB3.SA\", \"IRBR3.SA\", \"BBDC4.SA\", \"ABEV3.SA\", \n               \"YDUQ3.SA\", \"BBAS3.SA\", \"B3SA3.SA\", \"WEGE3.SA\", \"RADL3.SA\", \n               \"LREN3.SA\", \"BRFS3.SA\", \"CSAN3.SA\", \"HAPV3.SA\", \"SUZB3.SA\", \n               \"GRND3.SA\", \"MGLU3.SA\", \"BEEF3.SA\", \"EGIE3.SA\", \"HYPE3.SA\")\n\n# Get stock price data\nprices &lt;- tq_get(br_stocks, from = \"2018-01-01\", to = \"2024-01-01\")\n\n\n\n\n\n# A tibble: 28,773 × 8\n   symbol   date        open  high   low close   volume adjusted\n   &lt;chr&gt;    &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 RAIZ4.SA 2021-08-05  7.48  7.60  7.18  7.24 98849900     6.38\n 2 RAIZ4.SA 2021-08-06  7.25  7.35  7.03  7.10 28799900     6.26\n 3 RAIZ4.SA 2021-08-09  7.18  7.31  7.07  7.07 14491100     6.23\n 4 RAIZ4.SA 2021-08-10  7.12  7.16  7.05  7.10  9988600     6.26\n 5 RAIZ4.SA 2021-08-11  7.13  7.14  6.80  6.87 31978000     6.06\n 6 RAIZ4.SA 2021-08-12  6.97  6.98  6.75  6.96 17054100     6.14\n 7 RAIZ4.SA 2021-08-13  6.98  7.04  6.86  7.01 21141500     6.18\n 8 RAIZ4.SA 2021-08-16  6.97  7.12  6.78  7.02 13434600     6.19\n 9 RAIZ4.SA 2021-08-17  6.93  6.99  6.76  6.82 17772000     6.01\n10 RAIZ4.SA 2021-08-18  6.83  7     6.72  6.81  8841700     6.01\n# ℹ 28,763 more rows"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-2-historical-returns",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-2-historical-returns",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Step 2: Historical Returns",
    "text": "Step 2: Historical Returns"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-3-trading-signals",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-3-trading-signals",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Step 3: Trading Signals",
    "text": "Step 3: Trading Signals"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-4-reweighting",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-4-reweighting",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Step 4: Reweighting",
    "text": "Step 4: Reweighting"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-5-portfolio-performance",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-5-portfolio-performance",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Step 5: Portfolio Performance",
    "text": "Step 5: Portfolio Performance"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-2-define-the-trading-signals",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-2-define-the-trading-signals",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Step 2: Define the Trading Signals",
    "text": "Step 2: Define the Trading Signals\n\nAfter collecting the data, we need to make sure that our momentum strategy accurately creates the trading signals and use them at an appropriate timestamp:\n\nWe use the lag() function to calculate, for each symbol, the adjusted trading price back in \\(\\small 60\\) days\nAfter that, we calculate the returns - in this case, we will be using log-returns\nWith the 3-month returns properly calculated, we rank stocks in each date from highest-to-lowest returns, keeping only the five highest returns for each given date\nFinally, we create a new date column that stores the week where we will be evaluating the returns from investing in the top 5 stocks"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-3-compute-returns",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-3-compute-returns",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Step 3: Compute Returns",
    "text": "Step 3: Compute Returns\n\nCodeOutput\n\n\n\n# Compute monthly returns\nreturns &lt;- prices%&gt;%\n  group_by(symbol)%&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = monthlyReturn,\n               col_rename = 'monthly_return')%&gt;%\n  mutate(date = floor_date(date, \"month\"))%&gt;%\n  select(symbol, date, monthly_return)\n\n\n\n\n\n# A tibble: 10 × 3\n# Groups:   symbol [1]\n   symbol   date       monthly_return\n   &lt;chr&gt;    &lt;date&gt;              &lt;dbl&gt;\n 1 RAIZ4.SA 2021-08-01      -0.0193  \n 2 RAIZ4.SA 2021-09-01       0       \n 3 RAIZ4.SA 2021-10-01      -0.0429  \n 4 RAIZ4.SA 2021-11-01      -0.195   \n 5 RAIZ4.SA 2021-12-01       0.181   \n 6 RAIZ4.SA 2022-01-01      -0.000853\n 7 RAIZ4.SA 2022-02-01      -0.111   \n 8 RAIZ4.SA 2022-03-01       0.231   \n 9 RAIZ4.SA 2022-04-01      -0.0146  \n10 RAIZ4.SA 2022-05-01      -0.112"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-4-portfolio-performance",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-4-portfolio-performance",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Step 4: Portfolio Performance",
    "text": "Step 4: Portfolio Performance\n\nFinally, it is time to find out how did your strategy perform over time\n\nFor that, you will match returns to ranks for each symbol and date combination\nBecause we want to make sure that we are only looking at the specific breakpoint dates for the trading strategy, we use the inner_join() function\n\nWith that, we are able to collect, for each beginning-of-month, what was the return from the strategy up to that point!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#maximizer-function",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#maximizer-function",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Maximizer Function",
    "text": "Maximizer Function\n\n\nCode\n# Objective function to maximize Sharpe Ratio\nmomentum_strategy &lt;- function(params) {\n  lookback &lt;- round(params[1])  # Ensure integer\n  holding &lt;- round(params[2])\n  top_n &lt;- round(params[3])\n  \n  if (lookback &lt; 30 | holding &lt; 1 | top_n &lt; 2) return(-Inf)  # Prevent invalid params\n  \n  # Compute momentum rankings\n  ranks &lt;- prices%&gt;%\n    group_by(symbol)%&gt;%\n    arrange(symbol, date)%&gt;%\n    mutate(lagged_price = lag(adjusted, lookback))%&gt;%\n    mutate(momentum_return = log(adjusted / lagged_price))%&gt;%\n    drop_na()%&gt;%\n    ungroup()%&gt;%\n    mutate(rank_date = floor_date(date, \"month\"))%&gt;%\n    group_by(rank_date)%&gt;%\n    mutate(rank = rank(desc(momentum_return)))%&gt;%\n    filter(rank &lt;= top_n)%&gt;%\n    mutate(date = rank_date %m+% months(holding))%&gt;%  # Apply ranking next month\n    select(symbol, date, rank)\n  \n  # Compute monthly returns\n  returns &lt;- prices%&gt;%\n    group_by(symbol)%&gt;%\n    tq_transmute(select = adjusted,\n                 mutate_fun = monthlyReturn,\n                 col_rename = 'monthly_return')%&gt;%\n    mutate(date = floor_date(date, \"month\"))%&gt;%\n    select(symbol, date, monthly_return)\n  \n  # Merge ranking with next period's returns\n  strategy &lt;- inner_join(returns, ranks, by = c(\"symbol\", \"date\"))%&gt;%\n    arrange(date, rank)\n  \n  # Compute portfolio returns\n  portfolio &lt;- strategy%&gt;%\n    group_by(date)%&gt;%\n    summarize(portfolio_return = mean(monthly_return, na.rm = TRUE))\n  \n  # Compute Sharpe ratio\n  avg_return &lt;- mean(portfolio$portfolio_return, na.rm = TRUE)\n  sd_return &lt;- sd(portfolio$portfolio_return, na.rm = TRUE)\n  sharpe &lt;- avg_return / sd_return\n  \n  \n  # Store results for each combination\n  optimization_results &lt;&lt;- rbind(optimization_results, \n                                 data.frame(lookback = lookback,\n                                            holding = holding,\n                                            top_n = top_n,\n                                            sharpe_ratio = sharpe))\n  \n  return(-sharpe)  # Minimize negative Sharpe\n}"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#run-optimization",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#run-optimization",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Run Optimization",
    "text": "Run Optimization\n\nCodeOutput\n\n\n\noptimization_results=data.frame()\n\n# Run optimization\nset.seed(123)\nopt_result &lt;- DEoptim(\n  momentum_strategy,\n  lower = c(30, 1, 1),  # Min values for lookback, holding, top_n\n  upper = c(90, 5, 5), # Max values\n  DEoptim.control(itermax = 50)  # Number of iterations\n)\n\n# Print optimal parameters\nopt_params &lt;- opt_result$optim$bestmem\ncat(\"Optimal lookback:\", round(opt_params[1]), \"\\n\")\ncat(\"Optimal holding:\", round(opt_params[2]), \"\\n\")\ncat(\"Optimal top_n:\", round(opt_params[3]), \"\\n\")\n\n\n\n\n\nOptimal lookback: 69 \n\n\nOptimal holding: 3 \n\n\nOptimal top_n: 1"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#output-results",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#output-results",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Output Results",
    "text": "Output Results\n\nCodeOutput\n\n\n\n# Create a heatmap of Sharpe ratios\nggplot(optimization_results, aes(x = factor(lookback), y = factor(top_n), fill = sharpe_ratio)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"blue\", high = \"red\") +\n  labs(x = \"Lookback Period (Days)\", \n       y = \"Top N Stocks\", \n       fill = \"Sharpe Ratio\") +\n  theme_minimal() +\n  ggtitle(\"Heatmap of Sharpe Ratios by Lookback Period and Top N Stocks\")"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-efficient-portfolios",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#choosing-efficient-portfolios",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Choosing Efficient Portfolios",
    "text": "Choosing Efficient Portfolios\n\nNow that we understood how to analyze the performance of single stocks, let’s turn our attention to determine how an investor can analyze a portfolio of assets!\nLet’s start of with the simplest case: create a portfolio with two stocks, Amazon and Ferrari. Previously, we’ve shown that, for the analysis period, we had the following results in terms of risk and return:\n\n\n\n\n\n\n\n\nLet’s create \\(5\\) different portfolios using \\(\\pm20\\%\\) of allocation weights in each asset"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-2-define-the-trading-signals-1",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-2-define-the-trading-signals-1",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Step 2: Define the Trading Signals",
    "text": "Step 2: Define the Trading Signals\n\nCodeOutput\n\n\n\n# Create the rankings\nranks &lt;- prices%&gt;%\n  #Select relevant columns\n  select(symbol, date, adjusted)%&gt;%\n  #Rename price for conciseness\n  rename(price = adjusted)%&gt;%\n  #Group by symbol\n  group_by(symbol)%&gt;%\n  #Reorder by symbol and dat\n  arrange(symbol, date)%&gt;%\n  #Get the price from the last 60th trading day (roughly a 3-month lag)\n  mutate(lagged_price = lag(price, 60))%&gt;%  \n  mutate(momentum_return = log(price/lagged_price))%&gt;% #Using continuously compounded returns\n  drop_na()%&gt;% #Remove any NAs\n  group_by(year(date),month(date),symbol)%&gt;%\n  slice_head(n=1)%&gt;%\n  mutate(rank_date = floor_date(date, \"month\"))%&gt;% # Set the date where we will apply the momentum strategy \n  group_by(rank_date)%&gt;%\n  mutate(rank = rank(desc(momentum_return)))%&gt;%\n  filter(rank &lt;= 5)%&gt;% #Get only the top 5 stocks in terms of momentum\n  mutate(date = rank_date + months(1))%&gt;% # Apply ranking next month\n  ungroup()%&gt;%\n  select(rank,date,symbol)\n\n\n\n\n\n# A tibble: 10 × 3\n    rank date       symbol  \n   &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;   \n 1     1 2018-04-01 SUZB3.SA\n 2     2 2018-04-01 BBAS3.SA\n 3     3 2018-04-01 IRBR3.SA\n 4     4 2018-04-01 MGLU3.SA\n 5     5 2018-04-01 ITUB3.SA\n 6     1 2018-05-01 SUZB3.SA\n 7     2 2018-05-01 MGLU3.SA\n 8     3 2018-05-01 IRBR3.SA\n 9     4 2018-05-01 BBAS3.SA\n10     5 2018-05-01 ITUB3.SA"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-4-portfolio-performance-1",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#step-4-portfolio-performance-1",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Step 4: Portfolio Performance",
    "text": "Step 4: Portfolio Performance\n\nCodeOutput\n\n\n\n# Merge momentum ranking with next month’s returns\nstrategy_LO &lt;- inner_join(returns, ranks, by = c(\"symbol\", \"date\"))%&gt;%\n  arrange(date, rank)%&gt;%\n  group_by(date)%&gt;%\n  summarize(portfolio_return = mean(monthly_return, na.rm = TRUE))\n\n# Compute cumulative returns\nstrategy_LO%&gt;%\n  mutate(cumulative_return = cumprod(1 + portfolio_return) - 1)%&gt;%\n# Plot results\nggplot(aes(x = date, y = cumulative_return)) +\n    geom_line(color = \"blue\",size=2) +\n    geom_hline(yintercept=0,linetype='dashed')+\n    labs(title = \"Long-biased Momentum Portfolio Performance\",\n         subtitle = 'Selecting the top 5 stocks in terms of past returns, lookback period of 60 trading days, and monthly rebalancing',\n         y = \"Cumulative Returns\",\n         x = '')+\n    #Scales\n    scale_y_continuous(labels = percent,breaks=seq(-0.5,1.5,0.1))+\n    scale_x_date(date_breaks = '3 months')+\n    #Custom 'TidyQuant' theme\n    theme_minimal()+\n    #Adding further customizations\n    theme(legend.position='none',\n          axis.title = element_text(face='bold',size=15),\n          axis.text = element_text(size=10),\n          axis.text.x = element_text(angle=90),\n          plot.title = element_text(size=20,face='bold'),\n          plot.subtitle  = element_text(size=15))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#assessing-portfolio-performance",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#assessing-portfolio-performance",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Assessing portfolio performance",
    "text": "Assessing portfolio performance\n\nThere is so much you can do with the PerformanceAnalytics package - convert your newly created strategy_* objects into an .xts object by calling portfolio%&gt;%as.xts() and experiment using these pre-built functions for assessing historical performance:\n\ncharts.RollingPerformance()\ncharts.PerformanceSummary()\nchart.Histogram()\ntable.AnnualizedReturns()\n\nPlay around with changing the parameters of the strategy and document the effects of such changes!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#going-long-and-short",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#going-long-and-short",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Going long and short",
    "text": "Going long and short\n\nOur previous momentum strategy was defined as a long-biased strategy: we only selected the top 5 stocks in terms of past returns and built a long position\nA potential risk in this strategy is that, even if those performed well in the past, there is a downward trend in the market\nWe can add a little bit more complexity to our momentum based strategy by allowing long and short positions at the same time:\n\nCalculate the rolling 3-month returns as before\nSelect the top and worst performing firms in each period\nBuild a long position on the top firms, and a short position in the worst firms\nRecalculate metrics\n\nIn what follows, we will do small tweaks in our ranking definition so as to employ a long-short momentum strategy"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#redefine-the-trading-signals",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#redefine-the-trading-signals",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "(Re)define the Trading Signals",
    "text": "(Re)define the Trading Signals\n\nCodeOutput\n\n\n\n# Create the rankings\nranks &lt;- prices%&gt;%\n  #Select relevant columns\n  select(symbol, date, adjusted)%&gt;%\n  #Rename price for conciseness\n  rename(price = adjusted)%&gt;%\n  #Group by symbol\n  group_by(symbol)%&gt;%\n  #Reorder by symbol and dat\n  arrange(symbol, date)%&gt;%\n  #Get the price from the last 60th trading day (roughly a 3-month lag)\n  mutate(lagged_price = lag(price, 60))%&gt;%  \n  mutate(momentum_return = log(price/lagged_price))%&gt;% #Using continuously compounded returns\n  drop_na()%&gt;% #Remove any NAs\n  group_by(year(date),month(date),symbol)%&gt;%\n  slice_head(n=1)%&gt;%\n  mutate(rank_date = floor_date(date, \"month\"))%&gt;% # Set the date where we will apply the momentum strategy \n  group_by(rank_date)%&gt;%\n  mutate(rank = rank(desc(momentum_return)))%&gt;%\n  filter(rank %in% c(1:5,15:20))%&gt;% #Get only the top 5 stocks in terms of momentum\n  mutate(date = rank_date + months(1))%&gt;% # Apply ranking next month\n  ungroup()%&gt;%\n  select(symbol, date, rank)\n\n\n\n\n\n# A tibble: 10 × 3\n   symbol   date        rank\n   &lt;chr&gt;    &lt;date&gt;     &lt;dbl&gt;\n 1 SUZB3.SA 2018-04-01     1\n 2 BBAS3.SA 2018-04-01     2\n 3 IRBR3.SA 2018-04-01     3\n 4 MGLU3.SA 2018-04-01     4\n 5 ITUB3.SA 2018-04-01     5\n 6 WEGE3.SA 2018-04-01    15\n 7 RADL3.SA 2018-04-01    16\n 8 BEEF3.SA 2018-04-01    17\n 9 BRFS3.SA 2018-04-01    18\n10 SUZB3.SA 2018-05-01     1"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#setting-up-a-long-and-short-momentum-strategy",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#setting-up-a-long-and-short-momentum-strategy",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Setting up a long and short momentum strategy",
    "text": "Setting up a long and short momentum strategy\n\nCodeOutput\n\n\n\nstrategy_LS &lt;- inner_join(returns, ranks, by = c(\"symbol\", \"date\"))%&gt;%\n  arrange(date, rank)%&gt;%\n  mutate(monthly_return=ifelse(rank&gt;=15,-1*monthly_return,monthly_return))%&gt;%\n  group_by(date)%&gt;%\n  summarize(portfolio_return = mean(monthly_return, na.rm = TRUE))\n\n# Compute cumulative returns\nstrategy_LS%&gt;%\n  mutate(cumulative_return = cumprod(1 + portfolio_return) - 1)%&gt;%\n# Plot results\n  ggplot(aes(x = date, y = cumulative_return)) +\n    geom_line(color = \"red\",size=2) +\n    geom_hline(yintercept=0,linetype='dashed')+\n    labs(title = \"Long-Short Momentum Portfolio Performance\",\n         subtitle = 'Selecting the top/worst 5 stocks in terms of past returns, lookback period of 60 trading days, and monthly rebalancing',\n         y = \"Cumulative Returns\",\n         x = '')+\n    #Scales\n    scale_y_continuous(labels = percent,breaks=seq(-0.5,1.5,0.1))+\n    scale_x_date(date_breaks = '3 months')+\n    #Custom 'TidyQuant' theme\n    theme_minimal()+\n    #Adding further customizations\n    theme(legend.position='none',\n          axis.title = element_text(face='bold',size=15),\n          axis.text = element_text(size=10),\n          axis.text.x = element_text(angle=90),\n          plot.title = element_text(size=20,face='bold'),\n          plot.subtitle  = element_text(size=15))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#putting-all-together",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#putting-all-together",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Putting all together",
    "text": "Putting all together\n\nCodeOutput\n\n\n\nchart_strategy_LO=strategy_LO%&gt;%mutate(Type='Long-biased')\nchart_strategy_LS=strategy_LS%&gt;%mutate(Type='Long-Short')\n\nchart_strategy_LO%&gt;%\n  rbind(chart_strategy_LS)%&gt;%\n  group_by(Type)%&gt;%\n  # Compute cumulative returns\n  mutate(cumulative_return = cumprod(1 + portfolio_return) - 1)%&gt;%\n# Plot results\n  ggplot(aes(x = date, y = cumulative_return,group=Type,col=Type)) +\n    geom_line(size=2) +\n    geom_hline(yintercept=0,linetype='dashed')+\n    labs(title = \"Momentum Strategies\",\n         subtitle = 'Both long-biased and long-short strategies.',\n         y = \"Cumulative Returns\",\n         x = '')+\n    #Scales\n    scale_y_continuous(labels = percent,breaks=seq(-0.5,1.5,0.1))+\n    scale_x_date(date_breaks = '3 months')+\n    #Custom 'TidyQuant' theme\n    theme_minimal()+\n    #Adding further customizations\n    theme(legend.position='bottom',\n          axis.title = element_text(face='bold',size=15),\n          axis.text = element_text(size=10),\n          axis.text.x = element_text(angle=90),\n          plot.title = element_text(size=20,face='bold'),\n          plot.subtitle  = element_text(size=15))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#momentum-in-the-stock-market-continued",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#momentum-in-the-stock-market-continued",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "Momentum in the stock market, continued",
    "text": "Momentum in the stock market, continued\n\nHow can we implement a momentum strategy? In short, there are some key steps in defining the strategy parameters that needed to be taken into consideration:\n\nDefine the lookback period (e.g., \\(3\\), \\(6\\), or \\(12\\) months) to measure past returns.\nRank assets based on past performance\nConstruct a portfolio that goes long on the top-performing assets and eventually short on the worst performers\nRebalance periodically (e.g., monthly or quarterly) based on a weighting criteria\nCalculate the performance metrics\n\nIn what follows, we will be looking at a step-by-step guide for implementing such strategy using Brazilian stocks through a hands-on exercise"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#introduction-to-corporate-finance",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#introduction-to-corporate-finance",
    "title": "Introduction to Corporate Finance",
    "section": "Introduction to Corporate Finance",
    "text": "Introduction to Corporate Finance\nWhat is a Corporation, anyway? Think about any business opportunity, like opening a small pizza vendor facility…\n\nOn the one hand, there is an endless variety of real assets needed to carry on such business\nOn the other hand, these assets simply do not come for free – there has to be some way to acquire them!\n\n\nBroadly speaking, these are:\n\nInvestment Decisions \\(\\rightarrow\\) acquisition of real assets - e.g, oven, machinery, powerplant\nFinancing Decisions: \\(\\rightarrow\\) sale of financial assets - bank financing, equity\n\nNot surprisingly, these decisions are intertwinned:\n\nWhile Investment decisions define what is the set of assets to be organized…\nFinancing decisions define how these assets will be acquired!"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#examples-of-investment-and-financing-decisions",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#examples-of-investment-and-financing-decisions",
    "title": "Introduction to Corporate Finance",
    "section": "Examples of Investment and Financing Decisions",
    "text": "Examples of Investment and Financing Decisions"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#investment-decisions",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#investment-decisions",
    "title": "Introduction to Corporate Finance",
    "section": "Investment Decisions",
    "text": "Investment Decisions\n\nInvestment decisions are generally thought of as a capital investment made today to generate returns in the future\nThe extent to when these returns are expected to happen depends on the specific investment\nExamples of Investment decisions:\n\n\nResearch and Development (R&D) on creating/optimizing new products/services\nMachinery, Property, Pland, and Equipment\nOpening new stores\nAcquiring an operation from a company\nExpanding trade credit to leverage sales\nStock up warehouses with inventory during high seasonality"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#financing-decisions",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#financing-decisions",
    "title": "Introduction to Corporate Finance",
    "section": "Financing Decisions",
    "text": "Financing Decisions\n\nAfter you decide on what to invest: how are you going to fund it?\nOverall, a firm can raise money by two ways:\n\nA firm can borrow money from a lender \\(\\rightarrow\\) firm receives the money, but now has a promise to pay back the debt plus interest\nAlternatively, the firm can raise money from the shareholders \\(\\rightarrow\\) equity financing\n\nThe choice between the amount of equity vs is often referred to Capital Structure decisions\nWithin each type of financing, there are specific decisions that increase the complexity:\n\n\nWould the firm be better-off by having a 1-year or 20-year financing? Should it include collateral or not? Fixed or variable interest rate?\nFor equity financing, should the firm issue new equity or reinvest profits into its operations?\n\n\nQuestion: between investment and financing, which one is the most important?"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#managing-investment-and-financing-decisions",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#managing-investment-and-financing-decisions",
    "title": "Introduction to Corporate Finance",
    "section": "Managing investment and financing decisions",
    "text": "Managing investment and financing decisions\n\nWe saw that any business opportunity must be accompained by an investment and a financing decision\nManaging these decisions is a hard task, especially if you have to conduct the front-end of the business altogether\nTo this point, there has to be someone to organize these flows on behalf of the firm…\n\n\n\n\n\nInvestment and Financing Decisions (BMA?)"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#the-role-of-the-financial-manager",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#the-role-of-the-financial-manager",
    "title": "Introduction to Corporate Finance",
    "section": "The role of the Financial Manager",
    "text": "The role of the Financial Manager\n\n\n\n\nInvestment and Financing Decisions (BMA?)\n\n\n\nA Financial Manager1 stands between the firm and outside investors:\n\nOn the one hand, it helps managing the firm’s operations, particularly by helping to make good investment decisions\nOn the other hand, it deals with investors such as shareholders and financial institutions\n\n\n\nFollowing (BMA?), we use this term to refer to anyone (or a group of people) responsible for an investment or financing decision."
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#the-investment-trade-off",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#the-investment-trade-off",
    "title": "Introduction to Corporate Finance",
    "section": "The Investment Trade-off",
    "text": "The Investment Trade-off\n\n\n\n\nInvestment and Financing Decisions (BMA?)\n\n\n\nSuppose that firm has a proposed investment project (a real asset) and has cash on hand to finance the project\nThe Financial Manager has to decide whether to invest in the project:\n\nIf investing, cash goes to fund the project\nIf not investing, the firm can then pay out the cash to shareholders as an extra-dividend"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#the-investment-trade-off-continued",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#the-investment-trade-off-continued",
    "title": "Introduction to Corporate Finance",
    "section": "The Investment Trade-off, continued",
    "text": "The Investment Trade-off, continued\n\nAssume that this financial manager acts on behalf of the shareholder’s interests. What do they want the financial manager to do?\nThe answer depends on the project’s rate of return:\n\nIf the return offered by project is higher than what the shareholders can get elsewhere investing on their own \\(\\rightarrow\\) shareholders would be better off with the project\nIf it offers a return rate that is lower than what the shareholders can achieve on their own \\(\\rightarrow\\) shareholders would be better off by having the cash on their hands\n\nThis decision is tied to an important concept called the opportunity cost of capital:\n\nWhenever a corporation invests in a project, its shareholders lose the opportunity to invest the cash on their own\nCorporations increase value by accepting all investment projects that earn more than the opportunity cost of capital."
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#ibovespa-and-interest-rate-dynamics",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#ibovespa-and-interest-rate-dynamics",
    "title": "Introduction to Corporate Finance",
    "section": "Ibovespa and interest rate dynamics",
    "text": "Ibovespa and interest rate dynamics"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#how-to-take-actions-on-behalf-of-the-shareholders",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#how-to-take-actions-on-behalf-of-the-shareholders",
    "title": "Introduction to Corporate Finance",
    "section": "How to take actions on behalf of the shareholders",
    "text": "How to take actions on behalf of the shareholders\n\nShareholders differ in several dimensions:\n\nAge\nTastes\nConsumption Patterns\nWealth\nRisk Aversion\nInvestment Horizon\n\n\n\nQuestion: which criteria should the Financial Manager use to take investment and financing decisions?\n\nFortunately, there is a widely accepted criteria: maximize the current market value of shareholders’ investment!"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#shareholder-value-maximization",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#shareholder-value-maximization",
    "title": "Introduction to Corporate Finance",
    "section": "Shareholder Value Maximization",
    "text": "Shareholder Value Maximization\n\nMaximizing shareholder wealth is a sensible goal when the they have access to well-functioning financial markets:\n\nFinancial markets allow them to share risks and transport savings across time\nIt also gives them the flexibility to manage their own savings and investment plans\n\nWhy it works like this? Assume that the Financial Manager acts on behalf of the stockholders of the firm. A plausible assumption is that:\n\n\nShareholders want to be as rich as possible…\nAnd transform such wealth into his/her specific consumption pattern via borrowing/lending\nFinally, shareholders need to manage the risk of his/her chosen consumption pattern\n\n\nNote that points 2 and 3 can be done by the shareholder\nHow then can the financial manager help the firm’s stockholders? Increasing their wealth!"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#the-separation-of-ownership-and-control",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#the-separation-of-ownership-and-control",
    "title": "Introduction to Corporate Finance",
    "section": "The separation of ownership and control",
    "text": "The separation of ownership and control\n\nWhen we introduced the figure of the Financial Manager and the existence of financial markets, we implicitly discussed the separation of ownership and control\nIn other words, the shareholders of the firm cannot fully control what the managers do\nIs this a problem? Up to now, we are assuming that the Financial Manager should look after the interests of the shareholders\nAlthough this separation is necessary, there are reasons to think that managers could pursue their own objectives:\n\n\nMaximize their bonuses\nPass attractive, but risky projects to increase job safety\nOverinvest to show hard work\nTake on too much risk as the downside will be ultimately borne out by the shareholders"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#the-agency-problem",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#the-agency-problem",
    "title": "Introduction to Corporate Finance",
    "section": "The Agency Problem",
    "text": "The Agency Problem\n\n\n\n\n\n\n\nDefinition\n\n\nThe Agency Problem is a coordination issue present in corporate decisions. Between the manager and the shareholders of the firm:\n\nThe manager (or the agent) does not have incentives to maximize shareholder value; and\nShareholders (the principals) need to incur in monitoring costs to constrain value-destroying decisions\n\n\nThat sets up the stage for Corporate Governance: the design of actions that aims to align the interests of the different stakeholders of the firm and reduce Agency Costs\n\n\n\n\n\n\n\nExamples of internally enforced Corporate Governance mechanisms:\n\n\n\n\n\nThe creation of a Board of Directors\nCompensation plans based on metrics that are tied to maximizing shareholder value (stock options, restricted stock units, variable compensation, etc)\nVoluntary adoption of Corporate Governance principles, such as ESG practices"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#the-agency-problem-continued",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#the-agency-problem-continued",
    "title": "Introduction to Corporate Finance",
    "section": "The Agency Problem, continued",
    "text": "The Agency Problem, continued\n\n\n\n\n\n\n\nDefinition\n\n\nThe Agency Problem is a coordination issue present in corporate decisions. Between the manager and the shareholders of the firm:\n\nThe manager (or the agent) does not have incentives to maximize shareholder value; and\nShareholders (the principals) need to incur in monitoring costs to constrain value-destroying decisions\n\n\nThat sets up the stage for Corporate Governance: the design of actions that aims to align the interests of the different stakeholders of the firm and reduce Agency Costs\n\n\n\n\n\n\n\nExamples of externally enforced Corporate Governance mechanisms:\n\n\n\n\n\nLegal and regulatory requirements - SEC (U.S), Comissão de Valores Mobiliários (CVM) (Brazil)\nHostile Takeovers and Market Monitoring\nShareholder Pressure, also called the “Wall Street walk”"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#supplementary-reading",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#supplementary-reading",
    "title": "Introduction to Corporate Finance",
    "section": "Supplementary Reading",
    "text": "Supplementary Reading\n\nAfter-class reading (available on e-Class):\n\nCarbon Credit Markets\nWhat Every Leader Needs to Know About Carbon Credits\n\n\n\nMiscellaneous (social media, video content, etc):\n\nBarbarians at the Gate: movie from 1993 describing some corporate governance conflicts in the 80’s - link\nSucession: series reflecting a troublesome family business intertwined with a series of corporate governance scandals - link"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#references",
    "href": "quant-mkt/Lecture 1 - Introduction to Corporate Finance/index.html#references",
    "title": "Introduction to Corporate Finance",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#a-brief-introduction",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#a-brief-introduction",
    "title": "Introduction to Corporate Finance",
    "section": "A brief introduction",
    "text": "A brief introduction\n\nBackground\n\n\nUndegraduate + Master’s in Finance and Economics @ University of São Paulo (USP)\nPh.D. in Economics (2023) @ INSPER - Institute of Education and Research\n\n\nAcademic Affiliations\n\n\nJan/24 - actual: Assistant Professor @ Getulio Vargas Foundation (FGV-EAESP), São Paulo, BR\n\n\nPrivate Affiliations\n\n\nDec/21 - actual: Consultant and Data Scientist @ Circana"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#how-to-use-these-slides",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#how-to-use-these-slides",
    "title": "Introduction to Corporate Finance",
    "section": "How to use these slides",
    "text": "How to use these slides\n\nThese slides leverage Quarto, an open-source scientific and technical publishing system from Posit (formerly RStudio):\n\nCreate dynamic content with Python, R, Julia, and Observable\nPublish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more\nWrite using Pandoc markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\nFor our course, we’ll use the following notation:\n\n\nLink will be colored in dark-orange\nInline equations and variables will be rendered in gray\nCode chunks will be provided along with outputs (R and Python)"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#an-example-of-a-code-chunk",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#an-example-of-a-code-chunk",
    "title": "Introduction to Corporate Finance",
    "section": "An example of a code chunk",
    "text": "An example of a code chunk\n\nResultPythonR\n\n\n\n\n\n\n\n\nNote\n\n\nUse Show the Code code tabsets to display the code. You can use the {{&lt; bi clipboard &gt;}} buttom at the top-right to copy it to your session.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Generating data\nnp.random.seed(0)\ndat = pd.DataFrame({\n    'cond': np.repeat(['A', 'B'], 10),\n    'xvar': np.arange(1, 21) + np.random.normal(0, 3, 20),\n    'yvar': np.arange(1, 21) + np.random.normal(0, 3, 20)\n})\n\n# Plotting\nsns.set(style=\"whitegrid\")  # Set plot style\nplt.figure(figsize=(8, 6))  # Set figure size\nsns.scatterplot(data=dat, x='xvar', y='yvar', hue='cond', marker='o')  # Scatter plot\nsns.regplot(data=dat, x='xvar', y='yvar', scatter=False)  # Regression line\nplt.show()\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\ndat &lt;- data.frame(cond = rep(c(\"A\", \"B\"), each=10),\n                  xvar = 1:20 + rnorm(20,sd=3),\n                  yvar = 1:20 + rnorm(20,sd=3))\n\nggplot(dat, aes(x=xvar, y=yvar)) +\n  geom_point(shape=1) + \n  geom_smooth()"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#course-outline",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#course-outline",
    "title": "Introduction to Corporate Finance",
    "section": "Course Outline",
    "text": "Course Outline\n\nWe’ll cover the first section of course, which contemplates choice models:\n\nBinary Response Models: Logit/Probit\nMulti-choice Models: Multinomial Logit\nEconomic Implications of multi-choice models\nMarket-basket Analysis\n\nWe’ll keep a hands-on approach to be best of our capabilities, with handout exercises (code + analysis) and focus on practical applications\nEvaluation (first section) will consist of:\n\nCode coursework (DataCamp)\nHandout Exercise (deadline TBD)\n\nYou can download a notebook covering this first lecture using this link and the dataset using here"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#references",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#references",
    "title": "Introduction to Corporate Finance",
    "section": "References",
    "text": "References\n\nOur first section of the Marketing Analytics course will focus on the main applications from a practical standpoint\n\n\nFor a detailed discussion on the methodological approaches undertaken thoughout the lectures, we’ll follow:\n\n\n\nDiscrete Choice Methods with Simulation - Train (2009)\nEconometric Analysis - Greene (2011)\nEconometrics - Hayashi (2000)\n\n\n\nFor a broader discussion around the practical applications on Marketing, we’ll follow Chapman and Feit (2015)"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#discrete-choice-models",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#discrete-choice-models",
    "title": "Introduction to Corporate Finance",
    "section": "Discrete Choice Models",
    "text": "Discrete Choice Models\n\nMarketing practitioners are often concerned about consumers decisions around a defined set of choices:\n\nConsumers choose between a binary decision of buy/not buy, given product, consumer, and environmental characteristics\nFaced with a defined set of products, make a decision around which products to buy\nAfter experiencing a flow of consumption from a product or service up to time \\(t\\), decide whether to continue consumption at \\(t+1\\) or churn\n\nEliciting knowledge about individual choices shed light on consumer preferences and help marketers to understand how consumers value certain product attributes\nAggregating individual choices helps marketers understand how the demand curve for a given product or industry works"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#binary-choice-models",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#binary-choice-models",
    "title": "Introduction to Corporate Finance",
    "section": "Binary Choice Models",
    "text": "Binary Choice Models\n\nMotivation: among a defined set of two mutually exclusive alternatives, consumers make a decision by choosing one alternative in spite of the other:\n\n\n\\[\n\\small\nY_i=\\begin{cases}\n            1, & \\text{if $Y_i$ = a predefined alternative}\\\\\n            0, & \\text{otherwise}\n         \\end{cases}\n\\]\n\nThese models implicitly tell about consumer’s preferences regarding several consumption decisions. Say that we know that a set \\(X\\) of characteristics (which may be consumer-specific, environmental, or product) affect \\(i\\)’s decision. We can then model such decision by:\n\n\n\n\\[\n\\small Y_i=f(X_i)\n\\]\n\nOur starting point will be to define what \\(f(X)\\) should look like. Depending on the shape of this function, the interpretation becomes a probabilistic measurement"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#binary-choice-models---application",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#binary-choice-models---application",
    "title": "Introduction to Corporate Finance",
    "section": "Binary Choice Models - Application",
    "text": "Binary Choice Models - Application\n\nOne of the most widely applicable areas of binary choice modeling refers to churn modeling: the the percentage of customers who discontinue their use of a business’s products or services over a certain time period\nThis concept is important for businesses because it affects their revenue and offers insights into customer satisfaction and loyalty:\n\nA high churn rate may suggest there are issues with the product, service, or overall customer experience\nA low churn rate may suggest the existence of customer loyalty, high switching costs, or valuable attributes from the offer\n\nChurn modeling offers valuable insights to marketers as it is possible to derive actions ex-ante its occurrence and prevent customers from churning\nThis issue becomes more relevant when customer acquisition costs increase"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#practical-application-outline",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#practical-application-outline",
    "title": "Introduction to Corporate Finance",
    "section": "Practical Application Outline",
    "text": "Practical Application Outline\n\nYou will use a bank Customer Relationship Manager (CRM) data set comprising of 10,000 bank customers and its actual engagement status (whether or not he/she has churned)\nThis data set will be primarily based on this Kaggle notebook, although some adaptations have been made for teaching purposes"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#before-we-start-tech-setup",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#before-we-start-tech-setup",
    "title": "Introduction to Corporate Finance",
    "section": "Before we start: tech-setup",
    "text": "Before we start: tech-setup\n\nWe’ll be running code both in Python and R. All results will be shown in the first tabset, with their corresponding R and Python versions in the subsequent tabsets.\nBefore you start, ensure you have installed all the necessary packages:\n\n\nI’ve included all R packages in the R-dependencies.txt. Use the following code to install each package:\n\n\n\n#Require\npackages=read.delim('Assets/r-requirements.txt',header = FALSE)[,1]\n\n#Require\nlapply(packages,require,character.only=TRUE)\n\n\nFor Python libraries, use the terminal to install and then import all packages inside a notebook\n\n\n\n\npip install requirements.txt"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#quick-outline-of-the-dataset",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#quick-outline-of-the-dataset",
    "title": "Introduction to Corporate Finance",
    "section": "Quick outline of the dataset",
    "text": "Quick outline of the dataset"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#variable-descriptives",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#variable-descriptives",
    "title": "Introduction to Corporate Finance",
    "section": "Variable Descriptives",
    "text": "Variable Descriptives\n\nTo begin our investigation, let’s do a simple variable description of the variables that we have in our dataset:\n\n\n\nResultRPython\n\n\n\n\n'data.frame':   10000 obs. of  12 variables:\n $ customer_id     : int  15634602 15647311 15619304 15701354 15737888 15574012 15592531 15656148 15792365 15592389 ...\n $ credit_score    : int  619 608 502 699 850 645 822 376 501 684 ...\n $ country         : chr  \"France\" \"Spain\" \"France\" \"France\" ...\n $ gender          : chr  \"Female\" \"Female\" \"Female\" \"Female\" ...\n $ age             : int  42 41 42 39 43 44 50 29 44 27 ...\n $ tenure          : int  2 1 8 1 2 8 7 4 4 2 ...\n $ balance         : num  0 83.8 159.7 0 125.5 ...\n $ products_number : int  1 1 3 2 1 2 2 4 2 1 ...\n $ credit_card     : int  1 0 1 0 1 1 1 1 0 1 ...\n $ active_member   : int  1 1 0 0 1 0 1 0 1 1 ...\n $ estimated_salary: num  101.3 112.5 113.9 93.8 79.1 ...\n $ churn           : int  1 0 1 0 0 1 0 1 0 0 ...\n\n\n\n\n\n\nShow the code\nData=read.csv('Assets/bank-dataset.csv')\nData%&gt;%str()\n\n\n\n\n\n\nShow the code\n# Read the CSV file into a pandas DataFrame\nData = pd.read_csv('Assets/bank-dataset.csv')\n\n# Print the structure of the DataFrame\nprint(Data.describe())"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#analyzing-churn-distribution",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#analyzing-churn-distribution",
    "title": "Introduction to Corporate Finance",
    "section": "Analyzing churn distribution",
    "text": "Analyzing churn distribution\n\nResultRPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n(Data%&gt;%\n  select(where(is.numeric))%&gt;%\n  cor())[-10,10]%&gt;%\n  as.data.frame()%&gt;%\n  rename(Correlation='.')%&gt;%\n  rownames_to_column('Variable')%&gt;%\n  mutate(Sign=ifelse(Correlation&gt;0,'Positive','Negative'))%&gt;%\n  ggplot(aes(x=Variable,y=Correlation,fill=Sign))+\n  geom_col()+\n  coord_flip()+\n  theme_minimal()+\n  scale_fill_manual(values=c(Positive='darkgreen',Negative='red'))+\n  geom_text(aes(label=round(Correlation,2),hjust=ifelse(Correlation&lt;0,0.5,-0.5)),size=5)+\n  scale_y_continuous(limits = c(-1,1))+\n  labs(title='Churn correlation across numeric variables',\n       x='',\n       y='')+\n  theme(legend.position = 'hide',\n        plot.title = element_text(size=20,face='bold'),\n        axis.text = element_text(size=15))\n\n\n\n\n\n\nShow the code\n# Read the CSV file into a pandas DataFrame\nData = pd.read_csv('Assets/bank-dataset.csv')\n\n# Select numeric columns and calculate correlation\ncorrelation_df = Data.select_dtypes(include='number').corr().iloc[:-1, -1].reset_index()\n\n# Rename columns\ncorrelation_df.columns = ['Variable', 'Correlation']\n\n# Add 'Sign' column based on correlation values\ncorrelation_df['Sign'] = correlation_df['Correlation'].apply(lambda x: 'Positive' if x &gt; 0 else 'Negative')\n\n# Plotting\nplt.figure(figsize=(10, 6))\nbars = plt.barh(correlation_df['Variable'], correlation_df['Correlation'], color=[ 'darkgreen' if sign == 'Positive' else 'red' for sign in correlation_df['Sign']])\nplt.gca().invert_yaxis()\nplt.title('Churn correlation across numeric variables', fontsize=20, fontweight='bold')\nplt.xlabel('')\nplt.ylabel('')\nplt.xlim(-1, 1)\n\n# Adding annotations\nfor bar, correlation in zip(bars, correlation_df['Correlation']):\n    plt.text(bar.get_width() * (0.8 if correlation &lt; 0 else -0.8), bar.get_y() + bar.get_height() / 2, round(correlation, 2), \n             va='center', ha='center', fontsize=10)\n\nplt.show()"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#modeling-discrete-choice-models",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#modeling-discrete-choice-models",
    "title": "Introduction to Corporate Finance",
    "section": "Modeling Discrete Choice Models",
    "text": "Modeling Discrete Choice Models\n\nSay that we are interested in modeling the occurrence of churn, which is a discrete variable for each customer \\(i\\):\n\n\n\\[\nY_i=\n\\begin{cases}\n1, \\text{if the customer has churned}\\\\\n0,  \\text{if the customer is still an active client}\n\\end{cases}\n\\]\n\nIf we have a set of covariates, \\(X_i\\), then we can define the relationship:\n\n\n\n\\[\nY_i=f(X_i)\n\\]\n\nHow our \\(f(X_i)\\) should look like?\nWhat is the interpretation of the estimates that we’ll find?"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#linear-probability-models-lpm",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#linear-probability-models-lpm",
    "title": "Introduction to Corporate Finance",
    "section": "Linear Probability Models (LPM)",
    "text": "Linear Probability Models (LPM)\n\nThe simplest approach that we can take is to assume that \\(Y\\) is linear on the set of characteristics \\(X\\). In other words, \\(f(X)\\) is a linear model:\n\n\n\\[\nY_i= P(Y_i=1|X_i)=\\alpha + \\beta_1x_1+\\beta_2x_2+...+\\beta_nx_n+\\varepsilon_i\n\\]\n\nWe can then use Ordinary Least Squares (OLS) to model such relationship. Because the functional form of \\(f(X)\\) is assumed to be linear, we call this a linear probability model (LPM):\nBecause the relationship between \\(Y\\) and \\(X\\) is assumed to be linear, the changes in the probability (or likelihood) of churn are linear on the parameters \\(\\beta\\). For example, for \\(x_1\\):\n\n\n\n\\[\n\\dfrac{\\partial Y}{\\partial x_1}=\\beta_1\n\\]"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#is-the-lpm-a-biased-estimator",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#is-the-lpm-a-biased-estimator",
    "title": "Introduction to Corporate Finance",
    "section": "Is the LPM a biased estimator?",
    "text": "Is the LPM a biased estimator?\n\nIf we believe that the OLS assumptions are valid, \\(\\beta\\) is still a consistent estimator of the population average relationship between \\(Y\\) and \\(X\\), regardless of the type of dependent:\n\n\n\\[\n\\begin{align}\n& \\beta_{OLS}=(X'X)^{-1}X'Y\\\\\n& \\beta_{OLS}=(X'X)^{-1}X'(X\\beta+\\epsilon)=\\underbrace{(X'X)^{-1}(X'X)}_{I}\\beta+(X'X)^{-1}X'\\epsilon\\\\\n& \\beta_{OLS}=\\beta + (X'X)^{-1}X'\\epsilon\n\\end{align}\n\\]\n\nIf we assume that \\(X\\perp\\epsilon\\) - i.e, no ommited variables-, we see that \\(\\beta_{OLS}\\) is an unbiased estimator of the average effect of \\(X\\) on \\(Y\\):\n\n\n\n\\[\n\\text{if } X\\perp\\epsilon \\rightarrow \\beta_{OLS}=\\beta + (X'X)^{-1}\\underbrace{X'\\epsilon}_{=0}\\rightarrow \\beta_{OLS}=\\beta\n\\]"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#modeling-churn-via-lpm",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#modeling-churn-via-lpm",
    "title": "Introduction to Corporate Finance",
    "section": "Modeling churn via LPM",
    "text": "Modeling churn via LPM\n\nSuppose we want to understand the determinants of customer’s churn over time. For that, we’ll consider the following variables:\n\n\n\\[\n\\text{if } X=\\begin{bmatrix} CreditScore \\\\ D(Gender) \\\\ Age \\\\ Tenure \\\\ Balance \\\\ \\#Products \\\\ D(CreditCard) \\\\ D(Active) \\\\ Salary \\end{bmatrix} \\rightarrow Y \\sim X'\\beta+\\epsilon\n\\]"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#lpm-estimation",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#lpm-estimation",
    "title": "Introduction to Corporate Finance",
    "section": "LPM Estimation",
    "text": "LPM Estimation\n\nResultsRPython\n\n\n\n\n\nCall:\nlm(formula = churn ~ credit_score + gender + age + tenure + balance + \n    products_number + credit_card + active_member + estimated_salary, \n    data = Data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.77456 -0.23648 -0.12527  0.02732  1.20241 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.025e-01  3.387e-02  -3.027  0.00248 ** \ncredit_score     -9.264e-05  3.877e-05  -2.389  0.01690 *  \ngenderMale       -7.733e-02  7.531e-03 -10.267  &lt; 2e-16 ***\nage               1.131e-02  3.589e-04  31.505  &lt; 2e-16 ***\ntenure           -1.849e-03  1.296e-03  -1.426  0.15392    \nbalance           6.938e-04  6.306e-05  11.003  &lt; 2e-16 ***\nproducts_number  -4.262e-03  6.766e-03  -0.630  0.52873    \ncredit_card      -2.958e-03  8.222e-03  -0.360  0.71901    \nactive_member    -1.432e-01  7.532e-03 -19.016  &lt; 2e-16 ***\nestimated_salary  7.118e-05  6.516e-05   1.092  0.27474    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3746 on 9990 degrees of freedom\nMultiple R-squared:  0.1359,    Adjusted R-squared:  0.1351 \nF-statistic: 174.6 on 9 and 9990 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nShow the code\nData=read.csv('Assets/bank-dataset.csv')\n\nReg=lm(churn ~ credit_score + gender + age + tenure + balance + products_number + credit_card + active_member + estimated_salary,data = Data)\n\n\n\n\n\n\nShow the code\n# Read the CSV file into a pandas DataFrame\nData = pd.read_csv('Assets/bank-dataset.csv')\n\n# Convert 'gender' into dummy variables\nData = pd.get_dummies(Data, columns=['gender'], drop_first=True)\n\n# Define independent and dependent variables\nX = Data[['credit_score', 'gender_Male', 'age', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary']]\ny = Data['churn']\n\n# Add a constant to the independent variables matrix for the intercept\nX = sm.add_constant(X)\n\n# Fit the linear regression model\nmodel = sm.OLS(y, X.astype(float)).fit()\n\n# Print the regression summary\nprint(model.summary())"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#overall-analysis",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#overall-analysis",
    "title": "Introduction to Corporate Finance",
    "section": "Overall analysis",
    "text": "Overall analysis\n\nAll else equal, an additional year increases the likelihood of churning by:\n\n\n\\[\n\\partial Y/\\partial Age=\\beta_{Age}=0.011\n\\]\n\nMoving from the \\(25^{th}\\) to the \\(75^{th}\\) percentile of age increases the likelihood by \\(\\beta_{Age}\\times IQR(Age)=0.011\\times \\beta_{Age} \\approx0.1357006\\) or 14%\n\n\nMale customers tend to churn, on average, -7.73% less than Female customers:\n\n\n\n\\[\n\\partial Y/\\partial D(genderMale)=\\beta_{genderMale}=-0.077\n\\]\n\nSimilar analyses can be made for the following continuous variables (see Code in next slide)"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#analyses",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#analyses",
    "title": "Introduction to Corporate Finance",
    "section": "Analyses",
    "text": "Analyses\n\nResultRPython\n\n\n\n\n\n\n\n\nVariable\npartial_change\n\n\n\n\nactive_member\n-14.32%\n\n\nage\n13.57%\n\n\nbalance\n8.86%\n\n\ncredit_card\n-0.30%\n\n\ncredit_score\n-1.24%\n\n\nestimated_salary\n0.70%\n\n\ngenderMale\n-7.73%\n\n\nproducts_number\n-0.43%\n\n\ntenure\n-0.74%\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ncontinuous_vars= c('credit_score','age','tenure','balance','products_number','estimated_salary')\ndummy_vars=c('genderMale','credit_card','active_member')\n\nestimates=tidy(Reg)%&gt;%select(term,estimate)%&gt;%rename(Variable=term)\n\nData%&gt;%\n    select(all_of(continuous_vars))%&gt;%\n    apply(2,IQR,na.rm=TRUE)%&gt;%\n    as.data.frame()%&gt;%\n    rownames_to_column('Variable')%&gt;%\n    rename(Change='.')%&gt;%\n  rbind(data.frame(Variable=dummy_vars,Change=1))%&gt;%\n  left_join(estimates)%&gt;%\n  group_by(Variable)%&gt;%\n  summarize(partial_change=percent(Change*estimate,accuracy=0.01))%&gt;%\n  kable()%&gt;%\n  kable_styling(bootstrap_options = 'responsive')\n\n\n\n\n\n\nShow the code\n# Define continuous and dummy variables\ncontinuous_vars = ['credit_score', 'age', 'tenure', 'balance', 'products_number', 'estimated_salary']\ndummy_vars = ['gender_Male', 'credit_card', 'active_member']\n\n# Get coefficient estimates from the model\nestimates = model.params.reset_index()\nestimates.columns = ['Variable', 'estimate']\n\n# Calculate IQR for continuous variables\niqr_values = Data[continuous_vars].apply(lambda x: x.quantile(0.75) - x.quantile(0.25), axis=0)\niqr_df = pd.DataFrame({'Variable': iqr_values.index, 'Change': iqr_values.values})\n\n# Create a DataFrame for dummy variables\ndummy_df = pd.DataFrame({'Variable': dummy_vars, 'Change': 1})\n\n# Combine continuous and dummy variable dataframes\ncombined_df = pd.concat([iqr_df, dummy_df])\n\n# Merge with coefficient estimates\nresult_df = pd.merge(combined_df, estimates, on='Variable', how='left')\n\n# Calculate partial changes\nresult_df['partial_change'] = result_df['Change'] * result_df['estimate']*100\n\n# Format the results\nresult_df['partial_change'] = result_df['partial_change'].map(lambda x: f\"{x:.2f}%\")\n\n# Print the results\nprint(tabulate(result_df[['Variable', 'partial_change']], headers='keys', tablefmt='github'))"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#limitations-of-the-lpm",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#limitations-of-the-lpm",
    "title": "Introduction to Corporate Finance",
    "section": "Limitations of the LPM",
    "text": "Limitations of the LPM\n\nAlthough a consistent estimator of the average effect, the LPM has limitations when it comes to its practical implications to binary outcomes:\n\n\nChurn probabilities should lie within \\([0,1]\\), but the predicted probabilities, \\(\\hat{Y}\\), have continuous support \\((-\\infty,+\\infty)\\). When the goal is to predict outcomes, this creates probabilities that are outside of the ranges\nThere is an implicit assumption that the effects are linear, which may not hold true. Example, an increase of $1,000 in customer’s income may have significant impacts on churn likelihood when customers are from the bottom of the income distribution, but the effects should dampen as we move towards the top of the income distribution\nHeteroskedasticity\n\\(R^2\\) is not well-defined"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#limitations-of-the-lpm-1",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#limitations-of-the-lpm-1",
    "title": "Introduction to Corporate Finance",
    "section": "Limitations of the LPM",
    "text": "Limitations of the LPM\n\nHeteroskedasticityPoor Fit\\(R^2\\)\n\n\n\n\nEven though we can achieve consistent estimators, we’ll have heteroskedasticity in our estimates by construction. To see that, recall that the conditional expectation of Y given X is given by:\n\n\\[\n\\small E(Y|X)=\\underbrace{[P(Y=1|X]}_{X\\beta\\times 1} + \\underbrace{[1-P(Y=1|X)]\\times 0}_{=0} = X\\beta\n\\]\n\nGiven that the variance of a Bernoulli Distribution is given by \\(p \\times (1-p)\\), then:\n\n\\[\nV(Y|X)=P(Y|X)\\times[1-P(Y|X)]=X\\beta\\times(1-X\\beta) \\text{, which clearly depends on } X\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn most linear probability models, \\(R^2\\) has no meaningful interpretation\nSince the regression line can never fit the data perfectly if the dependent variable is binary and the regressors are continuous"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#introducing-non-linear-binary-models",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#introducing-non-linear-binary-models",
    "title": "Introduction to Corporate Finance",
    "section": "Introducing non-linear binary models",
    "text": "Introducing non-linear binary models\n\nRecall that the limitations of the LPM stem from the fact that \\(f(X)=X'\\beta\\). It might be that a linear relationship does not capture all aspects that a churn analysis should have!\nTo that point, we need to think about a new relationship, \\(Y=f(X)\\), that fulfills the following points:\n\n\nThe predicted outcomes, \\(\\hat{Y}\\), lie between 0 and 1\nThe effects do not need to be linear on the parameters\nIdeally, we’d want to apply a non-linear transformation in such a way that the relationship between Y and X is sigmoid (or S-shaped) curve: the changes in the predicted probability tend to go to zero as we approach the lower and upper bounds of the distribution of X\n\n\nThe most known cases are logistic regression (\\(f(x)=\\Lambda (x)\\)) and Probit (\\(f(x)=\\Phi(X)\\))"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logistic-regression",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logistic-regression",
    "title": "Introduction to Corporate Finance",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nLike any other transformation function, the idea behind using \\(\\Lambda(X)\\) lies on the latent variable approach: think about an unobserved component, \\(Y^\\star\\), which is a continuous variable, such as how much a consumer values a product.\nAlthough we do not observe \\(Y^\\star\\), we do observe the consumer’s decision of buying or not buying the product, depending on a given threshold:\n\n\n\\[\nY=\n\\begin{cases}\n1, \\text{ if }Y^\\star&gt;0\\\\\n0, \\text{ if }Y^\\star\\leq0\\\\\n\\end{cases}\n\\]\n\nTherefore, we can see that the probability of buying depends on a latent variable, which is not observed by the econometrician:\n\n\n\n\\[\nP(Y=1|X)=P(Y^\\star&gt;0|X)=P(\\underbrace{X\\beta+\\varepsilon}_{Y^{\\star}}&gt;0|X)\n\\]"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logistic-regression-continued",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logistic-regression-continued",
    "title": "Introduction to Corporate Finance",
    "section": "Logistic Regression, continued",
    "text": "Logistic Regression, continued\n\nWe can rearrange terms and find that:\n\n\n\\[\nP(Y=1|X)=P(\\underbrace{X\\beta+\\varepsilon}_{Y^{\\star}}&gt;0|X)\\rightarrow \\underbrace{P(\\varepsilon&gt;-X\\beta|X)\\equiv P(\\varepsilon&lt;X\\beta|X)}_{\\text{Under Simmetry}}\n\\]\n\nTherefore, this framework can be used to think about any cumulative density function which has the simmetry property. For the case of Logistic Regression, our transformation function that maps \\(Y^\\star\\) (how much consumer values a good) to \\(Y\\) (decision to buy or not buy) is:\n\n\n\n\\[\nf(Y^\\star)=\\Lambda(Y^\\star)=\\dfrac{\\exp(X\\beta)}{1+\\exp(X\\beta)}\n\\]\n\nEstimation is made using Maximum Likelihood estimators."
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#applying-logistic-regression",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#applying-logistic-regression",
    "title": "Introduction to Corporate Finance",
    "section": "Applying Logistic Regression",
    "text": "Applying Logistic Regression\n\nResultRPython\n\n\n\n\n\nCall:\nglm(formula = churn ~ credit_score + gender + age + tenure + \n    balance + products_number + credit_card + active_member + \n    estimated_salary, family = binomial(link = \"logit\"), data = Data)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -3.4760701  0.2414497 -14.397   &lt;2e-16 ***\ncredit_score     -0.0006521  0.0002777  -2.349   0.0188 *  \ngenderMale       -0.5429205  0.0539700 -10.060   &lt;2e-16 ***\nage               0.0728688  0.0025513  28.561   &lt;2e-16 ***\ntenure           -0.0147067  0.0092775  -1.585   0.1129    \nbalance           0.0050449  0.0004601  10.964   &lt;2e-16 ***\nproducts_number  -0.0361688  0.0463951  -0.780   0.4356    \ncredit_card      -0.0289865  0.0587461  -0.493   0.6217    \nactive_member    -1.0792900  0.0572260 -18.860   &lt;2e-16 ***\nestimated_salary  0.0004978  0.0004698   1.060   0.2893    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10109.8  on 9999  degrees of freedom\nResidual deviance:  8706.5  on 9990  degrees of freedom\nAIC: 8726.5\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\n\nShow the code\nData=read.csv('Assets/bank-dataset.csv')\n\nReg=glm(churn ~ credit_score + gender + age + tenure + balance + products_number + credit_card + active_member + estimated_salary,\n        family = binomial(link = \"logit\"),\n        data = Data)\n\nsummary(Reg)\n\n\n\n\n\n\nShow the code\n# Read the CSV file into a pandas DataFrame\nData = pd.read_csv('Assets/bank-dataset.csv')\n\n# Convert 'gender' into dummy variables\nData = pd.get_dummies(Data, columns=['gender'], drop_first=True)\n\n# Define independent and dependent variables\nX = Data[['credit_score', 'gender_Male', 'age', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary']]\ny = Data['churn']\n\n# Add a constant to the independent variables matrix for the intercept\nX = sm.add_constant(X)\n\n# Fit the linear regression model\nmodel = sm.Logit(y, X.astype(float)).fit()\n\n# Print the regression summary\nprint(model.summary())"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-1-probabilities-within-the-01-interval",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-1-probabilities-within-the-01-interval",
    "title": "Introduction to Corporate Finance",
    "section": "Logit Properties #1: probabilities within the [0,1] interval",
    "text": "Logit Properties #1: probabilities within the [0,1] interval\n\nOne interesting thing to note is that, due to the properties of \\(\\Lambda(\\cdot)\\), our estimated probabilities will fall within \\([0,1]\\) depending on \\(X\\):\n\n\nWhen \\(X\\rightarrow\\infty\\), the probability of buying in our example tends to 1:\n\n\n\\[\n\\Lambda(Y)=\\dfrac{\\exp(Y)}{1+\\exp(Y)}=\\dfrac{\\exp(X\\beta)}{1+\\exp(X\\beta)}\\rightarrow1\n\\]\n\nOn the other hand, when \\(X\\rightarrow -\\infty\\), the probability of buying in our example tends to 0:\n\n\n\n\\[\n\\Lambda(Y)=\\dfrac{\\exp(Y)}{1+\\exp(Y)}=\\dfrac{\\exp(X\\beta)}{1+\\exp(X\\beta)}\\rightarrow \\dfrac{0}{1}=0\n\\]"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-2-the-odds-ratio",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-2-the-odds-ratio",
    "title": "Introduction to Corporate Finance",
    "section": "Logit Properties #2: the odds-ratio",
    "text": "Logit Properties #2: the odds-ratio\n\nRecall that the logit estimation for \\(\\small P(Y|X)\\) is given by:\n\n\n\\[\n\\small\nP(Y)=\\dfrac{\\exp(X\\beta)}{1+\\exp(X\\beta)},\\text{ which we will call by } p\n\\]\n\nLooking at the inverse, \\(1/p\\), we can see that:\n\n\n\n\\[\n\\small\n\\dfrac{1}{p}=\\dfrac{1+\\exp(X\\beta)}{\\exp(X\\beta)}=1+\\dfrac{1}{\\exp{X\\beta}}\\rightarrow \\dfrac{1-p}{p}=\\dfrac{1}{\\exp{X\\beta}}\n\\]\n\nInverting and taking logs on both sides, we’ll have:\n\n\n\n\\[\n\\small \\log{\\bigg(\\dfrac{p}{1-p}}\\bigg)=X\\beta=\\alpha+\\beta_1x_1+\\beta_2x_2+...\\beta_kx_k\n\\]"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-2-the-odds-ratio-continued",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-2-the-odds-ratio-continued",
    "title": "Introduction to Corporate Finance",
    "section": "Logit Properties #2: the odds-ratio (continued)",
    "text": "Logit Properties #2: the odds-ratio (continued)\n\nTherefore, whenever we’re estimating a logit model, our transformation function, \\(\\Lambda(\\cdot)\\) is actually estimating \\(\\small \\dfrac{p}{1-p}\\):\n\n\n\\[\n\\small logit(p)=\\alpha+\\beta_1x_1+\\beta_2x_2+...\\beta_kx_k\n\\]\n\nThe term \\(\\small \\dfrac{p}{1-p}\\) is called odds-ratio, and is simply the ratio of the probability of success over the probability of failure\nHence, if we want to recover the impacts of any change in the odds-ratio due to our covariates, we can exponentiate our coefficients:\n\n\n\n\\[\n\\small \\text{if } \\log\\bigg(\\dfrac{p}{1-p}\\bigg)=\\alpha+\\beta_1x_1+\\beta_2x_2+...\\beta_kx_k\\rightarrow \\dfrac{p}{1-p}=\\exp(\\alpha+\\beta_1x_1+\\beta_2x_2+...\\beta_kx_k)\n\\]"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-2-the-odds-ratio-continued-1",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-2-the-odds-ratio-continued-1",
    "title": "Introduction to Corporate Finance",
    "section": "Logit Properties #2: the odds-ratio (continued)",
    "text": "Logit Properties #2: the odds-ratio (continued)\n\nExplanationResultRPython\n\n\n\n\nIn previous slides, we noted that the effect of Gender is \\(\\small\\beta_2=-0.54\\). Holding everything fixed, the odds-ratio between Gender=1 (Male) versus Gender=0 (Female) is given by:\n\n\\[\n\\small \\dfrac{p}{1-p}=\\exp(\\beta_2)=\\exp(-0.54)\\approx0.58\n\\]\n\nPut another way, the chances of being a churned client are \\(\\small 0.58-1=-0.42\\) or 42% less likely for men!\nSimilarly, if we look at Age (\\(\\beta_3=0.07\\)), an increase of 1 year increases the churn probability by:\n\n\\[\n\\exp(\\beta_1)=[\\exp(0.0728688)-1]\\approx 0.07 \\text{ or } 7.55\\%\n\\]\n\n\n\n\n\n\n\n\n\n\nOddsRatio\nMultiplier\nChangeOdds\n\n\n\n\n(Intercept)\n0.0309287\n1\n-96.907%\n\n\ncredit_score\n0.9993481\n100\n-6.519%\n\n\ngender\n0.5810488\n1\n-41.895%\n\n\nage\n1.0755894\n1\n7.559%\n\n\ntenure\n0.9854009\n1\n-1.460%\n\n\nbalance\n1.0050576\n10\n5.058%\n\n\nproducts_number\n0.9644775\n1\n-3.552%\n\n\ncredit_card\n0.9714295\n1\n-2.857%\n\n\nactive_member\n0.3398367\n1\n-66.016%\n\n\nestimated_salary\n1.0004980\n100\n4.980%\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nData=read.csv('Assets/bank-dataset.csv')%&gt;%mutate(gender=ifelse(gender=='Male',1,0))\n\nReg=glm(churn ~ credit_score + gender + age + tenure + balance + products_number + credit_card + active_member + estimated_salary,\n        family = binomial(link = \"logit\"),\n        data = Data)\n\ndata.frame(OddsRatio=exp(coefficients(Reg)),\n           Multiplier = c(1,100,1,1,1,10,1,1,1,100))%&gt;%\n  mutate(ChangeOdds=percent((OddsRatio-1)*Multiplier))%&gt;%\n  kable()%&gt;%\n  kable_styling(bootstrap_options = 'responsive',font_size = 10)\n\n\n\n\n\n\nShow the code\n# Read the CSV file into a pandas DataFrame\nData = pd.read_csv('Assets/bank-dataset.csv')\n\n# Convert 'gender' into dummy variables\nData = pd.get_dummies(Data, columns=['gender'], drop_first=True)\n\n# Define independent and dependent variables\nX = Data[['credit_score', 'gender_Male', 'age', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary']]\ny = Data['churn']\n\n# Add a constant to the independent variables matrix for the intercept\nX = sm.add_constant(X)\n\n# Fit the linear regression model\nmodel = sm.Logit(y, X.astype(float)).fit()\n\n# Calculate Odds Ratio and Change in Odds\nodds_ratio = pd.DataFrame({'OddsRatio': model.params.apply(lambda x: round(pow(2, x), 4)),\n                           'Multiplier': [1, 100, 1, 1, 1, 10, 1, 1, 1, 100]})\nodds_ratio['ChangeOdds'] = ((odds_ratio['OddsRatio'] - 1) * odds_ratio['Multiplier']).map(lambda x: f\"{x:.2%}\")\n\n# Display the results\nprint(odds_ratio)"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-3-different-marginal-effects",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-3-different-marginal-effects",
    "title": "Introduction to Corporate Finance",
    "section": "Logit Properties #3: different marginal effects",
    "text": "Logit Properties #3: different marginal effects\n\nExplanationResultRPython\n\n\n\n\nOne of the caveats of LPM was that the marginal effect was constant, which does not make a lot of sense from a probabilistic sense\nWith Logit, the marginal effects are not equal to \\(\\beta\\) anymore. To see that, take the derivative of \\(P(Y|X)\\) with respect to \\(x_1\\):\n\n\\[\n\\dfrac{\\partial\\Lambda(X\\beta)}{\\partial x_1}=\\beta_1 \\times\\dfrac{\\partial\\Lambda(X\\beta)}{\\partial X\\beta}\n\\]\n\nAs we can see, the effects are not going to be linear anymore!\nPut another way, given different levels of X, we may have different marginal effects on the probabilities - in what follows, we’ll analyze the case of age\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nData=read.csv('Assets/bank-dataset.csv')%&gt;%mutate(gender=ifelse(gender=='Male',1,0))\n\nReg=glm(churn ~ credit_score + gender + age + tenure + balance + products_number + credit_card + active_member + estimated_salary,\n        family = binomial(link = \"logit\"),\n        data = Data)\n\n#Set all other variables to the mean\nNewData=Data%&gt;%select(-c(churn,age))%&gt;%summarize(across(where(is.numeric),mean,na.rm=TRUE))\n\n#Include ranges for credit score\nNewData=NewData%&gt;%cbind(data.frame(age=seq(0,100)))\n\n#Predict\nPredict=predict(Reg,NewData,type='response')\n\n#Plot\nNewData%&gt;%\n  cbind(Predict)%&gt;%\n  ggplot(aes(x=age,y=Predict))+\n  geom_point()+\n  labs(title = 'Marginal effects of Age on Churn probabilities',\n       x='Age',\n       y='Predicted Probabilities')+\n  theme_minimal()+\n  scale_y_continuous(labels = percent)+\n  geom_hline(yintercept = c(0,1),linetype='dashed',color='red')\n\n\n\n\n\n\nShow the code\n# Load data\ndata = pd.read_csv('Assets/bank-dataset.csv')\ndata['gender'] = np.where(data['gender'] == 'Male', 1, 0)\n\n# Fit logistic regression model\nX = data[['credit_score', 'gender', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary','age']]\nX = sm.add_constant(X)  # Add constant term for intercept\ny = data['churn']\nmodel = sm.Logit(y, X).fit()\n\n# Set all other variables to the mean\nnew_data = X.drop(columns=['age']).mean(numeric_only=True).to_frame().transpose()\nnew_data = pd.DataFrame(np.repeat(new_data.values, 100, axis=0),columns=new_data.columns)\nage_df = pd.DataFrame(pd.Series(range(1, 101)), columns=['age'])\n\n# Concatenate with your existing DataFrame\nconcatenated_df = pd.concat([new_data, age_df], ignore_index=True,axis=1)\nconcatenated_df.columns=[*new_data.columns,'age']\n\n# Predict\npredicted = model.predict(concatenated_df)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(concatenated_df[['age']], predicted, marker='o', linestyle='-')\nplt.title('Marginal effects of Age on Churn probabilities')\nplt.xlabel('Age')\nplt.ylabel('Predicted Probabilities')\nplt.axhline(y=0, linestyle='--', color='red')\nplt.axhline(y=1, linestyle='--', color='red')\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter('{:.0%}'.format))\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-4-different-effects-by-categorical-variables",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-4-different-effects-by-categorical-variables",
    "title": "Introduction to Corporate Finance",
    "section": "Logit Properties #4: different effects by categorical variables",
    "text": "Logit Properties #4: different effects by categorical variables\n\nExplanationResultRPython\n\n\n\n\nAnother interesting use case is to re-do the same analysis before, but now varying also on categorical variables\nFor example, what is the difference in the probability of buying by men and women?\nIn order to do that, we can set all continous variables to their means and compare the estimated probabilities for gender=1 (male) and gender=0 (female)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nData=read.csv('Assets/bank-dataset.csv')%&gt;%mutate(gender=ifelse(gender=='Male',1,0))\n\nReg=glm(churn ~ credit_score + gender + age + tenure + balance + products_number + credit_card + active_member + estimated_salary,\n        family = binomial(link = \"logit\"),\n        data = Data)\n\n#Set all other variables to the mean\nNewData=Data%&gt;%select(-c(churn,gender))%&gt;%summarize(across(where(is.numeric),mean,na.rm=TRUE))\n\n#Include ranges for gender\nNewData=NewData%&gt;%cbind(data.frame(gender=c(1,0)))\n\n#Predict\nPredict=predict(Reg,NewData,type='response')\n\n#Plot\nNewData%&gt;%\n  cbind(Predict)%&gt;%\n  mutate(Sex=ifelse(gender==1,'Male','Female'))%&gt;%\n  ggplot(aes(x=Sex,y=Predict,fill=Sex))+\n  geom_col()+\n  geom_text(aes(label=percent(Predict)),size=5,position=position_stack(vjust = 0.5))+\n  labs(title = 'Marginal effects of gender on Churn probabilities',\n       x='Gender',\n       y='Predicted Probabilities')+\n  theme_minimal()+\n  scale_y_continuous(labels=percent)+\n    theme(legend.position = 'bottom',\n        axis.text = element_text(size=15),\n        plot.title = element_text(size=18,face='bold'))\n\n\n\n\n\n\nShow the code\n# Load data\ndata = pd.read_csv('Assets/bank-dataset.csv')\ndata['gender'] = np.where(data['gender'] == 'Male', 1, 0)\n\n# Fit logistic regression model\nX = data[['credit_score', 'age', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary','gender']]\nX = sm.add_constant(X)  # Add constant term for intercept\ny = data['churn']\nmodel = sm.Logit(y, X).fit()\n\n# Set all other variables to the mean\nnew_data = X.drop(columns=['gender']).mean(numeric_only=True).to_frame().transpose()\nnew_data = pd.DataFrame(np.repeat(new_data.values, 2, axis=0),columns=new_data.columns)\ngender_df = pd.DataFrame(pd.Series(range(0, 2)), columns=['gender'])\n\n# Concatenate with your existing DataFrame\nconcatenated_df = pd.concat([new_data, gender_df], ignore_index=True,axis=1)\nconcatenated_df.columns=[*new_data.columns,'gender']\n\n# Predict\npredicted = model.predict(concatenated_df)\n\n# Plot\nconcatenated_df['Predict'] = predicted\nconcatenated_df['Sex'] = np.where(concatenated_df['gender'] == 1, 'Male', 'Female')\n\nplt.figure(figsize=(10, 300))\nplt.bar(concatenated_df['Sex'], concatenated_df['Predict'], color=concatenated_df['Sex'].map({'Male': 'blue', 'Female': 'pink'}))\nfor i, value in enumerate(concatenated_df['Predict']):\n    plt.text(i, value, f'{value:.2%}', ha='center', va='bottom', fontsize=10)\nplt.title('Marginal effects of gender on Churn probabilities')\nplt.xlabel('Gender')\nplt.ylabel('Predicted Probabilities')\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#putting-all-together",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#putting-all-together",
    "title": "Introduction to Corporate Finance",
    "section": "Putting all together",
    "text": "Putting all together\n\nExplanationResultRPython\n\n\n\n\nWe saw that women tend to churn, approximately, \\(8\\) percentage points more than men\nIs this true across all levels of age? As age levels increase, the gap between men and women may become wider due to personal traits. On the other hand, it may be that gender differences are invariant to age\nIn order to do that, we can do a mix of the two last exercises:\n\n\nSet all continuous variables, with the exception of age, to their means\nCompare the estimated probabilities for gender=1 (male) and gender=0 (female)\nYou should have \\(200\\) rows in your data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nData=read.csv('Assets/bank-dataset.csv')%&gt;%mutate(gender=ifelse(gender=='Male',1,0))\n\nReg=glm(churn ~ credit_score + gender + age + tenure + balance + products_number + credit_card + active_member + estimated_salary,\n        family = binomial(link = \"logit\"),\n        data = Data)\n\n#Set all other variables to the mean\nNewData=Data%&gt;%select(-c(churn,gender,age))%&gt;%summarize(across(where(is.numeric),mean,na.rm=TRUE))\n\n#Include ranges for Age and Gender\nNewData=NewData%&gt;%\n  cbind(data.frame(age=seq(1,100)))%&gt;%\n  cbind(data.frame(gender=c(rep(1,100),rep(0,100))))\n\n#Predict\nPredict=predict(Reg,NewData,type='response')\n\n#Plot\nNewData%&gt;%\n  cbind(Predict)%&gt;%\n  mutate(Sex=ifelse(gender==1,'Male','Female'))%&gt;%\n  ggplot(aes(x=age,y=Predict,fill=Sex,col=Sex))+\n  geom_point()+\n  labs(title = 'Marginal effects of gender and age on Churn probabilities',\n       x='Age',\n       y='Predicted Probabilities')+\n  theme_minimal()+\n  scale_y_continuous(labels=percent)+\n    theme(legend.position = 'bottom',\n        axis.text = element_text(size=15),\n        plot.title = element_text(size=18,face='bold'))\n\n\n\n\n\n\nShow the code\n# Load data\ndata = pd.read_csv('Assets/bank-dataset.csv')\ndata['gender'] = np.where(data['gender'] == 'Male', 1, 0)\n\n# Fit logistic regression model\nX = data[['credit_score', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary','gender','age']]\nX = sm.add_constant(X)  # Add constant term for intercept\ny = data['churn']\nmodel = sm.Logit(y, X).fit()\n\n# Set all other variables to the mean\nnew_data = X.drop(columns=['gender','age']).mean(numeric_only=True).to_frame().transpose()\nnew_data = pd.DataFrame(np.repeat(new_data.values, 200, axis=0),columns=new_data.columns)\ngender_df = pd.DataFrame([*np.repeat(1,100,axis=0),*np.repeat(0,100,axis=0)], columns=['gender'])\nage_df = pd.DataFrame(pd.Series(range(1, 101)), columns=['age'])\nage_df =pd.concat([age_df,age_df],ignore_index=True,axis=0)\n\n# Concatenate with your existing DataFrame\nconcatenated_df = pd.concat([gender_df, age_df], ignore_index=True,axis=1)\nconcatenated_df = pd.concat([new_data,concatenated_df], ignore_index=True,axis=1)\nconcatenated_df.columns=[*new_data.columns,'gender','age']\n\n# Predict\npredicted = model.predict(concatenated_df)\n\n# Plot\nconcatenated_df['Predict'] = predicted\nconcatenated_df['Sex'] = np.where(concatenated_df['gender'] == 1, 'Male', 'Female')\n\nplt.figure(figsize=(10, 6))\nfor sex in concatenated_df['Sex'].unique():\n    plt.scatter(concatenated_df[concatenated_df['Sex'] == sex]['age'], concatenated_df[concatenated_df['Sex'] == sex]['Predict'],\n                label=sex, color='blue' if sex == 'Male' else 'pink')\nplt.title('Marginal effects of gender and age on Churn probabilities')\nplt.xlabel('Age')\nplt.ylabel('Predicted Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations",
    "title": "Introduction to Corporate Finance",
    "section": "Comparison across LPM and Logit estimations",
    "text": "Comparison across LPM and Logit estimations\n\nWe came across two estimators for binary choice models. Do they differ in terms of the responses?\nAlthough the coefficients from the different models are not directly comparable, we can use them to understand what are the partial effects:\nFor both cases, we’ll use all the other variables at their sample mean values:\n\nFor LPM, the effect will be a slope, i.e., the marginal change depending on Age will be constant\nFor Logit, the change in the predicted probability will vary depending on the Age reference point"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#comparing-across-lpm-and-logit-estimations---age",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#comparing-across-lpm-and-logit-estimations---age",
    "title": "Introduction to Corporate Finance",
    "section": "Comparing across LPM and Logit estimations - age",
    "text": "Comparing across LPM and Logit estimations - age\n\nResultRPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n#Read the data\nData=read.csv('Assets/bank-dataset.csv')%&gt;%mutate(gender=ifelse(gender=='Male',1,0))\n\n#Linear regression\nReg=lm(churn ~ credit_score + gender + age + tenure + balance + products_number + credit_card + active_member + estimated_salary,data = Data)\n\n#Set all other variables to the mean\nNewData=Data%&gt;%select(-c(churn,age))%&gt;%summarize(across(where(is.numeric),mean,na.rm=TRUE))\n\n#Include ranges for credit score\nNewData=NewData%&gt;%cbind(data.frame(age=seq(0,100)))\n\n#Predict LM\nLM_Predict=predict(Reg,NewData,type='response')\n\n#LOGIT regression\nReg=glm(churn ~ credit_score + gender + age + tenure + balance + products_number + credit_card + active_member + estimated_salary,data = Data,family=binomial(link='logit'))\n\n#Set all other variables to the mean\nNewData=Data%&gt;%select(-c(churn,age))%&gt;%summarize(across(where(is.numeric),mean,na.rm=TRUE))\n\n#Include ranges for credit score\nNewData=NewData%&gt;%cbind(data.frame(age=seq(0,100)))\n\n#Predict LM\nLOGIT_Predict=predict(Reg,NewData,type='response')\n\n#Convert into a data.frame\ndata.frame(LPM=LM_Predict,LOGIT=LOGIT_Predict)%&gt;%\n  gather(Model,Result)%&gt;%\n  cbind(Age=seq(0,100))%&gt;%\n  ggplot(aes(x=Age,y=Result,col=Model))+\n  geom_point(size=5)+\n  labs(title = 'Predicted Probabilities, varying on Age',\n       x='Age',\n       y='Predicted Probabilities')+\n  theme_minimal()+\n  scale_y_continuous(labels = percent)+\n  geom_hline(yintercept = c(0,1),linetype='dashed')+\n  geom_vline(xintercept = c(25,55),linetype='dashed')+\n    theme(legend.position = 'bottom',\n        axis.text = element_text(size=15),\n        plot.title = element_text(size=20,face='bold'))\n\n\n\n\n\n\nShow the code\n# Read the CSV file into a pandas DataFrame\nData = pd.read_csv('Assets/bank-dataset.csv')\n\n# Convert 'gender' into dummy variables\nData = pd.get_dummies(Data, columns=['gender'], drop_first=True)\n\n# Define independent and dependent variables\nX = Data[['credit_score', 'gender_Male', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary','age']]\ny = Data['churn']\n\n# Add a constant to the independent variables matrix for the intercept\nX = sm.add_constant(X)\n\n# Fit the linear regression model and the logit model\nlm_model = sm.OLS(y, X.astype(float)).fit()\nlogit_model = sm.Logit(y, X.astype(float)).fit()\n\n# Set all other variables to the mean\nnew_data = X.drop(columns=['age']).mean(numeric_only=True).to_frame().transpose()\nnew_data = pd.DataFrame(np.repeat(new_data.values, 100, axis=0),columns=new_data.columns)\nage_df = pd.DataFrame(pd.Series(range(1, 101)), columns=['age'])\n\n# Concatenate with your existing DataFrame\nconcatenated_df = pd.concat([new_data,age_df], ignore_index=True,axis=1)\nconcatenated_df.columns=[*new_data.columns,'age']\n\n# Predict\nlm_predicted = lm_model.predict(concatenated_df)\nlogit_predicted = logit_model.predict(concatenated_df)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(age_df, lm_predicted, label='LPM', marker='o', linestyle='-')\nplt.plot(age_df, logit_predicted, label='LOGIT', marker='o', linestyle='-')\n\n# Adding labels and title\nplt.xlabel('Age')\nplt.ylabel('Predicted Probabilities')\nplt.title('Comparison of LPM and Logit')\nplt.legend()\n\n# Displaying the plot\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations-continued",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations-continued",
    "title": "Introduction to Corporate Finance",
    "section": "Comparison across LPM and Logit estimations (continued)",
    "text": "Comparison across LPM and Logit estimations (continued)\n\nIn the LPM model, the effect of age is always the same – intuitively, \\(\\beta_{Age}\\), the partial derivative of Y with respect to Age, is the same for all levels\nOn the other hand, the changes when looking at the Logit model vary depending on the reference point for Age:\n\nFor values $[0,20], it increases modestly\nAround [21,60], the changes are exponential\nBetween [60,80], the changes start to become constant\nAfter 80, we see diminishing effects\n\nDo these differences matter in practice? It depends on what you’re looking…"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations-continued-1",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations-continued-1",
    "title": "Introduction to Corporate Finance",
    "section": "Comparison across LPM and Logit estimations (continued)",
    "text": "Comparison across LPM and Logit estimations (continued)\n\nOn the one hand, we clearly the weaknesses of the LPM when estimating probabilities for ages between \\([0,20]\\).\nOn the other hand, LPM and Logit will be almost identical between \\([30,60]\\)\n\n\nIf you tabulate the distribution of Age in your dataset, you’ll see that although LPM does a poor job in predicting probabilities for cases &lt;20, the sample proportion of these cases is less than 1%\nOn the other hand, its results are approximately the same as of Logit between \\([30,60]\\), which constitutes approximately 80% of the sample\n\n\nOk, do does it matter or not? Again, it depends…"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations-continued-2",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations-continued-2",
    "title": "Introduction to Corporate Finance",
    "section": "Comparison across LPM and Logit estimations (continued)",
    "text": "Comparison across LPM and Logit estimations (continued)\n\nLogit or LPM?\n\n\nIf your interest is to use the model results to predict probabilities for different age brackets, then yes, you should use Logit (or any model with the similar properties)\nIf, on the other hand, you’re just interest in knowing the effects for the average person in your sample, you can use LPM\n\n\nLPM is simpler, and we know very well the properties to analyze cases potential issues such as ommited variable bias\nIn the Logit world, there is no \\(R^2\\), but there are other ways to check the predictive ability and fit of models"
  },
  {
    "objectID": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#references-1",
    "href": "quant-mkt/Lecture 1 - Choice Models and Its Applications/index.html#references-1",
    "title": "Introduction to Corporate Finance",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nChapman, Chris, and Elea Mcdonnell Feit. 2015. R for Marketing Research and Analytics. Cham: Springer International Publishing.\n\n\nGreene, William H. 2011. Econometric Analysis. 7th ed. Upper Saddle River, NJ: Pearson.\n\n\nHayashi, Fumio. 2000. Econometrics. Princeton, NJ: Princeton University Press.\n\n\nTrain, Kenneth E. 2009. Discrete Choice Methods with Simulation. 2nd ed. Cambridge, England: Cambridge University Press."
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#a-brief-introduction",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#a-brief-introduction",
    "title": "Choice Models and its Applications",
    "section": "A brief introduction",
    "text": "A brief introduction\n\nBackground\n\n\nUndegraduate + Master’s in Finance and Economics @ University of São Paulo (USP)\nPh.D. in Economics (2023) @ INSPER - Institute of Education and Research\n\n\nAcademic Affiliations\n\n\nJan/24 - actual: Assistant Professor @ Getulio Vargas Foundation (FGV-EAESP), São Paulo, BR\n\n\nPrivate Affiliations\n\n\nDec/21 - actual: Consultant and Data Scientist @ Circana"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#how-to-use-these-slides",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#how-to-use-these-slides",
    "title": "Choice Models and its Applications",
    "section": "How to use these slides",
    "text": "How to use these slides\n\nThese slides leverage Quarto, an open-source scientific and technical publishing system from Posit (formerly RStudio):\n\nCreate dynamic content with Python, R, Julia, and Observable\nPublish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more\nWrite using Pandoc markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\nFor our course, we’ll use the following notation:\n\n\nLink will be colored in dark-orange\nInline equations and variables will be rendered in gray\nCode chunks will be provided along with outputs (in Python)"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#an-example-of-a-code-chunk",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#an-example-of-a-code-chunk",
    "title": "Choice Models and its Applications",
    "section": "An example of a code chunk",
    "text": "An example of a code chunk\n\nResultsPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Generating data\nnp.random.seed(0)\n\ndat = pd.DataFrame({\n    'cond': np.repeat(['A', 'B'], 10),\n    'xvar': np.arange(1, 21) + np.random.normal(0, 3, 20),\n    'yvar': np.arange(1, 21) + np.random.normal(0, 3, 20)\n})\n\ndat=tp.as_tibble(dat)\n\ntheme_plots= theme(\n  legend_position='bottom',\n  axis_title=element_text(face='bold',size=10),\n  plot_title=element_text(face='bold',size=15)\n  )\n\n# Plotting\nPlot=(\n  \n  ggplot(dat,aes(x='xvar',y='yvar',fill='cond',color='cond'))+\n              geom_point()+\n              labs(\n              title = 'An example of charting using the plotnine package',\n              subtitle = 'Randomly generated data',\n              x = 'X Variable',\n              y = 'Y Variable')\n            + scale_y_continuous(breaks = range(-10,25,5))\n            + scale_x_continuous(breaks = range(-10,25,5))\n            + theme_minimal()\n            + theme(\n              legend_position='bottom',\n              axis_title=element_text(face='bold',size=12),\n              axis_text=element_text(size=12),\n              plot_title=element_text(face='bold',size=20),\n              figure_size=(15,7)\n              )\n)\n\nPlot.show()"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#course-outline",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#course-outline",
    "title": "Choice Models and its Applications",
    "section": "Course Outline",
    "text": "Course Outline\n\nThe first section of the Quantitative Marketing course will cover choice models:\n\nBinary Response Models: Logit/Probit\nMulti-choice Models: Multinomial Logit\nEconomic Implications of multi-choice models\nMarket-basket Analysis and further topics on choice models\n\nTo the best of our capabilities, we will stick to a hands-on approach, focusing on practical applications and using the economic/econometric tools and concepts you’ve learned so far to interpret our findings\n\n\n\n\n\n\n\nHandouts\n\n\nTo maintain reproducibility, at the end of each class, a handout containing all the coding steps with detailed explanations will be provided. As such, students can download the file and replicate all the contents from each lecture on their end."
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#references",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#references",
    "title": "Choice Models and its Applications",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nChapman, Chris, and Elea Mcdonnell Feit. 2015. R for Marketing Research and Analytics. Cham: Springer International Publishing.\n\n\nGreene, William H. 2011. Econometric Analysis. 7th ed. Upper Saddle River, NJ: Pearson.\n\n\nHayashi, Fumio. 2000. Econometrics. Princeton, NJ: Princeton University Press.\n\n\nTrain, Kenneth E. 2009. Discrete Choice Methods with Simulation. 2nd ed. Cambridge, England: Cambridge University Press."
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#discrete-choice-models",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#discrete-choice-models",
    "title": "Choice Models and its Applications",
    "section": "Discrete Choice Models",
    "text": "Discrete Choice Models\n\nMarketing practitioners are often concerned about consumers decisions around a defined set of choices:\n\nConsumers choose between a binary decision of buy/not buy, given product, consumer, and environmental characteristics\nFaced with a defined set of products, make a decision around which products to buy\nAfter experiencing a flow of consumption from a product or service up to time \\(t\\), decide whether to continue consumption at \\(t+1\\) or churn\n\nEliciting knowledge about individual choices shed light on consumer preferences and help marketers to understand how consumers value certain product attributes\nAggregating individual choices helps marketers understand how the demand curve for a given product or industry works"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#binary-choice-models",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#binary-choice-models",
    "title": "Choice Models and its Applications",
    "section": "Binary Choice Models",
    "text": "Binary Choice Models\n\nMotivation: among a defined set of two mutually exclusive alternatives, consumers make a decision by choosing one alternative in spite of the other:\n\n\\[\n\\small\nY_i=\\begin{cases}\n            1, & \\text{if $Y_i$ = a predefined alternative}\\\\\n            0, & \\text{otherwise}\n         \\end{cases}\n\\]\n\nThese models implicitly tell about consumer’s preferences regarding several consumption decisions. Say that we know that a set \\(X\\) of characteristics (which may be consumer-specific, environmental, or product) affect \\(i\\)’s decision. We can then model such decision by:\n\n\\[\n\\small Y_i=f(X_i)\n\\]\n\nOur starting point will be to define what \\(f(X)\\) should look like. Depending on the shape of this function, the interpretation becomes a probabilistic measurement"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#binary-choice-models---application",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#binary-choice-models---application",
    "title": "Choice Models and its Applications",
    "section": "Binary Choice Models - Application",
    "text": "Binary Choice Models - Application\n\nOne of the most widely applicable areas of binary choice modeling refers to churn modeling:\n\n\n\n\n\n\n\nDefinition\n\n\nChurn refers to the the % of customers who discontinue their use of a business’s products or services over a certain time period. This concept is important for businesses because it affects their revenue and offers insights into customer satisfaction and loyalty:\n\nA high churn rate may suggest there are issues with the product, service, or overall customer experience\nA low churn rate may suggest the existence of customer loyalty, high switching costs, or valuable attributes from the offer\n\n\n\n\n\nChurn modeling offers valuable insights to marketers as it is possible to derive actions ex-ante its occurrence and prevent customers from churning\nThis issue becomes more relevant when customer acquisition costs increase"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#practical-application-outline",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#practical-application-outline",
    "title": "Choice Models and its Applications",
    "section": "Practical Application Outline",
    "text": "Practical Application Outline\n\nYou will use a bank Customer Relationship Manager (CRM) data set comprising of 10,000 bank customers and its actual engagement status (whether or not he/she has churned)\nThis data set will be primarily based on this Kaggle notebook, although some adaptations have been made for teaching purposes"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#before-we-start-tech-setup",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#before-we-start-tech-setup",
    "title": "Choice Models and its Applications",
    "section": "Before we start: tech-setup",
    "text": "Before we start: tech-setup\n\nTo install all necessary Python libraries, use the terminal to install and then import all packages inside a notebook. First, install all dependencies by downloading the requirements.txt file and running the following script on your terminal:\n\n\npip install requirements.txt\n\n\nAfter you have everything properly installed, it is time to load the packages in your session - make sure to load this at the beginning of your script/notebook:\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom plotnine import *\nfrom great_tables import GT, md\nfrom mizani.formatters import percent_format\n\n\n\n\n\n\n\nTech-setup\n\n\nIn the official page (link here) of our course, I have outlined a few steps for you to get started using Python and install all dependencies that we will be using."
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#a-quick-outline-of-the-dataset",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#a-quick-outline-of-the-dataset",
    "title": "Choice Models and its Applications",
    "section": "A quick outline of the dataset",
    "text": "A quick outline of the dataset"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#variable-descriptives",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#variable-descriptives",
    "title": "Choice Models and its Applications",
    "section": "Variable Descriptives",
    "text": "Variable Descriptives\n\nTo begin our investigation, let’s do a simple variable description of the variables that we have in our dataset:\n\n\nResultPython\n\n\n\n\n\n\n\n\n\n\nSummary Statistics\n\n\n\ncredit_score\nage\ntenure\nbalance\nproducts_number\ncredit_card\nactive_member\nestimated_salary\nchurn\n\n\n\n\ncount\n10,000\n10,000\n10,000\n$10,000\n10,000.00\n10,000.00\n10,000.00\n$10,000\n10,000.00\n\n\nmean\n651\n39\n5\n$76\n1.53\n0.71\n0.52\n$100\n0.20\n\n\nstd\n97\n10\n3\n$62\n0.58\n0.46\n0.50\n$58\n0.40\n\n\nmin\n350\n18\n0\n$0\n1.00\n0.00\n0.00\n$0\n0.00\n\n\n25%\n584\n32\n3\n$0\n1.00\n0.00\n0.00\n$51\n0.00\n\n\n50%\n652\n37\n5\n$97\n1.00\n1.00\n1.00\n$100\n0.00\n\n\n75%\n718\n44\n7\n$128\n2.00\n1.00\n1.00\n$149\n0.00\n\n\nmax\n850\n92\n10\n$251\n4.00\n1.00\n1.00\n$200\n1.00\n\n\n\n\n\n\n        \n\n\n\n\n\n# Read the CSV file into a pandas DataFrame\nData = pd.read_csv('Assets/bank-dataset.csv')\nSummary = Data.drop('customer_id',axis=1).describe().reset_index()\n\n# Print the structure of the DataFrame\nTable = (\n  GT(Summary)\n  .cols_align('center')\n  .tab_header(title=md(\"**Summary Statistics**\"))\n  .tab_stub('index')\n  .fmt_number(columns=['credit_score','age','tenure'],decimals = 0)\n  .fmt_number(columns=['products_number','credit_card','active_member','churn'],decimals = 2)\n  .fmt_currency(columns=['balance','estimated_salary'],decimals=0)\n  .opt_stylize(style=1,color='red')\n)\n\n#Output\nTable.tab_options(table_width=\"100%\",table_font_size=\"25px\")"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#analyzing-churn-distribution",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#analyzing-churn-distribution",
    "title": "Choice Models and its Applications",
    "section": "Analyzing churn distribution",
    "text": "Analyzing churn distribution\n\nResultPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Select numeric columns and calculate correlation with churn\ncorrelation_df = (\n  Data\n  .select_dtypes(include='number')\n  .corr()\n  .iloc[:-1, -1]\n  .reset_index()\n)\n\n# Rename columns\ncorrelation_df.columns = ['Variable', 'Correlation']\n\n# Add 'Sign' column based on correlation values\ncorrelation_df['Sign'] = correlation_df['Correlation'].apply(lambda x: 'Positive' if x &gt; 0 else 'Negative')\n\n#Chart\nPlot = (\n  ggplot(correlation_df, aes(x='Variable', y='Correlation', fill='Sign'))+\n  geom_col()+\n  geom_text(aes(label=correlation_df['Correlation'].round(2)),size=10, color='black',position=position_stack(vjust=0.5))+  # Add annotations\n  scale_fill_manual(values=['red','green'])+\n  coord_flip()+\n  labs(title='Churn correlation across numeric variables')+\n  theme_minimal()+\n  theme_plots +\n  theme(\n      axis_title_x=element_blank(),\n      axis_title_y=element_blank())\n  )\n\n#Display Output\nPlot.show()"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#modeling-discrete-choice-models",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#modeling-discrete-choice-models",
    "title": "Choice Models and its Applications",
    "section": "Modeling Discrete Choice Models",
    "text": "Modeling Discrete Choice Models\n\nSay that we are interested in modeling the occurrence of churn, which is a discrete variable for each customer \\(i\\):\n\n\\[\nY_i=\n\\begin{cases}\n1, \\text{if the customer has churned}\\\\\n0,  \\text{if the customer is still an active client}\n\\end{cases}\n\\]\n\nIf we have a set of covariates, \\(X_i\\), then we can define the relationship:\n\n\\[\nY_i=f(X_i)\n\\]\n\nHow our \\(f(X_i)\\) should look like?\nWhat is the interpretation of the estimates that we’ll find?"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#linear-probability-models-lpm",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#linear-probability-models-lpm",
    "title": "Choice Models and its Applications",
    "section": "Linear Probability Models (LPM)",
    "text": "Linear Probability Models (LPM)\n\nThe simplest approach that we can take is to assume that \\(Y\\) is linear on the set of characteristics \\(X\\). In other words, \\(f(X)\\) is a linear model:\n\n\\[\nY_i= P(Y_i=1|X_i)=\\alpha + \\beta_1x_1+\\beta_2x_2+...+\\beta_nx_n+\\varepsilon_i\n\\]\n\nWe can then use Ordinary Least Squares (OLS) to model such relationship. Because the functional form of \\(f(X)\\) is assumed to be linear, we call this a linear probability model (LPM):\nBecause the relationship between \\(Y\\) and \\(X\\) is assumed to be linear, the changes in the probability (or likelihood) of churn are linear on the parameters \\(\\beta\\). For example, for \\(x_1\\):\n\n\\[\n\\dfrac{\\partial Y}{\\partial x_1}=\\beta_1\n\\]"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#is-the-lpm-a-biased-estimator",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#is-the-lpm-a-biased-estimator",
    "title": "Choice Models and its Applications",
    "section": "Is the LPM a biased estimator?",
    "text": "Is the LPM a biased estimator?\n\nIf the OLS assumptions are valid, \\(\\beta_{OLS}\\) is still a consistent estimator of the population average relationship between \\(Y\\) and \\(X\\), regardless of the type of dependent variable:\n\n\\[\n\\begin{align}\n& \\beta_{OLS}=(X'X)^{-1}X'Y\\\\\n& \\beta_{OLS}=(X'X)^{-1}X'(X\\beta+\\epsilon)=\\underbrace{(X'X)^{-1}(X'X)}_{I}\\beta+(X'X)^{-1}X'\\epsilon\\\\\n& \\beta_{OLS}=\\beta + (X'X)^{-1}X'\\epsilon\n\\end{align}\n\\]\n\nIf we assume that \\(X\\perp\\epsilon\\) - i.e, no ommited variables-, we see that \\(\\beta_{OLS}\\) is an unbiased estimator of the average effect of \\(X\\) on \\(Y\\):\n\n\\[\n\\text{if } X\\perp\\epsilon \\rightarrow \\beta_{OLS}=\\beta + (X'X)^{-1}\\underbrace{X'\\epsilon}_{=0}\\rightarrow \\beta_{OLS}=\\beta\n\\]"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#modeling-churn-via-lpm",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#modeling-churn-via-lpm",
    "title": "Choice Models and its Applications",
    "section": "Modeling churn via LPM",
    "text": "Modeling churn via LPM\n\nSuppose we want to understand the determinants of customer’s churn over time. For that, we’ll consider the following variables:\n\n\\[\n\\text{if } X=\\begin{bmatrix} CreditScore \\\\ D(Gender) \\\\ Age \\\\ Tenure \\\\ Balance \\\\ \\#Products \\\\ D(CreditCard) \\\\ D(Active) \\\\ Salary \\end{bmatrix} \\rightarrow Y \\sim X'\\beta+\\epsilon\n\\]"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#lpm-estimation",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#lpm-estimation",
    "title": "Choice Models and its Applications",
    "section": "LPM Estimation",
    "text": "LPM Estimation\n\nResultsPython\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  churn   R-squared:                       0.136\nModel:                            OLS   Adj. R-squared:                  0.135\nMethod:                 Least Squares   F-statistic:                     174.6\nDate:                Fri, 04 Apr 2025   Prob (F-statistic):          8.76e-309\nTime:                        13:05:07   Log-Likelihood:                -4364.5\nNo. Observations:               10000   AIC:                             8749.\nDf Residuals:                    9990   BIC:                             8821.\nDf Model:                           9                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst               -0.1025      0.034     -3.027      0.002      -0.169      -0.036\ncredit_score     -9.264e-05   3.88e-05     -2.389      0.017      -0.000   -1.66e-05\ngender_Male         -0.0773      0.008    -10.267      0.000      -0.092      -0.063\nage                  0.0113      0.000     31.505      0.000       0.011       0.012\ntenure              -0.0018      0.001     -1.426      0.154      -0.004       0.001\nbalance              0.0007   6.31e-05     11.003      0.000       0.001       0.001\nproducts_number     -0.0043      0.007     -0.630      0.529      -0.018       0.009\ncredit_card         -0.0030      0.008     -0.360      0.719      -0.019       0.013\nactive_member       -0.1432      0.008    -19.016      0.000      -0.158      -0.128\nestimated_salary  7.118e-05   6.52e-05      1.092      0.275   -5.66e-05       0.000\n==============================================================================\nOmnibus:                     1513.285   Durbin-Watson:                   1.994\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             2319.357\nSkew:                           1.176   Prob(JB):                         0.00\nKurtosis:                       3.176   Cond. No.                     6.09e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 6.09e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n\n# Read the CSV file into a pandas DataFrame\nData = pd.read_csv('Assets/bank-dataset.csv')\n\n# Convert 'gender' into dummy variables\nData = pd.get_dummies(Data, columns=['gender'], drop_first=True)\n\n# Define independent and dependent variables\nX = Data[['credit_score', 'gender_Male', 'age', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary']]\ny = Data['churn']\n\n# Add a constant to the independent variables matrix for the intercept\nX = sm.add_constant(X)\n\n# Fit the linear regression model\nmodel = sm.OLS(y, X.astype(float)).fit()\n\n# Print the regression summary\nprint(model.summary())"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#overall-analysis",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#overall-analysis",
    "title": "Choice Models and its Applications",
    "section": "Overall analysis",
    "text": "Overall analysis\n\nAll else equal, an additional year increases the likelihood of churning by:\n\n\\[\n\\partial Y/\\partial Age=\\beta_{Age}\\approx 0.0113\n\\]\n\nMoving from the \\(25^{th}\\) to the \\(75^{th}\\) percentile of age increases the likelihood by \\(\\beta_{Age}\\times IQR(Age)=(44-32)\\times 0.0113 \\approx 14\\) or 14%\n\n\nMale customers tend to churn, on average, \\(7.7\\%\\) less than Female customers:\n\n\\[\n\\partial Y/\\partial D(genderMale)=\\beta_{genderMale}\\approx =-0.078\n\\]\n\nSimilar analyses can be made for the following continuous variables (see Code in next slide)"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#analyses",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#analyses",
    "title": "Choice Models and its Applications",
    "section": "Analyses",
    "text": "Analyses\n\nResultPython\n\n\n\n\n\n\n\n\n\n\nSummary Statistics\n\n\n\nChange in Covariate\nCoefficient from OLS\nChange on Outcome (in %)\n\n\n\n\ncredit_score\n134\n−0.0001\n−1.24%\n\n\nage\n12\n0.0113\n13.57%\n\n\ntenure\n4\n−0.0018\n−0.74%\n\n\nbalance\n128\n0.0007\n8.86%\n\n\nproducts_number\n1\n−0.0043\n−0.43%\n\n\nestimated_salary\n98\n0.0001\n0.70%\n\n\ngender_Male\n1\n−0.0773\n−7.73%\n\n\ncredit_card\n1\n−0.0030\n−0.30%\n\n\nactive_member\n1\n−0.1432\n−14.32%\n\n\n\n\n\n\n        \n\n\n\n\n\n# Define continuous and dummy variables\ncontinuous_vars = ['credit_score', 'age', 'tenure', 'balance', 'products_number', 'estimated_salary']\ndummy_vars = ['gender_Male', 'credit_card', 'active_member']\n\n# Get coefficient estimates from the model\nestimates = model.params.reset_index()\nestimates.columns = ['Variable', 'estimate']\n\n# Calculate IQR for continuous variables\niqr_df = Data[continuous_vars].apply(lambda x: x.quantile(0.75) - x.quantile(0.25), axis=0)\niqr_df = pd.DataFrame({'Variable': iqr_df.index, 'Change': iqr_df.values})\n\n# Create a DataFrame for dummy variables\ndummy_df = pd.DataFrame({'Variable': dummy_vars, 'Change': 1})\n\n# Combine continuous and dummy variable dataframes\ncombined_df = pd.concat([iqr_df, dummy_df])\n\n# Merge with coefficient estimates\nresult_df = pd.merge(combined_df, estimates, on='Variable', how='left')\n\n# Calculate partial changes\nresult_df['partial_change'] = (result_df['Change'] * result_df['estimate'])\n\n#Table\nTable= (\n  GT(result_df)\n  .cols_align('center')\n  .tab_stub(rowname_col='Variable')\n  .cols_label(\n    Change = 'Change in Covariate',\n    estimate = 'Coefficient from OLS',\n    partial_change = 'Change on Outcome (in %)'\n    )\n  .fmt_number(columns = 'Change',decimals=0)\n  .fmt_number(columns = 'estimate',decimals=4)\n  .fmt_percent(columns='partial_change',decimals=2)\n  .tab_header(title=md(\"**Summary Statistics**\"))\n  .opt_stylize(style=1,color='red')\n)\n\nTable.tab_options(table_width=\"100%\",table_font_size=\"25px\")"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#limitations-of-the-lpm",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#limitations-of-the-lpm",
    "title": "Choice Models and its Applications",
    "section": "Limitations of the LPM",
    "text": "Limitations of the LPM\n\nAlthough a consistent estimator of the average effect, the LPM has limitations when it comes to its practical implications to binary outcomes:\n\n\nChurn probabilities should lie within \\([0,1]\\), but the predicted probabilities, \\(\\hat{Y}\\), have continuous support \\((-\\infty,+\\infty)\\). When the goal is to predict outcomes, this creates probabilities that are outside of the ranges\nThere is an implicit assumption that the effects are linear, which may not hold true. Example, an increase of $1,000 in customer’s income may have significant impacts on churn likelihood when customers are from the bottom of the income distribution, but the effects should dampen as we move towards the top of the income distribution\nHeteroskedasticity\n\\(R^2\\) is not well-defined"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#limitations-of-the-lpm-1",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#limitations-of-the-lpm-1",
    "title": "Choice Models and its Applications",
    "section": "Limitations of the LPM",
    "text": "Limitations of the LPM\n\nPoor FitHeteroskedasticity\\(R^2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEven though we can achieve consistent estimators, we’ll have heteroskedasticity in our estimates by construction. To see that, recall that the conditional expectation of Y given X is given by:\n\n\\[\n\\small E(Y|X)=\\underbrace{[P(Y=1|X]}_{X\\beta\\times 1} + \\underbrace{[1-P(Y=1|X)]\\times 0}_{=0} = X\\beta\n\\]\n\nGiven that the variance of a Bernoulli Distribution is given by \\(p \\times (1-p)\\), then:\n\n\\[\nV(Y|X)=P(Y|X)\\times[1-P(Y|X)]=X\\beta\\times(1-X\\beta) \\text{, which clearly depends on } X\n\\]\n\n\n\nIn most linear probability models, \\(R^2\\) has no meaningful interpretation:\n\n\nRegression line can never fit the data perfectly if the dependent variable is binary and the regressors are continuous\nSince LPM predicts probabilities, the dependent variable is binary (\\(0\\) or \\(1\\)), making the interpretation of \\(R^2\\) problematic. The model often produces predicted values outside the \\([0,1]\\) range, leading to unreliable goodness-of-fit assessments"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#introducing-non-linear-binary-models",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#introducing-non-linear-binary-models",
    "title": "Choice Models and its Applications",
    "section": "Introducing non-linear binary models",
    "text": "Introducing non-linear binary models\n\nRecall that the limitations of the LPM stem from the fact that \\(f(X)=X'\\beta\\). It might be that a linear relationship does not capture all aspects that a churn analysis should have!\nTo that point, we need to think about a new relationship, \\(Y=f(X)\\), that fulfills the following points:\n\n\nThe predicted outcomes, \\(\\hat{Y}\\), lie between 0 and 1\nThe effects do not need to be linear on the parameters\nIdeally, we’d want to apply a non-linear transformation in such a way that the relationship between Y and X is sigmoid (or S-shaped) curve: the changes in the predicted probability tend to go to zero as we approach the lower and upper bounds of the distribution of X\n\n\nThe most known cases are logistic regression (\\(f(x)=\\Lambda (x)\\)) and Probit (\\(f(x)=\\Phi(X)\\))"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logistic-regression",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logistic-regression",
    "title": "Choice Models and its Applications",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nLike any other transformation function, the idea behind using \\(\\Lambda(X)\\) lies on the latent variable approach: think about an unobserved component, \\(Y^\\star\\), which is a continuous variable, such as how much a consumer values a product.\nAlthough we do not observe \\(Y^\\star\\), we do observe the consumer’s decision of buying or not buying the product, depending on a given threshold:\n\n\\[\nY=\n\\begin{cases}\n1, \\text{ if }Y^\\star&gt;0\\\\\n0, \\text{ if }Y^\\star\\leq0\\\\\n\\end{cases}\n\\]\n\nTherefore, we can see that the probability of buying depends on a latent variable, which is not observed by the econometrician:\n\n\\[\nP(Y=1|X)=P(Y^\\star&gt;0|X)=P(\\underbrace{X\\beta+\\varepsilon}_{Y^{\\star}}&gt;0|X)\n\\]"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logistic-regression-continued",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logistic-regression-continued",
    "title": "Choice Models and its Applications",
    "section": "Logistic Regression, continued",
    "text": "Logistic Regression, continued\n\nWe can rearrange terms and find that:\n\n\\[\nP(Y=1|X)=P(\\underbrace{X\\beta+\\varepsilon}_{Y^{\\star}}&gt;0|X)\\rightarrow \\underbrace{P(\\varepsilon&gt;-X\\beta|X)\\equiv P(\\varepsilon&lt;X\\beta|X)}_{\\text{Under Simmetry}}\n\\]\n\nTherefore, this framework can be used to think about any cumulative density function which has the simmetry property. For the case of Logistic Regression, our transformation function that maps \\(Y^\\star\\) (how much consumer values a good) to \\(Y\\) (decision to buy or not buy) is:\n\n\\[\nf(Y^\\star)=\\Lambda(Y^\\star)=\\dfrac{\\exp(X\\beta)}{1+\\exp(X\\beta)}\n\\]\n\nEstimation is made using Maximum Likelihood estimators."
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#applying-logistic-regression",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#applying-logistic-regression",
    "title": "Choice Models and its Applications",
    "section": "Applying Logistic Regression",
    "text": "Applying Logistic Regression\n\nResultPython\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.435323\n         Iterations 6\n\n\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                  churn   No. Observations:                10000\nModel:                          Logit   Df Residuals:                     9990\nMethod:                           MLE   Df Model:                            9\nDate:                Fri, 04 Apr 2025   Pseudo R-squ.:                  0.1388\nTime:                        13:05:08   Log-Likelihood:                -4353.2\nconverged:                       True   LL-Null:                       -5054.9\nCovariance Type:            nonrobust   LLR p-value:                1.475e-296\n====================================================================================\n                       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst               -3.4761      0.241    -14.397      0.000      -3.949      -3.003\ncredit_score        -0.0007      0.000     -2.349      0.019      -0.001      -0.000\ngender_Male         -0.5429      0.054    -10.060      0.000      -0.649      -0.437\nage                  0.0729      0.003     28.561      0.000       0.068       0.078\ntenure              -0.0147      0.009     -1.585      0.113      -0.033       0.003\nbalance              0.0050      0.000     10.964      0.000       0.004       0.006\nproducts_number     -0.0362      0.046     -0.780      0.436      -0.127       0.055\ncredit_card         -0.0290      0.059     -0.493      0.622      -0.144       0.086\nactive_member       -1.0793      0.057    -18.860      0.000      -1.191      -0.967\nestimated_salary     0.0005      0.000      1.060      0.289      -0.000       0.001\n====================================================================================\n\n\n\n\n\n# Read the CSV file into a pandas DataFrame\nData = pd.read_csv('Assets/bank-dataset.csv')\n\n# Convert 'gender' into dummy variables\nData = pd.get_dummies(Data, columns=['gender'], drop_first=True)\n\n# Define independent and dependent variables\nX = Data[['credit_score', 'gender_Male', 'age', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary']]\ny = Data['churn']\n\n# Add a constant to the independent variables matrix for the intercept\nX = sm.add_constant(X)\n\n# Fit the linear regression model\nmodel = sm.Logit(y, X.astype(float)).fit()\n\n# Print the regression summary\nprint(model.summary())"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-1-probabilities-within-the-01-interval",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-1-probabilities-within-the-01-interval",
    "title": "Choice Models and its Applications",
    "section": "Logit Properties #1: probabilities within the [0,1] interval",
    "text": "Logit Properties #1: probabilities within the [0,1] interval\n\nOne interesting thing to note is that, due to the properties of \\(\\Lambda(\\cdot)\\), our estimated probabilities will fall within \\([0,1]\\) depending on \\(X\\):\n\n\nWhen \\(X\\rightarrow\\infty\\), the probability of buying in our example tends to 1:\n\n\\[\n\\Lambda(Y)=\\dfrac{\\exp(Y)}{1+\\exp(Y)}=\\dfrac{\\exp(X\\beta)}{1+\\exp(X\\beta)}\\rightarrow1\n\\]\n\nOn the other hand, when \\(X\\rightarrow -\\infty\\), the probability of buying in our example tends to 0:\n\n\\[\n\\Lambda(Y)=\\dfrac{\\exp(Y)}{1+\\exp(Y)}=\\dfrac{\\exp(X\\beta)}{1+\\exp(X\\beta)}\\rightarrow \\dfrac{0}{1}=0\n\\]"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-2-the-odds-ratio",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-2-the-odds-ratio",
    "title": "Choice Models and its Applications",
    "section": "Logit Properties #2: the odds-ratio",
    "text": "Logit Properties #2: the odds-ratio\n\nRecall that the logit estimation for \\(\\small P(Y|X)\\) is given by:\n\n\\[\n\\small\nP(Y)=\\dfrac{\\exp(X\\beta)}{1+\\exp(X\\beta)},\\text{ which we will call by } p\n\\]\n\nLooking at the inverse, \\(1/p\\), we can see that:\n\n\\[\n\\small\n\\dfrac{1}{p}=\\dfrac{1+\\exp(X\\beta)}{\\exp(X\\beta)}=1+\\dfrac{1}{\\exp{X\\beta}}\\rightarrow \\dfrac{1-p}{p}=\\dfrac{1}{\\exp{X\\beta}}\n\\]\n\nInverting and taking logs on both sides, we’ll have:\n\n\\[\n\\small \\log{\\bigg(\\dfrac{p}{1-p}}\\bigg)=X\\beta=\\alpha+\\beta_1x_1+\\beta_2x_2+...\\beta_kx_k\n\\]"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-2-the-odds-ratio-continued",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-2-the-odds-ratio-continued",
    "title": "Choice Models and its Applications",
    "section": "Logit Properties #2: the odds-ratio (continued)",
    "text": "Logit Properties #2: the odds-ratio (continued)\n\nTherefore, whenever we’re estimating a logit model, our transformation function, \\(\\Lambda(\\cdot)\\) is actually estimating \\(\\small \\dfrac{p}{1-p}\\):\n\n\\[\n\\small logit(p)=\\alpha+\\beta_1x_1+\\beta_2x_2+...\\beta_kx_k\n\\]\n\nThe term \\(\\small \\dfrac{p}{1-p}\\) is called odds-ratio, and is simply the ratio of the probability of success over the probability of failure\nHence, if we want to recover the impacts of any change in the odds-ratio due to our covariates, we can exponentiate our coefficients:\n\n\\[\n\\small \\text{if } \\log\\bigg(\\dfrac{p}{1-p}\\bigg)=\\alpha+\\beta_1x_1+\\beta_2x_2+...\\beta_kx_k\\rightarrow \\dfrac{p}{1-p}=\\exp(\\alpha+\\beta_1x_1+\\beta_2x_2+...\\beta_kx_k)\n\\]"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-2-the-odds-ratio-continued-1",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-2-the-odds-ratio-continued-1",
    "title": "Choice Models and its Applications",
    "section": "Logit Properties #2: the odds-ratio (continued)",
    "text": "Logit Properties #2: the odds-ratio (continued)\n\nExplanationResultPython\n\n\n\n\nIn previous slides, we noted that the effect of Gender is \\(\\small\\beta_2=-0.54\\). Holding everything fixed, the odds-ratio between Gender=1 (Male) versus Gender=0 (Female) is given by:\n\n\\[\n\\small \\dfrac{p}{1-p}=\\exp(\\beta_2)=\\exp(-0.54)\\approx0.58\n\\]\n\nPut another way, the chances of being a churned client are \\(\\small 0.58-1=-0.42\\) or 42% less likely for men!\nSimilarly, if we look at Age (\\(\\beta_3=0.07\\)), one additional year increases churn probability by:\n\n\\[\n\\exp(\\beta_1)=[\\exp(0.0728688)-1]\\approx 0.07 \\text{ or } 7.55\\%\n\\]\n\n\n\n\n\n\n\n\n\n\n\nSummary Statistics\n\n\n\nOddsRatio\nMultiplier\nChangeOdds\n\n\n\n\nconst\n3.09%\n1\n−96.91%\n\n\ncredit_score\n99.93%\n100\n−7.00%\n\n\ngender_Male\n58.10%\n1\n−41.90%\n\n\nage\n107.56%\n1\n7.56%\n\n\ntenure\n98.54%\n1\n−1.46%\n\n\nbalance\n100.51%\n10\n5.10%\n\n\nproducts_number\n96.45%\n1\n−3.55%\n\n\ncredit_card\n97.14%\n1\n−2.86%\n\n\nactive_member\n33.98%\n1\n−66.02%\n\n\nestimated_salary\n100.05%\n100\n5.00%\n\n\n\n\n\n\n        \n\n\n\n\n\n# Calculate Odds Ratio and Change in Odds\nodds_ratio = pd.DataFrame({\n  'OddsRatio': model.params.apply(lambda x: round(np.exp(x), 4)),\n  'Multiplier': [1, 100, 1, 1, 1, 10, 1, 1, 1, 100]\n  })\n\nodds_ratio = odds_ratio.reset_index()  \n  \nodds_ratio['ChangeOdds'] = ((odds_ratio['OddsRatio'] - 1) * odds_ratio['Multiplier']).map(lambda x: f\"{x:.2%}\")\n\n#Table\nTable = (\n  GT(odds_ratio)\n  .cols_align('center')\n  .tab_stub(rowname_col='index')\n  .cols_label(\n    OddsRatio = 'Odds Ratio',\n    Multiplier = 'Multiplier',\n    ChangeOdds = 'Change in Probality (in p.p)'\n    )\n  .fmt_number(columns = 'odds',decimals=0)\n  .fmt_percent(columns=['OddsRatio','ChangeOdds'],decimals=2)\n  .tab_header(title=md(\"**Summary Statistics**\"))\n  .opt_stylize(style=1,color='red')\n)\n\nTable.tab_options(table_width=\"100%\",table_font_size=\"25px\")"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-3-different-marginal-effects",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-3-different-marginal-effects",
    "title": "Choice Models and its Applications",
    "section": "Logit Properties #3: different marginal effects",
    "text": "Logit Properties #3: different marginal effects\n\nExplanationResultPython\n\n\n\n\nOne of the caveats of LPM was that the marginal effect was constant, which does not make a lot of sense from a probabilistic sense\nWith Logit, the marginal effects are not equal to \\(\\beta\\) anymore. To see that, take the derivative of \\(P(Y|X)\\) with respect to \\(x_1\\):\n\n\\[\n\\dfrac{\\partial\\Lambda(X\\beta)}{\\partial x_1}=\\beta_1 \\times\\dfrac{\\partial\\Lambda(X\\beta)}{\\partial X\\beta}\n\\]\n\nAs we can see, the effects are not going to be linear anymore!\nPut another way, given different levels of X, we may have different marginal effects on the probabilities - in what follows, we’ll analyze the case of age\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Load data\ndata = pd.read_csv('Assets/bank-dataset.csv')\ndata['gender'] = np.where(data['gender'] == 'Male', 1, 0)\n\n# Fit logistic regression model\nX = data[['credit_score', 'gender', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary','age']]\nX = sm.add_constant(X)  # Add constant term for intercept\ny = data['churn']\nmodel = sm.Logit(y, X).fit()\n\n# Set all other variables to the mean\nnew_data = X.drop(columns=['age']).mean(numeric_only=True).to_frame().transpose()\nnew_data = pd.DataFrame(np.repeat(new_data.values, 100, axis=0),columns=new_data.columns)\nage_df = pd.DataFrame(pd.Series(range(1, 101)), columns=['age'])\n\n# Concatenate with your existing DataFrame\nconcatenated_df = pd.concat([new_data, age_df], ignore_index=True,axis=1)\nconcatenated_df.columns=[*new_data.columns,'age']\n\n# Predict on the new data\npredicted_df = pd.DataFrame({\n    'Age': age_df['age'],\n    'Predicted': model.predict(concatenated_df)\n    })\n\n# Plot\nPlot = (\n  ggplot(predicted_df,aes(x='Age',y='Predicted'))+\n  geom_point(color='blue',size=2)+\n  geom_hline(yintercept=[0,1],linetype='dashed',color='darkred')+\n  scale_y_continuous(labels = percent_format())+\n  scale_x_continuous(breaks = range(0,100,10))+\n  labs(\n    title ='Marginal effects of Age on Churn probabilities',\n    subtitle = 'Using the estimates from a Logit model.', \n    x = 'Age',\n    y = 'Predicted Probabilities')+\n  theme_minimal()+\n  theme(\n    legend_position='bottom',\n    axis_title=element_text(face='bold',size=12),\n    axis_text=element_text(size=12),\n    plot_title=element_text(face='bold',size=20),\n    figure_size=(15,7)\n    )\n  )\n  \nPlot.show()"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-4-different-effects-by-categorical-variables",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#logit-properties-4-different-effects-by-categorical-variables",
    "title": "Choice Models and its Applications",
    "section": "Logit Properties #4: different effects by categorical variables",
    "text": "Logit Properties #4: different effects by categorical variables\n\nExplanationResultPython\n\n\n\n\nAnother interesting use case is to re-do the same analysis before, but now varying also on categorical variables\nFor example, what is the difference in the probability of buying by men and women?\nIn order to do that, we can set all continous variables to their means and compare the estimated probabilities for gender=1 (male) and gender=0 (female)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Load data\ndata = pd.read_csv('Assets/bank-dataset.csv')\ndata['gender'] = np.where(data['gender'] == 'Male', 1, 0)\n\n# Fit logistic regression model\nX = data[['credit_score', 'age', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary','gender']]\nX = sm.add_constant(X)  # Add constant term for intercept\ny = data['churn']\nmodel = sm.Logit(y, X).fit()\n\n# Set all other variables to the mean\nnew_data = X.drop(columns=['gender']).mean(numeric_only=True).to_frame().transpose()\nnew_data = pd.DataFrame(np.repeat(new_data.values, 2, axis=0),columns=new_data.columns)\ngender_df = pd.DataFrame(pd.Series(range(0, 2)), columns=['gender'])\n\n# Concatenate with your existing DataFrame\nconcatenated_df = pd.concat([new_data, gender_df], ignore_index=True,axis=1)\nconcatenated_df.columns=[*new_data.columns,'gender']\n\n# Predict on the new data\npredicted_df = pd.DataFrame({\n    'Gender': gender_df['gender'],\n    'Predicted': model.predict(concatenated_df)\n    })\n\npredicted_df['Gender'] = np.where(concatenated_df['gender'] == 1, 'Male', 'Female')\n\n# Plot\nPlot = (\n  ggplot(predicted_df,aes(x='Gender',y='Predicted',fill='Gender'))+\n  geom_col()+\n  geom_text(aes(label=predicted_df['Predicted']*100),format_string=\"{:.2f}%\",size=20)+\n  scale_y_continuous(labels = percent_format())+\n  labs(\n    title ='Marginal effects of Gender on Churn probabilities',\n    subtitle = 'Using the estimates from a Logit model.', \n    x = 'Gender',\n    y = 'Predicted Probabilities',\n    fill = '')+\n  theme_minimal()+\n  theme(\n    legend_position='bottom',\n    axis_title=element_text(face='bold',size=12),\n    axis_text=element_text(size=12),\n    plot_title=element_text(face='bold',size=20),\n    figure_size=(15,6)\n    )\n  )\n  \nPlot.show()"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#putting-all-together",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#putting-all-together",
    "title": "Choice Models and its Applications",
    "section": "Putting all together",
    "text": "Putting all together\n\nExplanationResultPython\n\n\n\nWe saw that women tend to churn, approximately, \\(8\\) percentage points more than men\nIs this true across all levels of age? As age levels increase, the gap between men and women may become wider due to personal traits. On the other hand, it may be that gender differences are invariant to age\nIn order to do that, we can do a mix of the two last exercises:\n\n\nSet all continuous variables, with the exception of age, to their means\nCompare the estimated probabilities for gender=1 (male) and gender=0 (female)\nYou should have \\(200\\) rows in your data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Load data\ndata = pd.read_csv('Assets/bank-dataset.csv')\ndata['gender'] = np.where(data['gender'] == 'Male', 1, 0)\n\n# Fit logistic regression model\nX = data[['credit_score', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary','gender','age']]\nX = sm.add_constant(X)  # Add constant term for intercept\ny = data['churn']\nmodel = sm.Logit(y, X).fit()\n\n# Set all other variables to the mean\nnew_data = X.drop(columns=['gender','age']).mean(numeric_only=True).to_frame().transpose()\nnew_data = pd.DataFrame(np.repeat(new_data.values, 200, axis=0),columns=new_data.columns)\ngender_df = pd.DataFrame([*np.repeat(1,100,axis=0),*np.repeat(0,100,axis=0)], columns=['gender'])\nage_df = pd.DataFrame(pd.Series(range(1, 101)), columns=['age'])\nage_df =pd.concat([age_df,age_df],ignore_index=True,axis=0)\n\n# Concatenate with your existing DataFrame\nconcatenated_df = pd.concat([gender_df, age_df], ignore_index=True,axis=1)\nconcatenated_df = pd.concat([new_data,concatenated_df], ignore_index=True,axis=1)\nconcatenated_df.columns=[*new_data.columns,'gender','age']\n\n# Predict\npredicted = model.predict(concatenated_df)\nconcatenated_df['Predicted'] = predicted\nconcatenated_df['Gender'] = np.where(concatenated_df['gender'] == 1, 'Male', 'Female')\n\n# Plot\nPlot = (\n  ggplot(concatenated_df,aes(x='age',y='Predicted',fill='Gender'))+\n  geom_point(stroke=0,size=3)+\n  scale_y_continuous(labels = percent_format())+\n  scale_x_continuous(breaks = range(0,100,10))+\n  labs(\n    title ='Marginal effects of gender and age on Churn probabilities',\n    subtitle = 'Using the estimates from a Logit model.', \n    x = 'Age',\n    y = 'Predicted Probabilities',\n    fill = 'Gender')+\n  theme_minimal()+\n  theme(\n    legend_position='bottom',\n    axis_title=element_text(face='bold',size=12),\n    axis_text=element_text(size=12),\n    plot_title=element_text(face='bold',size=20),\n    figure_size=(15,7)\n    )\n  )\n  \nPlot.show()"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations",
    "title": "Choice Models and its Applications",
    "section": "Comparison across LPM and Logit estimations",
    "text": "Comparison across LPM and Logit estimations\n\nWe came across two estimators for binary choice models. Do they differ in terms of the responses?\nAlthough the coefficients from the different models are not directly comparable, we can use them to understand what are the partial effects:\nFor both cases, we’ll use all the other variables at their sample mean values:\n\nFor LPM, the effect will be a slope, i.e., the marginal change depending on Age will be constant\nFor Logit, the change in the predicted probability will vary depending on the Age reference point"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#comparing-across-lpm-and-logit-estimations---age",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#comparing-across-lpm-and-logit-estimations---age",
    "title": "Choice Models and its Applications",
    "section": "Comparing across LPM and Logit estimations - age",
    "text": "Comparing across LPM and Logit estimations - age\n\nResultPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Read the CSV file into a pandas DataFrame\nData = pd.read_csv('Assets/bank-dataset.csv')\n\n# Convert 'gender' into dummy variables\nData = pd.get_dummies(Data, columns=['gender'], drop_first=True)\n\n# Define independent and dependent variables\nX = Data[['credit_score', 'gender_Male', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary','age']]\ny = Data['churn']\n\n# Add a constant to the independent variables matrix for the intercept\nX = sm.add_constant(X)\n\n# Fit the linear regression model and the logit model\nlm_model = sm.OLS(y, X.astype(float)).fit()\nlogit_model = sm.Logit(y, X.astype(float)).fit()\n\n# Set all other variables to the mean\nnew_data = X.drop(columns=['age']).mean(numeric_only=True).to_frame().transpose()\nnew_data = pd.DataFrame(np.repeat(new_data.values, 100, axis=0),columns=new_data.columns)\nage_df = pd.DataFrame(pd.Series(range(1, 101)), columns=['age'])\n\n# Concatenate with your existing DataFrame\nconcatenated_df = pd.concat([new_data,age_df], ignore_index=True,axis=1)\nconcatenated_df.columns=[*new_data.columns,'age']\n\n# Predict\nlm_predicted = lm_model.predict(concatenated_df)\nlogit_predicted = logit_model.predict(concatenated_df)\n\npredicted_df = pd.DataFrame({\n  'LPM': lm_predicted,\n  'Logit': logit_predicted,\n  'Age': age_df['age']\n})\n\n\n# Plot\nPlot = (\n  ggplot(predicted_df,aes(x='Age'))+\n  geom_line(aes(y='LPM'),color='blue',size=2)+\n  geom_point(aes(y='Logit'),color='red',stroke=0,size=4)+\n  scale_y_continuous(labels = percent_format())+\n  scale_x_continuous(breaks = range(0,100,10))+\n  labs(\n    title ='Marginal effects of gender and age on Churn probabilities',\n    subtitle = 'Using the estimates from a Logit model and Linear Probability Model (LPM).', \n    x = 'Age',\n    y = 'Predicted Probabilities',\n    fill = 'Gender')+\n  theme_minimal()+\n  theme(\n    legend_position='bottom',\n    axis_title=element_text(face='bold',size=12),\n    axis_text=element_text(size=12),\n    plot_title=element_text(face='bold',size=20),\n    figure_size=(15,7)\n    )\n  )\n  \nPlot.show()"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations-continued",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations-continued",
    "title": "Choice Models and its Applications",
    "section": "Comparison across LPM and Logit estimations (continued)",
    "text": "Comparison across LPM and Logit estimations (continued)\n\nIn the LPM model, the effect of age is always the same – intuitively, \\(\\beta_{Age}\\), the partial derivative of Y with respect to Age, is the same for all levels\nOn the other hand, the changes when looking at the Logit model vary depending on the reference point for Age:\n\nFor values \\([0,20]\\), it increases modestly\nAround \\([21,60]\\), the changes are exponential\nBetween \\([60,80]\\), the changes start to become constant\nAfter 80, we see diminishing effects\n\nDo these differences matter in practice? It depends on what you’re looking… (more on the next slide)"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations-continued-1",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations-continued-1",
    "title": "Choice Models and its Applications",
    "section": "Comparison across LPM and Logit estimations (continued)",
    "text": "Comparison across LPM and Logit estimations (continued)\n\nOn the one hand, we clearly the weaknesses of the LPM when estimating probabilities for ages between \\([0,20]\\).\nOn the other hand, LPM and Logit will be almost identical between \\([30,60]\\)\n\n\nIf you tabulate the distribution of Age in your dataset, you’ll see that although LPM does a poor job in predicting probabilities for cases \\(&lt;20\\), the sample proportion of these cases is less than \\(1\\%\\)\nOn the other hand, its results are approximately the same as of Logit between \\([30,60]\\), which constitutes approximately \\(80\\%\\) of the sample, which indicates that for the specific sample you are looking at, a LPM is not a terrible idea"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations-continued-2",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#comparison-across-lpm-and-logit-estimations-continued-2",
    "title": "Choice Models and its Applications",
    "section": "Comparison across LPM and Logit estimations (continued)",
    "text": "Comparison across LPM and Logit estimations (continued)\n\nLogit or LPM?\n\n\nIf your interest is to use the model results to predict probabilities for different age brackets, then yes, you should use Logit (or any model with the similar properties)\nIf, on the other hand, you’re just interest in knowing the effects for the average person in your sample, you can use LPM\n\n\nLPM is simpler, and we know very well the properties to analyze cases potential issues such as ommited variable bias\nIn the Logit world, there is no \\(R^2\\), but there are other ways to check the predictive ability and fit of models"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#references-1",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#references-1",
    "title": "Choice Models and its Applications",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nChapman, Chris, and Elea Mcdonnell Feit. 2015. R for Marketing Research and Analytics. Cham: Springer International Publishing.\n\n\nGreene, William H. 2011. Econometric Analysis. 7th ed. Upper Saddle River, NJ: Pearson.\n\n\nHayashi, Fumio. 2000. Econometrics. Princeton, NJ: Princeton University Press.\n\n\nTrain, Kenneth E. 2009. Discrete Choice Methods with Simulation. 2nd ed. Cambridge, England: Cambridge University Press."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-markowitz-mean-variance-problem",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-markowitz-mean-variance-problem",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "The Markowitz Mean-Variance Problem",
    "text": "The Markowitz Mean-Variance Problem\n\nWe generalize the problem of finding the efficient portfolio for a given \\(N\\) number of assets as:\n\n\\[\n\\min_{\\{w_1,w_2,w_3,w_4,w_5\\}} \\sigma^2_p = w^T \\Sigma w,\n\\text{ such that:}\n\\\\\n\\begin{cases}\n\\sum_{i=1}^5 w_i=1 \\text{ (1)}\\\\\n0\\leq w_i \\leq 1, \\forall i \\text{ (2)}\n\\end{cases}\n\\]\n\nThis is also known as the Markowtiz Mean-Variance optimization problem for a long-only portfolio"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-markowitz-mean-variance-problem-continued",
    "href": "quant-fin/coursework/Lecture 7 - Building, Backtesting, and Evaluating Investment Strategies/index.html#the-markowitz-mean-variance-problem-continued",
    "title": "Building, Backtesting, and Evaluating Investment Strategies",
    "section": "The Markowitz Mean-Variance Problem, continued",
    "text": "The Markowitz Mean-Variance Problem, continued\n\nIn other words, you are solving the following problem: find the set of allocation weights \\(w_1,w_2,w_3,w_4\\), and \\(w_5\\) (the % that you allocate in each of the five stocks) that, when used to create a portfolio, are the ones that create the portfolio with the minimum variance among all possible combinations\nIn order to do that, you have to ensure that the weights add up to 100% (so you’re fully investing your capital), which is the first condition. The second conditions states that a stock cannot have negative weights nor have a weight that is greater than 100%\n\nImportant: Note that \\(w^T\\Sigma w\\) is nothing more than the matrix form of \\(\\sum_{i=1}^{N}w_i\\sigma_i^2+ 2\\sum_{i=1}^{N}\\sum_{j\\neq i}w_i w_j\\sigma_{i,j}\\), which is the variance of a portfolio that consists of \\(N\\) assets"
  },
  {
    "objectID": "quant-mkt.html#about-the-course",
    "href": "quant-mkt.html#about-the-course",
    "title": "Quantitative Methods",
    "section": "About the course",
    "text": "About the course\nThis is a graduate-level course in Quantitative Methods in Marketing for the Master’s Program in Economics and Finance (MEF), organized by the University of Navarra (UNAV). This course aims to provide a hands-on approach to some of the methods widely used to solve practical questions in the advertisement industry, nurturing students with enough methodological background to understand the methods’ differences, advantages and limitations, and ultimately apply them with enough technical rigor to solve business-oriented questions.\nThis course will be divided into two distinct sections:\n\nThe first section will focus on choice models, where students will be exposed to a series of techniques used in practical applications to estimate consumer’s choices over a defined set of possibilities, either that being a binary choice (i.e, opt-out of a subscription or renew) or a multinomial choice (i.e, choosing which product to buy from a set of possibilities).\nThe second section will be focused on market response models - measuring the effects of marketing efforts on sales metrics - and will discuss methods ranging from experimental design to panel-data estimation. This part of the course will be covered by Prof. Xavi Vidal-Berastain.\n\nAdditional papers and references not covered will be organized and outlined in the course’s official bibliography. Students are expected to apply the methods discussed in Sections 1 and 2 and replicate the codes developed during class. All coding routines will be discussed during class, and a handout notebook will be provided at the end of each class.\nThe evaluation will grade students based on their ability to develop, analyze, and discuss the results of a quantitative marketing case study. Students are expected to work in groups and create a report (code + discussions) applying some techniques discussed in class in a data case. The due date is 48 hours. Performance will be evaluated regarding reproducibility, organization, and quality of the discussion."
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2-solutions.html",
    "href": "quant-fin/datacases/data-case-2/data-case-2-solutions.html",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "This Data Case is part of the Practical Applications in Quantitative Finance course, held at FGV-EAESP’s undergraduate course in business. Carefully follow the instructions contained in the data case as well as eClass® before you make your submission.\n\n\nHedge funds operate in a highly competitive environment where their ability to generate alpha—returns beyond market factors—is constantly scrutinized. Many funds claim to deliver superior returns by exploiting inefficiencies, but how much of their performance is just exposure to well-known risk factors?\nAs part of the Strategic Investments Division at Sentinel Capital, a multi-billion-dollar Fund of Funds (FoF), your team has been tasked with evaluating the performance of eight hedge funds over the last decade. The ultimate goal? Constructing an optimal portfolio of hedge funds that maximizes returns while managing risk.\nTo do this, you’ll leverage R, tidyverse, tidyquant, and PortfolioAnalytics by applying factor models, portfolio optimization techniques, and rolling performance analysis. Your key tool for evaluation will be the Fama-French Five-Factor Model, which decomposes returns into market, size, value, profitability, and investment factors.\nYour dataset contains monthly returns from eight distinct hedge funds, each with a distinct allocation rationale:\n\nAlphaSynthesis Capital – A long/short equity fund with an emphasis on momentum-driven strategies.\nArbVantage Partners – A statistical arbitrage fund specializing in mean-reversion strategies.\nSentinel Macro Strategies – A global macro fund that trades based on macroeconomic indicators.\nQuantum Volatility Fund – A volatility arbitrage fund exploiting implied vs. realized volatility.\nHorizon Event-Driven – A fund that profits from corporate events such as mergers, spin-offs, and earnings surprises.\nDeepValue Asset Management – A deep-value fund investing in undervalued securities based on fundamental analysis.\nNova Growth Strategies – A growth-oriented hedge fund focusing on technology and high-beta stocks.\nVega Capital Neutral – A market-neutral strategy that seeks absolute returns with low beta exposure.\n\nYour task is to analyze these funds, compare them against the Fama-French three-factor model, and construct optimal hedge fund portfolios.\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®.\n\n\n\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\",\"PortfolioAnalytics\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('PortfolioAnalytics')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(PortfolioAnalytics)\n\n\n\n\n\n\n\nCallout\n\n\n\nDepending upon your R version that is currently installed, you may or may not have the right set of optimization routines properly set up in your session. To make sure that you can use the portfolio optimization functions from PortfolioAnalytics package, install the DEoptim (see details here) that is used on the backend.\n\ninstall.packages('DEoptim')\n\n\n\n\n\n\nDownload the hf_data.RDS file on eClass®. This dataset covers the period from January 2010, to December 2024 and shows the monthly returns for each hedge fund. Using the functions from the tidyverse and tidyquant packages, visualize the historical returns of all hedge funds. What trends or anomalies do you observe? Are there periods of exceptional gains or drawdowns? Which fund has shown to be the best performer during the study period?\n\n\n\n\n\n\nHint\n\n\n\n\nBecause hf_data is an data.frame object, you can pass the as.xts to convert it to xts and then call cumprod() to generate the cumulative returns for all assets. Do not forget to use the cumprod(1+x)-1 syntax to make sure that you are getting the correct compounded returns over time!\nNow, your resulting output can be converted back to a data.frame using as.data.frame() and getting the date column back using rownames_to_column('date'). To go from wide to long format, you can use the pivot_longer function, keeping only one column for the returns and another column that refers to each strategy at a given date.\nYou can pipe that into a ggplot call, mapping the date to the x-axis, the cumulative return column to the y-axis, and the geom_line() function to create a bar chart, using the group and color to color each fund differently. Add as many customizations you think are worth the effort.\n\n\n\n\n(cumprod(1+as.xts(hf_data))-1)%&gt;%\n  as.data.frame()%&gt;%\n  rownames_to_column('date')%&gt;%\n  mutate(date=as.Date(date))%&gt;%\n  pivot_longer(names_to='strategy',values_to = 'cum_return',cols=2:8)%&gt;%\n  ggplot(aes(x=date,y=cum_return,group=strategy,col=strategy))+\n  geom_line(size=1)+\n  scale_x_date(date_breaks = 'years',date_labels='%Y')+\n  scale_y_continuous(labels = label_percent(big.mark = '.'))+\n  labs(title='Comparison of hedge fund  strategies over time',\n       subtitle='Considering daily hedge fund returns.',\n       col='Strategy',\n       x='',\n       y='Cumulative Return')+\n  theme_minimal()+\n  theme(legend.position = 'bottom',\n        axis.text.x = element_text(angle=90,size=12),\n        axis.text.y = element_text(size=12),\n        axis.title = element_text(face='bold',size=15),\n        plot.title = element_text(face='bold',size=20),\n        plot.subtitle  = element_text(size=12))\n\n\n\n\n\n\n\n\n\n\n\nDownload the ff5_data.RDS file on eClass®. This file contains the historical monthly returns for the Fama-French 5 factor model for the U.S. stock market. Merge this data with your hedge fund dataset, hf_data, and run an Ordinary Least Squares (OLS) regression for each fund against the Fama-French factors using the following specification:\n\\[\n\\small E[R_{i,t}] = \\alpha + \\beta_s^M \\times \\underbrace{(E[R_m]− R_f)}_{\\text{Market}}  + \\beta_s^{SMB} \\times \\underbrace{E[R_{SMB}]}_{\\text{Size}} + \\beta_s^{HML} \\times \\underbrace{E[R_{HML}]}_{\\text{Book-to-Market.}} + \\beta_s^{RMW} \\times \\underbrace{E[R_{RMW}]}_{\\text{Profitability}} + \\beta_s^{CMA} \\times \\underbrace{E[R_{CMA}]}_{\\text{Investment}} + \\varepsilon_{i,t}\n\\]\nBased on the results you found, which funds exhibit significant (at 5% confidence levels) \\(\\alpha\\) after controlling for factor exposures? Store the names of the these funds in an object called positive_alpha_funds.\n\n\n\n\n\n\nHint\n\n\n\n\nBecause hf_data is in wide format, you can use the pivot_longer to transform it to a tidy format - assign it to another object (e.g, hf_returns) for later use.\nAfter that, merge this newly created that to the ff5_data object using the left_join function, and use mutate to create the excess returns for each fund by subtracting the risk-free column, assigning it to a new object (e.g, merged_df)\nUse the functional programming techniques from the purrr package (or do a for-loop) to map the lm and tidy functions to each strategy subset, and collect the results from \\(\\alpha\\) (the Intercept of the regression).\nCollect the results in an object and use ggplot to chart your results.\n\n\n\n\n#Read the FF5 Data\nff5_data=readRDS('Assets/ff5_data.RDS')\n\n#Convert the hf_data to a data.frame object and adjust columns\nhf_returns = hf_data%&gt;%\n  pivot_longer(cols = -date,names_to = 'fund',values_to = 'monthly_return')\n\n #Merge both datasets by date\nmerged_df &lt;- hf_returns%&gt;%\n  #Merge\n  left_join(ff5_data, by = \"date\")%&gt;%\n  #Pivot the data for each strategy\n  mutate(excess_return = monthly_return - RF)\n\n#Alpha estimation\nFF_results &lt;- merged_df%&gt;%\n  #Group by strategy\n  group_by(fund)%&gt;%\n  #Nest the data\n  nest()%&gt;%\n  #For each nest, map the lm() function and the tidy function\n  mutate(model = map(data, ~ lm(excess_return ~ MKT_MINUS_RF + HML + SMB + CMA + RMW, data = .)),\n         results = map(model, tidy))%&gt;%\n  #Unnest the results\n  unnest(results)%&gt;%\n  #Select the desired columns\n  select(fund, term, estimate, std.error, p.value)\n\n#Chart\nFF_results %&gt;%\n  filter(term=='(Intercept)')%&gt;%\n  mutate(stat_sig=ifelse(p.value&lt;0.05,'Statistically sig. at 1%','Not statistically sig. at 1%'))%&gt;%\n  ggplot(aes(x=reorder(fund,desc(estimate)),y=estimate,fill=stat_sig))+\n  geom_col(size=3)+\n  geom_text(aes(label = percent(estimate,accuracy=0.01),vjust=-1))+\n  #Annotations\n  labs(title='Which strategies did generate positive and statistically significant alphas?',\n       subtitle = 'Using the Fama-French three-factor model with monthly return data.',\n       x = 'Strategy',\n       y = 'Alpha (%)',\n       fill = 'Stat. Sig')+\n  #Scales\n  scale_y_continuous(labels = percent)+\n  #Custom theme minimal\n  theme_minimal()+\n  #Adding further customizations\n  theme(legend.position='bottom',\n        axis.title = element_text(face='bold',size=15),\n        axis.text = element_text(size=12),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15))\n\n\n\n\n\n\n\n#Positive alpha funds\npositive_alpha_funds=FF_results%&gt;%filter(term=='(Intercept)',estimate&gt;0)%&gt;%pull(fund)\n\n\n\n\nUsing the results from our previous OLS regressions, explain how exposed each fund was to each of the factor portfolios contained in the Fama-French 5 Factor model. How can this help you explain the over(under)performance of the top(worst) funds?\n\n\n\n\n\n\nHint\n\n\n\n\nUsing the estimates from the estimation, collect all the \\(\\beta\\)s from the regressions by filtering out the Intercept using the filter() function\nUse ggplot to chart the weights (factor betas) for each strategy. To display all strategies in one chart, you can map the factors to the x axis, the factor betas values to the y axis, use facet_wrap to facet your chart by each different fund.\n\n\n\n\nFF_results %&gt;%\n  filter(term != \"(Intercept)\")%&gt;%\n  group_by(fund)%&gt;%\n  ggplot(aes(x = reorder(term,desc(estimate)), y = estimate, fill = term)) +\n  geom_col(position = position_dodge())+\n  geom_label(aes(label = round(estimate,2)),col='black',fill='white')+\n  theme_minimal()+\n  facet_wrap(fund~.,ncol=2,nrow=4)+\n  #Annotations\n  labs(title = \"Fama-French Factor Loadings by Hedge Fund Strategy\",\n       x = \"Hedge Fund Strategy\",\n       y = \"Factor Loading\",\n       fill = 'Risk Factor')+\n  #Custom theme minimal\n  theme_minimal()+\n  #Adding further customizations\n  theme(legend.position='bottom',\n        axis.title = element_text(face='bold',size=15),\n        axis.text = element_blank(),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15))\n\n\n\n\n\n\n\n\n\n\n\nWay to go! Your boss was really impressed with how you were able to dissect the returns from individual hedge funds and understand which factors drove performance, and in which cases we were able to find positive returns that were uncorrelated with any of the risk factors. Now, it is time to start building portfolios by allocating money across each fund using the Markowitz Mean-Variance optimization rationale.\nTo do that, start by filtering your sample to consider only the hedge funds that were able to generate \\(\\alpha\\) throughout the whole period. Divide your sample using a \\(50\\%/50\\%\\) split, using information from \\(2010\\) until \\(2018\\) as a training set, and \\(2019\\) to \\(2024\\) as a test set. Construct a Minimum-Variance Portfolio of the following format:\n\\[\n\\min_{\\{w_1,w_2...,w_n\\}} \\sigma^2_p = w^T \\Sigma w,\n\\text{ such that:}\n\\\\\n\\begin{cases}\n\\sum_{i=1}^n w_i=1 \\text{ (1)}\\\\\n0.05\\leq w_i \\leq 0.50, \\forall i \\text{ (2)}\n\\end{cases}\n\\]\nIn other words, you are solving the following problem: find the set of allocation weights \\(w_1,w_2,w_3,...,w_n\\) (the % that you allocate in each of the strategies) that, when used to create a portfolio, are the ones that create the portfolio with the minimum variance among all possible combinations. In order to do that, you have to ensure that the weights add up to \\(100\\%\\) (so you’re fully investing your capital), which is the first condition. The second conditions states that neither stock can have a weight that is lower than \\(5\\%\\) nor have a weight that is greater than \\(50\\%\\).\nNote that \\(w^T\\Sigma w\\) is nothing more than the matrix form of \\(\\sum_{i=1}^{N}w_i\\sigma_i^2+ 2\\sum_{i=1}^{N}\\sum_{j\\neq i}w_i w_j\\sigma_{i,j}\\), which is the variance of a portfolio that consists of \\(N\\) assets.\nUsing the training sample and only the funds that have shown \\(\\alpha&gt;0\\) (regardless of the statistical significance), what is the optimal allocation? Store this into an object called min_var_allocation.\n\n\n\n\n\n\nHint\n\n\n\n\nFilter the hf_data to contain only the date and the columns that refer to the strategies with positive \\(\\alpha\\).\nUse the as.xts to transform the object from a data.frame to an xts object. This is crucial, as the functions from the PortfolioAnalytics operate on xts objects. With your newly created xts object, create two new objects, training_sample and testing_sample using the xts subsetting syntax:\n\n\ntraining_sample=(your_filtered_hf_data%&gt;%as.xts())['2010/2018']\ntest_sample=(your_filtered_hf_data%&gt;%as.xts())['2019/2024']\n\n\nUse the portfolio.spec and the optimize.portfolio functions from the PortfolioAnalytics package to define and optimize the portfolio using the syntax we have learned during class. If you are unsure on what to do here, please refer to the workbook.R file that we have gone through for a similar replication.\nFinally, use the extractWeights function to output a table with the optimal weights.\n\n\n\n\n#Transform into .xts and split the sample\nfiltered_hf_data=hf_data%&gt;%select(date,all_of(positive_alpha_funds))\ntraining_sample=(filtered_hf_data%&gt;%as.xts())['2010/2018']\ntest_sample=(filtered_hf_data%&gt;%as.xts())['2019/2024']\n\n\n#Risk Minimization\nmin_var_portfolio &lt;- portfolio.spec(assets = positive_alpha_funds)%&gt;% \n  add.objective(type = \"risk\", name = \"var\")%&gt;%\n  add.constraint(type = \"full_investment\")%&gt;%\n  add.constraint(type = \"box\", min = 0.05, max = 0.5)\n\nmin_var_allocation &lt;- training_sample %&gt;% \n  optimize.portfolio(\n    portfolio = min_var_portfolio\n  )\n\nIteration: 1 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\nIteration: 2 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\nIteration: 3 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\nIteration: 4 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\nIteration: 5 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\nIteration: 6 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\nIteration: 7 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\nIteration: 8 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\n[1] 0.054 0.440 0.052 0.454\n\n#Get the weights\nextractWeights(min_var_allocation)\n\nalphasynthesis     arbvantage       sentinel        horizon \n         0.054          0.440          0.052          0.454 \n\n\n\n\n\nUsing the training sample and only the funds that have shown \\(\\alpha&gt;0\\), develop a portfolio that maximizes returns instead of minimizing risk. What is the optimal allocation? Store this into an object called max_return_allocation.\n\n#Return Maximization\nmax_return_portfolio &lt;- portfolio.spec(assets = positive_alpha_funds)%&gt;% \n  add.objective(type = \"return\", name = \"mean\")%&gt;%\n  add.constraint(type = \"full_investment\")%&gt;%\n  add.constraint(type = \"box\", min = 0.05, max = 0.5)\n\nmax_return_allocation &lt;- training_sample %&gt;% \n  optimize.portfolio(\n    portfolio = max_return_portfolio\n    )\n\nIteration: 1 bestvalit: -0.010391 bestmemit:    0.358000    0.060000    0.492000    0.090000\nIteration: 2 bestvalit: -0.010391 bestmemit:    0.358000    0.060000    0.492000    0.090000\nIteration: 3 bestvalit: -0.010391 bestmemit:    0.358000    0.060000    0.492000    0.090000\nIteration: 4 bestvalit: -0.010391 bestmemit:    0.358000    0.060000    0.492000    0.090000\nIteration: 5 bestvalit: -0.010391 bestmemit:    0.358000    0.060000    0.492000    0.090000\nIteration: 6 bestvalit: -0.010391 bestmemit:    0.358000    0.060000    0.492000    0.090000\nIteration: 7 bestvalit: -0.010391 bestmemit:    0.358000    0.060000    0.492000    0.090000\n[1] 0.358 0.060 0.492 0.090\n\n#Get the weights\nextractWeights(max_return_allocation)\n\nalphasynthesis     arbvantage       sentinel        horizon \n         0.358          0.060          0.492          0.090 \n\n\n\n\n\nUsing the training sample and only the funds that have shown \\(\\alpha&gt;0\\), develop a portfolio that maximizes the Sharpe-Ratio - i.e, maximizes the risk-adjusted returns. What is the optimal allocation? Store this into an object called max_sharpe_allocation.\n\n#Return Maximization\nmax_sharpe_portfolio &lt;- portfolio.spec(assets = positive_alpha_funds)%&gt;% \n  add.objective(type = \"return\", name = \"mean\")%&gt;% \n  add.objective(type = \"risk\", name = \"StdDev\")%&gt;%\n  add.constraint(type = \"full_investment\")%&gt;%\n  add.constraint(type = \"box\", min = 0.05, max = 0.5)\n\nmax_sharpe_allocation &lt;- training_sample %&gt;% \n  optimize.portfolio(\n    portfolio = max_sharpe_portfolio,\n    SR= TRUE\n  )\n\nIteration: 1 bestvalit: 0.025895 bestmemit:    0.196000    0.338000    0.132000    0.334000\nIteration: 2 bestvalit: 0.024438 bestmemit:    0.134000    0.302000    0.082000    0.482000\nIteration: 3 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\nIteration: 4 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\nIteration: 5 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\nIteration: 6 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\nIteration: 7 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\nIteration: 8 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\nIteration: 9 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\nIteration: 10 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\n[1] 0.052 0.490 0.088 0.370\n\n#Get the weights\nextractWeights(max_sharpe_allocation)\n\nalphasynthesis     arbvantage       sentinel        horizon \n         0.052          0.490          0.088          0.370 \n\n\n\n\n\nCompare the optimal weights across each strategy and discuss the allocations. Is there any better allocation rule?\n\n\n\n\n\n\nHint\n\n\n\n\nAfter collecting your optimal weights using the extractWeights function and storing them into separate objects, use the rbind function to bind them together in a row-wise manner and transform it to a data.frame using the as.data.frame() function.\nAdd a new column that describes which strategy is contained in each row.\nUse the pivot_longer to transform the data to a tidy format, making it easier for you to pipe the output into a ggplot call.\n\nFor these steps, you can use the following syntax:\n\nrbind(extractWeights(min_var_allocation),\n      extractWeights(max_return_allocation),\n      extractWeights(max_sharpe_allocation))%&gt;%\n  as.data.frame()%&gt;%\n  mutate(objective=c('Minimize Variance','Maximize Returns','Maximize Sharpe'))%&gt;%\n  pivot_longer(1:4,names_to = 'fund',values_to = 'weights')\n\n\n\n\n#Chart weights\nrbind(extractWeights(min_var_allocation),\n      extractWeights(max_return_allocation),\n      extractWeights(max_sharpe_allocation))%&gt;%\n  as.data.frame()%&gt;%\n  mutate(objective=c('Minimize Variance','Maximize Returns','Maximize Sharpe'))%&gt;%\n  pivot_longer(1:4,names_to = 'fund',values_to = 'weights')%&gt;%\n  ggplot(aes(x = objective, y = weights, fill = fund)) +\n  geom_col() + \n  geom_text(aes(label = percent(weights)),\n            position = position_stack(vjust = .6),\n            size = 6) + \n  scale_x_discrete(labels = c(\"Maximize Return\", \"Minimize Risk\", \"Max Sharpe Ratio\")) +\n  scale_y_continuous(labels=scales::percent)+\n  labs(x = \"\", y = \"Allocation (%)\",\n       title = \"Asset Allocation by Optimization Criteria\")+\n  theme_minimal()+\n  theme(legend.position='bottom',\n        axis.title = element_text(face='bold',size=15),\n        axis.text = element_text(size=12),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15))\n\n\n\n\n\n\n\n\n\n\n\nCompare the historical performance from each fund in the testing sample. To do that, use the three sets of optimal weights you have just created (min_var_allocation, max_return_allocation, and max_sharpe_allocation) along with the tq_portfolio function to estimate the returns in the test sample, rebalancing it monthly. Which strategy did deliver the highest out-of-sample returns? Did they outperform a naive portfolio using equal weights to all funds?\n\n\n\n\n\n\nHint\n\n\n\n\nUse the hf_returns data you have created in the beginning of the exercise (i.e, the dataset of hedge fund returns in long format) and apply the filter function to keep only the rows where i) the fund is in the list of funds with positive alpha (i.e, it is in the positive_alpha_funds you have created before) and year(date)&gt;2018 (i.e, belongs to testing sample).\nUse the tq_portfolio function using the following syntax to calculate the historical returns of the portfolio:\n\n\n  tq_portfolio(\n    assets_col = fund, #This is the column that contains the fund names\n    returns_col = monthly_return, #Column that contains the monthly returns\n    weights = extractWeights(min_var_allocation), #The portfolio.spec you have created\n    col_rename = \"returns_min_var\",\n    rebalance_on = \"months\"\n  )\n\n\nDo this for all three portfolio specifications, assigning their results to separate objects. For the naive portfolio, you can assign a vector of equal weights using the rep(weight,number_of_replications) syntax.\n\n\n\n\n#Minimize Risk Portfolio\n\nmin_var_performance &lt;- hf_returns%&gt;%\n  filter(fund %in% positive_alpha_funds, year(date)&gt;2018)%&gt;%\n  tq_portfolio(\n    assets_col = fund,\n    returns_col = monthly_return,\n    weights = extractWeights(min_var_allocation),\n    col_rename = \"returns_min_var\",\n    rebalance_on = \"months\"\n  )\n\n#Maximize Return Portfolio\n\nmax_return_performance &lt;- hf_returns%&gt;%\n  filter(fund %in% positive_alpha_funds, year(date)&gt;2018)%&gt;%\n  tq_portfolio(\n    assets_col = fund,\n    returns_col = monthly_return,\n    weights = extractWeights(max_return_allocation),\n    col_rename = \"returns_max_return\",\n    rebalance_on = \"months\"\n  )\n\n#Naive Portfolio\n\nmax_sharpe_performance &lt;- hf_returns%&gt;%\n  filter(fund %in% positive_alpha_funds, year(date)&gt;2018)%&gt;%\n  tq_portfolio(\n    assets_col = fund,\n    returns_col = monthly_return,\n    weights = extractWeights(max_sharpe_allocation),\n    col_rename = \"returns_max_sharpe\",\n    rebalance_on = \"months\"\n  )\n\n#Naive Portfolio\n\nnaive_performance &lt;- hf_returns%&gt;%\n  filter(fund %in% positive_alpha_funds, year(date)&gt;2018)%&gt;%\n  tq_portfolio(\n    assets_col = fund,\n    returns_col = monthly_return,\n    weights = rep(0.25,4),\n    col_rename = \"returns_naive\",\n    rebalance_on = \"months\"\n  )\n\n#Chart\n\nnaive_performance%&gt;%\n  left_join(min_var_performance)%&gt;%\n  left_join(max_return_performance)%&gt;%\n  left_join(max_sharpe_performance)%&gt;%\n  pivot_longer(cols = \"returns_naive\":\"returns_max_sharpe\",\n               names_to = \"strategy\",\n               values_to = \"return\") %&gt;% \n  mutate(strategy = case_when(strategy == \"returns_max_return\" ~ \"Maximize Avg. Return\",\n                                strategy == \"returns_min_var\" ~ \"Minimize risk\",\n                                strategy == \"returns_naive\" ~ \"Equal Allocation\",\n                                strategy == \"returns_max_sharpe\" ~ \"Maximize Sharpe Ratio\")) %&gt;% \n  group_by(strategy) %&gt;% \n  mutate(cum_return = cumprod(1+return)-1) %&gt;% \n  ggplot(aes(x = date, y = cum_return, color = strategy)) + \n  geom_line(size = 1.2)+ \n  scale_y_continuous(label = scales::percent) +\n  labs(title = \"Historical Performance over test sample\", y = \"Cumulative Return\",\n       x = \"\",\n       color='') + \n  theme_minimal()+\n  theme(legend.position='bottom',\n      axis.title = element_text(face='bold',size=15),\n      axis.text = element_text(size=12),\n      plot.title = element_text(size=20,face='bold'),\n      plot.subtitle  = element_text(size=15))\n\n\n\n\n\n\n\n#Table\nnaive_performance%&gt;%\n  left_join(min_var_performance)%&gt;%\n  left_join(max_return_performance)%&gt;%\n  left_join(max_sharpe_performance)%&gt;%\n  as.xts()%&gt;%\n  Return.cumulative()\n\n                  returns_naive returns_min_var returns_max_return\nCumulative Return     0.9067833       0.8617784          0.8948185\n                  returns_max_sharpe\nCumulative Return          0.8771747\n\n\n\n\n\nUsing the max_return_portfolio object and the test_sample data cut, implement a rolling-window optimization to find the optimal portfolio weights. Use a training period and a rolling window of \\(12\\) months, and rebalance the portfolio monthly. How do these dynamically adjusted weights perform compared to static allocations?\n\n\n\n\n\n\nHint\n\n\n\n\nUsing the test_sample xts object you have created, apply the optimize.portfolio.rebalancing function, pointing to the max_return_portfolio and the aforementioned parameters for the training period, rolling window, and rebalancing period. You can use the following syntax:\n\n\n#Rolling optimization\nrolling_optimization &lt;- test_sample%&gt;%\n  optimize.portfolio.rebalancing(\n    portfolio = max_return_portfolio,\n    rebalance_on = \"months\",\n    training_period = 12,\n    rolling_window = 12\n  )\n\n\nAfter you have optimized your portfolio, call extractWeights and pipe that into ggplot to chart your results.\n\n\n\n\n#Chart\nextractWeights(rolling_optimization)%&gt;%\n  as.data.frame()%&gt;%\n  rownames_to_column(\"date\")%&gt;% \n  mutate(date = as.Date(date)) %&gt;% \n  pivot_longer(names_to = 'symbol',values_to = 'weights',cols=where(is.numeric)) %&gt;% \n  ggplot(aes(x = date, y = weights, fill = symbol)) +\n  labs(fill = \"\", x = \"\", y = \"Asset Weight\",\n       title = \"Weights under rolling optimization - Maximize Returns\") +\n  geom_col() +\n  scale_x_date(date_breaks = \"6 months\") +\n  scale_y_continuous(labels=scales::percent)+\n  theme_minimal()+\n  theme(legend.position='bottom',\n        axis.title = element_text(face='bold',size=15),\n        axis.text = element_text(size=12),\n        axis.text.x = element_text(angle=90),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15))\n\n\n\n\n\n\n\n\n\n\n\nLooking only at the test sample period (i.e, from \\(2019\\) to \\(2024\\), chart the historical performance of the dynamically-optimized portfolio vis-a-vis the static portfolios you have just created. Are there any gains stemming from dynamically optimizing the portfolio?\n\n\n\n\n\n\nHint\n\n\n\n\nUse the extractWeights() function on the dynamically optimized portfolio optimization object to collect a time series of weight allocations.\nWith that, use as_data_frame() and rownames_to_column(\"date\") to transform it to a data.frame with a date column identifier.\nFinally, transform it to long format using the pivot_longer() function.\nDo a similar procedure for the xts object where you stored the test sample returns (in my example, test_sample)\nUse left_join to merge both datasets. You can use a combination of mutate() and summarize() to calculate the portfolio returns on each date in a step-wise manner:\n\n\ndynamic_performance=left_join(weights,returns)%&gt;% # Your weights and returns data.frames\n  mutate(ind_return=returns*weights)%&gt;% # Calculate individual returns for each fund on each date\n  group_by(date)%&gt;% # Group by date\n  summarize(returns_dynamic=sum(ind_return,na.rm=TRUE)) # Summing up yields a weighted average of returns by each date\n\n\nJoin this data.frame with the other objects that contain the historical returns from the other allocation strategies and pipe that into ggplot.\n\n\n\n\nweights=extractWeights(rolling_optimization)%&gt;%\n  as.data.frame()%&gt;%\n  rownames_to_column(\"date\")%&gt;% \n  mutate(date = as.Date(date)) %&gt;% \n  pivot_longer(names_to = 'fund',values_to = 'weights',cols=where(is.numeric))\n\nreturns=test_sample%&gt;%\n  as.data.frame()%&gt;%\n  rownames_to_column(\"date\")%&gt;% \n  mutate(date = as.Date(date)) %&gt;% \n  pivot_longer(names_to = 'fund',values_to = 'returns',cols=where(is.numeric))\n\ndynamic_performance=left_join(weights,returns)%&gt;%\n  mutate(ind_return=returns*weights)%&gt;%\n  group_by(date)%&gt;%\n  summarize(returns_dynamic=sum(ind_return,na.rm=TRUE))\n\ndynamic_performance%&gt;%\n  left_join(min_var_performance)%&gt;%\n  left_join(max_return_performance)%&gt;%\n  left_join(max_sharpe_performance)%&gt;%\n  left_join(naive_performance)%&gt;%\n  pivot_longer(cols = \"returns_dynamic\":\"returns_naive\",\n               names_to = \"strategy\",\n               values_to = \"return\")%&gt;% \n  mutate(strategy = case_when(strategy == \"returns_max_return\" ~ \"Maximize Avg. Return\",\n                                strategy == \"returns_min_var\" ~ \"Minimize risk\",\n                                strategy == \"returns_naive\" ~ \"Equal Allocation\",\n                                strategy == \"returns_max_sharpe\" ~ \"Maximize Sharpe Ratio\",\n                                .default = \"Dynamic Portfolio\")) %&gt;%\n  group_by(strategy) %&gt;% \n  mutate(cum_return = cumprod(1+return)-1) %&gt;% \n  ggplot(aes(x = date, y = cum_return, color = strategy)) + \n  geom_line(size = 1.2)+ \n  scale_y_continuous(label = scales::percent) +\n  labs(title = \"Dynamic vis-a-vis static portfolios\", y = \"Cumulative Return\",\n       x = \"\",\n       color='') + \n  theme_minimal()+\n  theme(legend.position='bottom',\n        axis.title = element_text(face='bold',size=15),\n        axis.text = element_text(size=12),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15))\n\n\n\n\n\n\n\n#Table\ndynamic_performance%&gt;%\n  left_join(min_var_performance)%&gt;%\n  left_join(max_return_performance)%&gt;%\n  left_join(max_sharpe_performance)%&gt;%\n  left_join(naive_performance)%&gt;%\n  as.xts()%&gt;%\n  Return.cumulative()\n\n                  returns_dynamic returns_min_var returns_max_return\nCumulative Return        1.208376       0.7725337          0.7148009\n                  returns_max_sharpe returns_naive\nCumulative Return          0.7903378     0.7647151"
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#case-outline",
    "href": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#case-outline",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Hedge funds operate in a highly competitive environment where their ability to generate alpha—returns beyond market factors—is constantly scrutinized. Many funds claim to deliver superior returns by exploiting inefficiencies, but how much of their performance is just exposure to well-known risk factors?\nAs part of the Strategic Investments Division at Sentinel Capital, a multi-billion-dollar Fund of Funds (FoF), your team has been tasked with evaluating the performance of eight hedge funds over the last decade. The ultimate goal? Constructing an optimal portfolio of hedge funds that maximizes returns while managing risk.\nTo do this, you’ll leverage R, tidyverse, tidyquant, and PortfolioAnalytics by applying factor models, portfolio optimization techniques, and rolling performance analysis. Your key tool for evaluation will be the Fama-French Five-Factor Model, which decomposes returns into market, size, value, profitability, and investment factors.\nYour dataset contains monthly returns from eight distinct hedge funds, each with a distinct allocation rationale:\n\nAlphaSynthesis Capital – A long/short equity fund with an emphasis on momentum-driven strategies.\nArbVantage Partners – A statistical arbitrage fund specializing in mean-reversion strategies.\nSentinel Macro Strategies – A global macro fund that trades based on macroeconomic indicators.\nQuantum Volatility Fund – A volatility arbitrage fund exploiting implied vs. realized volatility.\nHorizon Event-Driven – A fund that profits from corporate events such as mergers, spin-offs, and earnings surprises.\nDeepValue Asset Management – A deep-value fund investing in undervalued securities based on fundamental analysis.\nNova Growth Strategies – A growth-oriented hedge fund focusing on technology and high-beta stocks.\nVega Capital Neutral – A market-neutral strategy that seeks absolute returns with low beta exposure.\n\nYour task is to analyze these funds, compare them against the Fama-French three-factor model, and construct optimal hedge fund portfolios.\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®."
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#tech-setup",
    "href": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#tech-setup",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\",\"PortfolioAnalytics\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('PortfolioAnalytics')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(PortfolioAnalytics)\n\n\n\n\n\n\n\nCallout\n\n\n\nDepending upon your R version that is currently installed, you may or may not have the right set of optimization routines properly set up in your session. To make sure that you can use the portfolio optimization functions from PortfolioAnalytics package, install the DEoptim (see details here) that is used on the backend.\n\ninstall.packages('DEoptim')"
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-1",
    "href": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-1",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Download the hf_data.RDS file on eClass®. This dataset covers the period from January 2010, to December 2024 and shows the monthly returns for each hedge fund. Using the functions from the tidyverse and tidyquant packages, visualize the historical returns of all hedge funds. What trends or anomalies do you observe? Are there periods of exceptional gains or drawdowns? Which fund has shown to be the best performer during the study period?\n\n\n\n\n\n\nHint\n\n\n\n\nBecause hf_data is an data.frame object, you can pass the as.xts to convert it to xts and then call cumprod() to generate the cumulative returns for all assets. Do not forget to use the cumprod(1+x)-1 syntax to make sure that you are getting the correct compounded returns over time!\nNow, your resulting output can be converted back to a data.frame using as.data.frame() and getting the date column back using rownames_to_column('date'). To go from wide to long format, you can use the pivot_longer function, keeping only one column for the returns and another column that refers to each strategy at a given date.\nYou can pipe that into a ggplot call, mapping the date to the x-axis, the cumulative return column to the y-axis, and the geom_line() function to create a bar chart, using the group and color to color each fund differently. Add as many customizations you think are worth the effort.\n\n\n\n\n(cumprod(1+as.xts(hf_data))-1)%&gt;%\n  as.data.frame()%&gt;%\n  rownames_to_column('date')%&gt;%\n  mutate(date=as.Date(date))%&gt;%\n  pivot_longer(names_to='strategy',values_to = 'cum_return',cols=2:8)%&gt;%\n  ggplot(aes(x=date,y=cum_return,group=strategy,col=strategy))+\n  geom_line(size=1)+\n  scale_x_date(date_breaks = 'years',date_labels='%Y')+\n  scale_y_continuous(labels = label_percent(big.mark = '.'))+\n  labs(title='Comparison of hedge fund  strategies over time',\n       subtitle='Considering daily hedge fund returns.',\n       col='Strategy',\n       x='',\n       y='Cumulative Return')+\n  theme_minimal()+\n  theme(legend.position = 'bottom',\n        axis.text.x = element_text(angle=90,size=12),\n        axis.text.y = element_text(size=12),\n        axis.title = element_text(face='bold',size=15),\n        plot.title = element_text(face='bold',size=20),\n        plot.subtitle  = element_text(size=12))"
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-2",
    "href": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-2",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Download the ff5_data.RDS file on eClass®. This file contains the historical monthly returns for the Fama-French 5 factor model for the U.S. stock market. Merge this data with your hedge fund dataset, hf_data, and run an Ordinary Least Squares (OLS) regression for each fund against the Fama-French factors using the following specification:\n\\[\n\\small E[R_{i,t}] = \\alpha + \\beta_s^M \\times \\underbrace{(E[R_m]− R_f)}_{\\text{Market}}  + \\beta_s^{SMB} \\times \\underbrace{E[R_{SMB}]}_{\\text{Size}} + \\beta_s^{HML} \\times \\underbrace{E[R_{HML}]}_{\\text{Book-to-Market.}} + \\beta_s^{RMW} \\times \\underbrace{E[R_{RMW}]}_{\\text{Profitability}} + \\beta_s^{CMA} \\times \\underbrace{E[R_{CMA}]}_{\\text{Investment}} + \\varepsilon_{i,t}\n\\]\nBased on the results you found, which funds exhibit significant (at 5% confidence levels) \\(\\alpha\\) after controlling for factor exposures? Store the names of the these funds in an object called positive_alpha_funds.\n\n\n\n\n\n\nHint\n\n\n\n\nBecause hf_data is in wide format, you can use the pivot_longer to transform it to a tidy format - assign it to another object (e.g, hf_returns) for later use.\nAfter that, merge this newly created that to the ff5_data object using the left_join function, and use mutate to create the excess returns for each fund by subtracting the risk-free column, assigning it to a new object (e.g, merged_df)\nUse the functional programming techniques from the purrr package (or do a for-loop) to map the lm and tidy functions to each strategy subset, and collect the results from \\(\\alpha\\) (the Intercept of the regression).\nCollect the results in an object and use ggplot to chart your results.\n\n\n\n\n#Read the FF5 Data\nff5_data=readRDS('Assets/ff5_data.RDS')\n\n#Convert the hf_data to a data.frame object and adjust columns\nhf_returns = hf_data%&gt;%\n  pivot_longer(cols = -date,names_to = 'fund',values_to = 'monthly_return')\n\n #Merge both datasets by date\nmerged_df &lt;- hf_returns%&gt;%\n  #Merge\n  left_join(ff5_data, by = \"date\")%&gt;%\n  #Pivot the data for each strategy\n  mutate(excess_return = monthly_return - RF)\n\n#Alpha estimation\nFF_results &lt;- merged_df%&gt;%\n  #Group by strategy\n  group_by(fund)%&gt;%\n  #Nest the data\n  nest()%&gt;%\n  #For each nest, map the lm() function and the tidy function\n  mutate(model = map(data, ~ lm(excess_return ~ MKT_MINUS_RF + HML + SMB + CMA + RMW, data = .)),\n         results = map(model, tidy))%&gt;%\n  #Unnest the results\n  unnest(results)%&gt;%\n  #Select the desired columns\n  select(fund, term, estimate, std.error, p.value)\n\n#Chart\nFF_results %&gt;%\n  filter(term=='(Intercept)')%&gt;%\n  mutate(stat_sig=ifelse(p.value&lt;0.05,'Statistically sig. at 1%','Not statistically sig. at 1%'))%&gt;%\n  ggplot(aes(x=reorder(fund,desc(estimate)),y=estimate,fill=stat_sig))+\n  geom_col(size=3)+\n  geom_text(aes(label = percent(estimate,accuracy=0.01),vjust=-1))+\n  #Annotations\n  labs(title='Which strategies did generate positive and statistically significant alphas?',\n       subtitle = 'Using the Fama-French three-factor model with monthly return data.',\n       x = 'Strategy',\n       y = 'Alpha (%)',\n       fill = 'Stat. Sig')+\n  #Scales\n  scale_y_continuous(labels = percent)+\n  #Custom theme minimal\n  theme_minimal()+\n  #Adding further customizations\n  theme(legend.position='bottom',\n        axis.title = element_text(face='bold',size=15),\n        axis.text = element_text(size=12),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15))\n\n\n\n\n\n\n\n#Positive alpha funds\npositive_alpha_funds=FF_results%&gt;%filter(term=='(Intercept)',estimate&gt;0)%&gt;%pull(fund)"
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-4",
    "href": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-4",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Using the results from our previous OLS regressions, explain how exposed each fund was to each of the factor portfolios contained in the Fama-French 5 Factor model. How can this help you explain the over(under)performance of the top(worst) funds?\n\n\n\n\n\n\nHint\n\n\n\n\nUsing the estimates from the estimation, collect all the \\(\\beta\\)s from the regressions by filtering out the Intercept using the filter() function\nUse ggplot to chart the weights (factor betas) for each strategy. To display all strategies in one chart, you can map the factors to the x axis, the factor betas values to the y axis, use facet_wrap to facet your chart by each different fund.\n\n\n\n\nFF_results %&gt;%\n  filter(term != \"(Intercept)\")%&gt;%\n  group_by(fund)%&gt;%\n  ggplot(aes(x = reorder(term,desc(estimate)), y = estimate, fill = term)) +\n  geom_col(position = position_dodge())+\n  geom_label(aes(label = round(estimate,2)),col='black',fill='white')+\n  theme_minimal()+\n  facet_wrap(fund~.,ncol=2,nrow=4)+\n  #Annotations\n  labs(title = \"Fama-French Factor Loadings by Hedge Fund Strategy\",\n       x = \"Hedge Fund Strategy\",\n       y = \"Factor Loading\",\n       fill = 'Risk Factor')+\n  #Custom theme minimal\n  theme_minimal()+\n  #Adding further customizations\n  theme(legend.position='bottom',\n        axis.title = element_text(face='bold',size=15),\n        axis.text = element_blank(),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15))"
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-4-1",
    "href": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-4-1",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Way to go! Your boss was really impressed with how you were able to dissect the returns from individual hedge funds and understand which factors drove performance, and in which cases we were able to find positive returns that were uncorrelated with any of the risk factors. Now, it is time to start building portfolios by allocating money across each fund using the Markowitz Mean-Variance optimization rationale.\nTo do that, start by filtering your sample to consider only the hedge funds that were able to generate \\(\\alpha\\) throughout the whole period. Divide your sample using a \\(50\\%/50\\%\\) split, using information from \\(2010\\) until \\(2018\\) as a training set, and \\(2019\\) to \\(2024\\) as a test set. Construct a Minimum-Variance Portfolio of the following format:\n\\[\n\\min_{\\{w_1,w_2...,w_n\\}} \\sigma^2_p = w^T \\Sigma w,\n\\text{ such that:}\n\\\\\n\\begin{cases}\n\\sum_{i=1}^n w_i=1 \\text{ (1)}\\\\\n0.05\\leq w_i \\leq 0.50, \\forall i \\text{ (2)}\n\\end{cases}\n\\]\nIn other words, you are solving the following problem: find the set of allocation weights \\(w_1,w_2,w_3,...,w_n\\) (the % that you allocate in each of the strategies) that, when used to create a portfolio, are the ones that create the portfolio with the minimum variance among all possible combinations. In order to do that, you have to ensure that the weights add up to \\(100\\%\\) (so you’re fully investing your capital), which is the first condition. The second conditions states that neither stock can have a weight that is lower than \\(5\\%\\) nor have a weight that is greater than \\(50\\%\\).\nNote that \\(w^T\\Sigma w\\) is nothing more than the matrix form of \\(\\sum_{i=1}^{N}w_i\\sigma_i^2+ 2\\sum_{i=1}^{N}\\sum_{j\\neq i}w_i w_j\\sigma_{i,j}\\), which is the variance of a portfolio that consists of \\(N\\) assets.\nUsing the training sample and only the funds that have shown \\(\\alpha&gt;0\\) (regardless of the statistical significance), what is the optimal allocation? Store this into an object called min_var_allocation.\n\n\n\n\n\n\nHint\n\n\n\n\nFilter the hf_data to contain only the date and the columns that refer to the strategies with positive \\(\\alpha\\).\nUse the as.xts to transform the object from a data.frame to an xts object. This is crucial, as the functions from the PortfolioAnalytics operate on xts objects. With your newly created xts object, create two new objects, training_sample and testing_sample using the xts subsetting syntax:\n\n\ntraining_sample=(your_filtered_hf_data%&gt;%as.xts())['2010/2018']\ntest_sample=(your_filtered_hf_data%&gt;%as.xts())['2019/2024']\n\n\nUse the portfolio.spec and the optimize.portfolio functions from the PortfolioAnalytics package to define and optimize the portfolio using the syntax we have learned during class. If you are unsure on what to do here, please refer to the workbook.R file that we have gone through for a similar replication.\nFinally, use the extractWeights function to output a table with the optimal weights.\n\n\n\n\n#Transform into .xts and split the sample\nfiltered_hf_data=hf_data%&gt;%select(date,all_of(positive_alpha_funds))\ntraining_sample=(filtered_hf_data%&gt;%as.xts())['2010/2018']\ntest_sample=(filtered_hf_data%&gt;%as.xts())['2019/2024']\n\n\n#Risk Minimization\nmin_var_portfolio &lt;- portfolio.spec(assets = positive_alpha_funds)%&gt;% \n  add.objective(type = \"risk\", name = \"var\")%&gt;%\n  add.constraint(type = \"full_investment\")%&gt;%\n  add.constraint(type = \"box\", min = 0.05, max = 0.5)\n\nmin_var_allocation &lt;- training_sample %&gt;% \n  optimize.portfolio(\n    portfolio = min_var_portfolio\n  )\n\nIteration: 1 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\nIteration: 2 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\nIteration: 3 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\nIteration: 4 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\nIteration: 5 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\nIteration: 6 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\nIteration: 7 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\nIteration: 8 bestvalit: 0.033117 bestmemit:    0.054000    0.440000    0.052000    0.454000\n[1] 0.054 0.440 0.052 0.454\n\n#Get the weights\nextractWeights(min_var_allocation)\n\nalphasynthesis     arbvantage       sentinel        horizon \n         0.054          0.440          0.052          0.454"
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-5",
    "href": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-5",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Using the training sample and only the funds that have shown \\(\\alpha&gt;0\\), develop a portfolio that maximizes returns instead of minimizing risk. What is the optimal allocation? Store this into an object called max_return_allocation.\n\n#Return Maximization\nmax_return_portfolio &lt;- portfolio.spec(assets = positive_alpha_funds)%&gt;% \n  add.objective(type = \"return\", name = \"mean\")%&gt;%\n  add.constraint(type = \"full_investment\")%&gt;%\n  add.constraint(type = \"box\", min = 0.05, max = 0.5)\n\nmax_return_allocation &lt;- training_sample %&gt;% \n  optimize.portfolio(\n    portfolio = max_return_portfolio\n    )\n\nIteration: 1 bestvalit: -0.010391 bestmemit:    0.358000    0.060000    0.492000    0.090000\nIteration: 2 bestvalit: -0.010391 bestmemit:    0.358000    0.060000    0.492000    0.090000\nIteration: 3 bestvalit: -0.010391 bestmemit:    0.358000    0.060000    0.492000    0.090000\nIteration: 4 bestvalit: -0.010391 bestmemit:    0.358000    0.060000    0.492000    0.090000\nIteration: 5 bestvalit: -0.010391 bestmemit:    0.358000    0.060000    0.492000    0.090000\nIteration: 6 bestvalit: -0.010391 bestmemit:    0.358000    0.060000    0.492000    0.090000\nIteration: 7 bestvalit: -0.010391 bestmemit:    0.358000    0.060000    0.492000    0.090000\n[1] 0.358 0.060 0.492 0.090\n\n#Get the weights\nextractWeights(max_return_allocation)\n\nalphasynthesis     arbvantage       sentinel        horizon \n         0.358          0.060          0.492          0.090"
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-6",
    "href": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-6",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Using the training sample and only the funds that have shown \\(\\alpha&gt;0\\), develop a portfolio that maximizes the Sharpe-Ratio - i.e, maximizes the risk-adjusted returns. What is the optimal allocation? Store this into an object called max_sharpe_allocation.\n\n#Return Maximization\nmax_sharpe_portfolio &lt;- portfolio.spec(assets = positive_alpha_funds)%&gt;% \n  add.objective(type = \"return\", name = \"mean\")%&gt;% \n  add.objective(type = \"risk\", name = \"StdDev\")%&gt;%\n  add.constraint(type = \"full_investment\")%&gt;%\n  add.constraint(type = \"box\", min = 0.05, max = 0.5)\n\nmax_sharpe_allocation &lt;- training_sample %&gt;% \n  optimize.portfolio(\n    portfolio = max_sharpe_portfolio,\n    SR= TRUE\n  )\n\nIteration: 1 bestvalit: 0.025895 bestmemit:    0.196000    0.338000    0.132000    0.334000\nIteration: 2 bestvalit: 0.024438 bestmemit:    0.134000    0.302000    0.082000    0.482000\nIteration: 3 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\nIteration: 4 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\nIteration: 5 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\nIteration: 6 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\nIteration: 7 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\nIteration: 8 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\nIteration: 9 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\nIteration: 10 bestvalit: 0.024390 bestmemit:    0.052000    0.490000    0.088000    0.370000\n[1] 0.052 0.490 0.088 0.370\n\n#Get the weights\nextractWeights(max_sharpe_allocation)\n\nalphasynthesis     arbvantage       sentinel        horizon \n         0.052          0.490          0.088          0.370"
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-7",
    "href": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-7",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Compare the optimal weights across each strategy and discuss the allocations. Is there any better allocation rule?\n\n\n\n\n\n\nHint\n\n\n\n\nAfter collecting your optimal weights using the extractWeights function and storing them into separate objects, use the rbind function to bind them together in a row-wise manner and transform it to a data.frame using the as.data.frame() function.\nAdd a new column that describes which strategy is contained in each row.\nUse the pivot_longer to transform the data to a tidy format, making it easier for you to pipe the output into a ggplot call.\n\nFor these steps, you can use the following syntax:\n\nrbind(extractWeights(min_var_allocation),\n      extractWeights(max_return_allocation),\n      extractWeights(max_sharpe_allocation))%&gt;%\n  as.data.frame()%&gt;%\n  mutate(objective=c('Minimize Variance','Maximize Returns','Maximize Sharpe'))%&gt;%\n  pivot_longer(1:4,names_to = 'fund',values_to = 'weights')\n\n\n\n\n#Chart weights\nrbind(extractWeights(min_var_allocation),\n      extractWeights(max_return_allocation),\n      extractWeights(max_sharpe_allocation))%&gt;%\n  as.data.frame()%&gt;%\n  mutate(objective=c('Minimize Variance','Maximize Returns','Maximize Sharpe'))%&gt;%\n  pivot_longer(1:4,names_to = 'fund',values_to = 'weights')%&gt;%\n  ggplot(aes(x = objective, y = weights, fill = fund)) +\n  geom_col() + \n  geom_text(aes(label = percent(weights)),\n            position = position_stack(vjust = .6),\n            size = 6) + \n  scale_x_discrete(labels = c(\"Maximize Return\", \"Minimize Risk\", \"Max Sharpe Ratio\")) +\n  scale_y_continuous(labels=scales::percent)+\n  labs(x = \"\", y = \"Allocation (%)\",\n       title = \"Asset Allocation by Optimization Criteria\")+\n  theme_minimal()+\n  theme(legend.position='bottom',\n        axis.title = element_text(face='bold',size=15),\n        axis.text = element_text(size=12),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15))"
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-8",
    "href": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-8",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Compare the historical performance from each fund in the testing sample. To do that, use the three sets of optimal weights you have just created (min_var_allocation, max_return_allocation, and max_sharpe_allocation) along with the tq_portfolio function to estimate the returns in the test sample, rebalancing it monthly. Which strategy did deliver the highest out-of-sample returns? Did they outperform a naive portfolio using equal weights to all funds?\n\n\n\n\n\n\nHint\n\n\n\n\nUse the hf_returns data you have created in the beginning of the exercise (i.e, the dataset of hedge fund returns in long format) and apply the filter function to keep only the rows where i) the fund is in the list of funds with positive alpha (i.e, it is in the positive_alpha_funds you have created before) and year(date)&gt;2018 (i.e, belongs to testing sample).\nUse the tq_portfolio function using the following syntax to calculate the historical returns of the portfolio:\n\n\n  tq_portfolio(\n    assets_col = fund, #This is the column that contains the fund names\n    returns_col = monthly_return, #Column that contains the monthly returns\n    weights = extractWeights(min_var_allocation), #The portfolio.spec you have created\n    col_rename = \"returns_min_var\",\n    rebalance_on = \"months\"\n  )\n\n\nDo this for all three portfolio specifications, assigning their results to separate objects. For the naive portfolio, you can assign a vector of equal weights using the rep(weight,number_of_replications) syntax.\n\n\n\n\n#Minimize Risk Portfolio\n\nmin_var_performance &lt;- hf_returns%&gt;%\n  filter(fund %in% positive_alpha_funds, year(date)&gt;2018)%&gt;%\n  tq_portfolio(\n    assets_col = fund,\n    returns_col = monthly_return,\n    weights = extractWeights(min_var_allocation),\n    col_rename = \"returns_min_var\",\n    rebalance_on = \"months\"\n  )\n\n#Maximize Return Portfolio\n\nmax_return_performance &lt;- hf_returns%&gt;%\n  filter(fund %in% positive_alpha_funds, year(date)&gt;2018)%&gt;%\n  tq_portfolio(\n    assets_col = fund,\n    returns_col = monthly_return,\n    weights = extractWeights(max_return_allocation),\n    col_rename = \"returns_max_return\",\n    rebalance_on = \"months\"\n  )\n\n#Naive Portfolio\n\nmax_sharpe_performance &lt;- hf_returns%&gt;%\n  filter(fund %in% positive_alpha_funds, year(date)&gt;2018)%&gt;%\n  tq_portfolio(\n    assets_col = fund,\n    returns_col = monthly_return,\n    weights = extractWeights(max_sharpe_allocation),\n    col_rename = \"returns_max_sharpe\",\n    rebalance_on = \"months\"\n  )\n\n#Naive Portfolio\n\nnaive_performance &lt;- hf_returns%&gt;%\n  filter(fund %in% positive_alpha_funds, year(date)&gt;2018)%&gt;%\n  tq_portfolio(\n    assets_col = fund,\n    returns_col = monthly_return,\n    weights = rep(0.25,4),\n    col_rename = \"returns_naive\",\n    rebalance_on = \"months\"\n  )\n\n#Chart\n\nnaive_performance%&gt;%\n  left_join(min_var_performance)%&gt;%\n  left_join(max_return_performance)%&gt;%\n  left_join(max_sharpe_performance)%&gt;%\n  pivot_longer(cols = \"returns_naive\":\"returns_max_sharpe\",\n               names_to = \"strategy\",\n               values_to = \"return\") %&gt;% \n  mutate(strategy = case_when(strategy == \"returns_max_return\" ~ \"Maximize Avg. Return\",\n                                strategy == \"returns_min_var\" ~ \"Minimize risk\",\n                                strategy == \"returns_naive\" ~ \"Equal Allocation\",\n                                strategy == \"returns_max_sharpe\" ~ \"Maximize Sharpe Ratio\")) %&gt;% \n  group_by(strategy) %&gt;% \n  mutate(cum_return = cumprod(1+return)-1) %&gt;% \n  ggplot(aes(x = date, y = cum_return, color = strategy)) + \n  geom_line(size = 1.2)+ \n  scale_y_continuous(label = scales::percent) +\n  labs(title = \"Historical Performance over test sample\", y = \"Cumulative Return\",\n       x = \"\",\n       color='') + \n  theme_minimal()+\n  theme(legend.position='bottom',\n      axis.title = element_text(face='bold',size=15),\n      axis.text = element_text(size=12),\n      plot.title = element_text(size=20,face='bold'),\n      plot.subtitle  = element_text(size=15))\n\n\n\n\n\n\n\n#Table\nnaive_performance%&gt;%\n  left_join(min_var_performance)%&gt;%\n  left_join(max_return_performance)%&gt;%\n  left_join(max_sharpe_performance)%&gt;%\n  as.xts()%&gt;%\n  Return.cumulative()\n\n                  returns_naive returns_min_var returns_max_return\nCumulative Return     0.9067833       0.8617784          0.8948185\n                  returns_max_sharpe\nCumulative Return          0.8771747"
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-9",
    "href": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-9",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Using the max_return_portfolio object and the test_sample data cut, implement a rolling-window optimization to find the optimal portfolio weights. Use a training period and a rolling window of \\(12\\) months, and rebalance the portfolio monthly. How do these dynamically adjusted weights perform compared to static allocations?\n\n\n\n\n\n\nHint\n\n\n\n\nUsing the test_sample xts object you have created, apply the optimize.portfolio.rebalancing function, pointing to the max_return_portfolio and the aforementioned parameters for the training period, rolling window, and rebalancing period. You can use the following syntax:\n\n\n#Rolling optimization\nrolling_optimization &lt;- test_sample%&gt;%\n  optimize.portfolio.rebalancing(\n    portfolio = max_return_portfolio,\n    rebalance_on = \"months\",\n    training_period = 12,\n    rolling_window = 12\n  )\n\n\nAfter you have optimized your portfolio, call extractWeights and pipe that into ggplot to chart your results.\n\n\n\n\n#Chart\nextractWeights(rolling_optimization)%&gt;%\n  as.data.frame()%&gt;%\n  rownames_to_column(\"date\")%&gt;% \n  mutate(date = as.Date(date)) %&gt;% \n  pivot_longer(names_to = 'symbol',values_to = 'weights',cols=where(is.numeric)) %&gt;% \n  ggplot(aes(x = date, y = weights, fill = symbol)) +\n  labs(fill = \"\", x = \"\", y = \"Asset Weight\",\n       title = \"Weights under rolling optimization - Maximize Returns\") +\n  geom_col() +\n  scale_x_date(date_breaks = \"6 months\") +\n  scale_y_continuous(labels=scales::percent)+\n  theme_minimal()+\n  theme(legend.position='bottom',\n        axis.title = element_text(face='bold',size=15),\n        axis.text = element_text(size=12),\n        axis.text.x = element_text(angle=90),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15))"
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-10",
    "href": "quant-fin/datacases/data-case-2/data-case-2-solutions.html#exercise-10",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Looking only at the test sample period (i.e, from \\(2019\\) to \\(2024\\), chart the historical performance of the dynamically-optimized portfolio vis-a-vis the static portfolios you have just created. Are there any gains stemming from dynamically optimizing the portfolio?\n\n\n\n\n\n\nHint\n\n\n\n\nUse the extractWeights() function on the dynamically optimized portfolio optimization object to collect a time series of weight allocations.\nWith that, use as_data_frame() and rownames_to_column(\"date\") to transform it to a data.frame with a date column identifier.\nFinally, transform it to long format using the pivot_longer() function.\nDo a similar procedure for the xts object where you stored the test sample returns (in my example, test_sample)\nUse left_join to merge both datasets. You can use a combination of mutate() and summarize() to calculate the portfolio returns on each date in a step-wise manner:\n\n\ndynamic_performance=left_join(weights,returns)%&gt;% # Your weights and returns data.frames\n  mutate(ind_return=returns*weights)%&gt;% # Calculate individual returns for each fund on each date\n  group_by(date)%&gt;% # Group by date\n  summarize(returns_dynamic=sum(ind_return,na.rm=TRUE)) # Summing up yields a weighted average of returns by each date\n\n\nJoin this data.frame with the other objects that contain the historical returns from the other allocation strategies and pipe that into ggplot.\n\n\n\n\nweights=extractWeights(rolling_optimization)%&gt;%\n  as.data.frame()%&gt;%\n  rownames_to_column(\"date\")%&gt;% \n  mutate(date = as.Date(date)) %&gt;% \n  pivot_longer(names_to = 'fund',values_to = 'weights',cols=where(is.numeric))\n\nreturns=test_sample%&gt;%\n  as.data.frame()%&gt;%\n  rownames_to_column(\"date\")%&gt;% \n  mutate(date = as.Date(date)) %&gt;% \n  pivot_longer(names_to = 'fund',values_to = 'returns',cols=where(is.numeric))\n\ndynamic_performance=left_join(weights,returns)%&gt;%\n  mutate(ind_return=returns*weights)%&gt;%\n  group_by(date)%&gt;%\n  summarize(returns_dynamic=sum(ind_return,na.rm=TRUE))\n\ndynamic_performance%&gt;%\n  left_join(min_var_performance)%&gt;%\n  left_join(max_return_performance)%&gt;%\n  left_join(max_sharpe_performance)%&gt;%\n  left_join(naive_performance)%&gt;%\n  pivot_longer(cols = \"returns_dynamic\":\"returns_naive\",\n               names_to = \"strategy\",\n               values_to = \"return\")%&gt;% \n  mutate(strategy = case_when(strategy == \"returns_max_return\" ~ \"Maximize Avg. Return\",\n                                strategy == \"returns_min_var\" ~ \"Minimize risk\",\n                                strategy == \"returns_naive\" ~ \"Equal Allocation\",\n                                strategy == \"returns_max_sharpe\" ~ \"Maximize Sharpe Ratio\",\n                                .default = \"Dynamic Portfolio\")) %&gt;%\n  group_by(strategy) %&gt;% \n  mutate(cum_return = cumprod(1+return)-1) %&gt;% \n  ggplot(aes(x = date, y = cum_return, color = strategy)) + \n  geom_line(size = 1.2)+ \n  scale_y_continuous(label = scales::percent) +\n  labs(title = \"Dynamic vis-a-vis static portfolios\", y = \"Cumulative Return\",\n       x = \"\",\n       color='') + \n  theme_minimal()+\n  theme(legend.position='bottom',\n        axis.title = element_text(face='bold',size=15),\n        axis.text = element_text(size=12),\n        plot.title = element_text(size=20,face='bold'),\n        plot.subtitle  = element_text(size=15))\n\n\n\n\n\n\n\n#Table\ndynamic_performance%&gt;%\n  left_join(min_var_performance)%&gt;%\n  left_join(max_return_performance)%&gt;%\n  left_join(max_sharpe_performance)%&gt;%\n  left_join(naive_performance)%&gt;%\n  as.xts()%&gt;%\n  Return.cumulative()\n\n                  returns_dynamic returns_min_var returns_max_return\nCumulative Return        1.208376       0.7725337          0.7148009\n                  returns_max_sharpe returns_naive\nCumulative Return          0.7903378     0.7647151"
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2.html",
    "href": "quant-fin/datacases/data-case-2/data-case-2.html",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "This Data Case is part of the Practical Applications in Quantitative Finance course, held at FGV-EAESP’s undergraduate course in business. Carefully follow the instructions contained in the data case as well as eClass® before you make your submission.\n\n\nHedge funds operate in a highly competitive environment where their ability to generate alpha—returns beyond market factors—is constantly scrutinized. Many funds claim to deliver superior returns by exploiting inefficiencies, but how much of their performance is just exposure to well-known risk factors?\nAs part of the Strategic Investments Division at Sentinel Capital, a multi-billion-dollar Fund of Funds (FoF), your team has been tasked with evaluating the performance of eight hedge funds over the last decade. The ultimate goal? Constructing an optimal portfolio of hedge funds that maximizes returns while managing risk.\nTo do this, you’ll leverage R, tidyverse, tidyquant, and PortfolioAnalytics by applying factor models, portfolio optimization techniques, and rolling performance analysis. Your key tool for evaluation will be the Fama-French Five-Factor Model, which decomposes returns into market, size, value, profitability, and investment factors.\nYour dataset contains monthly returns from eight distinct hedge funds, each with a distinct allocation rationale:\n\nAlphaSynthesis Capital – A long/short equity fund with an emphasis on momentum-driven strategies.\nArbVantage Partners – A statistical arbitrage fund specializing in mean-reversion strategies.\nSentinel Macro Strategies – A global macro fund that trades based on macroeconomic indicators.\nQuantum Volatility Fund – A volatility arbitrage fund exploiting implied vs. realized volatility.\nHorizon Event-Driven – A fund that profits from corporate events such as mergers, spin-offs, and earnings surprises.\nDeepValue Asset Management – A deep-value fund investing in undervalued securities based on fundamental analysis.\nNova Growth Strategies – A growth-oriented hedge fund focusing on technology and high-beta stocks.\nVega Capital Neutral – A market-neutral strategy that seeks absolute returns with low beta exposure.\n\nYour task is to analyze these funds, compare them against the Fama-French three-factor model, and construct optimal hedge fund portfolios.\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®.\n\n\n\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\",\"PortfolioAnalytics\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('PortfolioAnalytics')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(PortfolioAnalytics)\n\n\n\n\n\n\n\nCallout\n\n\n\nDepending upon your R version that is currently installed, you may or may not have the right set of optimization routines properly set up in your session. To make sure that you can use the portfolio optimization functions from PortfolioAnalytics package, install the DEoptim (see details here) that is used on the backend.\n\ninstall.packages('DEoptim')\n\n\n\n\n\n\nDownload the hf_data.RDS file on eClass®. This dataset covers the period from January 2010, to December 2024 and shows the monthly returns for each hedge fund. Using the functions from the tidyverse and tidyquant packages, visualize the historical returns of all hedge funds. What trends or anomalies do you observe? Are there periods of exceptional gains or drawdowns? Which fund has shown to be the best performer during the study period?\n\n\n\n\n\n\nHint\n\n\n\n\nBecause hf_data is an data.frame object, you can pass the as.xts to convert it to xts and then call cumprod() to generate the cumulative returns for all assets. Do not forget to use the cumprod(1+x)-1 syntax to make sure that you are getting the correct compounded returns over time!\nNow, your resulting output can be converted back to a data.frame using as.data.frame() and getting the date column back using rownames_to_column('date'). To go from wide to long format, you can use the pivot_longer function, keeping only one column for the returns and another column that refers to each strategy at a given date.\nYou can pipe that into a ggplot call, mapping the date to the x-axis, the cumulative return column to the y-axis, and the geom_line() function to create a bar chart, using the group and color to color each fund differently. Add as many customizations you think are worth the effort.\n\n\n\n\n\n\nDownload the ff5_data.RDS file on eClass®. This file contains the historical monthly returns for the Fama-French 5 factor model for the U.S. stock market. Merge this data with your hedge fund dataset, hf_data, and run an Ordinary Least Squares (OLS) regression for each fund against the Fama-French factors using the following specification:\n\\[\n\\small E[R_{i,t}] = \\alpha + \\beta_s^M \\times \\underbrace{(E[R_m]− R_f)}_{\\text{Market}}  + \\beta_s^{SMB} \\times \\underbrace{E[R_{SMB}]}_{\\text{Size}} + \\beta_s^{HML} \\times \\underbrace{E[R_{HML}]}_{\\text{Book-to-Market.}} + \\beta_s^{RMW} \\times \\underbrace{E[R_{RMW}]}_{\\text{Profitability}} + \\beta_s^{CMA} \\times \\underbrace{E[R_{CMA}]}_{\\text{Investment}} + \\varepsilon_{i,t}\n\\]\nBased on the results you found, which funds exhibit significant (at 5% confidence levels) \\(\\alpha\\) after controlling for factor exposures? Store the names of the these funds in an object called positive_alpha_funds.\n\n\n\n\n\n\nHint\n\n\n\n\nBecause hf_data is in wide format, you can use the pivot_longer to transform it to a tidy format - assign it to another object (e.g, hf_returns) for later use.\nAfter that, merge this newly created that to the ff5_data object using the left_join function, and use mutate to create the excess returns for each fund by subtracting the risk-free column, assigning it to a new object (e.g, merged_df)\nUse the functional programming techniques from the purrr package (or do a for-loop) to map the lm and tidy functions to each strategy subset, and collect the results from \\(\\alpha\\) (the Intercept of the regression).\nCollect the results in an object and use ggplot to chart your results.\n\n\n\n\n\n\nUsing the results from our previous OLS regressions, explain how exposed each fund was to each of the factor portfolios contained in the Fama-French 5 Factor model. How can this help you explain the over(under)performance of the top(worst) funds?\n\n\n\n\n\n\nHint\n\n\n\n\nUsing the estimates from the estimation, collect all the \\(\\beta\\)s from the regressions by filtering out the Intercept using the filter() function\nUse ggplot to chart the weights (factor betas) for each strategy. To display all strategies in one chart, you can map the factors to the x axis, the factor betas values to the y axis, use facet_wrap to facet your chart by each different fund.\n\n\n\n\n\n\nWay to go! Your boss was really impressed with how you were able to dissect the returns from individual hedge funds and understand which factors drove performance, and in which cases we were able to find positive returns that were uncorrelated with any of the risk factors. Now, it is time to start building portfolios by allocating money across each fund using the Markowitz Mean-Variance optimization rationale.\nTo do that, start by filtering your sample to consider only the hedge funds that were able to generate \\(\\alpha\\) throughout the whole period. Divide your sample using a \\(50\\%/50\\%\\) split, using information from \\(2010\\) until \\(2018\\) as a training set, and \\(2019\\) to \\(2024\\) as a test set. Construct a Minimum-Variance Portfolio of the following format:\n\\[\n\\min_{\\{w_1,w_2...,w_n\\}} \\sigma^2_p = w^T \\Sigma w,\n\\text{ such that:}\n\\\\\n\\begin{cases}\n\\sum_{i=1}^n w_i=1 \\text{ (1)}\\\\\n0.05\\leq w_i \\leq 0.50, \\forall i \\text{ (2)}\n\\end{cases}\n\\]\nIn other words, you are solving the following problem: find the set of allocation weights \\(w_1,w_2,w_3,...,w_n\\) (the % that you allocate in each of the strategies) that, when used to create a portfolio, are the ones that create the portfolio with the minimum variance among all possible combinations. In order to do that, you have to ensure that the weights add up to \\(100\\%\\) (so you’re fully investing your capital), which is the first condition. The second conditions states that neither stock can have a weight that is lower than \\(5\\%\\) nor have a weight that is greater than \\(50\\%\\).\nNote that \\(w^T\\Sigma w\\) is nothing more than the matrix form of \\(\\sum_{i=1}^{N}w_i\\sigma_i^2+ 2\\sum_{i=1}^{N}\\sum_{j\\neq i}w_i w_j\\sigma_{i,j}\\), which is the variance of a portfolio that consists of \\(N\\) assets.\nUsing the training sample and only the funds that have shown \\(\\alpha&gt;0\\) (regardless of the statistical significance), what is the optimal allocation? Store this into an object called min_var_allocation.\n\n\n\n\n\n\nHint\n\n\n\n\nFilter the hf_data to contain only the date and the columns that refer to the strategies with positive \\(\\alpha\\).\nUse the as.xts to transform the object from a data.frame to an xts object. This is crucial, as the functions from the PortfolioAnalytics operate on xts objects. With your newly created xts object, create two new objects, training_sample and testing_sample using the xts subsetting syntax:\n\n\ntraining_sample=(your_filtered_hf_data%&gt;%as.xts())['2010/2018']\ntest_sample=(your_filtered_hf_data%&gt;%as.xts())['2019/2024']\n\n\nUse the portfolio.spec and the optimize.portfolio functions from the PortfolioAnalytics package to define and optimize the portfolio using the syntax we have learned during class. If you are unsure on what to do here, please refer to the workbook.R file that we have gone through for a similar replication.\nFinally, use the extractWeights function to output a table with the optimal weights.\n\n\n\n\n\n\nUsing the training sample and only the funds that have shown \\(\\alpha&gt;0\\), develop a portfolio that maximizes returns instead of minimizing risk. What is the optimal allocation? Store this into an object called max_return_allocation.\n\n\n\nUsing the training sample and only the funds that have shown \\(\\alpha&gt;0\\), develop a portfolio that maximizes the Sharpe-Ratio - i.e, maximizes the risk-adjusted returns. What is the optimal allocation? Store this into an object called max_sharpe_allocation.\n\n\n\nCompare the optimal weights across each strategy and discuss the allocations. Is there any better allocation rule?\n\n\n\n\n\n\nHint\n\n\n\n\nAfter collecting your optimal weights using the extractWeights function and storing them into separate objects, use the rbind function to bind them together in a row-wise manner and transform it to a data.frame using the as.data.frame() function.\nAdd a new column that describes which strategy is contained in each row.\nUse the pivot_longer to transform the data to a tidy format, making it easier for you to pipe the output into a ggplot call.\n\nFor these steps, you can use the following syntax:\n\nrbind(extractWeights(min_var_allocation),\n      extractWeights(max_return_allocation),\n      extractWeights(max_sharpe_allocation))%&gt;%\n  as.data.frame()%&gt;%\n  mutate(objective=c('Minimize Variance','Maximize Returns','Maximize Sharpe'))%&gt;%\n  pivot_longer(1:4,names_to = 'fund',values_to = 'weights')\n\n\n\n\n\n\nCompare the historical performance from each fund in the testing sample. To do that, use the three sets of optimal weights you have just created (min_var_allocation, max_return_allocation, and max_sharpe_allocation) along with the tq_portfolio function to estimate the returns in the test sample, rebalancing it monthly. Which strategy did deliver the highest out-of-sample returns? Did they outperform a naive portfolio using equal weights to all funds?\n\n\n\n\n\n\nHint\n\n\n\n\nUse the hf_returns data you have created in the beginning of the exercise (i.e, the dataset of hedge fund returns in long format) and apply the filter function to keep only the rows where i) the fund is in the list of funds with positive alpha (i.e, it is in the positive_alpha_funds you have created before) and year(date)&gt;2018 (i.e, belongs to testing sample).\nUse the tq_portfolio function using the following syntax to calculate the historical returns of the portfolio:\n\n\n  tq_portfolio(\n    assets_col = fund, #This is the column that contains the fund names\n    returns_col = monthly_return, #Column that contains the monthly returns\n    weights = extractWeights(min_var_allocation), #The portfolio.spec you have created\n    col_rename = \"returns_min_var\",\n    rebalance_on = \"months\"\n  )\n\n\nDo this for all three portfolio specifications, assigning their results to separate objects. For the naive portfolio, you can assign a vector of equal weights using the rep(weight,number_of_replications) syntax.\n\n\n\n\n\n\nUsing the max_return_portfolio object and the test_sample data cut, implement a rolling-window optimization to find the optimal portfolio weights. Use a training period and a rolling window of \\(12\\) months, and rebalance the portfolio monthly. How do these dynamically adjusted weights perform compared to static allocations?\n\n\n\n\n\n\nHint\n\n\n\n\nUsing the test_sample xts object you have created, apply the optimize.portfolio.rebalancing function, pointing to the max_return_portfolio and the aforementioned parameters for the training period, rolling window, and rebalancing period. You can use the following syntax:\n\n\n#Rolling optimization\nrolling_optimization &lt;- test_sample%&gt;%\n  optimize.portfolio.rebalancing(\n    portfolio = max_return_portfolio,\n    rebalance_on = \"months\",\n    training_period = 12,\n    rolling_window = 12\n  )\n\n\nAfter you have optimized your portfolio, call extractWeights and pipe that into ggplot to chart your results.\n\n\n\n\n\n\nLooking only at the test sample period (i.e, from \\(2019\\) to \\(2024\\), chart the historical performance of the dynamically-optimized portfolio vis-a-vis the static portfolios you have just created. Are there any gains stemming from dynamically optimizing the portfolio?\n\n\n\n\n\n\nHint\n\n\n\n\nUse the extractWeights() function on the dynamically optimized portfolio optimization object to collect a time series of weight allocations.\nWith that, use as_data_frame() and rownames_to_column(\"date\") to transform it to a data.frame with a date column identifier.\nFinally, transform it to long format using the pivot_longer() function.\nDo a similar procedure for the xts object where you stored the test sample returns (in my example, test_sample)\nUse left_join to merge both datasets. You can use a combination of mutate() and summarize() to calculate the portfolio returns on each date in a step-wise manner:\n\n\ndynamic_performance=left_join(weights,returns)%&gt;% # Your weights and returns data.frames\n  mutate(ind_return=returns*weights)%&gt;% # Calculate individual returns for each fund on each date\n  group_by(date)%&gt;% # Group by date\n  summarize(returns_dynamic=sum(ind_return,na.rm=TRUE)) # Summing up yields a weighted average of returns by each date\n\n\nJoin this data.frame with the other objects that contain the historical returns from the other allocation strategies and pipe that into ggplot."
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2.html#case-outline",
    "href": "quant-fin/datacases/data-case-2/data-case-2.html#case-outline",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Hedge funds operate in a highly competitive environment where their ability to generate alpha—returns beyond market factors—is constantly scrutinized. Many funds claim to deliver superior returns by exploiting inefficiencies, but how much of their performance is just exposure to well-known risk factors?\nAs part of the Strategic Investments Division at Sentinel Capital, a multi-billion-dollar Fund of Funds (FoF), your team has been tasked with evaluating the performance of eight hedge funds over the last decade. The ultimate goal? Constructing an optimal portfolio of hedge funds that maximizes returns while managing risk.\nTo do this, you’ll leverage R, tidyverse, tidyquant, and PortfolioAnalytics by applying factor models, portfolio optimization techniques, and rolling performance analysis. Your key tool for evaluation will be the Fama-French Five-Factor Model, which decomposes returns into market, size, value, profitability, and investment factors.\nYour dataset contains monthly returns from eight distinct hedge funds, each with a distinct allocation rationale:\n\nAlphaSynthesis Capital – A long/short equity fund with an emphasis on momentum-driven strategies.\nArbVantage Partners – A statistical arbitrage fund specializing in mean-reversion strategies.\nSentinel Macro Strategies – A global macro fund that trades based on macroeconomic indicators.\nQuantum Volatility Fund – A volatility arbitrage fund exploiting implied vs. realized volatility.\nHorizon Event-Driven – A fund that profits from corporate events such as mergers, spin-offs, and earnings surprises.\nDeepValue Asset Management – A deep-value fund investing in undervalued securities based on fundamental analysis.\nNova Growth Strategies – A growth-oriented hedge fund focusing on technology and high-beta stocks.\nVega Capital Neutral – A market-neutral strategy that seeks absolute returns with low beta exposure.\n\nYour task is to analyze these funds, compare them against the Fama-French three-factor model, and construct optimal hedge fund portfolios.\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®."
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2.html#tech-setup",
    "href": "quant-fin/datacases/data-case-2/data-case-2.html#tech-setup",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\",\"tidyquant\",\"tidymodels\",\"xts\", \"glue\",\"scales\",\"PortfolioAnalytics\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('tidyquant')\n  install.packages('glue')\n  install.packages('scales')\n  install.packages('PortfolioAnalytics')\n\n#Load\n  library(tidyverse)\n  library(tidyquant)\n  library(tidymodels)\n  library(glue)\n  library(scales)\n  library(PortfolioAnalytics)\n\n\n\n\n\n\n\nCallout\n\n\n\nDepending upon your R version that is currently installed, you may or may not have the right set of optimization routines properly set up in your session. To make sure that you can use the portfolio optimization functions from PortfolioAnalytics package, install the DEoptim (see details here) that is used on the backend.\n\ninstall.packages('DEoptim')"
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-1",
    "href": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-1",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Download the hf_data.RDS file on eClass®. This dataset covers the period from January 2010, to December 2024 and shows the monthly returns for each hedge fund. Using the functions from the tidyverse and tidyquant packages, visualize the historical returns of all hedge funds. What trends or anomalies do you observe? Are there periods of exceptional gains or drawdowns? Which fund has shown to be the best performer during the study period?\n\n\n\n\n\n\nHint\n\n\n\n\nBecause hf_data is an data.frame object, you can pass the as.xts to convert it to xts and then call cumprod() to generate the cumulative returns for all assets. Do not forget to use the cumprod(1+x)-1 syntax to make sure that you are getting the correct compounded returns over time!\nNow, your resulting output can be converted back to a data.frame using as.data.frame() and getting the date column back using rownames_to_column('date'). To go from wide to long format, you can use the pivot_longer function, keeping only one column for the returns and another column that refers to each strategy at a given date.\nYou can pipe that into a ggplot call, mapping the date to the x-axis, the cumulative return column to the y-axis, and the geom_line() function to create a bar chart, using the group and color to color each fund differently. Add as many customizations you think are worth the effort."
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-2",
    "href": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-2",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Download the ff5_data.RDS file on eClass®. This file contains the historical monthly returns for the Fama-French 5 factor model for the U.S. stock market. Merge this data with your hedge fund dataset, hf_data, and run an Ordinary Least Squares (OLS) regression for each fund against the Fama-French factors using the following specification:\n\\[\n\\small E[R_{i,t}] = \\alpha + \\beta_s^M \\times \\underbrace{(E[R_m]− R_f)}_{\\text{Market}}  + \\beta_s^{SMB} \\times \\underbrace{E[R_{SMB}]}_{\\text{Size}} + \\beta_s^{HML} \\times \\underbrace{E[R_{HML}]}_{\\text{Book-to-Market.}} + \\beta_s^{RMW} \\times \\underbrace{E[R_{RMW}]}_{\\text{Profitability}} + \\beta_s^{CMA} \\times \\underbrace{E[R_{CMA}]}_{\\text{Investment}} + \\varepsilon_{i,t}\n\\]\nBased on the results you found, which funds exhibit significant (at 5% confidence levels) \\(\\alpha\\) after controlling for factor exposures? Store the names of the these funds in an object called positive_alpha_funds.\n\n\n\n\n\n\nHint\n\n\n\n\nBecause hf_data is in wide format, you can use the pivot_longer to transform it to a tidy format - assign it to another object (e.g, hf_returns) for later use.\nAfter that, merge this newly created that to the ff5_data object using the left_join function, and use mutate to create the excess returns for each fund by subtracting the risk-free column, assigning it to a new object (e.g, merged_df)\nUse the functional programming techniques from the purrr package (or do a for-loop) to map the lm and tidy functions to each strategy subset, and collect the results from \\(\\alpha\\) (the Intercept of the regression).\nCollect the results in an object and use ggplot to chart your results."
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-4",
    "href": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-4",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Using the results from our previous OLS regressions, explain how exposed each fund was to each of the factor portfolios contained in the Fama-French 5 Factor model. How can this help you explain the over(under)performance of the top(worst) funds?\n\n\n\n\n\n\nHint\n\n\n\n\nUsing the estimates from the estimation, collect all the \\(\\beta\\)s from the regressions by filtering out the Intercept using the filter() function\nUse ggplot to chart the weights (factor betas) for each strategy. To display all strategies in one chart, you can map the factors to the x axis, the factor betas values to the y axis, use facet_wrap to facet your chart by each different fund."
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-4-1",
    "href": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-4-1",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Way to go! Your boss was really impressed with how you were able to dissect the returns from individual hedge funds and understand which factors drove performance, and in which cases we were able to find positive returns that were uncorrelated with any of the risk factors. Now, it is time to start building portfolios by allocating money across each fund using the Markowitz Mean-Variance optimization rationale.\nTo do that, start by filtering your sample to consider only the hedge funds that were able to generate \\(\\alpha\\) throughout the whole period. Divide your sample using a \\(50\\%/50\\%\\) split, using information from \\(2010\\) until \\(2018\\) as a training set, and \\(2019\\) to \\(2024\\) as a test set. Construct a Minimum-Variance Portfolio of the following format:\n\\[\n\\min_{\\{w_1,w_2...,w_n\\}} \\sigma^2_p = w^T \\Sigma w,\n\\text{ such that:}\n\\\\\n\\begin{cases}\n\\sum_{i=1}^n w_i=1 \\text{ (1)}\\\\\n0.05\\leq w_i \\leq 0.50, \\forall i \\text{ (2)}\n\\end{cases}\n\\]\nIn other words, you are solving the following problem: find the set of allocation weights \\(w_1,w_2,w_3,...,w_n\\) (the % that you allocate in each of the strategies) that, when used to create a portfolio, are the ones that create the portfolio with the minimum variance among all possible combinations. In order to do that, you have to ensure that the weights add up to \\(100\\%\\) (so you’re fully investing your capital), which is the first condition. The second conditions states that neither stock can have a weight that is lower than \\(5\\%\\) nor have a weight that is greater than \\(50\\%\\).\nNote that \\(w^T\\Sigma w\\) is nothing more than the matrix form of \\(\\sum_{i=1}^{N}w_i\\sigma_i^2+ 2\\sum_{i=1}^{N}\\sum_{j\\neq i}w_i w_j\\sigma_{i,j}\\), which is the variance of a portfolio that consists of \\(N\\) assets.\nUsing the training sample and only the funds that have shown \\(\\alpha&gt;0\\) (regardless of the statistical significance), what is the optimal allocation? Store this into an object called min_var_allocation.\n\n\n\n\n\n\nHint\n\n\n\n\nFilter the hf_data to contain only the date and the columns that refer to the strategies with positive \\(\\alpha\\).\nUse the as.xts to transform the object from a data.frame to an xts object. This is crucial, as the functions from the PortfolioAnalytics operate on xts objects. With your newly created xts object, create two new objects, training_sample and testing_sample using the xts subsetting syntax:\n\n\ntraining_sample=(your_filtered_hf_data%&gt;%as.xts())['2010/2018']\ntest_sample=(your_filtered_hf_data%&gt;%as.xts())['2019/2024']\n\n\nUse the portfolio.spec and the optimize.portfolio functions from the PortfolioAnalytics package to define and optimize the portfolio using the syntax we have learned during class. If you are unsure on what to do here, please refer to the workbook.R file that we have gone through for a similar replication.\nFinally, use the extractWeights function to output a table with the optimal weights."
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-5",
    "href": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-5",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Using the training sample and only the funds that have shown \\(\\alpha&gt;0\\), develop a portfolio that maximizes returns instead of minimizing risk. What is the optimal allocation? Store this into an object called max_return_allocation."
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-6",
    "href": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-6",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Using the training sample and only the funds that have shown \\(\\alpha&gt;0\\), develop a portfolio that maximizes the Sharpe-Ratio - i.e, maximizes the risk-adjusted returns. What is the optimal allocation? Store this into an object called max_sharpe_allocation."
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-7",
    "href": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-7",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Compare the optimal weights across each strategy and discuss the allocations. Is there any better allocation rule?\n\n\n\n\n\n\nHint\n\n\n\n\nAfter collecting your optimal weights using the extractWeights function and storing them into separate objects, use the rbind function to bind them together in a row-wise manner and transform it to a data.frame using the as.data.frame() function.\nAdd a new column that describes which strategy is contained in each row.\nUse the pivot_longer to transform the data to a tidy format, making it easier for you to pipe the output into a ggplot call.\n\nFor these steps, you can use the following syntax:\n\nrbind(extractWeights(min_var_allocation),\n      extractWeights(max_return_allocation),\n      extractWeights(max_sharpe_allocation))%&gt;%\n  as.data.frame()%&gt;%\n  mutate(objective=c('Minimize Variance','Maximize Returns','Maximize Sharpe'))%&gt;%\n  pivot_longer(1:4,names_to = 'fund',values_to = 'weights')"
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-8",
    "href": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-8",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Compare the historical performance from each fund in the testing sample. To do that, use the three sets of optimal weights you have just created (min_var_allocation, max_return_allocation, and max_sharpe_allocation) along with the tq_portfolio function to estimate the returns in the test sample, rebalancing it monthly. Which strategy did deliver the highest out-of-sample returns? Did they outperform a naive portfolio using equal weights to all funds?\n\n\n\n\n\n\nHint\n\n\n\n\nUse the hf_returns data you have created in the beginning of the exercise (i.e, the dataset of hedge fund returns in long format) and apply the filter function to keep only the rows where i) the fund is in the list of funds with positive alpha (i.e, it is in the positive_alpha_funds you have created before) and year(date)&gt;2018 (i.e, belongs to testing sample).\nUse the tq_portfolio function using the following syntax to calculate the historical returns of the portfolio:\n\n\n  tq_portfolio(\n    assets_col = fund, #This is the column that contains the fund names\n    returns_col = monthly_return, #Column that contains the monthly returns\n    weights = extractWeights(min_var_allocation), #The portfolio.spec you have created\n    col_rename = \"returns_min_var\",\n    rebalance_on = \"months\"\n  )\n\n\nDo this for all three portfolio specifications, assigning their results to separate objects. For the naive portfolio, you can assign a vector of equal weights using the rep(weight,number_of_replications) syntax."
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-9",
    "href": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-9",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Using the max_return_portfolio object and the test_sample data cut, implement a rolling-window optimization to find the optimal portfolio weights. Use a training period and a rolling window of \\(12\\) months, and rebalance the portfolio monthly. How do these dynamically adjusted weights perform compared to static allocations?\n\n\n\n\n\n\nHint\n\n\n\n\nUsing the test_sample xts object you have created, apply the optimize.portfolio.rebalancing function, pointing to the max_return_portfolio and the aforementioned parameters for the training period, rolling window, and rebalancing period. You can use the following syntax:\n\n\n#Rolling optimization\nrolling_optimization &lt;- test_sample%&gt;%\n  optimize.portfolio.rebalancing(\n    portfolio = max_return_portfolio,\n    rebalance_on = \"months\",\n    training_period = 12,\n    rolling_window = 12\n  )\n\n\nAfter you have optimized your portfolio, call extractWeights and pipe that into ggplot to chart your results."
  },
  {
    "objectID": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-10",
    "href": "quant-fin/datacases/data-case-2/data-case-2.html#exercise-10",
    "title": "Data Case II - Factor Harvesting and Portfolio Optimization",
    "section": "",
    "text": "Looking only at the test sample period (i.e, from \\(2019\\) to \\(2024\\), chart the historical performance of the dynamically-optimized portfolio vis-a-vis the static portfolios you have just created. Are there any gains stemming from dynamically optimizing the portfolio?\n\n\n\n\n\n\nHint\n\n\n\n\nUse the extractWeights() function on the dynamically optimized portfolio optimization object to collect a time series of weight allocations.\nWith that, use as_data_frame() and rownames_to_column(\"date\") to transform it to a data.frame with a date column identifier.\nFinally, transform it to long format using the pivot_longer() function.\nDo a similar procedure for the xts object where you stored the test sample returns (in my example, test_sample)\nUse left_join to merge both datasets. You can use a combination of mutate() and summarize() to calculate the portfolio returns on each date in a step-wise manner:\n\n\ndynamic_performance=left_join(weights,returns)%&gt;% # Your weights and returns data.frames\n  mutate(ind_return=returns*weights)%&gt;% # Calculate individual returns for each fund on each date\n  group_by(date)%&gt;% # Group by date\n  summarize(returns_dynamic=sum(ind_return,na.rm=TRUE)) # Summing up yields a weighted average of returns by each date\n\n\nJoin this data.frame with the other objects that contain the historical returns from the other allocation strategies and pipe that into ggplot."
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Evaluating Performance: Statistical Tests",
    "text": "Evaluating Performance: Statistical Tests\n\nIn our previous discussions, we saw that \\(R^2\\), which was our measure of overall goodness-of-fit for linear regression models, really does not convey any relevant information for models like Logit\nFurthermore, the usual t-tests, used to test hypotheses around \\(\\hat{\\beta}\\), is not applicable here\n\nLogistic regression assumes errors follow the logistic distribution\nConsequently, the term \\(\\dfrac{(\\hat{\\beta}-\\beta_0)}{se(\\hat{\\beta})}\\) does not follow a t-distribution\n\nHow can we test make hypothesis around Logit models and assess overall accuracy?\nWe’ll make use of the fact that Logit models are estimated using a likelihood function, \\(\\mathcal{L}\\), in order to derive some important evaluation metrics"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Evaluating Performance: Statistical Tests (continued)",
    "text": "Evaluating Performance: Statistical Tests (continued)\n\nA model like logit is estimated using a maximum likelihood method. The likelihood of churning for a given individual \\(i\\), with observations \\((y_i,x_i)\\) can be written as\n\n\\[\n\\mathcal{L_i}(\\beta,y_i,x_i)=[\\Lambda(x_i\\beta)^{y_i}]\\times[1-\\Lambda(x_i\\beta)]^{1-y_i}\n\\]\nBecause we are assuming that all churn observations (i.e, customer decisions) are i.i.d, then the likelihood of the entire sample is just the product of the individual likelihoods:\n\\[\n\\mathcal{L}(\\beta,Y,X)=\\prod_{i=1}^{N}[\\Lambda(x_i\\beta)^{y_i}]\\times[1-\\Lambda(x_i\\beta)]^{1-y_i}\n\\]\n\nIn words, a maximum-likelihood estimator is trying to find the coefficients \\(\\beta\\) that make the sample of churn observations \\((y_1,y_2,...,y_n)=Y\\) more likely to occur!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued-1",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued-1",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Evaluating Performance: Statistical Tests (continued)",
    "text": "Evaluating Performance: Statistical Tests (continued)\n\nWhat happens on the back-end is that a maximum likelihood estimator will try to find the set of parameters \\(\\beta\\) that maximize the log-likelihood of the model, noting that the \\(\\log\\) is a monotonic function\nAt the end of the day, when we want to compare models, we can think about which model has the highest log-likelihood\nThere are three common tests that can be used to test this type of question: the Likelihood ratio (LR) test, the Wald test, and the Lagrange multiplier test (sometimes called a score test)\nThese tests are sometimes described as tests for differences among nested models, because one of the models can be said to be nested within the other:\n\nThe null hypothesis for all three tests is that the “smaller”, or “restricted” model, is the “true” model\nOn the other hand, a large test statistics indicate that the null hypothesis is false"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued-2",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued-2",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Evaluating Performance: Statistical Tests (continued)",
    "text": "Evaluating Performance: Statistical Tests (continued)\n\nRecall that our likelihood function is defined as:\n\n\\[\n\\small \\mathcal{L}(\\beta,Y,X)=\\prod_{i=1}^{N}[\\Lambda(x_i\\beta)^{y_i}]\\times[1-\\Lambda(x_i\\beta)]^{1-y_i}\n\\]\n\nTaking logs, the log-likelihood is given by:\n\n\\[\n\\small \\ell(\\beta) = \\log \\mathcal{L}(\\beta) = \\sum_{i=1}^{n} y_i \\log \\Lambda(x_i\\beta) + (1 - y_i) \\log (1 - \\Lambda(x_i\\beta))\n\\]\n\nOn the one hand, values of \\(\\beta\\) that are closer to the “true” relationship between the matrix \\(\\Lambda(X)\\) of covariates and \\(Y\\) increase the log-likelihood\nOn the other hand, values of \\(\\beta\\) that are unlikely to describe the sample relationship between \\(\\Lambda(X)\\) and \\(Y\\) decrease the log-likelihood"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued-3",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued-3",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Evaluating Performance: Statistical Tests (continued)",
    "text": "Evaluating Performance: Statistical Tests (continued)\n\n\n\n\n\n\nTo the purposes of our analysis, a constrained case, as a baseline, can be the case where the \\(Y\\) has no predictors (i.e, all \\(\\beta\\)’s are equal to zero)"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#statistical-tests-in-practice---wald-test",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#statistical-tests-in-practice---wald-test",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Statistical Tests in practice - Wald Test",
    "text": "Statistical Tests in practice - Wald Test\n\nResultPython\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.435323\n         Iterations 6\n\n\n\n\n\n\n\n\nSummary Statistics\n\n\n\nwald_stat\np_values\n\n\n\n\nconst\n−14.40\n0.00\n\n\ncredit_score\n−2.35\n0.02\n\n\ngender\n−10.06\n0.00\n\n\ntenure\n−1.59\n0.11\n\n\nbalance\n10.96\n0.00\n\n\nproducts_number\n−0.78\n0.44\n\n\ncredit_card\n−0.49\n0.62\n\n\nactive_member\n−18.86\n0.00\n\n\nestimated_salary\n1.06\n0.29\n\n\nage\n28.56\n0.00\n\n\n\n\n\n\n        \n\n\n\n\n\n# Load data\ndata = pd.read_csv('Assets/bank-dataset.csv')\ndata['gender'] = np.where(data['gender'] == 'Male', 1, 0)\n\n# Fit logistic regression model\nX = data[['credit_score', 'gender', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary','age']]\nX = sm.add_constant(X)  # Add constant term for intercept\ny = data['churn']\n\n# Fit logistic regression model\nreg = sm.Logit(y, X).fit()\n\n# Wald test for each coefficient\nwald_stat = reg.params / reg.bse\n\nstatistics = pd.DataFrame({\n  'parameter': X.columns,\n  'wald_stat': wald_stat,\n  'p_values': 1 - chi2.cdf(wald_stat**2, 1)\n})\n\nTable = (\n  GT(statistics)\n  .cols_align('center')\n  .tab_header(title=md(\"**Summary Statistics**\"))\n  .tab_stub('parameter')\n  .fmt_number(decimals = 2)\n  .opt_stylize(style=1,color='red')\n)\n\n#Output\nTable.show()\n\n#Save predictions and export it for later use\npredict=pd.DataFrame({'true_y': y,'predicted_y': reg.predict()})\npredict.to_csv('predicted.csv')"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#statistical-tests-in-practice---likelihood-ratio-test",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#statistical-tests-in-practice---likelihood-ratio-test",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Statistical Tests in practice - Likelihood-Ratio Test",
    "text": "Statistical Tests in practice - Likelihood-Ratio Test\n\nResultPython\n\n\n\n\nLikelihood Ratio Test:\n\n\nChi-square: 1403.0\n\n\nDegrees of freedom: 9.0\n\n\nP-value: 0.0\n\n\n\n\n\n# Perform likelihood ratio test (LRT)\nlr_test = reg.llr\ndf = reg.df_model\np_value = reg.llr_pvalue\n\nprint(\"Likelihood Ratio Test:\")\nprint(\"Chi-square:\", lr_test.round(0))\nprint(\"Degrees of freedom:\", df)\nprint(\"P-value:\", p_value.round(2))"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-pseudo-r2",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-pseudo-r2",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Evaluating Performance: Pseudo-\\(R^2\\)",
    "text": "Evaluating Performance: Pseudo-\\(R^2\\)\n\nExplanationResultPython\n\n\n\n\nMcFadden’s pseudo-\\(R^2\\) is an alternative metric for assessing a model’s performance that also takes into account the use of the log-likelihood function:\n\n\\[\n\\text{pseudo-}R^2=1-(\\mathcal{LL}_{FullModel}/\\mathcal{LL}_{\\beta=0})\n\\]\n\nIf your model doesn’t really predict the outcome better than the null model, and the statistic will be close to zero\nConversely, if the difference in log-likelihood is large, your test will approach 1\n\n\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.505489\n         Iterations 5\n\n\nMcFadden's Pseudo R-squared: 0.14\n\n\n\n\n\n# Calculate McFadden's pseudo R-squared\ndef pseudoR2(model):\n    L1 = model.llf\n    L0 = sm.Logit(y, sm.add_constant(np.ones_like(y))).fit().llf\n    return 1 - (L1 / L0)\n\npseudo_r2 = pseudoR2(reg)\nprint(\"McFadden's Pseudo R-squared:\", pseudo_r2.round(2))"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-accuracy",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-accuracy",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Evaluating Performance: Accuracy",
    "text": "Evaluating Performance: Accuracy\n\nHow the estimated results compare to actual choices from customers?\n\nOn the one hand, Logit predicted values relate to estimated probabilities\nOn the other hand, actual information on churn is binary\n\nFrom a practical perspective, one needs to map the estimated probabilities onto a categorization:\n\n\\[\n\\hat{Y}=\n\\begin{cases}\n1 \\text{, if } p&gt;p^\\star\\\\\n0 \\text{, if } p\\leq p^\\star\n\\end{cases}\n\\]\n\nBut how do we pick \\(p^\\star\\)?"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#introducing-the-confusion-matrix",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#introducing-the-confusion-matrix",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Introducing the Confusion Matrix",
    "text": "Introducing the Confusion Matrix\n\nA way to assess the choice of \\(p^\\star\\) is to analyze the confusion matrix:\n\nIt shows how much predictions were correct by each categorization: true positives and true negatives\nIt also shows how much predictions were incorrect by each categorization: false positives and false negatives\n\nIf we agnostically set \\(p^\\star=0.2037\\), which is the sample average of churn, our example would yield the following terms for us:\n\n\nThe number of actual churned customers that were ex-ante classified as churned\nThe number of actual churned customers that were ex-ante classified as not churned\nThe number of actual not churned customers that were ex-ante classified as churned\nThe number of actual not churned customers that were ex-ante classified as not churned"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#confusion-matrix",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#confusion-matrix",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\n\n\n\n\nGreen cells indicate the True Positive and True Negative cases\nRed cells indicate the False Positive and False Negative cases\n\n\nType I Errors (\\(153\\)) are the False Positive cases: we wrongly classified customers that did not churn as churned!\nType II Errors (\\(1788\\)) are the False Negative cases: we wrongly classified customers that did churn as not churned!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#how-should-a-good-estimator-look-like-looking-at-accuracy",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#how-should-a-good-estimator-look-like-looking-at-accuracy",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "How should a good estimator look like? Looking at Accuracy",
    "text": "How should a good estimator look like? Looking at Accuracy\n\nOverall, a good estimator should minimize the combination of Type I and Type II errors. In other words, we want our estimator to have the highest Accuracy as possible:\n\n\\[\n\\small ACC=\\dfrac{TP+TN}{TP+FP+TN+FN}=\\dfrac{1,387+5,486}{10,000}\\approx 69\\% \\text{ correct predictions}\n\\]\n\nNotwithstanding, there might be cases where the costs attributed to Type I and Type II errors are fairly different!\n\nFor example, sharing a discount coupon as a way to avoid losing a customer that was predicted to churn (Type I) while, in reality, it would not churn even without the coupon, has a much less significant cost\nNotwithstanding, losing a customer because we did not identify that he could churn (Type II) has a significant cost to the business"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#sensitivity",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#sensitivity",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Sensitivity",
    "text": "Sensitivity\n\nThe first question that will likely pop up during internal conversations is: how much of the churned customers we were able to correctly identify?\nThis question is of special interest in churn analysis, as the goal is to target these customers before they actually have their final decision!\nThe Sensivity (or the True Positive Rate) calculates the proportion of correctly identified churned customers by comparing the number of true positives with the total number of positives:\n\n\\[\nSensitivity=\\dfrac{TP}{TP+FN}=\\dfrac{1,387}{(1,387+650)}\\approx 68\\%\n\\]\n\nOverall, it seems that our model goes a decent job in identifying \\(68\\%\\) of the actual churned customers ahead of time!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#precision",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#precision",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Precision",
    "text": "Precision\n\nThe naivest way of identifying all churned customers is to set \\(p^\\star=0\\). In other words, if we classify all customers as churned, then, for sure, we’ll get all churned customers right!\nNotwithstanding, we are wrongly classifying some customers that would not churn in the future (false negatives). In practice, if we were to give coupons to every churn customer in potential, this action would cost us much more as we’re wasting money on customers that wouldn’t churn!\nThe Precision calculates the how precise our churn classification was by comparing the number of true positives with the total number of predicted positives:\n\n\\[\nPrecision=\\dfrac{TP}{TP+FP}=\\dfrac{1,387}{(1,387+2,477)}\\approx 35\\%\n\\]\n\nAlthough we hit a high number of churned customers, we have wrongly classified \\(1-35\\%=65\\%\\) of customers as potential churners!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#specificity",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#specificity",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Specificity",
    "text": "Specificity\n\nThe analog of the first question relates to how much of the non-churned customers we were able to correctly identify\nKnowing how much non-churned customers our model predicts shed light on how much we’re able to understand about customers that do not churn!\nThe Specificity (or True Negative Rate) calculates the proportion of correctly identified non-churned customers by comparing the number of true negatives with the total number of negatives:\n\n\\[\nSpecificity=\\dfrac{TN}{TN+FP}=\\dfrac{5,486}{(5,486+2,477)}\\approx 69\\%\n\\]"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#negative-predicted-value",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#negative-predicted-value",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Negative Predicted Value",
    "text": "Negative Predicted Value\n\nThe last piece that is still left to analyze is the precision of our estimates for non-churned classifications\nIn other words, out of all the non-churn predicted customers, how much were actually false negatives?\nThe Negative Predicted Value calculates the proportion of correctly identified non-churned customers by comparing the number of true negatives with the predicted negatives:\n\n\\[\nNPV=\\dfrac{TN}{TN+FN}=\\dfrac{5,486}{(5,486+650)}\\approx 89.37\\%\n\\]\n\nOut of all negative classifications, \\(81.3\\%\\) of them were correct!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#confusion-matrix-implementation",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#confusion-matrix-implementation",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Confusion Matrix Implementation",
    "text": "Confusion Matrix Implementation\n\nResultPython\n\n\n\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x00000219CD64FA10&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n# Predict churn using the trained logistic regression model\npredicted_churn = (\n  (reg.predict() &gt;= y.mean())  # Assuming a threshold of 0.5 for classification\n  .astype(int)\n  )\n\n#Estimate the Confusion Matrix\nCM=confusion_matrix(y,predicted_churn)\n\n#Chart \nConfusionMatrixDisplay.from_predictions(y, predicted_churn)\n\n#Title\nplt.title('Confusion Matrix')\n\n#Show\nplt.show()"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#putting-all-together",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#putting-all-together",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Putting all together",
    "text": "Putting all together\n\nAs we increase our threshold, \\(p^\\star\\), we minimize Type II error (i.e, we identify the churned customers) as we’re identifying the true positives\nAt the same time, however, we are increasing the Type I error, since there is going to be a higher number of false positives!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#finding-the-optimal-pstar",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#finding-the-optimal-pstar",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Finding the optimal \\(p^{\\star}\\)",
    "text": "Finding the optimal \\(p^{\\star}\\)\n\nExplanationResultPython\n\n\n\n\nIf, for example, we want to find \\(p^{\\star}\\) such that it maximizes the sum of specificity + sensitivity, we can:\n\n\nRedo our confusion matrix for each \\(p^{\\star}=\\{0,0.01,0.02,...,0.99,1\\}\\)\nCalculate the specificity and the sensitivity\nPick the threshold that maximizes the sum of both metrics\n\n\nNote that, depending upon the problem, we might want to use a different criterion to find the optimal \\(p^\\star\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Predict churn probabilities\nprobs = reg.predict()\n\n# Calculate specificity and sensitivity\nfpr, tpr, thresholds = roc_curve(y, probs)\nspecificity = 1 - fpr\nsensitivity = tpr\n\n# Find the best threshold value\nbest_threshold_index = np.argmax(specificity + sensitivity)\nbest_threshold = thresholds[best_threshold_index]\n\n#Create a pandas data.frame and plot it\n\nresult=pd.DataFrame({\n  'thresholds':thresholds,\n  'sp_se': specificity + sensitivity\n})\n\nPlot=(\n  ggplot(result, aes(x='thresholds',y='sp_se'))+\n  geom_point(size=0.5,color='darkorange')+\n  theme_minimal()+\n  geom_vline(xintercept=best_threshold,linetype='dashed')+\n  labs(x='Threshold',\n       y='Specificifity + Sensitivity',\n       title='Best value achieved around 0.22')+\n  theme(plot_title = element_text(size=15,face='bold'),\n        axis_text = element_text(size=12),\n        axis_title = element_text(size=12))\n  )\n\nPlot.show()"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#comparing-among-different-classifiers",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#comparing-among-different-classifiers",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Comparing among different classifiers",
    "text": "Comparing among different classifiers\n\nSay that, for some reason, you have ommitted age from the analysis. How can you assess how the predictive power of your model is going to deteriorate?\nOne way to do this is to analyze what we call Area under the Curve (or simply AUC):"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#auc-under-different-models",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#auc-under-different-models",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "AUC under different models",
    "text": "AUC under different models\n\nExplanationResultPython\n\n\n\n\nA Receiver Operating Characteristic curve (or simply ROC curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n\n\nThe True Positive Rate (or TPR)\nThe False Positive Rate (or FPR)\n\n\nThe AUC provides an aggregate measure of performance across all possible classification thresholds\nHowever, such metric is not very applicable metric whenever the costs of Type I and Type II errors are very different\n\n\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.479824\n         Iterations 6\n\n\nOptimization terminated successfully.\n         Current function value: 0.435323\n         Iterations 6\n\n\n\n\n\n\n\n\n\n\n\n\n# Define independent and dependent variables for Reg_1 (without 'age')\nX1 = X.drop('age',axis=1)\n\n# Define independent and dependent variables for Reg_2 (with 'age')\nX2 = X\n\n# Dependent variable\ny = data['churn']\n\n\n# Fit logistic regression models\nreg_1 = model = sm.Logit(y, X1.astype(float)).fit()\nreg_2 = model = sm.Logit(y, X2.astype(float)).fit()\n\n# Predict probabilities for both models\nprobs_1 = reg_1.predict()\nprobs_2 = reg_2.predict()\n\n# Compute ROC curves\nfpr_1, tpr_1, _ = roc_curve(y, probs_1)\nfpr_2, tpr_2, _ = roc_curve(y, probs_2)\n\n# Compute AUC scores\nauc_1 = roc_auc_score(y, probs_1)\nauc_2 = roc_auc_score(y, probs_2)\n\n# Plot ROC curves\nData=pd.concat([pd.DataFrame({'Model':'Without Age','FPR':fpr_1,'TPR':tpr_1}),\n          pd.DataFrame({'Model':'With Age','FPR':fpr_2,'TPR':tpr_2})],axis=0)\n\nPlot = (\n  \n  ggplot(Data,aes(x='FPR',y='TPR',fill='Model'))+\n  geom_point(stroke=0)+\n  theme_minimal()+\n  annotate(geom='text',x=0.5,y=0.5,label=('Model 1 (without Age): ' + auc_1.round(2).astype('str')))+\n  annotate(geom='text',x=0.4,y=0.9,label=('Model 2 (with Age): ' + auc_2.round(2).astype('str')))+\n  labs(x='FPR (1 - Specificity)',\n       y='TPR (Sensitivity)',\n       title='Model with Age outperforms the other for any threshold!')+\n  theme(plot_title = element_text(size=12,face='bold'),\n        axis_text = element_text(size=10),\n        axis_title = element_text(size=12),\n        legend_position ='bottom')\n)\n\nPlot.show()"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#other-topics-for-further-investigation",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#other-topics-for-further-investigation",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Other topics for further investigation",
    "text": "Other topics for further investigation\n\nPotential topics that you may want to dive in when looking at binary choice models:\n\nCompare different models in terms of predictive power, statistics etc, such as Probit, Random Forests, Suppor Vector Machines\nThink about other metrics to optimize your classificaiton results based on the question that you’re aiming to solve!\n\nBridging Econometrics with Machine Learning:\n\nUsing train/test splits and balanced samples\nUsing cross-validation folds\nComparing the classification performance across different [models specifications]^[For R, refer to the caret package. For Python, refer to scikit-learn\n\nAll in all, there is a growing literature on the role of machine learning methods for supervised learning applied to classification contexts1\n\nSee Machine Learning Methods That Economists Should Know About (Susan Atey) - click here to access"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#references",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#references",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#make-sure-to-load-all-necessary-packages",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#make-sure-to-load-all-necessary-packages",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Make sure to load all necessary packages",
    "text": "Make sure to load all necessary packages\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom matplotlib import pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\nfrom plotnine import *\nfrom great_tables import GT, md\nfrom mizani.formatters import percent_format\nfrom scipy.stats import chi2\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#thinking-about-consumer-choices",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#thinking-about-consumer-choices",
    "title": "Market Basket Analysis",
    "section": "Thinking about consumer choices",
    "text": "Thinking about consumer choices\n\nMany firms compile records of customer transactions. These records are very valuable to marketers and inform us about customers’ purchasing patterns:\n\n\nWhat are the ways in which we might optimize pricing or inventory given the purchase patterns?\nWhich relationships between the purchases and other customer information are more prevalent?\n\n\nSuch records may comprise an enormous number of data points yet with relatively little information in each single observation\nOur last lecture will examine a strategy to extract insight from transactions and co-occurrence data: association rule mining"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#thinking-about-consumer-choices-1",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#thinking-about-consumer-choices-1",
    "title": "Market Basket Analysis",
    "section": "Thinking about consumer choices",
    "text": "Thinking about consumer choices\n\nIntuition: when events occur together more often than one would expect from their individual rates of occurrence, such co-occurrence is an interesting pattern\n\nPeople tend to buy beer and olives together\nPasta consumption tends to be positively related to spices and sauces\n\nIt is all about the conditional expectation!\n\nSuppose that sausages represent 5% of the transaction\nAlso, ketchup brands represent 3% of the transactions\n\n\n\nIf the proportion of the hot-dog sales that also have ketchup is \\(\\small3\\%\\), then there is no significant relationship because this is what we would expect from the overall data\nIf, on the other hand, the proportion of ketch-up sales is \\(\\small25\\%\\) given a hot-dog sale, this indicates that the conditional probability given a hot-dog transaction is relevant!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#methodological-terms",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#methodological-terms",
    "title": "Market Basket Analysis",
    "section": "Methodological terms",
    "text": "Methodological terms\nOne way we can define these relationships is to characterize each situation:\n\nAn association is simply the co-occurrence of two or more things. Example: sausages and ketchup tend to be in the transaction\nA transaction is a set of items that co-occur in an observation. In marketing, a common transaction is the market basket: \\(\\{\\text{sausages},\\text{sauces},\\text{mustard},\\text{beer}\\}\\)\nA rule expresses the incidence across transactions of one set of items as a condition of another set of items. A condition in this sense does not imply a causal relationship, only an association of some strength, whether strong or weak\n\n\nTranslating our example using these terms, if we identify that ketch-up is generally bought along with sausages and sauces, then we have the following rule:\n\n\\[\n\\small\n\\{\\text{sausage},\\text{mustard}\\}\\implies\\{\\text{ketchup}\\}\n\\]"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#metrics",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#metrics",
    "title": "Market Basket Analysis",
    "section": "Metrics",
    "text": "Metrics\n\nAssociation rules are expressed with a few common metrics that reflect the rules of conditional probability:\n\n\nThe support is the proportion of all transactions that contain the set. If the set \\(\\small \\{\\text{sausage},\\text{ketchup}\\}\\) appears in 10 out of 200 transactions, the support is \\(\\small 0.05\\). What matters is that these items were in 10 transactions together - other items may be included.\nThe confidence is the support for the co-occurrence of all items in a rule, conditional on the support for the left hand set alone:\n\n\\[\n\\small\n\\text{Confidence}(\\text{sausage}\\implies \\text{ketchup})=\\dfrac{\\#(\\text{sausage } \\cap \\text{ketchup})}{\\#\\text{sausage}}\n\\]\n\nFor example, if sausage appears in \\(\\small200\\) transactions overall, and \\(\\small50\\) of these also contain ketchup, the confidence is \\(\\small 50/200 = 25\\%\\). In words, ketchup appears \\(\\small25\\%\\) of the time that sausage appears"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#metrics-1",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#metrics-1",
    "title": "Market Basket Analysis",
    "section": "Metrics",
    "text": "Metrics\n\nNote that Confidence in this context carries no implication about hypothesis testing, confidence intervals, or the like; it is only a measure of conditional association. Likewise, Confidence is not symmetric. If \\(\\small Support(\\text{\\{Sausage\\}})\\neq Support(\\text{\\{Ketchup\\}})\\):\n\n\\[\nConfidence(\\text{sausage}\\implies\\text{ketchup})\\neq Confidence(\\text{ketchup}\\implies\\text{sauage})\n\\]\n\nHow we can use these concepts to create “success” metrics for some associations? One of the most common ways is to define an increase (or “lift”):\n\n\\[\nlift(\\text{sausage}\\implies\\text{ketchup})=\\dfrac{support(\\text{sausage }\\cap \\text{ketchup})}{support(\\text{sausage })\\times support(\\text{ketchup})}\n\\]"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#understanding-lift",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#understanding-lift",
    "title": "Market Basket Analysis",
    "section": "Understanding lift",
    "text": "Understanding lift\n\nLift measures how the joint probability of the events compares to the case where the events are independent. If the individual appearances from sausage and ketchup are 75 and 80, respectively, we have:\n\n\\[\n\\small \\dfrac{(50/200)}{\\bigg(\\dfrac{75}{200}\\times\\dfrac{80}{200}\\bigg)}=\\dfrac{0.25}{0.375\\times0.4}\\approx 1.66\n\\]\n\nThe combination of sausages and ketchup is \\(\\small66\\%\\) higher than what we would expect if these items were bought independently!\nThis provides us with a metric of how relevant this association is, and we can take this into account when thinking about how we can exploit them in terms of marketing campaigns"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#wrapping-up-on-metrics",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#wrapping-up-on-metrics",
    "title": "Market Basket Analysis",
    "section": "Wrapping up on metrics",
    "text": "Wrapping up on metrics\n\nSupport, Confidence, and Lift tell us different things. When we search for some specific rules, we wish to find rules such that:\n\n\nExceed a minimum threshold on each item: sets that occur relatively frequently in transactions (high Support)\nShows strong conditional relationships (Confidence)\nAre more common than chance (Lift)\n\n\nIn what follows, we will use the groceries dataset to extract relevant combinations of purchases, calculate co-occurrence metrics, and devise action plans based on our findings"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#hands-on-exercise",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#hands-on-exercise",
    "title": "Market Basket Analysis",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\n\n\n\n\n\nDescription\n\n\n\nYou work as a big CPG retailer and just received access to its transaction database related to a sizable sample of their grocery sales. More specifically, the data set comprises lists of items purchased together (i.e, market baskets), where the individual items have been recorded as category labels instead of product names (i.e, sausages, ketchup, beer, etc). You have been prompted with the following task:\n\n\nWhich categories are frequently bought together?\nWhich action plans would you recommend and what should be the estimated effect?\n\n\n\n\n\nYou can download the groceries.csv data using the Download button below.\n\nNote: the groceries dataset has been adapted to fit the purpose of the lecture. You can find the original dataset here\n\n\n Download Raw data\n\n\n\\(\\rightarrow\\) This example has been taken from (Chapman and Feit 2015)"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#analyzing-the-dataset",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#analyzing-the-dataset",
    "title": "Market Basket Analysis",
    "section": "Analyzing the dataset",
    "text": "Analyzing the dataset\n\n\nLoading packages\n\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom matplotlib import pyplot as plt\nfrom plotnine import *\nfrom great_tables import GT, md\nfrom mizani.formatters import percent_format\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\n\n# Load the Groceries dataset\ndata = pd.read_csv('Assets/groceries.csv')\n\n\n# Load the Groceries dataset\ndata = pd.read_csv('Assets/groceries.csv')\ndata.head(10)\n\n\n\nDescribing the top 10 categories\n\nResult\n\ntop10=data.groupby(['itemDescription']).size().sort_values(ascending=False).head(10).reset_index()\ntop10.columns=['Category','Transactions']\n\ntheme_plots= theme(\n  legend_position='bottom',\n  axis_title=element_text(face='bold',size=10),\n  plot_title=element_text(face='bold',size=15)\n  )\n\nPlot = (\n  ggplot(top10, aes(x='reorder(Category,Transactions)', y='Transactions'))+\n  geom_col(fill='orange')+\n  geom_text(aes(label=top10['Transactions']),size=10, color='black',position=position_stack(vjust=0.5))+  # Add annotations\n  labs(\n    title='Top 10 Categories in terms of # of transactions',\n    subtitle='Considering all transactions where at a given category was bought.',\n    x='Category',\n    y='# of Transactions')+\n  theme_minimal()+\n  theme_plots +\n  theme(\n      axis_title_x=element_blank(),\n      axis_title_y=element_blank())\n    )\n\n#Display Output\nPlot.show()\n\nPython\nResult\n\ntop10=data.groupby(['itemDescription']).size().sort_values(ascending=False).head(10).reset_index()\ntop10.columns=['Category','Transactions']\n\ntheme_plots= theme(\n  legend_position='bottom',\n  axis_title=element_text(face='bold',size=10),\n  plot_title=element_text(face='bold',size=15)\n  )\n\nPlot = (\n  ggplot(top10, aes(x='reorder(Category,Transactions)', y='Transactions'))+\n  geom_col(fill='orange')+\n  geom_text(aes(label=top10['Transactions']),size=10, color='black',position=position_stack(vjust=0.5))+  # Add annotations\n  labs(\n    title='Top 10 Categories in terms of # of transactions',\n    subtitle='Considering all transactions where at a given category was bought.',\n    x='Category',\n    y='# of Transactions')+\n  theme_minimal()+\n  theme_plots +\n  theme(\n      axis_title_x=element_blank(),\n      axis_title_y=element_blank())\n    )\n\n#Display Output\nPlot.show()"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#looking-for-association-rules",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#looking-for-association-rules",
    "title": "Market Basket Analysis",
    "section": "Looking for Association Rules",
    "text": "Looking for Association Rules\n\nExplanationResultPython\n\n\n\nHow we can look for meaningful associations between items bought together?\nFirst, we need to transform our dataset in such a way that it has the following structure:\n\nEach row represents one transaction - i.e, a Customer \\(\\times\\) Date\nEach column represents a specific category\nFor each cell, we’ll do one-hot-encoding: if a given transaction happened to have a category in its basket, we’ll assign \\(\\small1\\). If not, we will assign zero\n\nIn this way, we’ll get the dataset in the correct format to start thinking about our three metrics: support, confidence, and lift\n\n\n\n\n\n\n\n\n\n\n\nTransaction-based data\n\n\n\nberries\nbeverages\nbottled beer\nbottled water\nbrandy\n\n\n\n\n1023-2014-06-13\n0\n0\n0\n0\n0\n\n\n1023-2014-12-30\n0\n0\n0\n0\n0\n\n\n1023-2015-01-06\n0\n0\n0\n0\n0\n\n\n1023-2015-06-28\n1\n0\n1\n0\n0\n\n\n1024-2015-08-10\n0\n0\n0\n0\n0\n\n\n1025-2014-02-06\n0\n0\n0\n0\n0\n\n\n1025-2014-03-10\n0\n0\n0\n0\n0\n\n\n1025-2015-08-19\n0\n0\n0\n0\n0\n\n\n1026-2014-03-08\n0\n0\n0\n0\n0\n\n\n1026-2014-06-18\n0\n0\n0\n0\n0\n\n\n\n\n\n\n        \n\n\n\n\n\ndata['Quantity'] = np.full_like(data['itemDescription'], 1).astype(int)\ndata['Invoice'] = data[\"Member_number\"].astype(str) + '-' + data[\"Date\"].astype(str)\n\nbasket = (data.groupby([\"Invoice\",\"itemDescription\"])[\"Quantity\"].\n          sum().\n          unstack().\n          reset_index().\n          fillna(0).\n          set_index(\"Invoice\")\n          )\n\ndef encode(x):\n    if x==0:\n        return 0\n    if x&gt;=1:\n        return 1\n    \nbasket_encode = basket.map(encode)\n\n\nTable = (\n  GT(basket_encode.iloc[90:100,10:15].reset_index())\n  .cols_align('center')\n  .tab_header(title=md(\"**Transaction-based data**\"))\n  .tab_stub('Invoice')\n  .fmt_number(decimals = 0)\n  .opt_stylize(style=1,color='red')\n)\n\n#Output\nTable.tab_options(table_width=\"100%\",table_font_size=\"25px\")"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#creating-rules",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#creating-rules",
    "title": "Market Basket Analysis",
    "section": "Creating Rules",
    "text": "Creating Rules\n\nExplanationResultPython\n\n\n\nWith the dataset already in place, we can start looking for reasonable associations\nAs transactions dataset tend to have a reasonably high number of items, we may want to switch our attention to items that represent a given threshold of support\nWe will look for items that represent, at least, \\(\\small0.5\\%\\) of the transactions (\\(\\small \\approx 20\\) times)\nConditional on these items, we’ll look for all combinations and check for the ones that we have reasonable lift\nWe’ll export our results to a .csv file for easier manipulation\n\n\n\n\n\n      antecedents     consequents  ...  certainty  kulczynski\n0   (frankfurter)          (beer)  ...   0.016154    0.089214\n1          (beer)   (frankfurter)  ...   0.004558    0.089214\n2        (yogurt)        (cereal)  ...   0.007000    0.081082\n3        (cereal)        (yogurt)  ...   0.010239    0.081082\n4        (cereal)          (soda)  ...   0.001611    0.079906\n5          (soda)        (cereal)  ...   0.000962    0.079906\n6  (bottled beer)    (whole milk)  ...  -0.000126    0.101549\n7    (whole milk)  (bottled beer)  ...  -0.000032    0.101549\n8        (cereal)    (whole milk)  ...  -0.011316    0.102551\n9    (whole milk)        (cereal)  ...  -0.003875    0.102551\n\n[10 rows x 14 columns]\n\n\n\n\n\nfrequent_itemsets = apriori(basket_encode, min_support=0.005, use_colnames=True)\n\nrules = association_rules(frequent_itemsets, metric=\"lift\").sort_values(\"lift\", ascending=False).reset_index(drop=True)\n\nrules.to_csv('Associations.csv')\n\nrules.head(10)"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#discussion---market-basket-analysis",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#discussion---market-basket-analysis",
    "title": "Market Basket Analysis",
    "section": "Discussion - Market Basket Analysis",
    "text": "Discussion - Market Basket Analysis\n\nAll in all, we were able to find four rules with relevant lift levels:\n\n\\((Frankfurter \\implies Beer)\\)\n\\((Beer \\implies Frankfurter)\\)\n\\((Yogurt \\implies Cereal)\\)\n\\((Cereal \\implies Yogurt)\\)\n\nWhich marketing actions can be done based on this information?\n\nPromotions: buy one, get discount on the other\nIn-store management: place items near the same aisle\nCoupouning: provides coupons for joint purchases\nSpecial packs"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#discussion---market-basket-analysis-continued",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#discussion---market-basket-analysis-continued",
    "title": "Market Basket Analysis",
    "section": "Discussion - Market Basket Analysis (continued)",
    "text": "Discussion - Market Basket Analysis (continued)\n\nMarket-basket analysis (and association rules in general) are not testing hypothesis, but simply finding potentially reasonable associations!\nBridging association rules with decision-making: one can leverage the relevant associations and do a series of other studies in such a way to understand consumption patterns:\n\n\nUnderstand socioeconomic characteristics of individuals who had the relevant association rules (who they are, what they eat, how much they spend, etc)\nConduct qualitative surveys to understand the specific reasons why the association is appearing (randomness? soccer games?)\nPerform other statistical analyses to analyze the data\nConduct experiments: what if we halve the price of beer but increase the price of frankfurters? Is this going to generate higher profits?"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#wrapping-up-whatve-done-so-far",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#wrapping-up-whatve-done-so-far",
    "title": "Market Basket Analysis",
    "section": "Wrapping-up: what’ve done so far",
    "text": "Wrapping-up: what’ve done so far\n\nSo far, you looked at how consumers make binary choices, both on a binary and multinomial fashion…\nAnd saw ways to understand associations between different but related choices\n\nLooking ahead\n\nExplore different classification models that seek to provide similar answers\nBridge traditional econometrics with the recent machine learning literature: cross-validation, train/test sets, hyperparameter tuning etc\nIf your interest is on causality, we need to think about clever ways to insulate our models from ommited variable bias\nThink about when you need to use a more complex model, and why!\n\nI hope you had as much fun as I did!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#reading-the-data",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#reading-the-data",
    "title": "Market Basket Analysis",
    "section": "Reading the data",
    "text": "Reading the data\n\nResultPython\n\n\n\n\n   Unnamed: 0  Member_number        Date itemDescription\n0           1           1808  2015-07-21  tropical fruit\n1           2           2552  2015-01-05      whole milk\n2           3           2300  2015-09-19       pip fruit\n3           4           1187  2015-12-12            beer\n4           5           3037  2015-02-01      whole milk\n5           6           4941  2015-02-14      rolls/buns\n6           7           4501  2015-05-08            beer\n7           8           3803  2015-12-23      pot plants\n8           9           2762  2015-03-20      whole milk\n9          10           4119  2015-02-12  tropical fruit\n\n\n\n\n\n# Load the Groceries dataset\ndata = pd.read_csv('Assets/groceries.csv')\ndata.head(10)"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#describing-the-top-10-categories",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#describing-the-top-10-categories",
    "title": "Market Basket Analysis",
    "section": "Describing the Top 10 Categories",
    "text": "Describing the Top 10 Categories\n\nResultPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntop10=data.groupby(['itemDescription']).size().sort_values(ascending=False).head(10).reset_index()\ntop10.columns=['Category','Transactions']\n\ntheme_plots= theme(\n  legend_position='bottom',\n  axis_title=element_text(face='bold',size=10),\n  plot_title=element_text(face='bold',size=15)\n  )\n\nPlot = (\n  ggplot(top10, aes(x='reorder(Category,Transactions)', y='Transactions'))+\n  geom_col(fill='orange')+\n  geom_text(aes(label=top10['Transactions']),size=10, color='black',position=position_stack(vjust=0.5))+  # Add annotations\n  labs(\n    title='Top 10 Categories in terms of # of transactions',\n    subtitle='Considering all transactions where at a given category was bought.',\n    x='Category',\n    y='# of Transactions')+\n  theme_minimal()+\n  theme(\n    legend_position='bottom',\n    axis_title=element_blank(),\n    axis_text=element_text(size=12),\n    plot_title=element_text(face='bold',size=20),\n    figure_size=(15,7)\n    )\n  )\n\n#Display Output\nPlot.show()"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#make-sure-to-load-all-necessary-packages",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#make-sure-to-load-all-necessary-packages",
    "title": "Market Basket Analysis",
    "section": "Make sure to load all necessary packages",
    "text": "Make sure to load all necessary packages\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom matplotlib import pyplot as plt\nfrom plotnine import *\nfrom great_tables import GT, md\nfrom mizani.formatters import percent_format\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#tech-setup",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#tech-setup",
    "title": "Market Basket Analysis",
    "section": "Tech-setup",
    "text": "Tech-setup\n\n\n\n\n\n\nTech-setup\n\n\nAll coding steps will be done using Python. If you need help on setting up your machine, please refer to this link for help\n\n\n\n\nBefore you start, make sure to import and load all the necessary packages:\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom matplotlib import pyplot as plt\nfrom plotnine import *\nfrom great_tables import GT, md\nfrom mizani.formatters import percent_format\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules"
  },
  {
    "objectID": "quant-mkt.html#tech-setup-i-setting-up-the-correct-python-version",
    "href": "quant-mkt.html#tech-setup-i-setting-up-the-correct-python-version",
    "title": "Quantitative Methods",
    "section": "Tech-setup I: setting up the correct Python version",
    "text": "Tech-setup I: setting up the correct Python version\nThroughout the course, students will also be exposed to a series of practical coding sessions, where the instructors will walk through practical applications of the methods discussed in class. All coding sessions will be developed using Python. Using the correct Python version ensures compatibility with dependencies, preventing installation failures and runtime errors. Some packages require specific versions for stability or features, while newer releases may break existing code.\nTo make sure you are fully able to perform all analyses, we will use 3.11.0 version, which you can download using this link."
  },
  {
    "objectID": "quant-mkt.html#tech-setup-ii-dependencies-and-environments",
    "href": "quant-mkt.html#tech-setup-ii-dependencies-and-environments",
    "title": "Quantitative Methods",
    "section": "Tech-setup II: dependencies and environments",
    "text": "Tech-setup II: dependencies and environments\nAfter you are done with your Python installation, it is time to install all package dependencies that will be used throughout the lectures. To that point, ensure to call pip install your_package on your terminal all Python packages that we’ll be using. If you are using Python for other applications, is always recommended to create a separate virtual environment (venv) using your terminal. Virtual environments help isolate projects with different Python versions, avoiding conflicts and making sure that the outputs are reproducible.\nFor Windows users, use the Windows button + R and type cmd:\n\npython -m venv /path/to/new/virtual/environment-name\n\nNote that environment-name can be any name of your choice. After you do this, ensure that your machine points to the environment by running:\n\ncd /path/to/new/virtual/environment\n\nFinally, activate your environment so that Python knows that we’ll be using it:\n\npython activate environment/Scripts/activate\n\nIf you have followed all the steps, you should see your enviroment-name surround by parenthesis. You’re now ready to install the packages that we’ll need! In order to do that, I’ve already stored it for you inside a notepad:\n\nmatplotlib\npandas\nnumpy\nstatsmodels\njupyter\nscikit-learn\nplotnine\ngreat_tables\ntidypolars\npyarrow\n\nNow, create a .txt file called requirements.txt and copy-paste the contents outlined above. These are the python packages that we’ll require in order to get things moving. After you’ve done this, place the requirements.txt file inside your enviroment. Before you install any packages, you can check the packages that are currently installed in your environment and store in a .txt file by calling:\n\npip freeze &gt;&gt; installed_packages.txt\n\nNot surprisingly, you’ll noticed that this action has created an empty file. In other words, there are still no packages installed in your newly created environment. Now, to install the packages inside requirements.txt, do:\n\npip install -r requirements.txt\n\nThis can take some time, depending on the packages and your current computer settings. After you’re done with this part, you can use pip freeze installed.packages.txt again and see that there are new dependencies installed!\n\n\n\n\n\n\n2. Using Quarto\n\n\n\nIn the first section of this course, you’ll be assigned with a data case, where you’ll need to manipulate code and write your insights altogether. I want to encourage you to give Quarto a try.\nTo install Quarto, follow this link and choose your Operating System. RStudio will automatically locate it and make it as an option whenever creating a new file with Ctrl+N. Why you should give Quarto a try:\n\nIt has multi-language support (Python, R, Julia, JavaScript), parses equations and mathematical notations via pandoc, and integrates seamlessly with GitHub\nAdvanced document formatting and output options: you can choose pdf, html, docx, or even a reveal.js presentation (like the ones from this course!)\nIt is easy, intuitive, and lets you focus on the most important aspect of your work\n\nThere are several IDEs that integrate well with Quarto. In special, I’d recommend RStudio, VSCode, and Positron. While the former is more suited for R programming, it does work quite well for rendering Quarto documents using both R and Python. VSCode and Positron (which is built on top of the open-source VSCode), on the other hand, provide support for Quarto through the Extensions panel.\n\n\n\n\n\n\n\n\n3. Contents\n\n\n\nAt the end of this page, you find the persistent links to all lectures of the course. As they are continuously updated with fixes and new implementations, you might expect some changes from time to time in the contents of each file.\n\nHit F for full-screen mode\nIf you are interest in getting a .pdf version of the slides, hit E to switch to print mode and then Ctrl + P\n\n\n\n\n\n\n\n\n\n4. Replications\n\n\n\nAlong with the slides, each lecture will also contain a replication file, in .qmd format, containing a thorough discussion for all examples that have been showcased during the in-class meeting. In this way, you can follow along and replicate all contents from the lectures on your end.\nTo do that, download the file, open it up in your preferred IDE, and render the Quarto document using the Render button (shortcut: Ctrl+Shift+K). You can find the permanent links to the replication files (.qmd) below:\n\nBinary Models and its Applications - access here\nEvaluating Logistic Regression Outcomes - access here\nMultinomial Choice Models and its applications - access here\nMarket Basket Analysis - access here"
  },
  {
    "objectID": "quant-mkt.html#tech-setup-setting-up-your-environment",
    "href": "quant-mkt.html#tech-setup-setting-up-your-environment",
    "title": "Quantitative Methods",
    "section": "Tech-setup: setting up your environment",
    "text": "Tech-setup: setting up your environment\nThroughout the course, students will also be exposed to a series of practical coding sessions, where the instructors will walk through practical applications of the methods discussed in class. All coding sessions will be developed using Python. Using the correct Python version ensures compatibility with dependencies, preventing installation failures and runtime errors. Some packages require specific versions for stability or features, while newer releases may break existing code.\nTo make sure you are fully able to perform all analyses, I recommend using the latest Python version (3.13.2), which you can download using this link.\nAfter you are done with your Python installation, it is time to install all package dependencies that will be used throughout the lectures. To that point, ensure to call pip install your_package on your terminal all Python packages that we’ll be using. If you are using Python for other applications, is always recommended to create a separate virtual environment (venv) using your terminal. Virtual environments help isolate projects with different Python versions, avoiding conflicts and making sure that the outputs are reproducible.\nFor Windows users, use the Windows button + R and type cmd:\n\npython -m venv /path/to/new/virtual/environment-name\n\nNote that environment-name can be any name of your choice. After you do this, ensure that your machine points to the environment by running:\n\ncd /path/to/new/virtual/environment\n\nFinally, activate your environment so that Python knows that we’ll be using it:\n\npython activate environment/Scripts/activate\n\nIf you have followed all the steps, you should see your enviroment-name surround by parenthesis. You’re now ready to install the packages that we’ll need! In order to do that, I’ve already stored it for you inside a notepad:\n\nmatplotlib\npandas\nnumpy\nstatsmodels\njupyter\nscikit-learn\nplotnine\ngreat_tables\npyarrow\nmlxtend\n\nNow, create a .txt file called requirements.txt and copy-paste the contents outlined above. These are the python packages that we’ll require in order to get things moving. After you’ve done this, place the requirements.txt file inside your enviroment. Before you install any packages, you can check the packages that are currently installed in your environment and store in a .txt file by calling:\n\npip freeze &gt;&gt; installed_packages.txt\n\nNot surprisingly, you’ll noticed that this action has created an empty file. In other words, there are still no packages installed in your newly created environment. Now, to install the packages inside requirements.txt, do:\n\npip install -r requirements.txt\n\nThis can take some time, depending on the packages and your current computer settings. After you’re done with this part, you can use pip freeze installed.packages.txt again and see that there are new dependencies installed!\n\n\n\n\n\n\n2. Using Quarto\n\n\n\nIn the first section of this course, you’ll be assigned with a data case, where you’ll need to manipulate code and write your insights altogether. I want to encourage you to give Quarto a try:\n\nIt has multi-language support (Python, R, Julia, JavaScript), parses equations and mathematical notations via pandoc, and integrates seamlessly with GitHub\nAdvanced document formatting and output options: you can choose pdf, html, docx, or even a reveal.js presentation (like the ones from this course!)\nIt is easy, intuitive, and lets you focus on the most important aspect of your work\n\nTo install Quarto, follow this link and choose your Operating System. RStudio will automatically locate it and make it as an option whenever creating a new file with Ctrl+N.\nThere are several IDEs that integrate well with Quarto. In special, I’d recommend RStudio, VSCode, and Positron. While the former is more suited for R programming, it does work quite well for rendering Quarto documents using both R and Python. VSCode and Positron (which is built on top of the open-source VSCode), on the other hand, provide support for Quarto through the Extensions panel.\n\n\n\n\n\n\n\n\n3. Replications\n\n\n\nAlong with the slides, each lecture will also contain a replication file with all examples that have been showcased during the in-class meeting. In this way, you can follow along and replicate all contents from the lectures on your end.\nTo ensure everyone can easily follow and replicate the code used in our lectures, we will be using Google Colab. Colab is a free, cloud-based platform that allows you to run Python code directly in your browser without any installation. It supports real-time collaboration and comes pre-installed with most of the libraries we’ll need.\nTo make sure you are able to replicate those notebooks, please follow, for each lecture, the instructions present on ADI. You can simply download all necessary files, drag and drop them to the Collab notebook, and run the code.\nFor each lecture, please refer to the following links:\n\nBinary Models and its Applications - access here\nEvaluating Logistic Regression Outcomes - access here\nMultinomial Choice Models and its applications - access here\nMarket Basket Analysis - access here"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#bibliography-and-references",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#bibliography-and-references",
    "title": "Choice Models and its Applications",
    "section": "Bibliography and References",
    "text": "Bibliography and References\n\nAlthough emphasis will be given to the practical aspects (i.e, coding) around the subjects, it is expected that students are well-versed in statistics/econometrics\nFor a detailed discussion around the methodological approaches discussed throughout the lectures, you can refer to the following textbooks (graduate-level):\n\nDiscrete Choice Methods with Simulation - (Train 2009)\nEconometric Analysis - (Greene 2011)\nEconometrics - (Hayashi 2000)\n\nFor a broader discussion around the practical applications on Marketing topics, we’ll follow (Chapman and Feit 2015)\n\n\\(\\rightarrow\\) For those that want to extend their knowledge beyond the topics covered in the lectures, the professor will provide a range of academic papers, as well as policy/whitepapers that cover further topics on Quantitative Marketing"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#introduction-to-discrete-choice-models",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#introduction-to-discrete-choice-models",
    "title": "Choice Models and its Applications",
    "section": "Introduction to Discrete Choice Models",
    "text": "Introduction to Discrete Choice Models\n\nMarketing practitioners are often concerned about consumers decisions around a defined set of choices:\n\nConsumers choose between a binary decision of buy/not buy, given product, consumer, and environmental characteristics\nFaced with a defined set of products, make a decision around which products to buy\nAfter experiencing a flow of consumption from a product or service up to time \\(t\\), decide whether to continue consumption at \\(\\small t+1\\) or churn\n\nEliciting knowledge about individual choices shed light on consumer preferences and help marketers to understand how consumers value certain product attributes\nAggregating individual choices helps marketers understand how the demand curve for a given product or industry works"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#binary-choice-models---churn",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#binary-choice-models---churn",
    "title": "Choice Models and its Applications",
    "section": "Binary Choice Models - Churn",
    "text": "Binary Choice Models - Churn\n\nOne of the most widely applicable areas of binary choice modeling refers to churn modeling:\n\n\n\n\n\n\n\nDefinition\n\n\nChurn refers to the the % of customers who discontinue their use of a business’s products or services over a certain time period. This concept is important for businesses because it affects their revenue and offers insights into customer satisfaction and loyalty:\n\nA high churn rate may suggest there are issues with the product, service, or overall customer experience\nA low churn rate may suggest the existence of customer loyalty, high switching costs, or valuable attributes from the offer\n\n\n\n\n\nChurn modeling offers valuable insights to marketers as it is possible to derive actions ex-ante its occurrence and prevent customers from churning\nThis issue becomes more relevant when customer acquisition costs increase"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#hands-on-exercise",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#hands-on-exercise",
    "title": "Choice Models and its Applications",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\n\n\n\n\n\nDescription\n\n\n\nYou work as a quantitative analyst at a big regional bank and just got access to the bank’s Customer Relationship Manager (CRM) data, comprising a sample of 10,000 bank customers and its actual engagement status (whether or not he/she has churned) over the study period. Your manager wants you to use this data and investigate the following points:\n\n\nWhat is the percentage of customers that are churning during the study period?\nWhat are the key aspects that drove historical churn?\nWhich action plans would you recommend so as to preemptively identify and avoid future churn from customers?\n\n\n\n\n\nYou can download the bank-dataset.csv data using the Download button below\n\nNote: this dataset is be primarily based on this Kaggle notebook, although some adaptations have been made for teaching purposes\n\n\n Download Raw data"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#hands-on-exercise-1",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#hands-on-exercise-1",
    "title": "Choice Models and its Applications",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\n\n\n\n\n\nDescription\n\n\n\nYou work as a quantitative analyst at a big regional bank and just got access to the bank’s Customer Relationship Manager (CRM) data, comprising a sample of 10,000 bank customers and its actual engagement status (whether or not he/she has churned) over the study period. Your manager wants you to use this data and investigate the following points:\n\n\nWhat is the percentage of customers that are churning during the study period?\nWhat are the key aspects that drove historical churn?\n\n\n\n\n\nYou can download the bank-dataset.csv data using the Download button below.\n\nNote: this dataset is be primarily based on this Kaggle notebook, although some adaptations have been made for teaching purposes\n\n\n Download Raw data"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#analyzing-churn-correlation-with-customer-characteristics",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#analyzing-churn-correlation-with-customer-characteristics",
    "title": "Choice Models and its Applications",
    "section": "Analyzing churn correlation with customer characteristics",
    "text": "Analyzing churn correlation with customer characteristics\n\nResultPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Select numeric columns and calculate correlation with churn\ncorrelation_df = (\n  Data\n  .select_dtypes(include='number')\n  .corr()\n  .iloc[:-1, -1]\n  .reset_index()\n)\n\n# Rename columns\ncorrelation_df.columns = ['Variable', 'Correlation']\n\n# Add 'Sign' column based on correlation values\ncorrelation_df['Sign'] = correlation_df['Correlation'].apply(lambda x: 'Positive' if x &gt; 0 else 'Negative')\n\n#Chart\nPlot = (\n  ggplot(correlation_df, aes(x='Variable', y='Correlation', fill='Sign'))+\n  geom_col()+\n  geom_text(aes(label=correlation_df['Correlation'].round(2)),size=10, color='black',position=position_stack(vjust=0.5))+  # Add annotations\n  scale_fill_manual(values=['red','green'])+\n  coord_flip()+\n  labs(title='Churn correlation across numeric variables')+\n  theme_minimal()+\n  theme(\n    legend_position='bottom',\n    axis_title=element_blank(),\n    axis_text=element_text(size=12),\n    plot_title=element_text(face='bold',size=20),\n    figure_size=(15,7)\n    )\n)\n\n#Display Output\nPlot.show()"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#wrapping-up",
    "href": "quant-mkt/coursework/Lecture 1 - Choice Models and Its Applications/index.html#wrapping-up",
    "title": "Choice Models and its Applications",
    "section": "Wrapping-up",
    "text": "Wrapping-up\nLogit or LPM?\n\nIf your interest is to use the model results to predict probabilities for different age brackets, then yes, you should use Logit (or any model with the similar properties)\nIf, on the other hand, you’re just interest in knowing the effects for the average person in your sample (or extrapolating a causal effect if applicable), you can use LPM\n\nStrengths and weaknesses\n\nLPM is simpler, and we know very well the properties to analyze cases potential issues such as ommited variable bias\nIn the Logit world, there is no \\(R^2\\), but there are other ways to check the predictive ability and fit of models\n\n\\(\\rightarrow\\) In our next lecture, we will understand how to evaluate Logit models using metrics that are more suitable for the task!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#tech-setup",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#tech-setup",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Tech-setup",
    "text": "Tech-setup\n\n\n\n\n\n\nTech-setup\n\n\nAll coding steps will be done using Python. If you need help on setting up your machine, please refer to this link for help\n\n\n\n\nBefore you start, make sure to import and load all the necessary packages:\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom matplotlib import pyplot as plt\nfrom plotnine import *\nfrom great_tables import GT, md\nfrom mizani.formatters import percent_format\nfrom scipy.stats import chi2\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#a-graphical-intuition-of-the-maximum-likelihood-estimator",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#a-graphical-intuition-of-the-maximum-likelihood-estimator",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "A graphical intuition of the maximum likelihood estimator",
    "text": "A graphical intuition of the maximum likelihood estimator\n\nSince \\(X\\) and \\(Y\\) are given, the only way to change the \\(\\ell(\\beta)\\) is to change the estimates of the coefficients until we find the vector \\(\\beta^\\star\\) that maximizes the probability (likelihood) of describing the sample"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#the-wald-test",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#the-wald-test",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "The Wald Test",
    "text": "The Wald Test\n\n\n\n\n\n\nDefinition\n\n\nThe Wald Test focus on whether a set of coefficients \\(\\hat{\\beta}\\) are collectively significantly different from a given \\(\\beta_0\\) vector:\n\\[\n\\mathcal{W} = \\frac{(\\hat{\\beta}-\\beta_0)^2}{\\widehat{\\text{Var}}(\\hat{\\beta})}\n\\]\n\n\n\n\nThe Wald test can be used to test a single hypothesis on multiple parameters, as well as to test jointly multiple hypotheses on single/multiple parameters:\n\nThe Wald test helps determine if a set of independent variables are collectively significant in a model or if individual variables add value to the model\nAsymptotically, \\(\\mathcal{W}\\) follows a \\(\\chi^2(Q)\\) distribution, with \\(Q\\) being the number of restrictions"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#likelihood-ratio-test",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#likelihood-ratio-test",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Likelihood-Ratio Test",
    "text": "Likelihood-Ratio Test\n\n\n\n\n\n\nDefinition\n\n\nThe [Likelihood Ratio] compares the likelihoods of restricted and unrestricted models:\n\\[\nLR = 2 \\big[\\ell(\\hat{\\beta}_0) - \\ell(\\hat{\\beta})\\big]\n\\]\n\n\n\n\nIt compares the gain in the log-likelihood if we use an unrestricted model\nAsymptotically, the test statistic follows a \\(\\chi^2(Q)\\) distribution, with \\(Q\\) being the number of restrictions"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#the-wald-test-1",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#the-wald-test-1",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "The Wald Test",
    "text": "The Wald Test\n\n\n\n\n\n\nDefinition\n\n\nThe Wald Test tests whether a set of coefficients \\(beta_j\\) are significantly different from zero:\n$$\n = \n$$\n\n\n\n\nAsymptotically, \\(\\mathcal{W}\\) follows a \\(\\chi^2(n)\\) distribution, with \\(n\\) being the number of parameters"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#statistical-tests-in-practice---the-lagrante-multiplier-test",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#statistical-tests-in-practice---the-lagrante-multiplier-test",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Statistical Tests in practice - The Lagrante Multiplier Test",
    "text": "Statistical Tests in practice - The Lagrante Multiplier Test\n\nResultPython\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.435323\n         Iterations 6\n\n\n\n\n\n\n\n\nSummary Statistics\n\n\n\nwald_stat\np_values\n\n\n\n\nconst\n−14.40\n0.00\n\n\ncredit_score\n−2.35\n0.02\n\n\ngender\n−10.06\n0.00\n\n\ntenure\n−1.59\n0.11\n\n\nbalance\n10.96\n0.00\n\n\nproducts_number\n−0.78\n0.44\n\n\ncredit_card\n−0.49\n0.62\n\n\nactive_member\n−18.86\n0.00\n\n\nestimated_salary\n1.06\n0.29\n\n\nage\n28.56\n0.00\n\n\n\n\n\n\n        \n\n\n\n\n\n# Load data\ndata = pd.read_csv('Assets/bank-dataset.csv')\ndata['gender'] = np.where(data['gender'] == 'Male', 1, 0)\n\n# Fit logistic regression model\nX = data[['credit_score', 'gender', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary','age']]\nX = sm.add_constant(X)  # Add constant term for intercept\ny = data['churn']\n\n# Fit logistic regression model\nreg = sm.Logit(y, X).fit()\n\n# Wald test for each coefficient\nwald_stat = reg.params / reg.bse\n\nstatistics = pd.DataFrame({\n  'parameter': X.columns,\n  'wald_stat': wald_stat,\n  'p_values': 1 - chi2.cdf(wald_stat**2, 1)\n})\n\nTable = (\n  GT(statistics)\n  .cols_align('center')\n  .tab_header(title=md(\"**Summary Statistics**\"))\n  .tab_stub('parameter')\n  .fmt_number(decimals = 2)\n  .opt_stylize(style=1,color='red')\n)\n\n#Output\nTable.show()\n\n#Save predictions and export it for later use\npredict=pd.DataFrame({'true_y': y,'predicted_y': reg.predict()})\npredict.to_csv('predicted.csv')"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#the-lagrange-multiplier-test",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#the-lagrange-multiplier-test",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "The Lagrange Multiplier Test",
    "text": "The Lagrange Multiplier Test\n\n\n\n\n\n\nDefinition\n\n\nThe [Lagrange Multiplier] test evaluates if including an additional parameter significantly improves the model. Based on the score function, \\(S(\\beta)\\), which is the first derivative of the log-likelihood, the test-statistic is defined as:\n\\[\nLM = S(\\hat{\\beta}_0)^\\top I(\\hat{\\beta}_0)^{-1} S(\\hat{\\beta}_0)\n\\] where \\(I(\\hat{\\beta}_0)^{-1}\\) is the Fisher Information Criteria, or the inverse of the Hessian Matrix of \\(\\hat{\\beta}\\)\n\n\n\n\nIn a given way, the Lagrange Multiplier analyses the slope of the first derivative of the maximum likelihood function:\n\nLarge values of the test statistic indicates that the slope vector departs significantly from 0 - there is still improvements to be made so as to maximize the log-likelihood\nAsymptotically, the test statistic follows a \\(\\chi^2(Q)\\) distribution, with \\(Q\\) being the number of restrictions"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#statistical-tests-in-practice",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#statistical-tests-in-practice",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Statistical Tests in practice",
    "text": "Statistical Tests in practice\n\nResultPython\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.435323\n         Iterations 6\n\n\n\n\n\n\n\n\nSummary Statistics\n\n\n\nwald_stat\np_values\n\n\n\n\nconst\n−14.40\n0.00\n\n\ncredit_score\n−2.35\n0.02\n\n\ngender\n−10.06\n0.00\n\n\ntenure\n−1.59\n0.11\n\n\nbalance\n10.96\n0.00\n\n\nproducts_number\n−0.78\n0.44\n\n\ncredit_card\n−0.49\n0.62\n\n\nactive_member\n−18.86\n0.00\n\n\nestimated_salary\n1.06\n0.29\n\n\nage\n28.56\n0.00\n\n\n\n\n\n\n        \n\n\n\n\n\n# Load data\ndata = pd.read_csv('Assets/bank-dataset.csv')\ndata['gender'] = np.where(data['gender'] == 'Male', 1, 0)\n\n# Fit logistic regression model\nX = data[['credit_score', 'gender', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary','age']]\nX = sm.add_constant(X)  # Add constant term for intercept\ny = data['churn']\n\n# Fit logistic regression model\nreg = sm.Logit(y, X).fit()\n\n# Wald test for each coefficient\nwald_stat = reg.params / reg.bse\n\nstatistics = pd.DataFrame({\n  'parameter': X.columns,\n  'wald_stat': wald_stat,\n  'p_values': 1 - chi2.cdf(wald_stat**2, 1)\n})\n\nTable = (\n  GT(statistics)\n  .cols_align('center')\n  .tab_header(title=md(\"**Summary Statistics**\"))\n  .tab_stub('parameter')\n  .fmt_number(decimals = 2)\n  .opt_stylize(style=1,color='red')\n)\n\n#Output\nTable.show()\n\n#Save predictions and export it for later use\npredict=pd.DataFrame({'true_y': y,'predicted_y': reg.predict()})\npredict.to_csv('predicted.csv')"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#other-topics-for-further-investigation-continued",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#other-topics-for-further-investigation-continued",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Other topics for further investigation, continued",
    "text": "Other topics for further investigation, continued\n\nThere is a growing literature on the role of machine learning methods for supervised learning applied to classification contexts.\n\n\nMachine Learning Methods That Economists Should Know About - Susan Atey - link here\nClassification Model Performance"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#bridging-econometrics-with-machine-learning",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#bridging-econometrics-with-machine-learning",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Bridging Econometrics with Machine Learning",
    "text": "Bridging Econometrics with Machine Learning\n\nPotential topics that you may want to dive in when looking at binary choice models:\n\nCompare different models in terms of predictive power, statistics etc, such as Probit, Random Forests, Suppor Vector Machines\nFine-tune the metrics to optimize your classification results based on the question that you’re aiming to solve\nUsing train/test splits and balanced samples\nUsing cross-validation folds\nComparing the classification performance across different [models specifications]^[For R, refer to the caret package. For Python, refer to scikit-learn\n\nAll in all, there is a growing literature on the role of machine learning methods for supervised learning applied to classification contexts1\n\nSee Machine Learning Methods That Economists Should Know About (Susan Atey) - click here to access"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#hands-on-exercise",
    "href": "quant-mkt/coursework/Lecture 2 -  Evaluating Logistic Regression Outcomes/index.html#hands-on-exercise",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\n\n\n\n\n\n\nDescription\n\n\n\nAs you progress through your work on the bank CRM dataset, there are a couple of questions that are still oustanding:\n\n\nHow do Logit estimates translate to economic interpretation?\nHow can we assess the performance of this classifier model and compare that to other models?\nAre there any ways for testing whether a specific variable improves the model performance?\n\n\n\n\n\nWe will continue to use the same dataset as before - you can download the bank-dataset.csv data using the Download button below\n\nNote: this dataset is be primarily based on this Kaggle notebook, although some adaptations have been made for teaching purposes\n\n\n Download Raw data"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#other-choice-based-applications-not-covered",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#other-choice-based-applications-not-covered",
    "title": "Market Basket Analysis",
    "section": "Other choice-based applications not covered",
    "text": "Other choice-based applications not covered\n\nMarketers also have a wide range of other techniques that can be used to draw relevant insights from transaction data - those include, but are not limited, to:\n\n\nCojoint Analysis: understand how customers value different features, helping to determine the optimal combination of attributes that maximize preferences and willingness to pay:\n\nHow much users are willing to pay for ad-free vs. premium content?\nDo customers care more about organic ingredients or low calories?\n\nMulti-touch Attribution: determine how different customer touchpoints contribute to conversions throughout the consumer journey:\n\nWhat are the most influential touchpoints (e.g, Google Search, META, TikTok)? How users interact before conversion?\n\n\n\\(\\rightarrow\\) For an in-depth discussion of other methodologies applied to a marketing context, refer to (Chapman and Feit 2015)"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#references",
    "href": "quant-mkt/coursework/Lecture 4 -  Market Basket Analysis/index.html#references",
    "title": "Market Basket Analysis",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nChapman, Chris, and Elea Mcdonnell Feit. 2015. R for Marketing Research and Analytics. Cham: Springer International Publishing."
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#tech-setup",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#tech-setup",
    "title": "Multiple Choice Models",
    "section": "Tech-setup",
    "text": "Tech-setup\n\n\n\n\n\n\nTech-setup\n\n\nAll coding steps will be done using Python. If you need help on setting up your machine, please refer to this link for help\n\n\n\n\nBefore you start, make sure to import and load all the necessary packages:\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom matplotlib import pyplot as plt\nfrom plotnine import *\nfrom great_tables import GT, md\nfrom mizani.formatters import percent_format\nfrom statsmodels.discrete.discrete_model import MNLogit\nfrom statsmodels.tools.tools import add_constant"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#recap-on-logit-models",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#recap-on-logit-models",
    "title": "Multiple Choice Models",
    "section": "Recap on Logit Models",
    "text": "Recap on Logit Models\n\nFrom previous classes, the idea behind using \\(\\Lambda(X)\\) lies on the latent variable approach: think about an unobserved component, \\(Y^\\star\\), which is a continuous variable, such as how much a consumer values a product.\nAlthough we do not observe \\(Y^\\star\\), we do observe consumers’ decisions of buying or not buying the product, depending on a given threshold:\n\n\\[\n  Y = \\begin{cases}\n      1, & \\text{if } Y^* &gt; 0 \\\\\n      0, & \\text{if } Y^* \\leq 0\n  \\end{cases}\n\\]\n\nTherefore, we can see that the probability of buying depends on a latent variable, which is not observed by the econometrician:\n\n\\[\n  P(Y = 1 | X) = P(Y^* &gt; 0 | X) = P(X \\beta + \\varepsilon &gt; 0 | X)\n\\]"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#logistic-regression-logit-models",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#logistic-regression-logit-models",
    "title": "Multiple Choice Models",
    "section": "Logistic Regression (Logit) Models",
    "text": "Logistic Regression (Logit) Models\n\nWe saw that Logit models are just an example of a case where the transformation function is:\n\n\\[\n  f(Y^*) = \\Lambda(Y^*) = \\frac{\\exp(X\\beta)}{1 + \\exp(X\\beta)}\n\\]\n\nWhenever we’re estimating a logit model, our transformation function, \\(\\Lambda(\\cdot)\\), is actually estimating \\(\\log[p/(1 - p)]\\):\n\n\\[\n  \\text{logit}(p) = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k\n\\]\n\nThe term \\(\\frac{p}{1 - p}\\) is called odds-ratio, and is simply the ratio of the probability of success over the probability of failure"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#hands-on-exercise",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#hands-on-exercise",
    "title": "Multiple Choice Models",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\n\n\n\n\n\nDescription\n\n\n\nYou were hired as a consultant to work on a research project that aims to understand how individuals travel around Spain. A survey was applied to more than 100 customers seeking to understand their consumption patterns when it comes to travelling around the country using buses, cars, trains, or airplanes. You have been prompted with the following task:\n\n\nHow customers decide which transportation method to use?\nWhat would have been the effect of decreasing transportation costs in terms of market-shares for each transportation?\n\n\n\n\n\nYou can download the transportation-dataset.csv data using the Download button below.\n\nNote: the groceries dataset has been adapted to fit the purpose of the lecture. You can find the original dataset here\n\n\n Download Raw data\n\n\n\\(\\rightarrow\\) This example has been taken from (Train 2009)"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#from-discrete-to-multiple-choice",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#from-discrete-to-multiple-choice",
    "title": "Multiple Choice Models",
    "section": "From Discrete to Multiple Choice",
    "text": "From Discrete to Multiple Choice\n\nIn practice, decisions are, in general, more complicated than a simply binary choice:\n\nConsumers choose between a product given a finite set of choices;\nPassengers choose which transportation method is the preferred one for their trips;\nIndividuals choose which payment method is the best to accommodate their budget constraints.\n\nIn more practical cases, the set of choices \\(k\\) is, in general, greater than \\(2\\). In this lecture, we will see that we can accommodate the estimation method used for Logit for cases where \\(k &gt; 2\\).\nThis method is generally called multinomial logit, and like logit, is just an example of generalized linear models, a broader class of models that generalize the multiple linear regression model"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#bridging-binary-to-multi-choice-models-continued",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#bridging-binary-to-multi-choice-models-continued",
    "title": "Multiple Choice Models",
    "section": "Bridging Binary to Multi-Choice Models (continued)",
    "text": "Bridging Binary to Multi-Choice Models (continued)\n\nRecall that, in a binary setting, our probability measure using a logit was:\n\n\\[\n  P(Y) = \\frac{\\exp(X\\beta)}{1 + \\exp(X\\beta)}\n\\] which we will call by \\(p\\)\n\nLet’s write the probabilities of each scenario \\(y=1\\) and \\(y=0\\) for a given customer \\(i\\) as:\n\n\\[\n\\begin{cases}\n  P(y_i = 1 \\mid x_i) = p_{i,1} \\\\\n  P(y_i = 0 \\mid x_i) = p_{i,0}\n  \\end{cases}\n\\]\n\nWe can use the intuition on these probabilities to arrive at a general formulation."
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#bridging-binary-to-multi-choice-models-continued-1",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#bridging-binary-to-multi-choice-models-continued-1",
    "title": "Multiple Choice Models",
    "section": "Bridging Binary to Multi-Choice Models (continued)",
    "text": "Bridging Binary to Multi-Choice Models (continued)\n\nSuppose that we consider \\(y = 0\\) (in our previous example, non-churned customers) as the baseline reference. Then, the logit regression for this customer \\(i\\) is simply the estimation of the log odds-ratio:\n\n\\[\n\\log \\left( \\frac{p_{i,1}}{1 - p_{i,1}} \\right) = \\log \\left( \\frac{p_{i,1}}{p_{i,0}} \\right) = X\\beta\n\\]\n\nIn words, we are estimating the log ratio of probabilities, taking into account that the baseline category is \\(y = 0\\)\nRelating this to the churn example, we are modeling the increase in the probability of churning (relative to non-churning), based on observations \\(X\\).\nWhat if we have more than two categories?"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#bridging-binary-to-multi-choice-models-continued-2",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#bridging-binary-to-multi-choice-models-continued-2",
    "title": "Multiple Choice Models",
    "section": "Bridging Binary to Multi-Choice Models (continued)",
    "text": "Bridging Binary to Multi-Choice Models (continued)\n\nSuppose now that we have \\(k &gt; 2\\). For example, instead of thinking about a churned customer, we can observe its actual bank choice: \\(\\{Santander, KutxaBank, BankInter\\}\\)\nOur set has now three potential choices. We can write the probability of customer \\(i\\) choosing bank \\(k = z\\) as:\n\n\\[\n\\begin{cases}\n  P(y_i = 1 | x_i) = p_{i,1} \\\\\n  P(y_i = 2 | x_i) = p_{i,2}, \\text{ such that} \\quad \\sum_{k=1}^{3} p_{i,k} = 1 \\\\\n  P(y_i = 3 | x_i) = p_{i,3}\n  \\end{cases}\n\\]"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#multinomial-logit",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#multinomial-logit",
    "title": "Multiple Choice Models",
    "section": "Multinomial Logit",
    "text": "Multinomial Logit\n\nLet’s say that we want to fit a model to understand customer’s preferences around bank services providers. Formally, we have information on covariates \\(X\\). Then, we want to fit a model such that we can recover the probability of choosing alternative \\(k\\) based on \\(X\\).\nFor that, fix \\(k = 1\\) (in our case, Santander) to be the baseline category. Then:\n\n\\[\n  \\log\\bigg(\\frac{p_{i,2}}{p_{i,1}}\\bigg) = \\alpha_2 + \\beta_{2,1} + \\dots + \\beta_{2,j}\n\\]\n\\[\n  \\log\\bigg(\\frac{p_{i,3}}{p_{i,1}}\\bigg) = \\alpha_3 + \\beta_{3,1} + \\dots + \\beta_{3,j}\n\\]\n\nIn words, for each bank \\(k\\) other than \\(k = 1\\) (Santander), we will have a separate equation that measures the log odds-ratio of preferring bank \\(k\\) over Santander\nTherefore, if the response variable has \\(k\\) possible categories, there will be \\(k - 1\\) equations."
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#about-the-dataset",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#about-the-dataset",
    "title": "Multiple Choice Models",
    "section": "About the dataset",
    "text": "About the dataset\n\nWe will use a different dataset to understand multi-choice models.\nThis data comes from Greene (2003) and consists of a survey about the preferred travel transportation method (air, bus, car, or train for a sample of individuals.\nWe observe the following characteristics:\n\nCharacteristics of the choice vehicle cost, waiting time, travel time\nCharacteristics of the individuals: income, family size\nThe actual choice made by each individual: \\(\\{air, bus, train, car\\}\\)\n\nWe want to understand how both offer characteristics as well as individual characteristics drive the decision to use a specific transportation method"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#quick-outline-of-the-dataset",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#quick-outline-of-the-dataset",
    "title": "Multiple Choice Models",
    "section": "Quick outline of the dataset",
    "text": "Quick outline of the dataset"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#estimating-a-multinomial-logit",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#estimating-a-multinomial-logit",
    "title": "Multiple Choice Models",
    "section": "Estimating a Multinomial Logit",
    "text": "Estimating a Multinomial Logit\n\nA Multinomial Logit model is estimated by maximizing the likelihood of observing the decisions for each consumer \\(i\\)\nAs before, the estimation of the parameters of this model by maximum likelihood proceeds by maximization of the multinomial likelihood with the probabilities viewed as functions of the parameters\nDifferently from the binary case, we’ll now have one equation for each \\(k \\neq 1\\). Note that it really makes no difference which category we pick as the reference cell, because we can always convert from one formulation to the other—similar case with defining a dummy variable"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#estimating-a-multinomial-logit-1",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#estimating-a-multinomial-logit-1",
    "title": "Multiple Choice Models",
    "section": "Estimating a Multinomial Logit",
    "text": "Estimating a Multinomial Logit\n\nResultPython\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.753382\n         Iterations 11\n\n\n                          MNLogit Regression Results                          \n==============================================================================\nDep. Variable:                   mode   No. Observations:                  210\nModel:                        MNLogit   Df Residuals:                      198\nMethod:                           MLE   Df Model:                            9\nDate:                Fri, 04 Apr 2025   Pseudo R-squ.:                  0.4424\nTime:                        13:05:46   Log-Likelihood:                -158.21\nconverged:                       True   LL-Null:                       -283.76\nCovariance Type:            nonrobust   LLR p-value:                 5.853e-49\n==============================================================================\n  mode=bus       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -10.6695      2.381     -4.482      0.000     -15.336      -6.003\nsize           0.1787      0.623      0.287      0.774      -1.042       1.399\nincome        -0.0317      0.023     -1.366      0.172      -0.077       0.014\ntravel         0.0481      0.009      5.308      0.000       0.030       0.066\n------------------------------------------------------------------------------\n  mode=car       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -11.5148      2.325     -4.953      0.000     -16.071      -6.958\nsize           1.1665      0.545      2.140      0.032       0.098       2.235\nincome        -0.0071      0.021     -0.343      0.732      -0.048       0.034\ntravel         0.0463      0.009      5.124      0.000       0.029       0.064\n------------------------------------------------------------------------------\nmode=train       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -9.3876      2.318     -4.049      0.000     -13.932      -4.844\nsize           0.9217      0.562      1.640      0.101      -0.180       2.023\nincome        -0.0633      0.022     -2.869      0.004      -0.107      -0.020\ntravel         0.0467      0.009      5.166      0.000       0.029       0.064\n==============================================================================\n\n\n\n\n\n# Read the CSV file into a Pandas DataFrame\nData = pd.read_csv('Assets/transportation-dataset.csv')\n\n# Filter rows where choice is 'yes' and reset index\nData = Data[Data['choice'] == 'yes'].reset_index(drop=True)\n\n# Define features and target variable\nX = Data[['size', 'income', 'travel']]\ny = Data['mode']\n\n# Add a constant term\nX = add_constant(X)\n\n# Fit multinomial logistic regression model\nmultinomial_model = MNLogit(y, X)\nmultinomial_result = multinomial_model.fit()\n\n# Display summary statistics\nprint(multinomial_result.summary())"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#predicting-probabilities",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#predicting-probabilities",
    "title": "Multiple Choice Models",
    "section": "Predicting probabilities",
    "text": "Predicting probabilities\n\nFor each of the estimated categories \\(k\\) (bus, train, car), the probability that individual \\(i\\) has chosen it is:\n\n\\[\np_{k,i} = \\frac{\\exp(\\alpha_k + \\beta_{k,1} + \\beta_{k,2} + \\dots + \\beta_{k,j})}\n{1 + \\sum_{k=2}^{4} \\exp(\\alpha_k + \\beta_{k,1} + \\beta_{k,2} + \\dots + \\beta_{k,j})}\n\\]\n\nRecall that we’ve recovered this probability by fixing a reference category (in this case, air).\nTherefore, to recover the probabilities of individuals choosing air:\n\n\\[\n  p_{1,i} = 1 - \\sum_{k=2}^{4} p_{k,i}\n\\]"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#predicting-probabilities-continued",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#predicting-probabilities-continued",
    "title": "Multiple Choice Models",
    "section": "Predicting Probabilities (continued)",
    "text": "Predicting Probabilities (continued)\n\nResultPython\n\n\n\n\n\n\n\n\n\n\nSummary Statistics\n\n\n\nMean\nStandard Deviation\nMedian\nMinimum\nMaximum\n\n\n\n\nair\n27.62%\n41.31%\n0.36%\n0.00%\n99.92%\n\n\nbus\n14.29%\n13.07%\n10.92%\n0.01%\n48.02%\n\n\ncar\n28.10%\n24.37%\n22.88%\n0.05%\n91.16%\n\n\ntrain\n30.00%\n23.87%\n28.82%\n0.01%\n74.96%\n\n\n\n\n\n\n        \n\n\n\n\n\nprobabilities = multinomial_result.predict()\n\n# Convert to tibble (DataFrame in pandas)\nprobabilities_df = pd.DataFrame(probabilities,columns=['air','bus','car','train'])\n\n# Reshape DataFrame to long format\nprobabilities_df = probabilities_df.stack().reset_index()\nprobabilities_df.columns = ['obs_num', 'mode', 'value']\n\n# Group by 'mode' and summarize statistics\nsummary_df = probabilities_df.groupby('mode').agg({'value': ['mean', 'std', 'median', 'min', 'max']})\nsummary_df.columns = ['_'.join(col).strip() for col in summary_df.columns.values]\n\n# Display the summary DataFrame\n\n#Table\nTable= (\n  GT(summary_df.reset_index())\n  .cols_align('center')\n  .tab_stub(rowname_col='mode')\n  .fmt_percent()\n  .cols_label(\n    value_mean = 'Mean',\n    value_std = 'Standard Deviation',\n    value_median = 'Median',\n    value_min = 'Minimum',\n    value_max = 'Maximum'\n    )\n  .tab_header(title=md(\"**Summary Statistics**\"))\n  .opt_stylize(style=1,color='red')\n)\n\nTable.tab_options(table_width=\"100%\",table_font_size=\"25px\")"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#assessing-performance",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#assessing-performance",
    "title": "Multiple Choice Models",
    "section": "Assessing Performance",
    "text": "Assessing Performance\n\nExplanationResultPython\n\n\n\nBecause we’re using the very same maximization procedure as used in logit models, we can use the same maximum likelihood statistical tests:\n\nThe Wald test\nThe Likelihood-Ratio test\nThe Lagrange Multiplier test\n\nAlso, we can tabulate the results in such a way that we’d ideally want all observations to lie at the diagonal cells of our matrix\n\n\n\n\n\n\n\n\n\n\n\nAccuracy of the model is 64.76%\n\n\n\nAir\nBus\nCar\nTrain\n\n\n\n\nair\n54\n0\n3\n1\n\n\nbus\n1\n8\n5\n16\n\n\ncar\n5\n5\n30\n19\n\n\ntrain\n0\n4\n15\n44\n\n\n\n\n\n\n        \n\n\n\n\n\n# Define class labels mapping\nclass_mapping = {0: 'air', 1: 'bus', 2: 'car', 3: 'train'}\n\n# Predict class labels\npredicted_labels = multinomial_result.predict(X).idxmax(axis=1).map(class_mapping)\nactual_labels = y\n\n# Combine predicted labels with original data\nData['Predicted'] = predicted_labels\nData['Actual'] = actual_labels\n\n# Create contingency table\ncontingency_table = pd.crosstab(index=Data['Actual'], columns=Data['Predicted']).reset_index()\n\n# Calculate accuracy\naccuracy = \"{:.2%}\".format((Data['Actual'] == Data['Predicted']).mean())\n\n# Display the contingency table and accuracy\nTable= (\n  GT(contingency_table)\n  .cols_align('center')\n  .tab_stub(rowname_col='Actual')\n  .cols_label(\n    air = 'Air',\n    bus = 'Bus',\n    car = 'Car',\n    train = 'Train',\n    )\n  .tab_header(title=\"Accuracy of the model is \" + accuracy)\n  .opt_stylize(style=1,color='red')\n)\n\nTable.tab_options(table_width=\"100%\",table_font_size=\"25px\")"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#counterfactual-exercises",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#counterfactual-exercises",
    "title": "Multiple Choice Models",
    "section": "Counterfactual exercises",
    "text": "Counterfactual exercises\n\nAs marketers, we’re generally interested in understanding how consumers would react as-if they were exposed to a given situation:\n\nWhat happens to the probability of using a plane as the size of the family grows?\nHow many customers would upgrade their travel plans to air if they have received an increase in income?\nIf the price of air decreases significantly, how would customers distribute among other transportation modes?\n\nIn what follows, we’ll analyze how these three changes affect the distribution of transportation choices."
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#exercise-1-increasing-size",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#exercise-1-increasing-size",
    "title": "Multiple Choice Models",
    "section": "Exercise 1: increasing size",
    "text": "Exercise 1: increasing size\n\nExplanationResultPython\n\n\n\nWhat happens to the probability of using each mode as the size of the family grows?\nIn order to analyze that, we will:\n\nMake a copy of the original matrix of covariates, \\(X\\)\nUse a loop iterating different sets of customers, with size ranging from \\(1\\) to \\(4\\)\nPredict the probabilities for each choice and averaging them out across customers\nAppend the results\n\n\nResult: Consumers tend to switch over to cars more aggressively\n\n\n\n\n\n\n\n\n\n\nCounterfactual Exercise 1 - Probabilities\n\n\n\nAir\nBus\nCar\nTrain\n\n\n\n\n1\n29.10%\n20.71%\n21.31%\n28.88%\n\n\n2\n26.34%\n11.12%\n30.24%\n32.30%\n\n\n3\n23.19%\n5.33%\n38.63%\n32.85%\n\n\n4\n19.58%\n2.38%\n46.42%\n31.62%\n\n\n5\n15.24%\n1.03%\n54.15%\n29.58%\n\n\n\n\n\n\n        \n\n\n\n\n\n# Define class labels mapping\nclass_mapping = {0: 'air', 1: 'bus', 2: 'car', 3: 'train'}\n\n#Define size buckets\nsize_buckets = np.arange(1,6,1)\n\n#Store the results\nresults=pd.DataFrame()\n\nfor i in size_buckets:\n\n  # Create counterfactual DataFrame with current values of X except for and different sizes\n  X_Temp=X.copy()\n  X_Temp['size']=i\n\n  # Predict probabilities for counterfactual data\n  counterfactual_probs = pd.DataFrame({'estimate': multinomial_result.predict(X_Temp).mean(axis=0)}).T\n  counterfactual_probs.columns=counterfactual_probs.columns.map(class_mapping)\n  counterfactual_probs['size']=i\n  counterfactual_probs=counterfactual_probs.reindex(columns=['size','air','bus','car','train'])\n  \n  #Append\n  results=pd.concat([results,counterfactual_probs],axis=0,ignore_index=True)\n\n# Display the contingency table and accuracy\nTable= (\n  GT(results)\n  .cols_align('center')\n  .tab_stub(rowname_col='size')\n  .fmt_percent()\n  .cols_label(\n    size= 'Size',\n    air = 'Air',\n    bus = 'Bus',\n    car = 'Car',\n    train = 'Train',\n    )\n  .tab_header(title=md(\"**Counterfactual Exercise 1 - Probabilities**\"))\n  .opt_stylize(style=1,color='red')\n)\n\nTable.tab_options(table_width=\"100%\",table_font_size=\"25px\")"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#exercise-2-increasing-income",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#exercise-2-increasing-income",
    "title": "Multiple Choice Models",
    "section": "Exercise 2: increasing income",
    "text": "Exercise 2: increasing income\n\nExplanationResultPython\n\n\n\nHow many customers would upgrade their travel plans to air if they receive an increase in income?\nIn order to analyze that, we will:\n\nMake a copy of the original matrix of covariates, \\(X\\)\nUse a loop iterating different sets of customers, with income ranging from \\(0\\) to \\(100\\) by increments of \\(10\\)\nPredict the probabilities for each choice and averaging them out across customers\nAppend the results\n\n\n\n\n\n\n\n\n\n\n\n\nCounterfactual Exercise 2 - Probabilities\n\n\n\nAir\nBus\nCar\nTrain\n\n\n\n\n0\n22.47%\n11.01%\n9.11%\n57.41%\n\n\n10\n24.07%\n13.01%\n13.67%\n49.25%\n\n\n20\n25.46%\n14.72%\n19.53%\n40.29%\n\n\n30\n26.67%\n15.82%\n26.33%\n31.17%\n\n\n40\n27.70%\n16.11%\n33.44%\n22.75%\n\n\n50\n28.56%\n15.57%\n40.15%\n15.71%\n\n\n60\n29.27%\n14.38%\n45.99%\n10.35%\n\n\n70\n29.86%\n12.79%\n50.78%\n6.57%\n\n\n80\n30.35%\n11.04%\n54.55%\n4.06%\n\n\n90\n30.78%\n9.31%\n57.46%\n2.45%\n\n\n100\n31.16%\n7.72%\n59.66%\n1.46%\n\n\n\n\n\n\n        \n\n\n\n\n\n#Define income buckets\nincome_buckets = np.arange(0,110,10)\n\n#Store the results\nresults=pd.DataFrame()\n\nfor i in income_buckets:\n\n  # Create counterfactual DataFrame with current values of X except for and different sizes\n  X_Temp=X.copy()\n  X_Temp['income']=i\n\n  # Predict probabilities for counterfactual data\n  counterfactual_probs = pd.DataFrame({'estimate': multinomial_result.predict(X_Temp).mean(axis=0)}).T\n  counterfactual_probs.columns=counterfactual_probs.columns.map(class_mapping)\n  counterfactual_probs['income']=i\n  counterfactual_probs=counterfactual_probs.reindex(columns=['income','air','bus','car','train'])\n  \n  #Append\n  results=pd.concat([results,counterfactual_probs],axis=0,ignore_index=True)\n\n# Display the contingency table and accuracy\nTable= (\n  GT(results)\n  .cols_align('center')\n  .tab_stub(rowname_col='income')\n  .fmt_percent()\n  .cols_label(\n    income= 'Income Level',\n    air = 'Air',\n    bus = 'Bus',\n    car = 'Car',\n    train = 'Train',\n    )\n  .tab_header(title=md(\"**Counterfactual Exercise 2 - Probabilities**\"))\n  .opt_stylize(style=1,color='red')\n)\n\nTable.tab_options(table_width=\"100%\",table_font_size=\"25px\")"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#exercise-3-decreasing-travel",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#exercise-3-decreasing-travel",
    "title": "Multiple Choice Models",
    "section": "Exercise 3: decreasing travel",
    "text": "Exercise 3: decreasing travel\n\nExplanationResultPython\n\n\n\nHow many customers would upgrade their travel plans to air if travel costs decrease?\nIn order to analyze that, we will:\n\nMake a copy of the original matrix of covariates, \\(X\\)\nCreate a range of different customers, with travel costs set to \\(10\\%-90\\%\\) of the original cost for each customer\nPredict the probabilities for each choice and averaging them out across customers\nAppend the results\n\n\n\n\n\n\n\n\n\n\n\n\nCounterfactual Exercise 3 - Probabilities\n\n\n\nAir\nBus\nCar\nTrain\n\n\n\n\n0%\n99.97%\n0.00%\n0.02%\n0.01%\n\n\n10%\n99.03%\n0.03%\n0.64%\n0.30%\n\n\n20%\n90.16%\n0.81%\n4.80%\n4.23%\n\n\n30%\n69.24%\n3.70%\n13.76%\n13.29%\n\n\n40%\n60.21%\n5.23%\n17.65%\n16.91%\n\n\n50%\n54.74%\n6.39%\n19.33%\n19.54%\n\n\n60%\n48.35%\n7.84%\n21.23%\n22.57%\n\n\n70%\n41.30%\n9.57%\n23.50%\n25.63%\n\n\n80%\n35.30%\n11.28%\n25.54%\n27.88%\n\n\n90%\n30.86%\n12.84%\n27.05%\n29.24%\n\n\n100%\n27.62%\n14.29%\n28.10%\n30.00%\n\n\n\n\n\n\n        \n\n\n\n\n\n#Define travel buckets\ntravel_buckets = np.arange(0, 1.1, 0.1)\n\n#Store the results\nresults=pd.DataFrame()\n\nfor i in travel_buckets:\n\n  # Create counterfactual DataFrame with current values of X except for and different sizes\n  X_Temp=X.copy()\n  X_Temp['travel']=X_Temp['travel']*i\n\n  # Predict probabilities for counterfactual data\n  counterfactual_probs = pd.DataFrame({'estimate': multinomial_result.predict(X_Temp).mean(axis=0)}).T\n  counterfactual_probs.columns=counterfactual_probs.columns.map(class_mapping)\n  counterfactual_probs['travel']= \"{:.0%}\".format(i)\n  counterfactual_probs=counterfactual_probs.reindex(columns=['travel','air','bus','car','train'])\n  \n  #Append\n  results=pd.concat([results,counterfactual_probs],axis=0,ignore_index=True)\n\n# Display the predicted probabilities DataFrame\nresults=results.reindex(columns=['travel','air','bus','car','train'])\n\n\n# Display the contingency table and accuracy\nTable= (\n  GT(results)\n  .cols_align('center')\n  .tab_stub(rowname_col='travel')\n  .fmt_percent()\n  .cols_label(\n    travel= '% of Original Travel Cost',\n    air = 'Air',\n    bus = 'Bus',\n    car = 'Car',\n    train = 'Train',\n    )\n  .tab_header(title=md(\"**Counterfactual Exercise 3 - Probabilities**\"))\n  .opt_stylize(style=1,color='red')\n)\n\nTable.tab_options(table_width=\"100%\",table_font_size=\"25px\")"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#limitations-of-the-multinomial-logit",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#limitations-of-the-multinomial-logit",
    "title": "Multiple Choice Models",
    "section": "Limitations of the Multinomial Logit",
    "text": "Limitations of the Multinomial Logit\n\nRecall that we estimated our predicted probabilities by looking at:\n\n\\[\n  p_{k,i} = \\frac{\\exp(\\alpha_k + \\beta_{k,1} + \\beta_{k,2} + \\dots + \\beta_{k,j})}{1 + \\sum_{k=2}^{4} \\exp(\\alpha_k + \\beta_{k,1} + \\beta_{k,2} + \\dots + \\beta_{k,j})}\n\\]\n\nAs a consequence, the probabilities of two choices are the same if they have the same characteristics\nThis creates room for unreasonable substitution patterns. One of the most important properties of the multinomial logit model is the Independence from Irrelevant Alternatives (IIA)\nThe IIA property states that for any individual, the ratio of probabilities of choosing two alternatives is independent of the availability or attributes of any other alternatives"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#limitations-of-the-multinomial-logit-continued",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#limitations-of-the-multinomial-logit-continued",
    "title": "Multiple Choice Models",
    "section": "Limitations of the Multinomial Logit (continued)",
    "text": "Limitations of the Multinomial Logit (continued)\n\nConsider the probability of choosing mode=bus \\(k = 1\\) and mode=train \\(k = 2\\). According to the multinomial model, the probabilities of choosing each alternative are:\n\n\\[\n  p_{2,i} = \\frac{\\exp(\\alpha_2 + \\beta_{2,1} + \\beta_{2,2} + \\dots + \\beta_{2,j})}{1 + \\sum_{k=2}^{4} \\exp(\\alpha_k + \\beta_{k,1} + \\beta_{k,2} + \\dots + \\beta_{k,j})}\n\\]\n\\[\n  p_{3,i} = \\frac{\\exp(\\alpha_3 + \\beta_{3,1} + \\beta_{3,2} + \\dots + \\beta_{3,j})}{1 + \\sum_{k=2}^{4} \\exp(\\alpha_k + \\beta_{k,1} + \\beta_{k,2} + \\dots + \\beta_{k,j})}\n\\]\n\nProblem: this ratio is independent of the availability and attributes of \\(k = 4\\) car:\n\\[\n\\frac{p_{2,i}}{p_{3,i}} = \\frac{\\exp(\\alpha_2 + \\beta_{2,1} + \\beta_{2,2} + \\dots + \\beta_{2,j})}{\\exp(\\alpha_3 + \\beta_{3,1} + \\beta_{3,2} + \\dots + \\beta_{3,j})} \\equiv \\frac{\\exp^{bus}}{\\exp^{train}}\n\\]"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#the-red-busblue-bus-paradox",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#the-red-busblue-bus-paradox",
    "title": "Multiple Choice Models",
    "section": "The Red-Bus/Blue-Bus Paradox",
    "text": "The Red-Bus/Blue-Bus Paradox\n\nThe IIA property limits responses to changes predicted by the multinomial logit model\nA well-known paradox illustrating this is the red-bus/blue-bus paradox:\n1.Suppose you have two alternatives with identical properties: car and red bus\n\nIf their observable attributes are exactly the same, we should expect the market share of each alternative to be the same (i.e., \\(50\\%\\) each)\n\nNow, assume a blue bus category is introduced with the same observable characteristics. You would expect the new probabilities to be:\n\nCar: \\(50\\%\\)\n\nRed bus: \\(25\\%\\)\nBlue bus: \\(25\\%\\)\n\nProblem: due to the IIA, a multinomial logit model would predict that each option has 33%, which is clearly counterintuitive, as it violates rational decision-making - remember, in terms of valued features, red and blue buses are essentially the same!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#limitations-of-the-multinomial-logit-continued-1",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#limitations-of-the-multinomial-logit-continued-1",
    "title": "Multiple Choice Models",
    "section": "Limitations of the Multinomial Logit (continued)",
    "text": "Limitations of the Multinomial Logit (continued)\n\nTo which extent do the implications of the IIA affect our problem? In order to see that, suppose that you have an increase of \\(10\\%\\) in the cost of using bus\nIf that is the case, individuals who stop using buses due to cost increases are predicted to distribute themselves among the remaining modes in proportion to the initial probabilities of choosing the remaining modes\nIn other words, the ratio of probabilities across the other choices remain intact\n\n\nDoes this sound reasonable? Likely not! If there’s an increase in bus prices, we would expect air and bus passengers to react differently to these changes:\n\nSince bus and train are closely related, one would expect marginal bus users to move towards train\nAs a consequence, the amount of marginal users moving to air should be much less"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#rethinking-our-use-of-multinomial-logit",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#rethinking-our-use-of-multinomial-logit",
    "title": "Multiple Choice Models",
    "section": "Rethinking our use of multinomial logit",
    "text": "Rethinking our use of multinomial logit\n\nHow we can change this? Adding individual preference heterogeneity into the choices:\n\n\\[\nV_{i,k}=  \\underbrace{\\alpha_k+X\\beta_{k}}_{\\text{Average utility for choice k}} + \\underbrace{\\mu_{i,k}}_{\\text{Customer-specific utility for k}}\n\\]\n\nIn the red bus/blue bus paradox, this would allow \\(50\\%\\) of the customers to really prefer cars\nIn our practical example, this would allow air passengers to stick with their choices regardless of the changes in bus fares\n\n\nImplementations:\n\nUse of nested logit models: first, they choose whether they’ll go with high/low cost; after that, they choose the specific transportation method\nBLP estimation - see pyBLP for an implementation using Python"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#references",
    "href": "quant-mkt/coursework/Lecture 3 - Multiple Choice Models/index.html#references",
    "title": "Multiple Choice Models",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nTrain, Kenneth E. 2009. Discrete Choice Methods with Simulation. 2nd ed. Cambridge, England: Cambridge University Press."
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#tech-setup",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#tech-setup",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Tech-setup",
    "text": "Tech-setup\n\n\n\n\n\n\nTech-setup\n\n\nAll coding steps will be done using Python. If you need help on setting up your machine, please refer to this link for help\n\n\n\n\nBefore you start, make sure to import and load all the necessary packages:\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom matplotlib import pyplot as plt\nfrom plotnine import *\nfrom great_tables import GT, md\nfrom mizani.formatters import percent_format\nfrom scipy.stats import chi2\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Evaluating Performance: Statistical Tests",
    "text": "Evaluating Performance: Statistical Tests\n\nIn our previous discussions, we saw that \\(R^2\\), which was our measure of overall goodness-of-fit for linear regression models, really does not convey any relevant information for models like Logit\nFurthermore, the usual t-tests, used to test hypotheses around \\(\\hat{\\beta}\\), is not applicable here\n\nLogistic regression assumes errors follow the logistic distribution\nConsequently, the term \\(\\dfrac{(\\hat{\\beta}-\\beta_0)}{se(\\hat{\\beta})}\\) does not follow a t-distribution\n\nHow can we test make hypothesis around Logit models and assess overall accuracy?\nWe’ll make use of the fact that Logit models are estimated using a likelihood function, \\(\\mathcal{L}\\), in order to derive some important evaluation metrics"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#hands-on-exercise",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#hands-on-exercise",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\n\n\n\n\n\n\nDescription\n\n\n\nAs you progress through your work on the bank CRM dataset, there are a couple of questions that are still oustanding:\n\n\nHow do Logit estimates translate to economic interpretation?\nHow can we assess the performance of this classifier model and compare that to other models?\nAre there any ways for testing whether a specific variable improves the model performance?\n\n\n\n\n\nWe will continue to use the same dataset as before - you can download the bank-dataset.csv data using the Download button below\n\nNote: this dataset is be primarily based on this Kaggle notebook, although some adaptations have been made for teaching purposes\n\n\n Download Raw data"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Evaluating Performance: Statistical Tests (continued)",
    "text": "Evaluating Performance: Statistical Tests (continued)\n\nA model like logit is estimated using a maximum likelihood method. The likelihood of churning for a given individual \\(i\\), with observations \\((y_i,x_i)\\) can be written as\n\n\\[\n\\mathcal{L_i}(\\beta,y_i,x_i)=[\\Lambda(x_i\\beta)^{y_i}]\\times[1-\\Lambda(x_i\\beta)]^{1-y_i}\n\\]\nBecause we are assuming that all churn observations (i.e, customer decisions) are i.i.d, then the likelihood of the entire sample is just the product of the individual likelihoods:\n\\[\n\\mathcal{L}(\\beta,Y,X)=\\prod_{i=1}^{N}[\\Lambda(x_i\\beta)^{y_i}]\\times[1-\\Lambda(x_i\\beta)]^{1-y_i}\n\\]\n\nIn words, a maximum-likelihood estimator is trying to find the coefficients \\(\\beta\\) that make the sample of churn observations \\((y_1,y_2,...,y_n)=Y\\) more likely to occur!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued-1",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued-1",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Evaluating Performance: Statistical Tests (continued)",
    "text": "Evaluating Performance: Statistical Tests (continued)\n\nWhat happens on the back-end is that a maximum likelihood estimator will try to find the set of parameters \\(\\beta\\) that maximize the log-likelihood of the model, noting that the \\(\\log\\) is a monotonic function\nAt the end of the day, when we want to compare models, we can think about which model has the highest log-likelihood\nThere are three common tests that can be used to test this type of question: the Likelihood ratio (LR) test, the Wald test, and the Lagrange multiplier test (sometimes called a score test)\nThese tests are sometimes described as tests for differences among nested models, because one of the models can be said to be nested within the other:\n\nThe null hypothesis for all three tests is that the “smaller”, or “restricted” model, is the “true” model\nOn the other hand, a large test statistics indicate that the null hypothesis is false"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued-2",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued-2",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Evaluating Performance: Statistical Tests (continued)",
    "text": "Evaluating Performance: Statistical Tests (continued)\n\nRecall that our likelihood function is defined as:\n\n\\[\n\\small \\mathcal{L}(\\beta,Y,X)=\\prod_{i=1}^{N}[\\Lambda(x_i\\beta)^{y_i}]\\times[1-\\Lambda(x_i\\beta)]^{1-y_i}\n\\]\n\nTaking logs, the log-likelihood is given by:\n\n\\[\n\\small \\ell(\\beta) = \\log \\mathcal{L}(\\beta) = \\sum_{i=1}^{n} y_i \\log \\Lambda(x_i\\beta) + (1 - y_i) \\log (1 - \\Lambda(x_i\\beta))\n\\]\n\nOn the one hand, values of \\(\\beta\\) that are closer to the “true” relationship between the matrix \\(\\Lambda(X)\\) of covariates and \\(Y\\) increase the log-likelihood\nOn the other hand, values of \\(\\beta\\) that are unlikely to describe the sample relationship between \\(\\Lambda(X)\\) and \\(Y\\) decrease the log-likelihood"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#a-graphical-intuition-of-the-maximum-likelihood-estimator",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#a-graphical-intuition-of-the-maximum-likelihood-estimator",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "A graphical intuition of the maximum likelihood estimator",
    "text": "A graphical intuition of the maximum likelihood estimator\n\nSince \\(X\\) and \\(Y\\) are given, the only way to change the \\(\\ell(\\beta)\\) is to change the estimates of the coefficients until we find the vector \\(\\beta^\\star\\) that maximizes the probability (likelihood) of describing the sample"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued-3",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-statistical-tests-continued-3",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Evaluating Performance: Statistical Tests (continued)",
    "text": "Evaluating Performance: Statistical Tests (continued)\n\n\n\n\n\n\nTo the purposes of our analysis, a constrained case, as a baseline, can be the case where the \\(Y\\) has no predictors (i.e, all \\(\\beta\\)’s are equal to zero)"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#the-wald-test",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#the-wald-test",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "The Wald Test",
    "text": "The Wald Test\n\n\n\n\n\n\nDefinition\n\n\nThe Wald Test focus on whether a set of coefficients \\(\\hat{\\beta}\\) are collectively significantly different from a given \\(\\beta_0\\) vector:\n\\[\n\\mathcal{W} = \\frac{(\\hat{\\beta}-\\beta_0)^2}{\\widehat{\\text{Var}}(\\hat{\\beta})}\n\\]\n\n\n\n\nThe Wald test can be used to test a single hypothesis on multiple parameters, as well as to test jointly multiple hypotheses on single/multiple parameters:\n\nThe Wald test helps determine if a set of independent variables are collectively significant in a model or if individual variables add value to the model\nAsymptotically, \\(\\mathcal{W}\\) follows a \\(\\chi^2(Q)\\) distribution, with \\(Q\\) being the number of restrictions"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#the-lagrange-multiplier-test",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#the-lagrange-multiplier-test",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "The Lagrange Multiplier Test",
    "text": "The Lagrange Multiplier Test\n\n\n\n\n\n\nDefinition\n\n\nThe [Lagrange Multiplier] test evaluates if including an additional parameter significantly improves the model. Based on the score function, \\(S(\\beta)\\), which is the first derivative of the log-likelihood, the test-statistic is defined as:\n\\[\nLM = S(\\hat{\\beta}_0)^\\top I(\\hat{\\beta}_0)^{-1} S(\\hat{\\beta}_0)\n\\] where \\(I(\\hat{\\beta}_0)^{-1}\\) is the Fisher Information Criteria, or the inverse of the Hessian Matrix of \\(\\hat{\\beta}\\)\n\n\n\n\nIn a given way, the Lagrange Multiplier analyses the slope of the first derivative of the maximum likelihood function:\n\nLarge values of the test statistic indicates that the slope vector departs significantly from 0 - there is still improvements to be made so as to maximize the log-likelihood\nAsymptotically, the test statistic follows a \\(\\chi^2(Q)\\) distribution, with \\(Q\\) being the number of restrictions"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#likelihood-ratio-test",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#likelihood-ratio-test",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Likelihood-Ratio Test",
    "text": "Likelihood-Ratio Test\n\n\n\n\n\n\nDefinition\n\n\nThe [Likelihood Ratio] compares the likelihoods of restricted and unrestricted models:\n\\[\nLR = 2 \\big[\\ell(\\hat{\\beta}_0) - \\ell(\\hat{\\beta})\\big]\n\\]\n\n\n\n\nIt compares the gain in the log-likelihood if we use an unrestricted model\nAsymptotically, the test statistic follows a \\(\\chi^2(Q)\\) distribution, with \\(Q\\) being the number of restrictions"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#statistical-tests-in-practice---wald-test",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#statistical-tests-in-practice---wald-test",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Statistical Tests in practice - Wald Test",
    "text": "Statistical Tests in practice - Wald Test\n\nResultPython\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.435323\n         Iterations 6\n\n\n\n\n\n\n\n\nSummary Statistics\n\n\n\nwald_stat\np_values\n\n\n\n\nconst\n−14.40\n0.00\n\n\ncredit_score\n−2.35\n0.02\n\n\ngender\n−10.06\n0.00\n\n\ntenure\n−1.59\n0.11\n\n\nbalance\n10.96\n0.00\n\n\nproducts_number\n−0.78\n0.44\n\n\ncredit_card\n−0.49\n0.62\n\n\nactive_member\n−18.86\n0.00\n\n\nestimated_salary\n1.06\n0.29\n\n\nage\n28.56\n0.00\n\n\n\n\n\n\n        \n\n\n\n\n\n# Load data\ndata = pd.read_csv('Assets/bank-dataset.csv')\ndata['gender'] = np.where(data['gender'] == 'Male', 1, 0)\n\n# Fit logistic regression model\nX = data[['credit_score', 'gender', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary','age']]\nX = sm.add_constant(X)  # Add constant term for intercept\ny = data['churn']\n\n# Fit logistic regression model\nreg = sm.Logit(y, X).fit()\n\n# Wald test for each coefficient\nwald_stat = reg.params / reg.bse\n\nstatistics = pd.DataFrame({\n  'parameter': X.columns,\n  'wald_stat': wald_stat,\n  'p_values': 1 - chi2.cdf(wald_stat**2, 1)\n})\n\nTable = (\n  GT(statistics)\n  .cols_align('center')\n  .tab_header(title=md(\"**Summary Statistics**\"))\n  .tab_stub('parameter')\n  .fmt_number(decimals = 2)\n  .opt_stylize(style=1,color='red')\n)\n\n#Output\nTable.tab_options(table_width=\"100%\",table_font_size=\"25px\")\n\n#Save predictions and export it for later use\npredict=pd.DataFrame({'true_y': y,'predicted_y': reg.predict()})\npredict.to_csv('predicted.csv')"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#statistical-tests-in-practice---likelihood-ratio-test",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#statistical-tests-in-practice---likelihood-ratio-test",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Statistical Tests in practice - Likelihood-Ratio Test",
    "text": "Statistical Tests in practice - Likelihood-Ratio Test\n\nResultPython\n\n\n\n\nLikelihood Ratio Test:\n\n\nChi-square: 1403.0\n\n\nDegrees of freedom: 9.0\n\n\nP-value: 0.0\n\n\n\n\n\n# Perform likelihood ratio test (LRT)\nlr_test = reg.llr\ndf = reg.df_model\np_value = reg.llr_pvalue\n\nprint(\"Likelihood Ratio Test:\")\nprint(\"Chi-square:\", lr_test.round(0))\nprint(\"Degrees of freedom:\", df)\nprint(\"P-value:\", p_value.round(2))"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-pseudo-r2",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-pseudo-r2",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Evaluating Performance: Pseudo-\\(R^2\\)",
    "text": "Evaluating Performance: Pseudo-\\(R^2\\)\n\nExplanationResultPython\n\n\n\n\nMcFadden’s pseudo-\\(R^2\\) is an alternative metric for assessing a model’s performance that also takes into account the use of the log-likelihood function:\n\n\\[\n\\text{pseudo-}R^2=1-(\\mathcal{LL}_{FullModel}/\\mathcal{LL}_{\\beta=0})\n\\]\n\nIf your model doesn’t really predict the outcome better than the null model, and the statistic will be close to zero\nConversely, if the difference in log-likelihood is large, your test will approach 1\n\n\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.505489\n         Iterations 5\n\n\nMcFadden's Pseudo R-squared: 0.14\n\n\n\n\n\n# Calculate McFadden's pseudo R-squared\ndef pseudoR2(model):\n    L1 = model.llf\n    L0 = sm.Logit(y, sm.add_constant(np.ones_like(y))).fit().llf\n    return 1 - (L1 / L0)\n\npseudo_r2 = pseudoR2(reg)\nprint(\"McFadden's Pseudo R-squared:\", pseudo_r2.round(2))"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-accuracy",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#evaluating-performance-accuracy",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Evaluating Performance: Accuracy",
    "text": "Evaluating Performance: Accuracy\n\nHow the estimated results compare to actual choices from customers?\n\nOn the one hand, Logit predicted values relate to estimated probabilities\nOn the other hand, actual information on churn is binary\n\nFrom a practical perspective, one needs to map the estimated probabilities onto a categorization:\n\n\\[\n\\hat{Y}=\n\\begin{cases}\n1 \\text{, if } p&gt;p^\\star\\\\\n0 \\text{, if } p\\leq p^\\star\n\\end{cases}\n\\]\n\nBut how do we pick \\(p^\\star\\)?"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#introducing-the-confusion-matrix",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#introducing-the-confusion-matrix",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Introducing the Confusion Matrix",
    "text": "Introducing the Confusion Matrix\n\nA way to assess the choice of \\(p^\\star\\) is to analyze the confusion matrix:\n\nIt shows how much predictions were correct by each categorization: true positives and true negatives\nIt also shows how much predictions were incorrect by each categorization: false positives and false negatives\n\nIf we agnostically set \\(p^\\star=0.2037\\), which is the sample average of churn, our example would yield the following terms for us:\n\n\nThe number of actual churned customers that were ex-ante classified as churned\nThe number of actual churned customers that were ex-ante classified as not churned\nThe number of actual not churned customers that were ex-ante classified as churned\nThe number of actual not churned customers that were ex-ante classified as not churned"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#confusion-matrix",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#confusion-matrix",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\n\n\n\n\nDiagonal cells indicate the True Positive and True Negative cases\nOff-Diagonal cells indicate the False Positive and False Negative cases\n\n\nType I Errors (\\(2,477\\)) are the False Positive cases: we wrongly classified customers that did not churn as churned!\nType II Errors (\\(650\\)) are the False Negative cases: we wrongly classified customers that did churn as not churned!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#how-should-a-good-estimator-look-like-looking-at-accuracy",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#how-should-a-good-estimator-look-like-looking-at-accuracy",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "How should a good estimator look like? Looking at Accuracy",
    "text": "How should a good estimator look like? Looking at Accuracy\n\nOverall, a good estimator should minimize the combination of Type I and Type II errors. In other words, we want our estimator to have the highest Accuracy as possible:\n\n\\[\n\\small ACC=\\dfrac{TP+TN}{TP+FP+TN+FN}=\\dfrac{1,387+5,486}{10,000}\\approx 69\\% \\text{ correct predictions}\n\\]\n\nNotwithstanding, there might be cases where the costs attributed to Type I and Type II errors are fairly different!\n\nFor example, sharing a discount coupon as a way to avoid losing a customer that was predicted to churn (Type I) while, in reality, it would not churn even without the coupon, has a much less significant cost\nNotwithstanding, losing a customer because we did not identify that he could churn (Type II) has a significant cost to the business"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#sensitivity",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#sensitivity",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Sensitivity",
    "text": "Sensitivity\n\nThe first question that will likely pop up during internal conversations is: how much of the churned customers we were able to correctly identify?\nThis question is of special interest in churn analysis, as the goal is to target these customers before they actually have their final decision!\nThe Sensivity (or the True Positive Rate) calculates the proportion of correctly identified churned customers by comparing the number of true positives with the total number of positives:\n\n\\[\nSensitivity=\\dfrac{TP}{TP+FN}=\\dfrac{1,387}{(1,387+650)}\\approx 68\\%\n\\]\n\nOverall, it seems that our model goes a decent job in identifying \\(68\\%\\) of the actual churned customers ahead of time!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#precision",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#precision",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Precision",
    "text": "Precision\n\nThe naivest way of identifying all churned customers is to set \\(p^\\star=0\\). In other words, if we classify all customers as churned, then, for sure, we’ll get all churned customers right!\nNotwithstanding, we are wrongly classifying some customers that would not churn in the future (false negatives). In practice, if we were to give coupons to every churn customer in potential, this action would cost us much more as we’re wasting money on customers that wouldn’t churn!\nThe Precision calculates the how precise our churn classification was by comparing the number of true positives with the total number of predicted positives:\n\n\\[\nPrecision=\\dfrac{TP}{TP+FP}=\\dfrac{1,387}{(1,387+2,477)}\\approx 35\\%\n\\]\n\nAlthough we hit a high number of churned customers, we have wrongly classified \\(1-35\\%=65\\%\\) of customers as potential churners!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#specificity",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#specificity",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Specificity",
    "text": "Specificity\n\nThe analog of the first question relates to how much of the non-churned customers we were able to correctly identify\nKnowing how much non-churned customers our model predicts shed light on how much we’re able to understand about customers that do not churn!\nThe Specificity (or True Negative Rate) calculates the proportion of correctly identified non-churned customers by comparing the number of true negatives with the total number of negatives:\n\n\\[\nSpecificity=\\dfrac{TN}{TN+FP}=\\dfrac{5,486}{(5,486+2,477)}\\approx 69\\%\n\\]"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#negative-predicted-value",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#negative-predicted-value",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Negative Predicted Value",
    "text": "Negative Predicted Value\n\nThe last piece that is still left to analyze is the precision of our estimates for non-churned classifications\nIn other words, out of all the non-churn predicted customers, how much were actually false negatives?\nThe Negative Predicted Value calculates the proportion of correctly identified non-churned customers by comparing the number of true negatives with the predicted negatives:\n\n\\[\nNPV=\\dfrac{TN}{TN+FN}=\\dfrac{5,486}{(5,486+650)}\\approx 89.37\\%\n\\]\n\nOut of all negative classifications, \\(81.3\\%\\) of them were correct!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#confusion-matrix-implementation",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#confusion-matrix-implementation",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Confusion Matrix Implementation",
    "text": "Confusion Matrix Implementation\n\nResultPython\n\n\n\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x000002AE9F7C6BA0&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n# Predict churn using the trained logistic regression model\npredicted_churn = (\n  (reg.predict() &gt;= y.mean())  # Assuming a threshold of 0.5 for classification\n  .astype(int)\n  )\n\n#Estimate the Confusion Matrix\nCM=confusion_matrix(y,predicted_churn)\n\n#Chart \nConfusionMatrixDisplay.from_predictions(y, predicted_churn)\n\n#Title\nplt.title('Confusion Matrix')\n\n#Show\nplt.show()"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#putting-all-together",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#putting-all-together",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Putting all together",
    "text": "Putting all together\n\nAs we increase our threshold, \\(p^\\star\\), we minimize Type II error (i.e, we identify the churned customers) as we’re identifying the true positives\nAt the same time, however, we are increasing the Type I error, since there is going to be a higher number of false positives!"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#finding-the-optimal-pstar",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#finding-the-optimal-pstar",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Finding the optimal \\(p^{\\star}\\)",
    "text": "Finding the optimal \\(p^{\\star}\\)\n\nExplanationResultPython\n\n\n\n\nIf, for example, we want to find \\(p^{\\star}\\) such that it maximizes the sum of specificity + sensitivity, we can:\n\n\nRedo our confusion matrix for each \\(p^{\\star}=\\{0,0.01,0.02,...,0.99,1\\}\\)\nCalculate the specificity and the sensitivity\nPick the threshold that maximizes the sum of both metrics\n\n\nNote that, depending upon the problem, we might want to use a different criterion to find the optimal \\(p^\\star\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Predict churn probabilities\nprobs = reg.predict()\n\n# Calculate specificity and sensitivity\nfpr, tpr, thresholds = roc_curve(y, probs)\nspecificity = 1 - fpr\nsensitivity = tpr\n\n# Find the best threshold value\nbest_threshold_index = np.argmax(specificity + sensitivity)\nbest_threshold = thresholds[best_threshold_index]\n\n#Create a pandas data.frame and plot it\n\nresult=pd.DataFrame({\n  'thresholds':thresholds,\n  'sp_se': specificity + sensitivity\n})\n\nPlot=(\n  ggplot(result, aes(x='thresholds',y='sp_se'))+\n  geom_point(size=0.5,color='darkorange')+\n  theme_minimal()+\n  geom_vline(xintercept=best_threshold,linetype='dashed')+\n  labs(x='Threshold',\n       y='Specificifity + Sensitivity',\n       title='Best value achieved around 0.22')+\n  theme(plot_title = element_text(size=15,face='bold'),\n        axis_text = element_text(size=12),\n        axis_title = element_text(size=12),\n        figure_size=(15,6))\n  )\n\nPlot.show()"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#comparing-among-different-classifiers",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#comparing-among-different-classifiers",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Comparing among different classifiers",
    "text": "Comparing among different classifiers\n\nSay that, for some reason, you have ommitted age from the analysis. How can you assess how the predictive power of your model is going to deteriorate?\nOne way to do this is to analyze what we call Area under the Curve (or simply AUC):"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#auc-under-different-models",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#auc-under-different-models",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "AUC under different models",
    "text": "AUC under different models\n\nExplanationResultPython\n\n\n\n\nA Receiver Operating Characteristic curve (or simply ROC curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n\n\nThe True Positive Rate (or TPR)\nThe False Positive Rate (or FPR)\n\n\nThe AUC provides an aggregate measure of performance across all possible classification thresholds\nHowever, such metric is not very applicable metric whenever the costs of Type I and Type II errors are very different\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Define independent and dependent variables for Reg_1 (without 'age')\nX1 = X.drop('age',axis=1)\n\n# Define independent and dependent variables for Reg_2 (with 'age')\nX2 = X\n\n# Dependent variable\ny = data['churn']\n\n\n# Fit logistic regression models\nreg_1 = model = sm.Logit(y, X1.astype(float)).fit()\nreg_2 = model = sm.Logit(y, X2.astype(float)).fit()\n\n# Predict probabilities for both models\nprobs_1 = reg_1.predict()\nprobs_2 = reg_2.predict()\n\n# Compute ROC curves\nfpr_1, tpr_1, _ = roc_curve(y, probs_1)\nfpr_2, tpr_2, _ = roc_curve(y, probs_2)\n\n# Compute AUC scores\nauc_1 = roc_auc_score(y, probs_1)\nauc_2 = roc_auc_score(y, probs_2)\n\n# Plot ROC curves\nData=pd.concat([pd.DataFrame({'Model':'Without Age','FPR':fpr_1,'TPR':tpr_1}),\n          pd.DataFrame({'Model':'With Age','FPR':fpr_2,'TPR':tpr_2})],axis=0)\n\nPlot = (\n  \n  ggplot(Data,aes(x='FPR',y='TPR',fill='Model'))+\n  geom_point(stroke=0)+\n  theme_minimal()+\n  annotate(geom='text',x=0.5,y=0.5,label=('Model 1 (without Age): ' + auc_1.round(2).astype('str')))+\n  annotate(geom='text',x=0.4,y=0.9,label=('Model 2 (with Age): ' + auc_2.round(2).astype('str')))+\n  labs(x='FPR (1 - Specificity)',\n       y='TPR (Sensitivity)',\n       title='Model with Age outperforms the other for any threshold!')+\n  theme(plot_title = element_text(size=12,face='bold'),\n        axis_text = element_text(size=10),\n        axis_title = element_text(size=12),\n        legend_position ='bottom',\n        figure_size=(15,6))\n)\n\nPlot.show()"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#bridging-econometrics-with-machine-learning",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#bridging-econometrics-with-machine-learning",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "Bridging Econometrics with Machine Learning",
    "text": "Bridging Econometrics with Machine Learning\n\nPotential topics that you may want to dive in when looking at binary choice models:\n\nCompare different models in terms of predictive power, statistics etc, such as Probit, Random Forests, Suppor Vector Machines\nFine-tune the metrics to optimize your classification results based on the question that you’re aiming to solve\nUsing train/test splits and balanced samples\nUsing cross-validation folds\nComparing the classification performance across different [models specifications]^[For R, refer to the caret package. For Python, refer to scikit-learn\n\nAll in all, there is a growing literature on the role of machine learning methods for supervised learning applied to classification contexts1\n\nSee Machine Learning Methods That Economists Should Know About (Susan Atey) - click here to access"
  },
  {
    "objectID": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#references",
    "href": "quant-mkt/coursework/Lecture 2 - Evaluating Logistic Regression Outcomes/index.html#references",
    "title": "Evaluating Logistic Regresion Outcomes",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments"
  },
  {
    "objectID": "quant-mkt.html#contents",
    "href": "quant-mkt.html#contents",
    "title": "Quantitative Methods",
    "section": "Contents",
    "text": "Contents\nBelow you can find the persistent links to all lectures of the course. As they are continuously updated with fixes and new implementations, you might expect some changes from time to time in the contents of each file.\n\n\n\n\n\n\nNote\n\n\n\n\nHit F for full-screen mode\nIf you are interest in getting a .pdf version of the slides, hit E to switch to print mode and then Ctrl + P"
  },
  {
    "objectID": "quant-fin.html#contents",
    "href": "quant-fin.html#contents",
    "title": "Practical Applications in Quantitative Finance",
    "section": "Contents",
    "text": "Contents\nBelow you can find the persistent links to all lectures of the course. As they are continuously updated with fixes and new implementations, you might expect some changes from time to time in the contents of each file.\nTo access the slides from our guest-lecture (Renato Lerípio, Kapitalo Investimentos, May 2025), click here.\n\n\n\n\n\n\nNote\n\n\n\n\nHit F for full-screen mode\nIf you are interest in getting a .pdf version of the slides, hit E to switch to print mode and then Ctrl + P"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#outline",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#outline",
    "title": "Equity Valuation and Simulation",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\nTidy Finance (Scheuch, Voigt, and Weiss 2023)\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\n\n\n\n\n\n\n\n\nCoding Replications\n\n\nFor coding replications, whenever applicable, please follow this page or hover on the specific slides with containing coding chunks.\n\nEnsure that you have your  session properly set-up according to the instructions outlined in the course webpage\nIn the webpage, you can also find a detailed discussion of the examples covered in this lecture"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#disclaimer",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#disclaimer",
    "title": "Equity Valuation and Simulation",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\n\n\n\n\nDisclaimer\n\n\nThe information presented in this lecture is for educational and informational purposes only and should not be construed as investment advice. Nothing discussed constitutes a recommendation to buy, sell, or hold any financial instrument or security. Investment decisions should be made based on individual research and consultation with a qualified financial professional. The presenter assumes no responsibility for any financial decisions made based on this content.\nAll code used in this lecture is publicly available and is also shared on my GitHub page. Participants are encouraged to review, modify, and use the code for their own learning and research purposes. However, no guarantees are made regarding the accuracy, completeness, or suitability of the code for any specific application.\nFor any questions or concerns, please feel free to reach out via email at lucas.macoris@fgv.br"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#dealing-with-uncertainty-in-valuation-models",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#dealing-with-uncertainty-in-valuation-models",
    "title": "Equity Valuation and Simulation",
    "section": "Dealing with Uncertainty in Valuation Models",
    "text": "Dealing with Uncertainty in Valuation Models\n\nTraditional valuation methods (e.g., Discounted Cash Flow) rely on single-point estimates:\n\nWe assume a given level of revenue growth…\nWe also fix the appropriate discount rates, \\(r\\), over time…\nFinally, we come up with assumptions regarding the firm’s Terminal Value (i.e, the perpetuity value)!\n\nNote, however, that these estimates are subject to significant uncertainty:\n\nMarket conditions\nCompetition\nRegulation\nMacroeconomic and exogenous shocks, such as COVID-19\n\n\n\nQuestion: how can get take into account the role of uncertainty in valuation models?"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#dealing-with-uncertainty-in-valuation-models-continued",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#dealing-with-uncertainty-in-valuation-models-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Dealing with Uncertainty in Valuation Models, continued",
    "text": "Dealing with Uncertainty in Valuation Models, continued\n\nSuppose we have the following Discounted Cash Flow estimate:\n\n\\[\n\\text{Firm Value} = \\sum_{t=1}^{T} \\frac{FCF_t}{(1 + r)^t}\n\\]\n\n\\(FCF_t\\) depends on a variety of firm-level factors, such as sales growth, gross margins, taxes, among others\nSimilarly, \\(r\\), the discount rate, if modeled using the CAPM, depends upon factors such as the sensitivity to market risk (\\(\\beta\\)), the risk-free rate, \\(r_f\\), and the market risk premium\n\n\nChanges in those variables can severely affect the outcomes of our valuation: for example, if the realized \\(FCF\\)’s are lower than what we assumed, we might be overestimating the firm’s value!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#sensitivity-analysis",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#sensitivity-analysis",
    "title": "Equity Valuation and Simulation",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\n\nA first way in which we can take uncertainty into account is through Sensitivity Analysis:\n\n\n\n\n\n\n\nDefinition\n\n\n\nSensitivity analysis tests how changes in a single input affect the valuation output, while all other variables are held constant. It is useful to identify key value drivers and assess their impact.\n\n\n\n\n\nFix the main drivers of your outcome variable (e.g, growth rate of COGS, growth rate of Sales, Cost of Capital, etc)\nFor each driver, create scenarios in where you vary the input of interest within a given range\nFor that specific scenario, collect the new outcome variable calculated when everything is fixed, but the specific driver has changed\nRepeat this across all scenarios and drivers"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#scenario-analysis",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#scenario-analysis",
    "title": "Equity Valuation and Simulation",
    "section": "Scenario Analysis",
    "text": "Scenario Analysis\n\nAs described before, a clear limitation of Sensitivity Analysis is the fact that only one input is evaluated at a time\nWhat if we wanted to see the combined effect of multiple variables changing at the same time?\n\n\n\n\n\n\n\nDefinition\n\n\n\nA Scenario Analysis evaluates the effect of multiple variables changing at the same time\n\n\nIt is used to create alternative scenarios (e.g, Best, Base, and Worst Case Scenarios) based on business logic\nIt is more realistic than sensitivity analysis, but still limited to a few discrete outcomes\n\n\nIt is very reasonable to assume that more than one driver is going to change at a time. To do that, we can create a grid of values and simulate changes in inputs at the same time"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#monte-carlo-simulations-core-idea",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#monte-carlo-simulations-core-idea",
    "title": "Equity Valuation and Simulation",
    "section": "Monte Carlo Simulations: Core Idea",
    "text": "Monte Carlo Simulations: Core Idea\n\nA Monte Carlo Simulation is a computational technique that uses random sampling to model the distribution of outcomes.\nSteps:\n\nDefine uncertain variables as probability distributions (e.g., normal, lognormal).\nGenerate a large number of random scenarios for these variables.\nCompute the outcome (e.g., firm value) for each scenario.\nAnalyze the distribution of results (e.g., mean, percentiles, risk of loss).\n\n\nSimulated DCF Example\nLet \\(FCF_t \\sim \\mathcal{N}(\\mu_t, \\sigma_t^2)\\), and \\(r \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)\\)\n\\[\n\\text{Firm Value}^{(i)} = \\sum_{t=1}^{T} \\frac{FCF_t^{(i)}}{(1 + r^{(i)})^t}\n\\]\n\nRepeat for \\(i = 1,2,..., N\\) simulations.\nGet a distribution of firm values, not just a point estimate."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#why-monte-carlo",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#why-monte-carlo",
    "title": "Equity Valuation and Simulation",
    "section": "Why Monte Carlo?",
    "text": "Why Monte Carlo?\n\nAccounts for interactions between variables.\nGenerates probabilistic insights:\n\nValue at Risk (VaR)\nConfidence intervals\nProbability of breaching debt covenants\n\nIncreasingly used in:\n\nCapital budgeting\nReal options valuation\nRisk management and credit modeling"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#hands-on-exercise",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#hands-on-exercise",
    "title": "Equity Valuation and Simulation",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\nRefer back to NetScape valuation model that you have worked on prior classes. Using Monte Carlo simulations, you were asked to stress-test such model by analyzing what would have happened to the base scenario as-if some inputs were changed\n\n\n\n\n\n\n\nInstructions\n\n\n\nWe will be using the NetScape case we have worked on previous classes\nYour first task is to translate the valuation model in such a way that you can seamlessly replicate the baseline results\nAfter that, you will be prompted with a series of questions that will require you to simulate \\(N\\) scenarios and analyze the effects on NetScape’s value\n\n\n\n\nGiven that the baseline price, at the time of the valuation, was $28.46, what would be your recommendation for this stock?"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#references",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#references",
    "title": "Equity Valuation and Simulation",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#example",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#example",
    "title": "Equity Valuation and Simulation",
    "section": "Example:",
    "text": "Example:\nIf DCF valuation is:\n$$ = _{t=1}^{T} \nWe can vary \\(r\\) (discount rate) and observe the change in value:\n\n\n\nDiscount Rate (\\(r\\))\nFirm Value (in millions)\n\n\n\n\n\\(6\\%\\)\n\\(\\$125\\)\n\n\n\\(8\\%\\)\n\\(\\$110\\)\n\n\n\\(10\\%\\)\n\\(\\$98\\)\n\n\n\n\nHelps answer: “How sensitive is the valuation to our assumptions?”\nLimitation: One variable at a time; doesn’t capture interactions."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#example-scenarios",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#example-scenarios",
    "title": "Equity Valuation and Simulation",
    "section": "Example Scenarios:",
    "text": "Example Scenarios:\n\n\n\n\n\n\n\n\n\n\nScenario\nRevenue Growth\nDiscount Rate\nTerminal Value\nFirm Value\n\n\n\n\nBest Case\n8%\n6%\nHigh\n$140M\n\n\nBase Case\n5%\n8%\nMedium\n$110M\n\n\nWorst Case\n2%\n10%\nLow\n$85M\n\n\n\n\nHighlights range of possible outcomes under plausible assumptions.\nStill lacks the probabilistic richness of full simulations."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#sensitivity-analysis-example",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#sensitivity-analysis-example",
    "title": "Equity Valuation and Simulation",
    "section": "Sensitivity Analysis, Example",
    "text": "Sensitivity Analysis, Example\n\nTo illustrate the use of Sensitivity Analysis, we can vary \\(r\\), the discount rate, and observe the change in the firm’s value for each distinct \\(r\\):\n\n\n\n\nDiscount Rate (\\(r\\))\nFirm Value (in millions)\n\n\n\n\n\\(6\\%\\)\n\\(\\$125\\)\n\n\n\\(8\\%\\)\n\\(\\$110\\)\n\n\n\\(10\\%\\)\n\\(\\$98\\)\n\n\n\nKey Points on Sensitivity Analysis\n\nOn the one hand, it helps to answer how sensitive is the valuation to some of the assumptions that were used in the model\nOn the other hand, there is a clear limitation: the fact that we are varying one variable at a time doesn’t capture interactions between variables!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#scenario-analysis-continued",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#scenario-analysis-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Scenario Analysis, continued",
    "text": "Scenario Analysis, continued\n\n\n\n\n\n\n\n\n\n\nScenario\nRevenue Growth\nDiscount Rate\nTerminal Value\nFirm Value\n\n\n\n\nBest Case\n\\(8\\%\\)\n\\(6\\%\\)\nHigh\n\\(\\$140M\\)\n\n\nBase Case\n\\(5\\%\\)\n\\(8\\%\\)\nMedium\n\\(\\$110M\\)\n\n\nWorst Case\n\\(2\\%\\)\n\\(10\\%\\)\nLow\n\\(\\$85M\\)\n\n\n\nKey Points on Scenario Analysis\n\nIt highlights a range of possible outcomes under plausible assumptions\nHowever, it still lacks a probability component: we still don’t know how likely each of those scenarios are:\n\n\nBest, Base, and Worst cases are limited in the way that they can produce scenarios because we assume that all variables will go into the same direction\nFurthermore, our realized outcome fallS under a combination of those scenarios"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#from-scenarios-to-simulations",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#from-scenarios-to-simulations",
    "title": "Equity Valuation and Simulation",
    "section": "From Scenarios to Simulations",
    "text": "From Scenarios to Simulations\n\nWhat if we could extend the logit of scenario simulation to \\(N&gt;&gt;3\\) scenarios? To do that, we can use simulation techniques, such as the Monte Carlo simulation:\n\n\n\n\n\n\n\nDefinition:\n\n\nA Monte Carlo Simulation is a computational technique that uses random sampling to model the distribution of outcomes for a given random variable:\n\nWe define the parameters of our simulation as probability distributions (e.g., normal, lognormal).\nGenerate a large number of random scenarios for these variables\nCompute the outcome (e.g., firm value) for each scenario\nAnalyze the distribution of the desired results (e.g., mean, percentiles, risk of loss)\n\n\n\n\n\nWith Monte Carlo simulations, instead of getting a range of potential outcomes for the firm’s value, we actually get an empirical distribution of values!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#simulated-dcf-example",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#simulated-dcf-example",
    "title": "Equity Valuation and Simulation",
    "section": "Simulated DCF Example",
    "text": "Simulated DCF Example\nIn order to see how we can apply Monte Carlo Simulation to assess the distribution of Firm Value, let \\(FCF_t \\sim \\mathcal{N}(\\mu_t, \\sigma_t^2)\\), and \\(r \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)\\)\n\\[\n\\text{Firm Value}^{(i)} = \\sum_{t=1}^{T} \\frac{FCF_t^{(i)}}{(1 + r^{(i)})^t}\n\\]\n\nRepeat for \\(i = 1,2,..., N\\) simulations.\nGet a distribution of firm values**, not just a point estimate"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#monte-carlo-implementation",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#monte-carlo-implementation",
    "title": "Equity Valuation and Simulation",
    "section": "Monte Carlo Implementation",
    "text": "Monte Carlo Implementation\n\nTo see how we can apply Monte Carlo Simulation to assess the distribution of Firm Value, let’s assume that both the free cash flow and the discount rate are random variables:\n\n\\(FCF_t \\sim \\mathcal{N}(\\mu_t, \\sigma_t^2)\\)\n\\(r \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)\\)\n\nIf we know the distribution of those random variables, we can draw \\(N\\) observations for each random variable and use it to calculate the desired outcome (i.e, the firm’s value). For draw \\(i\\), the estimated value of the firm is simply:\n\n\\[\n\\text{Firm Value}^{(i)} = \\sum_{t=1}^{T} \\frac{FCF_t^{(i)}}{(1 + r^{(i)})^t}\n\\]\n\nRepeat for \\(i = 1,2,..., N\\) simulations\nGet a distribution of firm values, not just a point estimate"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#monte-carlo-implementation-1",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#monte-carlo-implementation-1",
    "title": "Equity Valuation and Simulation",
    "section": "Monte Carlo implementation",
    "text": "Monte Carlo implementation\n\nSuppose we are using the Gordon’s Growth Model, where the value of a firm is given by the infinite stream of dividends:\n\n\\[\nV_t= \\dfrac{D_1}{r-g}\n\\]\nwhere \\(D1\\) s the dividend next year, \\(r\\) is the discount rate, and \\(g\\) is the perpetual growth rate. If we assume the following distributions:\n\nSince \\(D1\\) is the next period’s dividend, and it is fixed to \\(\\$2\\)\n\\(r \\sim \\mathcal{N}(0.08,0.01^2)\\)\n\\(g \\sim \\mathcal{N}(0.03,0.005^2)\\)\n\n\nIn what follows, we will simulate \\(10,000\\) distinct scenarios for draws of \\(r\\) and \\(g\\) and see how the \\(V_t\\) distributes over all potential combinations"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#monte-carlo-implementation-continued",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#monte-carlo-implementation-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Monte Carlo Implementation, continued",
    "text": "Monte Carlo Implementation, continued\n\nCodeOutput\n\n\n\n#Set seed for reproducibility\nset.seed(123)\n\n# Parameters\nn_sim &lt;- 10000\nD1 &lt;- 2\n\n# Simulate r and g\nr &lt;- rnorm(n_sim, mean = 0.08, sd = 0.01)\ng &lt;- rnorm(n_sim, mean = 0.03, sd = 0.005)\n\n# Compute values\nvalue &lt;- D1 / (r - g)\n\n# Filter for valid values where r &gt; g\nvalue &lt;- value[is.finite(value) & (r &gt; g)]\n\n# Plot\nlibrary(ggplot2)\nlibrary(scales)\n\nggplot(data.frame(Value = value), aes(x = Value)) +\n  geom_histogram(bins = 60, fill = \"#2c7fb8\", color = \"white\", alpha = 0.8) +\n  scale_x_continuous(labels=scales::dollar)+\n  labs(\n    title = \"Monte Carlo Valuation Distribution (Gordon Growth Model)\",\n    x = \"Firm Value\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size=20)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#step-1-hardcoding-the-assumptions",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#step-1-hardcoding-the-assumptions",
    "title": "Equity Valuation and Simulation",
    "section": "Step 1: Hardcoding the assumptions",
    "text": "Step 1: Hardcoding the assumptions\n\nRecall that our Free Cash Flow estimation in peiod \\(t\\), \\(FCF_t\\), is given by:\n\n\\[\nFCF_t = EBIT\\times (1-\\tau) \\pm \\text{Depreciation} \\pm \\text{CAPEX} \\pm \\Delta NWC\n\\] Where \\(\\tau\\) is the marginal tax-rate, CAPEX is Capital Expenditures, and NWC is Net Working Capital\n\nIn what follows, we will simulate the value of Netscape using a 10-year discounted cash flow (DCF) model with structured assumptions on top of the historical revenue levels from 1995"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#monte-carlo-setup-multi-year-dcf-netscape-example",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#monte-carlo-setup-multi-year-dcf-netscape-example",
    "title": "Equity Valuation and Simulation",
    "section": "Monte Carlo Setup – Multi-Year DCF (Netscape Example)",
    "text": "Monte Carlo Setup – Multi-Year DCF (Netscape Example)\nWe simulate the value of Netscape using a 10-year discounted cash flow (DCF) model with structured assumptions for revenue growth, margins, CapEx, and cost of capital.\n\nBase year revenue (1995): \\(\\$33,250\\) (in thousands)\nRevenue growth: constant \\(65\\%\\) per year\nOperating cost structure (as % of revenue):\n\n\nCost of Sales: \\(10.44\\%\\)\nR&D: \\(36.76\\%\\)\nOther Operating Expenses: decreasing from 80% to 20%\n\n\nCAPEX: decreasing from \\(45\\%\\) to \\(10\\%\\) of revenue\nDepreciation: constant at 5.5% of revenue\nNet Working Capital (NWC): assumed zero\nTax rate: \\(34\\%\\)\nDiscount rate (WACC): \\(r_t = r_f + \\beta \\times \\text{MRP} = 6.71\\% + 1.5 \\cdot 7.5\\% = 17.96\\%\\)\nTerminal growth rate: \\(4\\%\\)\nTerminal discount rate: \\(17.96\\%\\)\nShares outstanding: \\(38,000\\)\n\nThese assumptions will feed into a Monte Carlo engine or deterministic DCF."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#step-1-hardcoding-the-assumptions-continued",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#step-1-hardcoding-the-assumptions-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Step 1: Hardcoding the assumptions, continued",
    "text": "Step 1: Hardcoding the assumptions, continued\n\n#Number of years\nn_years=10\n\n# Assumptions (copied and simplified)\nnetscape_assumptions &lt;- list(\n  revenue_growth = rep(0.65,n_years),\n  cost_of_sales_pct = rep(0.1044,n_years),\n  rd_pct = rep(0.3676,n_years),\n  tax_rate = rep(0.34,n_years),\n  other_op_exp_pct = c(0.80,0.65,0.55,0.45,0.35,0.25,rep(0.2,4)),\n  capex_pct = c(0.45,0.4,0.3,0.2,rep(0.1,6)),\n  nwc_pct = rep(0,n_years),\n  depreciation_pct = rep(0.055,n_years),\n  beta=rep(1.5,n_years),\n  rf=rep(0.0671,n_years),\n  mrp=rep(0.075,n_years),\n  shares_outstanding = 38000,\n  terminal_growth = 0.04,\n  terminal_r=0.1796\n)\n\n# Base year (1995)\nnetscape_base_rev &lt;- 33250"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#step-2-getting-the-forecasted-free-cash-flow",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#step-2-getting-the-forecasted-free-cash-flow",
    "title": "Equity Valuation and Simulation",
    "section": "Step 2: Getting the forecasted Free Cash Flow",
    "text": "Step 2: Getting the forecasted Free Cash Flow\n\nAfter we got our assumptions in place, it is time to put them together and generate the Free Cash Flow values for \\(t=1,2,...,10\\)\nFor that, we will create a function, get_projections, that has two arguments:\n\nassumptions: a named list containing vectors of financial assumptions (growth rates, margins, etc) for each year\nbase: the base year revenue (e.g., from 1995).\n\nWe initialize an empty data frame projection with columns for each financial metric over the 10-year period, and fill out the results using the Free Cash Flow definition\nWith this function, you should be able to replicate the Free Cash Flows from the base scenario by calling get_projection(netscape_assumptions,netscape_base_rev)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#step-2-getting-the-forecasted-free-cash-flow-continued",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#step-2-getting-the-forecasted-free-cash-flow-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Step 2: Getting the forecasted Free Cash Flow, continued",
    "text": "Step 2: Getting the forecasted Free Cash Flow, continued\n\n# Projection over 10 years\nget_projection &lt;- function(assumptions,base){\n  \n  projection=data.frame(\n      Year = 1:n_years,\n      Revenue = NA,\n      EBIT = NA,\n      Taxes = NA,\n      NOPAT = NA,\n      Depreciation = NA,\n      CAPEX = NA,\n      NWC = NA,\n      Delta_NWC = NA,\n      Discount_Rate=NA)\n\n  # Fill in projections for revenue\n  for (t in 1:n_years) {\n    if (t == 1) {\n      projection$Revenue[t] &lt;- base * (1 + assumptions$revenue_growth[t])\n    } else {\n      projection$Revenue[t] &lt;- projection$Revenue[t - 1] * (1 + assumptions$revenue_growth[t])\n    }\n  }\n  \n  #Fill in FCF terms\n  projection$EBIT &lt;- projection$Revenue * (1 - assumptions$cost_of_sales_pct - assumptions$rd_pct - assumptions$other_op_exp_pct - assumptions$depreciation_pct)\n  projection$Taxes &lt;- projection$EBIT * assumptions$tax_rate\n  projection$NOPAT &lt;- projection$EBIT - projection$Taxes\n  projection$Depreciation &lt;- projection$Revenue * assumptions$depreciation_pct\n  projection$CAPEX &lt;- projection$Revenue * assumptions$capex_pct\n  projection$NWC &lt;- projection$Revenue*assumptions$nwc_pct\n  projection$Delta_NWC &lt;- 0\n  projection$Discount_Rate &lt;- 1/(1+(assumptions$rf+assumptions$mrp*assumptions$beta))^projection$Year\n  \n  return(projection)\n}"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-1-changing-revenue-growth",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-1-changing-revenue-growth",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 1: changing revenue growth",
    "text": "Exercise 1: changing revenue growth\n\nCodeOutput\n\n\n\n  #Set seed for reproducibility\n  set.seed(123)\n\n  # Initialize an empty dataframe\n  results &lt;- numeric(0)\n  sim_assumptions &lt;- netscape_assumptions\n  n_sim=10000\n  \n  #Run the simulation\n  for (i in 1:n_sim){\n\n    # Simulate revenue growth for 10 years from a normal distribution\n    rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n    \n    # Modify assumptions with simulated growth\n    sim_assumptions$revenue_growth &lt;- rev_growth\n    \n    # Compute EV for this simulation\n    ev &lt;- get_EV(sim_assumptions,\n                 netscape_base_rev,\n                 sim_assumptions$terminal_r,\n                 sim_assumptions$terminal_growth)\n    \n    results[i] &lt;- ev\n  }\n  \n\n  #Create a ggplot histogram\n  results%&gt;%\n    as.tibble()%&gt;%\n    ggplot(aes(x = value)) +\n    geom_histogram(fill = \"skyblue\", color = \"white\", bins = 50)+\n    scale_x_continuous(labels = scales::dollar) +\n    labs(\n      title = \"Monte Carlo Simulation of Enterprise Value\",\n      subtitle = paste0('Drawing ',comma(n_sim), ' simulations of Revenue Growth.'), \n      x = \"Enterprise Value (USD, Thousands)\",\n      y = \"Frequency\"\n    ) +\n    theme_minimal(base_size = 14)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-2-changing-revenue-growth",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-2-changing-revenue-growth",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 2: changing revenue growth",
    "text": "Exercise 2: changing revenue growth"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-1-changing-revenue-growth-1",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-1-changing-revenue-growth-1",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 1: changing revenue growth",
    "text": "Exercise 1: changing revenue growth"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-1-changing-revenue-growth-2",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-1-changing-revenue-growth-2",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 1: changing revenue growth",
    "text": "Exercise 1: changing revenue growth"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-1-changing-revenue-growth-3",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-1-changing-revenue-growth-3",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 1: changing revenue growth",
    "text": "Exercise 1: changing revenue growth"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-1-changing-revenue-growth-4",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-1-changing-revenue-growth-4",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 1: changing revenue growth",
    "text": "Exercise 1: changing revenue growth"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#valuing-the-company-using-fcf",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#valuing-the-company-using-fcf",
    "title": "Equity Valuation and Simulation",
    "section": "Valuing the company using FCF",
    "text": "Valuing the company using FCF\n\nWe will now create two additional functions:\n\nThe get_dcf() function will calculate the Discounted Cash Flow for the whole period, based on the projections we have just created. It returns a scalar value representing the present value of projected Free Cash Flows\nThe get_terminal_value() will calculate an estimate of the present value of the perpetuity (in Year 0) based on the projections and the assumptions regarding the long-term r and g. ItcComputes the terminal value of the firm beyond the projection horizon using a growing perpetuity\n\nAdding up these two values yields the total value of the firm, \\(V\\), which is embedded in the get_EV() function:\n\n\nget_EV(netscape_assumptions,\n       netscape_base_rev,\n       netscape_assumptions$terminal_r,\n       netscape_assumptions$terminal_growth)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#code-1",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#code-1",
    "title": "Equity Valuation and Simulation",
    "section": "Code",
    "text": "Code\n\n# Projection over 10 years\nget_projection &lt;- function(assumptions,base){\n  \n  projection=data.frame(\n      Year = 1:n_years,\n      Revenue = NA,\n      EBIT = NA,\n      Taxes = NA,\n      NOPAT = NA,\n      Depreciation = NA,\n      CAPEX = NA,\n      NWC = NA,\n      Delta_NWC = NA,\n      Discount_Rate=NA)\n\n  # Fill in projections for revenue\n  for (t in 1:n_years) {\n    if (t == 1) {\n      projection$Revenue[t] &lt;- base * (1 + assumptions$revenue_growth[t])\n    } else {\n      projection$Revenue[t] &lt;- projection$Revenue[t - 1] * (1 + assumptions$revenue_growth[t])\n    }\n  }\n  \n  #Fill in FCF terms\n  projection$EBIT &lt;- projection$Revenue * (1 - assumptions$cost_of_sales_pct - assumptions$rd_pct - assumptions$other_op_exp_pct - assumptions$depreciation_pct)\n  projection$Taxes &lt;- projection$EBIT * assumptions$tax_rate\n  projection$NOPAT &lt;- projection$EBIT - projection$Taxes\n  projection$Depreciation &lt;- projection$Revenue * assumptions$depreciation_pct\n  projection$CAPEX &lt;- projection$Revenue * assumptions$capex_pct\n  projection$NWC &lt;- projection$Revenue*assumptions$nwc_pct\n  projection$Delta_NWC &lt;- 0\n  projection$Discount_Rate &lt;- 1/(1+(assumptions$rf+assumptions$mrp*assumptions$beta))^projection$Year\n  \n  return(projection)\n}\n\nget_projection(netscape_assumptions,netscape_base_rev)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#output-1",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#output-1",
    "title": "Equity Valuation and Simulation",
    "section": "Output",
    "text": "Output\n\n\n   Year    Revenue        EBIT      Taxes      NOPAT Depreciation     CAPEX NWC\n1     1   54862.50  -17940.038  -6099.613 -11840.425     3017.438  24688.12   0\n2     2   90523.12  -16022.593  -5447.682 -10574.911     4978.772  36209.25   0\n3     3  149363.16  -11500.963  -3910.327  -7590.636     8214.974  44808.95   0\n4     4  246449.21    5668.332   1927.233   3741.099    13554.706  49289.84   0\n5     5  406641.19   50016.867  17005.735  33011.132    22365.266  40664.12   0\n6     6  670957.97  149623.627  50872.033  98751.594    36902.688  67095.80   0\n7     7 1107080.65  302233.017 102759.226 199473.791    60889.436 110708.06   0\n8     8 1826683.07  498684.478 169552.722 329131.755   100467.569 182668.31   0\n9     9 3014027.06  822829.388 279761.992 543067.396   165771.488 301402.71   0\n10   10 4973144.65 1357668.491 461607.287 896061.204   273522.956 497314.47   0\n   Delta_NWC Discount_Rate\n1          0     0.8477450\n2          0     0.7186716\n3          0     0.6092502\n4          0     0.5164888\n5          0     0.4378508\n6          0     0.3711859\n7          0     0.3146710\n8          0     0.2667607\n9          0     0.2261451\n10         0     0.1917134"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-2-changing-revenue-growth-and-cogs",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-2-changing-revenue-growth-and-cogs",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 2: Changing Revenue Growth and COGS",
    "text": "Exercise 2: Changing Revenue Growth and COGS\n\nCodeOutput\n\n\n\n  #Set seed for reproducibility\n  set.seed(123)\n\n  # Initialize an empty dataframe\n  results &lt;- numeric(0)\n  sim_assumptions &lt;- netscape_assumptions\n  n_sim=10000\n\n#Run the simulation\nfor (i in 1:n_sim){\n  \n  # Simulate revenue growth for 10 years from a normal distribution\n  rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n  cogs_perc &lt;- rnorm(n_years, mean = 0.10, sd = 0.025)\n  \n  # Modify assumptions with simulated growth\n  sim_assumptions$revenue_growth &lt;- rev_growth\n  sim_assumptions$cost_of_sales_pct &lt;- cogs_perc\n  \n  # Compute EV for this simulation\n  ev &lt;- get_EV(sim_assumptions,\n               netscape_base_rev,\n               sim_assumptions$terminal_r,\n               sim_assumptions$terminal_growth)\n  \n  results[i] &lt;- ev\n}\n\n#Create a ggplot histogram\nresults%&gt;%\n  as.tibble()%&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(fill = \"skyblue\", color = \"white\", bins = 50)+\n  scale_x_continuous(labels = scales::dollar) +\n  labs(\n    title = \"Monte Carlo Simulation of Enterprise Value\",\n    subtitle = paste0('Drawing ',comma(n_sim), ' simulations of Revenue Growth and % COGS.'), \n    x = \"Enterprise Value (USD, Thousands)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 20)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-3-changing-revenue-growth-cogs-and-capex",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-3-changing-revenue-growth-cogs-and-capex",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 3: Changing Revenue Growth, COGS, and CAPEX",
    "text": "Exercise 3: Changing Revenue Growth, COGS, and CAPEX\n\nCodeOutput\n\n\n\n#Set seed for reproducibility\nset.seed(123)\n\n# Initialize an empty dataframe\nresults &lt;- numeric(0)\nsim_assumptions &lt;- netscape_assumptions\nn_sim=10000\n\n#Run the simulation\nfor (i in 1:n_sim){\n  \n  # Simulate revenue growth for 10 years from a normal distribution\n  rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n  cogs_perc &lt;- rnorm(n_years, mean = 0.10, sd = 0.025)\n  capex_perc &lt;- seq(rnorm(1, mean = 0.45, sd = 0.1),to=0.1,length.out = 10)\n  \n  # Modify assumptions with simulated growth\n  sim_assumptions$revenue_growth &lt;- rev_growth\n  sim_assumptions$cost_of_sales_pct &lt;- cogs_perc\n  sim_assumptions$capex_pct &lt;- capex_perc\n  \n  # Compute EV for this simulation\n  ev &lt;- get_EV(sim_assumptions,\n               netscape_base_rev,\n               sim_assumptions$terminal_r,\n               sim_assumptions$terminal_growth)\n  \n  results[i] &lt;- ev\n}\n\n#Create a ggplot histogram\nresults%&gt;%\n  as.tibble()%&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(fill = \"skyblue\", color = \"white\", bins = 50)+\n  scale_x_continuous(labels = scales::dollar) +\n  labs(\n    title = \"Monte Carlo Simulation of Enterprise Value\",\n    subtitle = paste0('Drawing ',comma(n_sim), ' simulations of Revenue Growth, % COGS, and % CAPEX.'), \n    x = \"Enterprise Value (USD, Thousands)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 20)\n\nfinal_results=results"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-4-different-levels-for-terminal-r-and-g",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-4-different-levels-for-terminal-r-and-g",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 4: Different levels for terminal \\(r\\) and \\(g\\)",
    "text": "Exercise 4: Different levels for terminal \\(r\\) and \\(g\\)\n\nCodeOutput\n\n\n\n#Set seed for reproducibility\nset.seed(123)\n\n# Initialize an empty dataframe\nresults &lt;- numeric(0)\nnew_data &lt;- data.frame()\nsim_assumptions &lt;- netscape_assumptions\nn_sim=1000\ng_sequence &lt;- seq(0.05,0.03,length.out=10)\nr_sequence &lt;- seq(0.15,0.225,length.out=10)\n\nfor(s in 1:10){\n  \n  #Run the simulation\n  for (i in 1:n_sim){\n    \n    # Simulate revenue growth for 10 years from a normal distribution\n    rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n    cogs_perc &lt;- rnorm(n_years, mean = 0.10, sd = 0.025)\n    capex_perc &lt;- seq(rnorm(1, mean = 0.45, sd = 0.1),to=0.1,length.out = 10)\n    \n    # Modify assumptions with simulated growth\n    sim_assumptions$revenue_growth &lt;- rev_growth\n    sim_assumptions$cost_of_sales_pct &lt;- cogs_perc\n    sim_assumptions$capex_pct &lt;- capex_perc\n    sim_assumptions$terminal_growth = g_sequence[s]\n    sim_assumptions$terminal_r = r_sequence[s]\n    \n    # Compute EV for this simulation\n    ev &lt;- get_EV(sim_assumptions,\n                 netscape_base_rev,\n                 sim_assumptions$terminal_r,\n                 sim_assumptions$terminal_growth)\n    \n    results[i] &lt;- ev\n  }\n  \n  #Store the Pairs\n  new_data=new_data%&gt;%\n    rbind(data.frame(Scenario = paste0('Scenario ', s),\n                     EV=results))\n          \n  message(paste0('Finished simulation for pair number ',s,'.'))\n}\n\n\n#install.packages('ggridges')\nlibrary(ggridges)\n\n#Create a ggplot histogram\nnew_data%&gt;%\n  mutate(Scenario = factor(Scenario,levels=paste0('Scenario ',10:1)))%&gt;%\n  ggplot(aes(x = EV,\n             y = Scenario,\n             fill=..x..)) +\n  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01)+\n  scale_x_continuous(labels = scales::dollar) +\n  scale_fill_viridis_c(name = \"Enterprise Value\", option = \"plasma\",labels=scales::dollar)+\n  labs(\n    title = \"Monte Carlo Simulation of Enterprise Value\",\n    subtitle = paste0('Drawing ',comma(n_sim), ' simulations, best-to-worse scenarios.'), \n    x = \"\",\n    y = \"Frequency\",\n    fill = \"Enterprise Value (USD, Thousands)\")+\n  theme_minimal(base_size = 14)+\n  theme(legend.position = 'bottom',\n        legend.title = element_text(size = 14),\n        legend.text = element_text(size = 12),\n        legend.key.height = unit(0.6, \"cm\"),\n        legend.key.width = unit(2.5, \"cm\"))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-5-independently-distributed-variables",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-5-independently-distributed-variables",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 5: Independently Distributed Variables",
    "text": "Exercise 5: Independently Distributed Variables\n\nCodeOutput\n\n\n\nindep_results &lt;- numeric(0)\nsim_assumptions &lt;- netscape_assumptions\nn_sim=10000\n\n#Run the simulation\nfor (i in 1:n_sim){\n  \n  # Simulate revenue growth for 10 years from a normal distribution\n  rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n  cogs_perc &lt;- rnorm(n_years, mean = 0.10, sd = 0.025)\n  capex_perc &lt;- seq(rnorm(1, mean = 0.45, sd = 0.1),to=0.1,length.out = 10)\n  r_terminal &lt;- rnorm(1, mean = 0.1796, sd = 0.05)\n  g_terminal &lt;- rnorm(1, mean = 0.03, sd = 0.01)\n  \n  # Modify assumptions with simulated growth\n  sim_assumptions$revenue_growth &lt;- rev_growth\n  sim_assumptions$cost_of_sales_pct &lt;- cogs_perc\n  sim_assumptions$capex_pct &lt;- capex_perc\n  sim_assumptions$terminal_growth = g_terminal\n  sim_assumptions$terminal_r = r_terminal\n  \n  # Compute EV for this simulation\n  ev &lt;- get_EV(sim_assumptions,\n               netscape_base_rev,\n               sim_assumptions$terminal_r,\n               sim_assumptions$terminal_growth)\n  \n  indep_results[i] &lt;- ev\n}\n\n\n#Create a ggplot histogram\nindep_results%&gt;%\n  as.tibble()%&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(fill = \"skyblue\", color = \"white\", bins = 50)+\n  scale_x_continuous(labels = scales::dollar) +\n  labs(\n    title = \"Monte Carlo Simulation of Enterprise Value\",\n    subtitle = paste0('Drawing ',comma(n_sim), ' simulations. Terminal growth and discount assumed to be i.i.d'), \n    x = \"Enterprise Value (USD, Thousands)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 14)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-6-correlated-variables",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-6-correlated-variables",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 6: Correlated Variables",
    "text": "Exercise 6: Correlated Variables\n\nCodeOutput\n\n\n\nlibrary(MASS)\n\n\n# Initialize an empty dataframe\ncorr_results &lt;- numeric(0)\nsim_assumptions &lt;- netscape_assumptions\nn_sim=10000\n\n# Means and standard deviations for r and g\nmu &lt;- c(r = 0.1796, g = 0.03)\nsd_r &lt;- 0.015\nsd_g &lt;- 0.005\nrho &lt;- -0.8  # specified correlation\n\n# Covariance matrix\nsigma &lt;- matrix(c(sd_r^2,\n                  rho * sd_r * sd_g,\n                  rho * sd_r * sd_g,\n                  sd_g^2\n), nrow = 2)\n\n# Simulate 10000 draws\nsim_bivariate &lt;- mvrnorm(n = n_sim, mu = mu, Sigma = sigma)\n\n#Run the simulation\nfor (i in 1:n_sim){\n  \n  # Simulate revenue growth for 10 years from a normal distribution\n  rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n  cogs_perc &lt;- rnorm(n_years, mean = 0.10, sd = 0.025)\n  capex_perc &lt;- seq(rnorm(1, mean = 0.45, sd = 0.1),to=0.1,length.out = 10)\n\n  # Modify assumptions with simulated growth\n  sim_assumptions$revenue_growth &lt;- rev_growth\n  sim_assumptions$cost_of_sales_pct &lt;- cogs_perc\n  sim_assumptions$capex_pct &lt;- capex_perc\n  sim_assumptions$terminal_r = sim_bivariate[i,1]\n  sim_assumptions$terminal_growth = sim_bivariate[i,2]\n  \n  # Compute EV for this simulation\n  ev &lt;- get_EV(sim_assumptions,\n               netscape_base_rev,\n               sim_assumptions$terminal_r,\n               sim_assumptions$terminal_growth)\n  \n  corr_results[i] &lt;- ev\n}\n\n#Chart\nrbind(data.frame(Model='i.i.d',Values=indep_results),\n      data.frame(Model='Bivariate Normal',Values=corr_results))%&gt;%\n  ggplot(aes(x = Values,color=Model)) +\n  geom_density(aes(y=after_stat(count)),size=1)+\n  scale_x_continuous(labels = scales::dollar) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    title = \"Monte Carlo Simulation of Enterprise Value\",\n    subtitle = paste0('Drawing ',comma(n_sim), ' simulations. Terminal growth and discount taken from a bivariate normal distribution with rho = -0.8'), \n    x = \"Enterprise Value (USD, Thousands)\",\n    y = \"Frequency\")+\n  theme_minimal(base_size = 14)+\n  theme(legend.position='bottom')"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#final-question-should-we-invest",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#final-question-should-we-invest",
    "title": "Equity Valuation and Simulation",
    "section": "Final Question: should we invest?",
    "text": "Final Question: should we invest?\n\nCodeOutput\n\n\n\n#Create a ggplot histogram\nprices=final_results%&gt;%\n  as.tibble()%&gt;%\n  mutate(Price=value/netscape_assumptions$shares_outstanding)%&gt;%\n  mutate(Situation=ifelse(Price&gt;=28.37,'Undervalued','Overvalued'))\n\nprices%&gt;%\n  ggplot(aes(x = Price)) +\n  geom_histogram(aes(fill = Situation), bins = 50)+\n  scale_x_continuous(labels = scales::dollar)+\n  scale_fill_manual(values=c('darkgreen','darkred'))+\n  geom_vline(xintercept=28.37,linetype='dashed',size=1)+\n  annotate(geom='text',x=40,y=500,label= paste0('True Price is higher than \\nbaseline price only in ',\n                                                percent(mean(prices$Price&gt;=28.37)),\n                                                ' of the cases!'))+\n  labs(\n    title = \"Monte Carlo Simulation of Share Price\",\n    subtitle = paste0('Drawing 10,000 simulations'), \n    x = \"Enterprise Value (USD, Thousands)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 20)+\n  theme(legend.position='bottom')"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#valuing-the-company-using-fcf-continued",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#valuing-the-company-using-fcf-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Valuing the company using FCF, continued",
    "text": "Valuing the company using FCF, continued\n\nget_dcf &lt;- function(projection){\n  \n  dcf=projection%&gt;%\n    mutate(FCF = NOPAT + Depreciation - CAPEX - Delta_NWC)%&gt;%\n    reframe(DCF=FCF*Discount_Rate)%&gt;%\n    pull(DCF)%&gt;%\n    sum()\n  \n  return(dcf)\n  \n}\n\nget_terminal_value &lt;- function(projection,r,g){\n\n  last_fcf=projection%&gt;%\n    mutate(FCF = NOPAT + Depreciation - CAPEX - Delta_NWC)%&gt;%\n    pull(FCF)%&gt;%\n    tail(1)\n  \n  return(last_fcf*(1+g)/(r-g)/(1+r)^(n_years+1))\n}\n\nget_EV &lt;-function(assumptions,base,r,g){\n\n  PV_FCF = get_projection(assumptions,base)%&gt;%get_dcf()\n  PV_Terminal = get_projection(assumptions,base)%&gt;%get_terminal_value(r,g)\n  \n  return(PV_FCF+PV_Terminal)\n\n  }"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-6-independent-versus-correlated-variables",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#exercise-6-independent-versus-correlated-variables",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 6: Independent versus Correlated Variables",
    "text": "Exercise 6: Independent versus Correlated Variables\n\nCodeOutput\n\n\n\nlibrary(MASS)\n\n#Set seed for reproducibility\nset.seed(123)\n\n# Initialize an empty dataframe\nindep_results &lt;- numeric(0)\ncorr_results &lt;- numeric(0)\nn_sim=10000\n\n# Means and standard deviations for r and g\nmu &lt;- c(r = 0.1796, g = 0.03)\nsd_r &lt;- 0.025\nsd_g &lt;- 0.001\n\n#Case 1: i.i.d variables\nsim_assumptions &lt;- netscape_assumptions\nrho &lt;- 0  # specified correlation of zero\n\n# Covariance matrix\nsigma &lt;- matrix(c(sd_r^2,\n                  rho * sd_r * sd_g,\n                  rho * sd_r * sd_g,\n                  sd_g^2\n), nrow = 2)\n\n# Simulate 10000 draws\nsim_bivariate &lt;- mvrnorm(n = n_sim, mu = mu, Sigma = sigma)\n\n#Run the simulation for independent variables\nfor (i in 1:n_sim){\n  \n  # Simulate revenue growth for 10 years from a normal distribution\n  rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n  cogs_perc &lt;- rnorm(n_years, mean = 0.10, sd = 0.025)\n  capex_perc &lt;- seq(rnorm(1, mean = 0.45, sd = 0.1),to=0.1,length.out = 10)\n\n  # Modify assumptions with simulated growth\n  sim_assumptions$revenue_growth &lt;- rev_growth\n  sim_assumptions$cost_of_sales_pct &lt;- cogs_perc\n  sim_assumptions$capex_pct &lt;- capex_perc\n  sim_assumptions$terminal_r = sim_bivariate[i,1]\n  sim_assumptions$terminal_growth = sim_bivariate[i,2]\n  \n  # Compute EV for this simulation\n  ev &lt;- get_EV(sim_assumptions,\n               netscape_base_rev,\n               sim_assumptions$terminal_r,\n               sim_assumptions$terminal_growth)\n  \n  indep_results[i] &lt;- ev\n}\n\n#Case 2: correlated variables\n\nsim_assumptions &lt;- netscape_assumptions\nrho &lt;- -0.5\n\n# Covariance matrix\nsigma &lt;- matrix(c(sd_r^2,\n                  rho * sd_r * sd_g,\n                  rho * sd_r * sd_g,\n                  sd_g^2\n), nrow = 2)\n\n# Simulate 10000 draws\nsim_bivariate &lt;- mvrnorm(n = n_sim, mu = mu, Sigma = sigma)\n\n#Run the simulation for independent variables\nfor (i in 1:n_sim){\n  \n  # Simulate revenue growth for 10 years from a normal distribution\n  rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n  cogs_perc &lt;- rnorm(n_years, mean = 0.10, sd = 0.025)\n  capex_perc &lt;- seq(rnorm(1, mean = 0.45, sd = 0.1),to=0.1,length.out = 10)\n\n  # Modify assumptions with simulated growth\n  sim_assumptions$revenue_growth &lt;- rev_growth\n  sim_assumptions$cost_of_sales_pct &lt;- cogs_perc\n  sim_assumptions$capex_pct &lt;- capex_perc\n  sim_assumptions$terminal_r = sim_bivariate[i,1]\n  sim_assumptions$terminal_growth = sim_bivariate[i,2]\n  \n  # Compute EV for this simulation\n  ev &lt;- get_EV(sim_assumptions,\n               netscape_base_rev,\n               sim_assumptions$terminal_r,\n               sim_assumptions$terminal_growth)\n  \n  corr_results[i] &lt;- ev\n}\n\n#Chart\nrbind(data.frame(Model='i.i.d',Values=indep_results),\n      data.frame(Model='Bivariate Normal',Values=corr_results))%&gt;%\n  ggplot(aes(x = Values,fill=Model)) +\n  geom_density(aes(y=after_stat(count)),position='identity',alpha=0.5)+\n  scale_x_continuous(labels = scales::dollar) +\n  scale_y_continuous(labels = scales::comma)+\n  labs(\n    title = \"Monte Carlo Simulation of Enterprise Value\",\n    subtitle = paste0('Drawing ',comma(n_sim), ' simulations. Terminal growth and discount taken from a bivariate normal distribution with rho = -0.8'), \n    x = \"Enterprise Value (USD, Thousands)\",\n    y = \"Frequency\")+\n  theme_minimal(base_size = 14)+\n  theme(legend.position='bottom')"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#final-thoughts",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#final-thoughts",
    "title": "Equity Valuation and Simulation",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\nMonte Carlo Simulation is a powerful technique to handle model uncertainty:\n\nIt helps managers think beyond the point estimate of the valuation model and handle what-if questions\nIt also helps stress-testing the valuation model\n\nIt should be used along with business logic to help tailor the simulation exercise:\n\nWe don’t want to create overly complex simulations\nWe can incorporate more structure to create meaningful scenarios - for example, imposing a correlation structure between the variables"
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#disclaimer",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#disclaimer",
    "title": "Course Introduction",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\n\n\n\n\nDisclaimer\n\n\nThe information presented in this lecture is for educational and informational purposes only and should not be construed as investment advice. Nothing discussed constitutes a recommendation to buy, sell, or hold any financial instrument or security. Investment decisions should be made based on individual research and consultation with a qualified financial professional. The presenter assumes no responsibility for any financial decisions made based on this content.\nAll publicly available content used in this lecture is available and also shared on my GitHub page. Participants are encouraged to review, modify, and use it for their own learning and research purposes. However, no guarantees are made regarding the accuracy, completeness, or suitability of the code for any specific application.\nFor any questions or concerns, please feel free to reach out via email at lucas.macoris@fgv.br"
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#references",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#references",
    "title": "Course Introduction",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ."
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#welcome-to-the-course",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#welcome-to-the-course",
    "title": "Course Introduction",
    "section": "Welcome to the Course",
    "text": "Welcome to the Course\n\nOverview and Course Organization\nGrading and Evaluations\nNavigating through the syllabus\nHow you can get the best of this course\nOverall Q&A\nIntroduction to Corporate Finance"
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#overview-and-course-organization",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#overview-and-course-organization",
    "title": "Course Introduction",
    "section": "Overview and course organization",
    "text": "Overview and course organization\n\nThis is a hands-on, applied course on Valuation designed for professional master’s students in management. Throughout the course, students will:\n\nExplore the core concepts and methodologies used in valuation\nDevelop the ability to critically analyze corporate performance, investment opportunities, and strategic decisions\n\nBasic text-book\n\nWe will follow Corporate Finance (Berk and DeMarzo 2023), as our text-book\nHarvard Business Review (selected cases) - access granted on e-Class®\n\nSupplementary Reading\n\nSelected academic papers"
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#grading-and-evaluations",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#grading-and-evaluations",
    "title": "Course Introduction",
    "section": "Grading and Evaluations",
    "text": "Grading and Evaluations\n\nGrading will be composed of the following activities:\n\nCapstone Project (Excel format) (50%)\nCapstone Project Presentation (PowerPoint format) (30%)\nTake-home Deliverables (10%)\nActive In-class Participation (10%)\n\n\nYou can find the details of any of these activities in the official syllabus (available on eClass)\nIn case of any questions, feel free to reach out to lucas.macoris@fgv.br\n\n\n\n\n\n\n\n\n\nOffice-hours\n\n\nI also host office-hours (by appointment) on Thursdays, 5PM-6PM. In these sessions, I’ll be more than happy to help you with anything you need from this course. Use the Office-hour Appointments link at the bottom of this slide to schedule some time (or click here)."
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#getting-the-best-of-this-course",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#getting-the-best-of-this-course",
    "title": "Course Introduction",
    "section": "Getting the best of this course",
    "text": "Getting the best of this course"
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#getting-the-best-of-this-course-1",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#getting-the-best-of-this-course-1",
    "title": "Course Introduction",
    "section": "Getting the best of this course",
    "text": "Getting the best of this course\n\nHow you can get the best of this course\n\nFollow FGV-EAESP code of conduct\nBe organized: pay attention to pre-readings and deliverables!\nBe proactive: ask questions and participate in discussions, proactively study the mandatory reading\nTake the lead on your learning: you are the ultimate responsible for your success!\n\n\n\nHow the professor can facilitate you getting the best of this course\n\nAll mandatory content will be provided ahead of time\nProvide a safe and open space for questions, both in-person and remote\nMotivate students, both from the academic and practitioner standpoints\nProvide opportunities to extend knowledge beyond the mandatory readings"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#extensions",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#extensions",
    "title": "Equity Valuation and Simulation",
    "section": "Extensions",
    "text": "Extensions\nQuestion: how can we improve our simulation exercise?\n\nIn what follows, we’ll take a look at several ways by which you could extend your simulation coding exercise to cope with real-world features. To illustrate, we will adopt a simple valuation approach of the following form:\n\n\\[\n\\small V_i = \\sum_{t=1}^{t=5}\\dfrac{FCF_t}{(1+r)^t}+ PV_0\\bigg(\\dfrac{FCF_5\\times(1+g)}{r-g}\\bigg)\n\\]\n\nIn words, we are breaking down the firms value into:\n\nA 1-5 year horizon\nA growing perpetuity, discounted back to Year 0\n\nFrom that, we can extend the simulation routines to allow for important features that are present in real-world data"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#base-case-simulation",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#base-case-simulation",
    "title": "Equity Valuation and Simulation",
    "section": "Base Case Simulation",
    "text": "Base Case Simulation\n\nOutlineCodeOutput\n\n\n\nThe base case denotes the Free Cash Flow in period \\(t\\) as:\n\n\\[\nFCF_t = Sales_t\\times(1-COGS) \\pm CAPEX \\pm \\Delta NWC\n\\]\n\nInitial Sales levels are \\(\\$100\\)\nThe percent of Cost of Goods Sold (COGS) is \\(60\\%\\)\nCapital Expenditures (CAPEX) is \\(15\\%\\) of Sales\nNet Working Capital investment is \\(5%\\) of sales\n\n\n\n\nlibrary(glue)\nlibrary(MASS)\n\nset.seed(123)\nn_sim &lt;- 10000\n\n# Parameters\ninitial_sales &lt;- 1000\ncogs_pct &lt;- 0.6\ncapex_pct &lt;- 0.15\nnwc_pct &lt;- 0.05\n\nsimulate_values &lt;- function(r, g, label='All') {\n  firm_value &lt;- numeric(n_sim)\n  \n  for (i in 1:n_sim) {\n    sales &lt;- initial_sales\n    nwc_last &lt;- sales * nwc_pct\n    fa_last &lt;- sales * capex_pct\n    fcf &lt;- numeric(5)\n    \n    for (t in 1:5) {\n      sales &lt;- sales * (1 + g[i])\n      nwc_current &lt;- sales * nwc_pct\n      fa_current &lt;- sales * capex_pct\n      \n      delta_nwc &lt;- nwc_current - nwc_last\n      delta_capex &lt;- fa_current - fa_last\n      \n      nwc_last &lt;- nwc_current\n      fa_last &lt;- fa_current\n      \n      fcf[t] &lt;- sales * (1 - cogs_pct) - delta_capex - delta_nwc\n    }\n    \n    fcf_terminal &lt;- fcf[5] * (1 + g[i])\n    perp_value &lt;- fcf_terminal / (r[i] - g[i])\n    \n    discounted_fcf &lt;- sum(fcf / (1 + r[i])^(1:5))\n    discounted_perp &lt;- perp_value / (1 + r[i])^5\n    \n    firm_value[i] &lt;- discounted_fcf + discounted_perp\n  }\n  \n  firm_value &lt;- firm_value[is.finite(firm_value) & (r &gt; g)]\n  \n  data.frame(Value = firm_value, Scenario = label)\n}\n\n# Normal distributions\nr_norm &lt;- rnorm(n_sim, mean = 0.08, sd = 0.01)\ng_norm &lt;- rnorm(n_sim, mean = 0.03, sd = 0.005)\ndf_norm &lt;- simulate_values(r_norm, g_norm, \"Normal\")\n\nbase_norm &lt;- simulate_values(r_norm, g_norm, \"Normal\")\n\nggplot(base_norm, aes(x = Value)) +\n  geom_histogram(bins = 60, fill = \"#2c7fb8\", color = \"white\", alpha = 0.8)+\n  scale_x_continuous(labels = dollar) +\n  labs(\n    title = \"Monte Carlo Valuation Distribution Normal vs. Alternative Distributions\",\n    subtitle = glue('Based on {comma(n_sim)} simulations.'),\n    x = \"Firm Value\",\n    y = \"Frequency\",\n    fill = \"Scenario\",\n    color = \"Scenario\"\n  ) +\n  theme_minimal(base_size = 20)+\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#extension-1-alternative-distributions",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#extension-1-alternative-distributions",
    "title": "Equity Valuation and Simulation",
    "section": "Extension 1: Alternative Distributions",
    "text": "Extension 1: Alternative Distributions\n\nOutlineCodeOutput\n\n\n\nKey Point: Monte Carlo doesn’t assume normality — any distribution that fits the problem can be used!\n\nWhile normal distributions are common, other distributions (e.g., uniform, log-normal, etc.) can also be used:\nFor example, if \\(r \\sim \\text{Uniform}(0.05, 0.10)\\), the growth rate could be chosen randomly from a uniform distribution between \\(5\\%\\) and \\(10\\%\\)\n\nDistributions like the log-normal may be more appropriate for modeling returns, as they respect the non-negativity constraint - $r $. Therefore, for a given variable \\(X\\), it could be sampled from any distribution\nIn what follows, we’ll simulate results drawing from an Uniform distribution for \\(r\\) and a Beta for \\(g\\)\n\n\n\n\n# Baseline case\nr_norm &lt;- rnorm(n_sim, mean = 0.08, sd = 0.01)\ng_norm &lt;- rnorm(n_sim, mean = 0.03, sd = 0.005)\ndf_norm &lt;- simulate_values(r_norm, g_norm, \"Normal\")\n\n\n# Alternative distributions\nr_unif &lt;- runif(n_sim, min = 0.06, max = 0.10)\ng_beta &lt;- 0.06 * rbeta(n_sim, 2, 5)\ndf_alt &lt;- simulate_values(r_unif, g_beta, \"Alternative\")\n\n# Combine results\ndf_all &lt;- rbind(df_norm, df_alt)\n\n# Plot combined chart\n\nggplot(df_all, aes(x = Value, fill = Scenario, color = Scenario)) +\n  geom_density(alpha = 0.5, size = 1) +\n  scale_x_continuous(labels = dollar) +\n  scale_fill_manual(values = c(\"Normal\" = \"#2c7fb8\", \"Alternative\" = \"#f03b20\")) +\n  scale_color_manual(values = c(\"Normal\" = \"#2c7fb8\", \"Alternative\" = \"#f03b20\")) +\n  labs(\n    title = \"Monte Carlo Valuation Distribution Normal vs. Alternative Distributions\",\n    subtitle = glue('Based on {comma(n_sim)} simulations.'),\n    x = \"Firm Value\",\n    y = \"Density\",\n    fill = \"Scenario\",\n    color = \"Scenario\"\n  ) +\n  theme_minimal(base_size = 20)+\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#extension-2-correlated-variables",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#extension-2-correlated-variables",
    "title": "Equity Valuation and Simulation",
    "section": "Extension 2: Correlated Variables",
    "text": "Extension 2: Correlated Variables\n\nOutlineCodeOutput\n\n\n\nIn the baseline case, we have each random variable sampled independently from a normal distribution (i.e, i.i.d sampling). This can lead to higher variability and wider spread in the results that are unlikely to happen\nBy introducing a correlation structure between variables, we can constrain the possible outcomes, narrowing the distribution, leading to more realistic and stable simulations where changes in one variable influence the others\nIn what follows, we will keep \\(r\\) and \\(g\\) mean and standard deviation, but will now assume that the correlation between these two variables is \\(\\rho=0.9\\):\n\nWhenever \\(g\\) is high, it might be because the firm is in its earlier stages, so \\(r\\) should be higher\nWhenever \\(g\\) is low, firm may have reached its market peak, so \\(r\\) should be lower\n\n\n\n\n\nr_norm &lt;- rnorm(n_sim, mean = 0.08, sd = 0.01)\ng_norm &lt;- rnorm(n_sim, mean = 0.03, sd = 0.005)\ndf_norm &lt;- simulate_values(r_norm, g_norm, \"Uncorrelated\")\n\n# Simulate r and g with negative correlation\nmu_r &lt;- 0.08\nmu_g &lt;- 0.03\nsigma_r &lt;- 0.01\nsigma_g &lt;- 0.005\ncorr_rg &lt;- 0.9\n\n# Covariance matrix\ncov_matrix &lt;- matrix(c(sigma_r^2, corr_rg * sigma_r * sigma_g, corr_rg * sigma_r * sigma_g, sigma_g^2), \n                     nrow = 2, ncol = 2)\n\n# Simulate r and g from the bivariate normal distribution\nr_g_correlated &lt;- mvrnorm(n_sim, mu = c(mu_r, mu_g), Sigma = cov_matrix)\nr_corr &lt;- r_g_correlated[, 1]\ng_corr &lt;- r_g_correlated[, 2]\ndf_corr &lt;- simulate_values(r_corr, g_corr, \"Correlated\")\n\n# Combine results\ndf_all &lt;- rbind(df_norm, df_corr)\n\nggplot(df_all, aes(x = Value, fill = Scenario, color = Scenario)) +\n  geom_density(alpha = 0.6, size = 1.5) +\n  scale_x_continuous(labels = dollar, limits = c(0, 20000)) +  # Set x-axis limits\n  scale_fill_manual(values = c(\"Uncorrelated\" = \"#2c7fb8\", \"Correlated\" = \"#f03b20\")) +\n  scale_color_manual(values = c(\"Uncorrelated\" = \"#2c7fb8\", \"Correlated\" = \"#f03b20\")) +\n  labs(\n    title = \"Monte Carlo Valuation Distribution Uncorrelated vs. Strong Correlation\",\n    x = \"Firm Value\",\n    y = \"Density\",\n    fill = \"Scenario\",\n    color = \"Scenario\"\n  ) +\n  theme_minimal(base_size = 20)+\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#extension-3-varying-the-sample-size",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#extension-3-varying-the-sample-size",
    "title": "Equity Valuation and Simulation",
    "section": "Extension 3: Varying the Sample Size",
    "text": "Extension 3: Varying the Sample Size\n\nOutlineCodeOutput\n\n\n\nIn short, your Monte Carlo simulation outcome will depend on the number of simulations: the more simulations we run, the more stable and reliable the estimates become.\n\nWith smaller samples, (e.g., \\(n&lt;100\\) or \\(n&lt;1,000\\), results show high variability and may not capture the true distribution.\nWith larger samples, as \\(n\\) (e.g., \\(100,000\\) or \\(1,000,000\\)), the estimates converge toward the expected value\n\nAll in all, increasing sample size reduces random noise, improving the precision and the stability of simulation results\n\n\n\n\n# Simulation sizes\nn_sim_values &lt;- c(100, 1000, 5000, 10000, 100000, 1000000)\n\n# Store all results\nall_results &lt;- data.frame()\n\n# Run simulations\n\nfor (n in n_sim_values) {\n  r_norm &lt;- rnorm(n, mean = 0.08, sd = 0.01)\n  g_norm &lt;- rnorm(n, mean = 0.03, sd = 0.005)\n  \n  df &lt;- simulate_values(r_norm, g_norm, comma(n))\n  all_results &lt;- bind_rows(all_results, df)\n}\n\nall_results$Scenario &lt;-factor(all_results$Scenario,levels=comma(n_sim_values))\n\n# Plot histograms overlaid in one chart\nggplot(all_results, aes(x = Value, fill = Scenario)) +\n  geom_histogram(aes(y=..density..),bins = 60, color = \"white\") +\n  geom_density(size=0.5,fill=NA)+\n  scale_x_continuous(labels = scales::dollar,limits=c(0,25000)) +\n  facet_wrap(Scenario~.,scales='free')+\n  labs(\n    title = \"Monte Carlo Valuation Distribution by Number of Simulations\",\n    subtitle = \"Varying the sample size for draws\",\n    x = \"Firm Value\",\n    y = \"Density\",\n    fill = \"Simulation Size\"\n  ) +\n  theme_minimal(base_size = 15) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#allow-for-bootstraping",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#allow-for-bootstraping",
    "title": "Equity Valuation and Simulation",
    "section": "Allow for bootstraping",
    "text": "Allow for bootstraping\n\nOutlineCodeCode\n\n\n\nBootstraping is a resampling technique that draws samples with replacement from observed data\n\nIt is useful to incorporate empirical variability without assuming a specific parametric distribution\nHelps capture the uncertainty present in historical or observed data\n\nGiven a sample \\(x_1, x_2, \\dots, x_N\\), draw \\(B\\) bootstrap samples:\n\n\\[\n    x_1^*, x_2^*, \\dots, x_m^* \\quad \\text{with replacement from} \\quad x_1, \\dots, x_N\n\\] - Compute the statistic of interest (e.g., mean, median) on each bootstrap sample\n\nBootstrapping can be used, for example, if we have a very limited series of historical values for our random values (e.g, \\(n=20\\)) but want to incorporate variability into the analysis\n\n\n\n\n#Number of simulations\nn_sim=10000\n\n# Historical series\nhistorical_cogs &lt;- c(0.58, 0.6, 0.62, 0.59, 0.61,0.75,0.54,0.66,0.29,0.78)\nhistorical_capex &lt;- c(0.48, 0.5, 0.52, 0.49, 0.51,0.40,0.52,0.59,0.58,0.81)\n\n# For each simulation, bootstrap 10 values and take the mean → length n_sim vector\nboot_cogs_pct &lt;- replicate(n_sim, mean(sample(historical_cogs, 10, replace = TRUE)))\nboot_capex_pct &lt;- replicate(n_sim, mean(sample(historical_capex, 10, replace = TRUE)))\n\nsimulate_values &lt;- function(r, g, cogs_pct, capex_pct, label = 'Simulation') {\n  firm_value &lt;- numeric(n_sim)\n  \n  for (i in 1:n_sim) {\n    sales &lt;- initial_sales\n    nwc_last &lt;- sales * nwc_pct\n    fa_last &lt;- sales * capex_pct[i]\n    fcf &lt;- numeric(5)\n    \n    for (t in 1:5) {\n      sales &lt;- sales * (1 + g[i])\n      nwc_current &lt;- sales * nwc_pct\n      fa_current &lt;- sales * capex_pct[i]\n      \n      delta_nwc &lt;- nwc_current - nwc_last\n      delta_capex &lt;- fa_current - fa_last\n      \n      nwc_last &lt;- nwc_current\n      fa_last &lt;- fa_current\n      \n      fcf[t] &lt;- sales * (1 - cogs_pct[i]) - delta_capex - delta_nwc\n    }\n    \n    fcf_terminal &lt;- fcf[5] * (1 + g[i])\n    perp_value &lt;- fcf_terminal / (r[i] - g[i])\n    \n    discounted_fcf &lt;- sum(fcf / (1 + r[i])^(1:5))\n    discounted_perp &lt;- perp_value / (1 + r[i])^5\n    \n    firm_value[i] &lt;- discounted_fcf + discounted_perp\n  }\n  \n  firm_value &lt;- firm_value[is.finite(firm_value) & (r &gt; g)]\n  \n  data.frame(Value = firm_value, Scenario = label)\n}\n\n\n# Normal scenario (fixed percentages for COGS and CAPEX)\ndf_norm &lt;- simulate_values(r_norm, g_norm, cogs_pct = rep(0.6,n_sim), capex_pct = rep(0.5,n_sim), label = \"Normal\")\n\n# Bootstrap scenario (bootstrapped mean percentages)\ndf_bootstrap &lt;- simulate_values(r_norm, g_norm, cogs_pct = boot_cogs_pct, capex_pct = boot_capex_pct, label = \"Bootstrap\")\n\n#Plot\nggplot() +\n  geom_density(data = df_norm, aes(x = Value, fill = \"Normal\"), alpha = 0.5) +\n  geom_density(data = df_bootstrap, aes(x = Value, fill = \"Bootstrap\"), alpha = 0.5) +\n  scale_x_continuous(labels = dollar)+\n  labs(\n    title = \"Monte Carlo Valuation Distribution: Normal vs. Bootstrap\",\n    x = \"Firm Value\",\n    y = \"Frequency\",\n    fill = 'Approach',\n    color = \"Scenario\"\n  ) +\n  theme_minimal(base_size = 20) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#overall-thoughts-on-monte-carlo-simulation",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#overall-thoughts-on-monte-carlo-simulation",
    "title": "Equity Valuation and Simulation",
    "section": "Overall thoughts on Monte Carlo Simulation",
    "text": "Overall thoughts on Monte Carlo Simulation\n\nMonte Carlo Simulation is a powerful technique to handle model uncertainty:\n\nIt helps managers think beyond the point estimate of the valuation model and handle what-if questions\nIt also helps stress-testing the valuation model\n\nIt should be used along with business logic to help tailor the simulation exercise:\n\nWe don’t want to create overly complex simulations\nWe can incorporate more structure to create meaningful scenarios - for example, imposing a correlation structure between the variables\n\nBy setting up proper simulation parameters (number of simulations, the distributions and its parameters, etc), we can leverage the role of uncertainty in valuation models in a much more pronounced way and guide decision-making!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#extension-4-bootstraping",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#extension-4-bootstraping",
    "title": "Equity Valuation and Simulation",
    "section": "Extension 4: Bootstraping",
    "text": "Extension 4: Bootstraping\n\nOutlineCodeOutput\n\n\n\nBootstraping is a resampling technique that draws samples with replacement from observed data\n\nIt is useful to incorporate empirical variability without assuming a specific parametric distribution\nHelps capture the uncertainty present in historical or observed data\n\nGiven a sample \\(x_1, x_2, \\dots, x_N\\), draw \\(B\\) bootstrap samples, you draw a given number of samples with replacement and compute the statistic of interest (e.g., mean, median) on each bootstrap sample\nBootstrapping can be used, for example, if we have a very limited series of historical values for our random values (e.g, \\(n=20\\)) but want to incorporate variability into the analysis\n\n\n\n\n#Number of simulations\nn_sim=10000\n\n# Historical series\nhistorical_cogs &lt;- c(0.58, 0.6, 0.62, 0.59, 0.61,0.75,0.54,0.66,0.29,0.78)\nhistorical_capex &lt;- c(0.48, 0.5, 0.52, 0.49, 0.51,0.40,0.52,0.59,0.58,0.81)\n\n# For each simulation, bootstrap 10 values and take the mean → length n_sim vector\nboot_cogs_pct &lt;- replicate(n_sim, mean(sample(historical_cogs, 10, replace = TRUE)))\nboot_capex_pct &lt;- replicate(n_sim, mean(sample(historical_capex, 10, replace = TRUE)))\n\nsimulate_values &lt;- function(r, g, cogs_pct, capex_pct, label = 'Simulation') {\n  firm_value &lt;- numeric(n_sim)\n  \n  for (i in 1:n_sim) {\n    sales &lt;- initial_sales\n    nwc_last &lt;- sales * nwc_pct\n    fa_last &lt;- sales * capex_pct[i]\n    fcf &lt;- numeric(5)\n    \n    for (t in 1:5) {\n      sales &lt;- sales * (1 + g[i])\n      nwc_current &lt;- sales * nwc_pct\n      fa_current &lt;- sales * capex_pct[i]\n      \n      delta_nwc &lt;- nwc_current - nwc_last\n      delta_capex &lt;- fa_current - fa_last\n      \n      nwc_last &lt;- nwc_current\n      fa_last &lt;- fa_current\n      \n      fcf[t] &lt;- sales * (1 - cogs_pct[i]) - delta_capex - delta_nwc\n    }\n    \n    fcf_terminal &lt;- fcf[5] * (1 + g[i])\n    perp_value &lt;- fcf_terminal / (r[i] - g[i])\n    \n    discounted_fcf &lt;- sum(fcf / (1 + r[i])^(1:5))\n    discounted_perp &lt;- perp_value / (1 + r[i])^5\n    \n    firm_value[i] &lt;- discounted_fcf + discounted_perp\n  }\n  \n  firm_value &lt;- firm_value[is.finite(firm_value) & (r &gt; g)]\n  \n  data.frame(Value = firm_value, Scenario = label)\n}\n\n\n# Normal scenario (fixed percentages for COGS and CAPEX)\ndf_norm &lt;- simulate_values(r_norm, g_norm, cogs_pct = rep(0.6,n_sim), capex_pct = rep(0.5,n_sim), label = \"Normal\")\n\n# Bootstrap scenario (bootstrapped mean percentages)\ndf_bootstrap &lt;- simulate_values(r_norm, g_norm, cogs_pct = boot_cogs_pct, capex_pct = boot_capex_pct, label = \"Bootstrap\")\n\n#Plot\nggplot() +\n  geom_density(data = df_norm, aes(x = Value, fill = \"Normal\"), alpha = 0.5) +\n  geom_density(data = df_bootstrap, aes(x = Value, fill = \"Bootstrap\"), alpha = 0.5) +\n  scale_x_continuous(labels = dollar)+\n  labs(\n    title = \"Monte Carlo Valuation Distribution: Normal vs. Bootstrap\",\n    x = \"Firm Value\",\n    y = \"Frequency\",\n    fill = 'Approach',\n    color = \"Scenario\"\n  ) +\n  theme_minimal(base_size = 20) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#extension-4-bootstrapping",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valution and Simulation/index.html#extension-4-bootstrapping",
    "title": "Equity Valuation and Simulation",
    "section": "Extension 4: Bootstrapping",
    "text": "Extension 4: Bootstrapping\n\nOutlineCodeOutput\n\n\n\nBootstrapping is a resampling technique that draws samples with replacement from observed data\n\nIt is useful to incorporate empirical variability without assuming a specific parametric distribution\nHelps capture the uncertainty present in historical or observed data\n\nGiven a sample \\(x_1, x_2, \\dots, x_N\\), draw \\(B\\) bootstrap samples, you draw a given number of samples with replacement and compute the statistic of interest (e.g., mean, median) on each bootstrap sample\nBootstrapping can be used, for example, if we have a very limited series of historical values for our random values (e.g, \\(n=20\\)) but want to incorporate variability into the analysis\n\n\n\n\n#Number of simulations\nn_sim=10000\n\n# Historical series\nhistorical_cogs &lt;- c(0.58, 0.6, 0.62, 0.59, 0.61,0.75,0.54,0.66,0.29,0.78)\nhistorical_capex &lt;- c(0.48, 0.5, 0.52, 0.49, 0.51,0.40,0.52,0.59,0.58,0.81)\n\n# For each simulation, bootstrap 10 values and take the mean → length n_sim vector\nboot_cogs_pct &lt;- replicate(n_sim, mean(sample(historical_cogs, 10, replace = TRUE)))\nboot_capex_pct &lt;- replicate(n_sim, mean(sample(historical_capex, 10, replace = TRUE)))\n\nsimulate_values &lt;- function(r, g, cogs_pct, capex_pct, label = 'Simulation') {\n  firm_value &lt;- numeric(n_sim)\n  \n  for (i in 1:n_sim) {\n    sales &lt;- initial_sales\n    nwc_last &lt;- sales * nwc_pct\n    fa_last &lt;- sales * capex_pct[i]\n    fcf &lt;- numeric(5)\n    \n    for (t in 1:5) {\n      sales &lt;- sales * (1 + g[i])\n      nwc_current &lt;- sales * nwc_pct\n      fa_current &lt;- sales * capex_pct[i]\n      \n      delta_nwc &lt;- nwc_current - nwc_last\n      delta_capex &lt;- fa_current - fa_last\n      \n      nwc_last &lt;- nwc_current\n      fa_last &lt;- fa_current\n      \n      fcf[t] &lt;- sales * (1 - cogs_pct[i]) - delta_capex - delta_nwc\n    }\n    \n    fcf_terminal &lt;- fcf[5] * (1 + g[i])\n    perp_value &lt;- fcf_terminal / (r[i] - g[i])\n    \n    discounted_fcf &lt;- sum(fcf / (1 + r[i])^(1:5))\n    discounted_perp &lt;- perp_value / (1 + r[i])^5\n    \n    firm_value[i] &lt;- discounted_fcf + discounted_perp\n  }\n  \n  firm_value &lt;- firm_value[is.finite(firm_value) & (r &gt; g)]\n  \n  data.frame(Value = firm_value, Scenario = label)\n}\n\n\n# Normal scenario (fixed percentages for COGS and CAPEX)\ndf_norm &lt;- simulate_values(r_norm, g_norm, cogs_pct = rep(0.6,n_sim), capex_pct = rep(0.5,n_sim), label = \"Normal\")\n\n# Bootstrap scenario (bootstrapped mean percentages)\ndf_bootstrap &lt;- simulate_values(r_norm, g_norm, cogs_pct = boot_cogs_pct, capex_pct = boot_capex_pct, label = \"Bootstrap\")\n\n#Plot\nggplot() +\n  geom_density(data = df_norm, aes(x = Value, fill = \"Normal\"), alpha = 0.5) +\n  geom_density(data = df_bootstrap, aes(x = Value, fill = \"Bootstrap\"), alpha = 0.5) +\n  scale_x_continuous(labels = dollar)+\n  labs(\n    title = \"Monte Carlo Valuation Distribution: Normal vs. Bootstrap\",\n    x = \"Firm Value\",\n    y = \"Frequency\",\n    fill = 'Approach',\n    color = \"Scenario\"\n  ) +\n  theme_minimal(base_size = 20) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#outline",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#outline",
    "title": "Introduction to Shiny",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\nMastering Shiny (Wickham 2021)\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\n\n\n\n\n\n\n\n\nCoding Replications\n\n\nFor coding replications, whenever applicable, please follow this page or hover on the specific slides with containing coding chunks.\n\nEnsure that you have your  session properly set-up according to the instructions outlined in the course webpage\nIn the webpage, you can also find a detailed discussion of the examples covered in this lecture"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#disclaimer",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#disclaimer",
    "title": "Introduction to Shiny",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\n\n\n\n\nDisclaimer\n\n\nThe information presented in this lecture is for educational and informational purposes only and should not be construed as investment advice. Nothing discussed constitutes a recommendation to buy, sell, or hold any financial instrument or security. Investment decisions should be made based on individual research and consultation with a qualified financial professional. The presenter assumes no responsibility for any financial decisions made based on this content.\nAll code used in this lecture is publicly available and is also shared on my GitHub page. Participants are encouraged to review, modify, and use the code for their own learning and research purposes. However, no guarantees are made regarding the accuracy, completeness, or suitability of the code for any specific application.\nFor any questions or concerns, please feel free to reach out via email at lucas.macoris@fgv.br"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#what-else",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#what-else",
    "title": "Introduction to Shiny",
    "section": "What else?",
    "text": "What else?\n\nDuring this semester, you have been exposed to a series of practical applications in topics related to finance:\n\nCollecting and manipulating data\nBacktesting Investment Strategies\nEvaluating Mutual Fund performance\nStress-testing Equity Valuation Models\nAnalyzing M&A announcements and marketing sentiment around news\n\n\nWhat’s left for us to do?\nIt is time to ship your insights to a broader audience - I introduce you Shiny!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#references",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#references",
    "title": "Introduction to Shiny",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nWickham, Hadley. 2021. Mastering Shiny. O’Reilly Media. https://mastering-shiny.org/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#introduction-to-shiny",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#introduction-to-shiny",
    "title": "Introduction to Shiny",
    "section": "Introduction to Shiny",
    "text": "Introduction to Shiny\n\n\n\n\n\n\nDefinition\n\n\nShiny is an R package that makes it easy to build interactive web applications directly from your session. It was developed by Posit (previously RStudio) to bridge the gap between data analysis and web interfaces.\n\n\n\n\nIt allows for real-time data visualization and analysis using a web browser, updating outputs automatically when inputs change\nRecently, its usage has been extended to Python users, offering similar reactive programming\n\n\nKey Features\n\nEasy integration with ggplot2, dplyr, and other tidyverse tools, with customizable UI layouts using built-in layout functions, or plain HTML and CSS\nSupports user input through sliders, dropdowns, buttons, and more\nEasy to deploy, host, and ship your application to the internet!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#key-features",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#key-features",
    "title": "Introduction to Shiny",
    "section": "Key Features",
    "text": "Key Features\n\nIt is a reactive programming model: updates outputs automatically when inputs change\nEasy integration with ggplot2, dplyr, and other tidyverse tools\nCustomizable UI layouts using built-in layout functions, or plain HTML and CSS\nSupports user input through sliders, dropdowns, buttons, and more\nEasy to deploy, host, and ship your application to the internet!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#typical-use-cases",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#typical-use-cases",
    "title": "Introduction to Shiny",
    "section": "Typical Use Cases",
    "text": "Typical Use Cases\n\nShiny can be used in a variety of contexts, including but not limited to:\n\n\n\nInteractive Dashboards and Reports: whether you have inputs that change over time, or need, Shiny can leverage user interaction by asking inputs and also providing interactivity, such as downloading results\nPrototyping Data Science products: creating a minimum-viable-product using Shiny is one way of testing an idea before allocating a substantial amount of resources into it. Shiny Apps are simple yet powerful, and can be customized using the best resources on HTML, JavaScript, and CSS\nTools for non-technical stakeholders: Shiny helps to bridge the gap between R and Python users with non-technical audiences, allowing different agents to draw insights from the data without the need for extensive coding"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#shiny-app-structure",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#shiny-app-structure",
    "title": "Introduction to Shiny",
    "section": "Shiny App Structure",
    "text": "Shiny App Structure\n\nA general purpose Shiny app has the following structure\n\nThe ui: defines the layout and appearance of the app (i.e, the frontend)\nThe server: Defines the logic and behavior of the app (i.e, the backend)\nFinally, you can combine both to launch the app\n\n\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#extensions-and-tools",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#extensions-and-tools",
    "title": "Introduction to Shiny",
    "section": "Extensions and Tools",
    "text": "Extensions and Tools\n\nshinydashboard for admin-style dashboards\nshinyjs and shinyWidgets for enhanced UI/UX\nshinytest and shinyloadtest for testing and performance"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#example-1",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#example-1",
    "title": "Introduction to Shiny",
    "section": "Example #1",
    "text": "Example #1"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#example-2",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#example-2",
    "title": "Introduction to Shiny",
    "section": "Example #2",
    "text": "Example #2"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#example-3",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#example-3",
    "title": "Introduction to Shiny",
    "section": "Example #3",
    "text": "Example #3"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#hosting-shiny-applications",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#hosting-shiny-applications",
    "title": "Introduction to Shiny",
    "section": "Hosting Shiny Applications",
    "text": "Hosting Shiny Applications\n\nWay to go: you just did your very first app!\nNow, the question is… how can you share that with your audience? Luckily, there are several ways in which you can host your application1:\n\n\nshinyapps.io (by Posit): by and large, this is the easiest option for hosting Shiny apps for beginners. Fully managed, no server setup required\nPosit Connect: enterprise-grade publishing platform, supports Shiny (R and Python), Quarto, Dash, Flask, and more. It offers authentication, scheduled reports, and usage analytics. Suitable for internal tools and collaborative data products\nDocker: containerize Shiny apps for portability and reproducibility. Useful for deployment in Kubernetes or cloud-native environments. Can be combined with CI/CD workflows\n\nFor detailed content, please refer to a discussion regarding hosting options here."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#shiny-app-structure-getting-started",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#shiny-app-structure-getting-started",
    "title": "Introduction to Shiny",
    "section": "Shiny App Structure (getting started)",
    "text": "Shiny App Structure (getting started)\n\nA general purpose Shiny app has the following structure:\n\nThe ui: defines the layout and appearance of the app (i.e, the frontend)\nThe server: Defines the logic and behavior of the app (i.e, the backend)\nFinally, the shinyApp function creates app objects from an explicit UI/server pair:\n\n\n\nshinyApp(ui, server)\n\n\nThis function uses the ui and server definition and launches a local app in your browser for quick visualization and troubleshooting\n\nHow it works\n\nEvery Shiny app has the same structure: an app.R file that contains ui and server\nYou can create a Shiny app by making a new directory and saving an app.R file inside it. It is recommended that each app will live in its own unique directory"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#a-minimal-working-example-in-finance",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#a-minimal-working-example-in-finance",
    "title": "Introduction to Shiny",
    "section": "A minimal working example (in finance)",
    "text": "A minimal working example (in finance)\n\nYou boss got impressed with the analysis you did as a buy-side analyst at Pierpoint Capital. Unfortunately, he did not take the Practical Applications in Quantitative Finance course, so he really can’t replicate your findings. Your task is to help him ship your analysis to the whole organization\n\n\nCreate stunning visuals that allow interactivity\nAllow users to change parameters of your analysis\nFinally, provide users with the ability to download their findings\n\n\n\n\n\n\n\nSpecific Instructions\n\n\n\nWe will be extensively using several contents from (Wickham 2021). For the sake of brevity, we will not dive into the nitty-gritty details, but rather explore some functions that can be used within a Shiny context\nWe will leverage ShinyAI, an LLM specifically desined to help developing a web framework for data driven apps. You can ask questions about how to use Shiny, to explain how certain things work, or even ask to build a Shiny app for you."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#a-basic-shiny-app",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#a-basic-shiny-app",
    "title": "Introduction to Shiny",
    "section": "A basic Shiny app",
    "text": "A basic Shiny app\n\nIn what follows, we will begin by creating a very simply app that provides a summary performance of selected stocks over time, constructed using the following UI/UX structure:\n\nUsers start by selecting a group of stocks from a list and create a portfolio\nAfter that, they provide a start and end date for the analysis\nThe expected outcome should be a chart that compares the cumulative returns of all selected assets over time\n\nEach section will contain three tabs:\n\nThe Outline section will provide a very quick explanation of what we need to add to the app\nThe Prompt section described the prompt we will be asking ShinyAI to help us with\nFinally, the Code section will contain the updated code generated by ShinyAI, which can be fed into your R section"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#my-first-shiny-app",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#my-first-shiny-app",
    "title": "Introduction to Shiny",
    "section": "My first Shiny app",
    "text": "My first Shiny app\n\nOutlinePromptCode\n\n\n\nIn what follows, we will begin by creating a very simply app that provides a summary performance of selected stocks over time, constructed using the following UI/UX structure:\n\nUsers provide a start and end date for the analysis of the Magnificent 7 stocks\nThe expected outcome should be a chart that compares the cumulative returns of all selected assets over time\n\nEach section will contain three tabs:\n\nThe Outline section will provide a very quick explanation of what we need to add to the app\nThe Prompt section describes what we will be asking ShinyAI to help us with - as expected, this should be a trial-and-error process between prompts\nFinally, the Code section will contain the updated code generated by ShinyAI, which can be fed into your R section\n\n\n\n\n\nCreate a Shiny app that visualizes the cumulative returns of the 'Magnificent 7' tech stocks (Apple, Microsoft, Amazon, NVIDIA, Alphabet/Google, Meta/Facebook, and Tesla). The app should have the following features:\n\nThe only user input should be a date range selector in the sidebar, with a default range of the past year and a minimum selectable date of January 1, 2015\n\nDisplay a line chart showing the cumulative returns of all Magnificent 7 stocks over the selected time period, with company names shown in the legend instead of ticker symbols\n\nInclude a data table below the chart that summarizes key performance metrics for each stock: company name, start date, end date, total return, annualized return, and volatility\n\nUse the tidyquant package to download stock price data from Yahoo Finance and calculate the returns\n\nUse the tidyverse for data manipulation and ggplot2 for visualization\n\n\n\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(bslib)\nlibrary(scales)\n\n# Define the Magnificent 7 stocks\nmagnificent_7 &lt;- c(\n  \"AAPL\", # Apple\n  \"MSFT\", # Microsoft\n  \"AMZN\", # Amazon\n  \"NVDA\", # NVIDIA\n  \"GOOGL\", # Alphabet (Google)\n  \"META\", # Meta (Facebook)\n  \"TSLA\"  # Tesla\n)\n\nui &lt;- page_sidebar(\n  title = \"Magnificent 7 Return Comparison\",\n  \n  sidebar = sidebar(\n    title = \"Date Range\",\n    \n    dateRangeInput(\n      \"date_range\",\n      \"Select date range:\",\n      start = Sys.Date() - 365,\n      end = Sys.Date(),\n      min = \"2015-01-01\",\n      max = Sys.Date()\n    )\n  ),\n  \n  card(\n    full_screen = TRUE,\n    card_header(\"Cumulative Returns of Magnificent 7 Stocks\"),\n    plotOutput(\"returns_plot\", height = \"500px\")\n  ),\n  \n  card(\n    card_header(\"Performance Summary\"),\n    dataTableOutput(\"data_summary\")\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  # Reactive expression to get and process stock data\n  stock_data &lt;- reactive({\n    # Validate that we have dates\n    req(input$date_range)\n    \n      # Download data\n      tq_data &lt;- tq_get(\n        magnificent_7,\n        from = input$date_range[1],\n        to = input$date_range[2],\n        get = \"stock.prices\"\n      )\n      \n      # Check if we got data\n      req(nrow(tq_data) &gt; 0)\n      \n      # Calculate daily returns\n      returns_data &lt;- tq_data %&gt;%\n        group_by(symbol) %&gt;%\n        tq_transmute(\n          select = adjusted,\n          mutate_fun = periodReturn,\n          period = \"daily\",\n          col_rename = \"daily_return\"\n        )\n      \n      # Calculate cumulative returns\n      cum_returns &lt;- returns_data %&gt;%\n        group_by(symbol) %&gt;%\n        mutate(cumulative_return = cumprod(1 + daily_return) - 1) %&gt;%\n        ungroup()\n      \n      list(\n        raw_data = tq_data,\n        cum_returns = cum_returns\n      )\n    })\n\n  # Generate returns plot\n  output$returns_plot &lt;- renderPlot({\n    req(stock_data())\n    \n    cum_returns &lt;- stock_data()$cum_returns\n    \n    # Create a named vector for better labels\n    stock_names &lt;- c(\n      \"AAPL\" = \"Apple\",\n      \"MSFT\" = \"Microsoft\",\n      \"AMZN\" = \"Amazon\",\n      \"NVDA\" = \"NVIDIA\",\n      \"GOOGL\" = \"Alphabet\",\n      \"META\" = \"Meta\",\n      \"TSLA\" = \"Tesla\"\n    )\n    \n    ggplot(cum_returns, aes(x = date, y = cumulative_return, color = symbol)) +\n      geom_line(linewidth = 1) +\n      scale_y_continuous(labels = percent) +\n      scale_color_discrete(labels = stock_names) +\n      labs(\n        title = \"Cumulative Returns of Magnificent 7 Tech Stocks\",\n        x = \"Date\",\n        y = \"Cumulative Return\",\n        color = \"Company\"\n      ) +\n      theme_minimal() +\n      theme(\n        legend.position = \"bottom\",\n        legend.box = \"horizontal\",\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\")\n      )\n  })\n  \n  # Generate data summary\n  output$data_summary &lt;- renderDataTable({\n    req(stock_data())\n    \n    cum_returns &lt;- stock_data()$cum_returns\n    \n    # Stock display names for the summary table\n    stock_names &lt;- c(\n      \"AAPL\" = \"Apple\",\n      \"MSFT\" = \"Microsoft\",\n      \"AMZN\" = \"Amazon\",\n      \"NVDA\" = \"NVIDIA\",\n      \"GOOGL\" = \"Alphabet\",\n      \"META\" = \"Meta\",\n      \"TSLA\" = \"Tesla\"\n    )\n    \n    # Summary statistics for each asset\n    summary_data &lt;- cum_returns %&gt;%\n      group_by(symbol) %&gt;%\n      summarize(\n        Start_Date = min(date),\n        End_Date = max(date),\n        Total_Return = last(cumulative_return) %&gt;% round(4),\n        Annualized_Return = ((1 + last(cumulative_return))^(252 / n())) - 1 %&gt;% round(4),\n        Volatility = sd(daily_return, na.rm = TRUE) * sqrt(252) %&gt;% round(4)\n      ) %&gt;%\n      mutate(\n        Company = stock_names[symbol],\n        Total_Return = percent(Total_Return, accuracy = 0.01),\n        Annualized_Return = percent(Annualized_Return, accuracy = 0.01),\n        Volatility = percent(Volatility, accuracy = 0.01)\n      ) %&gt;%\n      select(Company, Start_Date, End_Date, Total_Return, Annualized_Return, Volatility)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#example",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#example",
    "title": "Introduction to Shiny",
    "section": "Example",
    "text": "Example\n\nOutlinePromptCode"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#outline",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#outline",
    "title": "Equity Valuation and Simulation",
    "section": "Outline",
    "text": "Outline\n\nThis lecture is mainly based the following textbooks:\n\nTidy Finance (Scheuch, Voigt, and Weiss 2023)\nR for Data Science (Wickham, Mine Cetinkaya-Rundel, and Grolemund 2023)\n\n\n\n\n\n\n\n\nCoding Replications\n\n\nFor coding replications, whenever applicable, please follow this page or hover on the specific slides with containing coding chunks.\n\nEnsure that you have your  session properly set-up according to the instructions outlined in the course webpage\nIn the webpage, you can also find a detailed discussion of the examples covered in this lecture"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#disclaimer",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#disclaimer",
    "title": "Equity Valuation and Simulation",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\n\n\n\n\nDisclaimer\n\n\nThe information presented in this lecture is for educational and informational purposes only and should not be construed as investment advice. Nothing discussed constitutes a recommendation to buy, sell, or hold any financial instrument or security. Investment decisions should be made based on individual research and consultation with a qualified financial professional. The presenter assumes no responsibility for any financial decisions made based on this content.\nAll code used in this lecture is publicly available and is also shared on my GitHub page. Participants are encouraged to review, modify, and use the code for their own learning and research purposes. However, no guarantees are made regarding the accuracy, completeness, or suitability of the code for any specific application.\nFor any questions or concerns, please feel free to reach out via email at lucas.macoris@fgv.br"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#dealing-with-uncertainty-in-valuation-models",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#dealing-with-uncertainty-in-valuation-models",
    "title": "Equity Valuation and Simulation",
    "section": "Dealing with Uncertainty in Valuation Models",
    "text": "Dealing with Uncertainty in Valuation Models\n\nTraditional valuation methods (e.g., Discounted Cash Flow) rely on single-point estimates:\n\nWe assume a given level of revenue growth…\nWe also fix the appropriate discount rates, \\(r\\), over time…\nFinally, we come up with assumptions regarding the firm’s Terminal Value (i.e, the perpetuity value)!\n\nNote, however, that these estimates are subject to significant uncertainty:\n\nMarket conditions\nCompetition\nRegulation\nMacroeconomic and exogenous shocks, such as COVID-19\n\n\n\nQuestion: how can get take into account the role of uncertainty in valuation models?"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#dealing-with-uncertainty-in-valuation-models-continued",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#dealing-with-uncertainty-in-valuation-models-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Dealing with Uncertainty in Valuation Models, continued",
    "text": "Dealing with Uncertainty in Valuation Models, continued\n\nSuppose we have the following Discounted Cash Flow estimate:\n\n\\[\n\\text{Firm Value} = \\sum_{t=1}^{T} \\frac{FCF_t}{(1 + r)^t}\n\\]\n\n\\(FCF_t\\) depends on a variety of firm-level factors, such as sales growth, gross margins, taxes, among others\nSimilarly, \\(r\\), the discount rate, if modeled using the CAPM, depends upon factors such as the sensitivity to market risk (\\(\\beta\\)), the risk-free rate, \\(r_f\\), and the market risk premium\n\n\nChanges in those variables can severely affect the outcomes of our valuation: for example, if the realized \\(FCF\\)’s are lower than what we assumed, we might be overestimating the firm’s value!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#sensitivity-analysis",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#sensitivity-analysis",
    "title": "Equity Valuation and Simulation",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\n\nA first way in which we can take uncertainty into account is through Sensitivity Analysis:\n\n\n\n\n\n\n\nDefinition\n\n\n\nSensitivity analysis tests how changes in a single input affect the valuation output, while all other variables are held constant. It is useful to identify key value drivers and assess their impact.\n\n\n\n\n\nFix the main drivers of your outcome variable (e.g, growth rate of COGS, growth rate of Sales, Cost of Capital, etc)\nFor each driver, create scenarios in where you vary the input of interest within a given range\nFor that specific scenario, collect the new outcome variable calculated when everything is fixed, but the specific driver has changed\nRepeat this across all scenarios and drivers"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#sensitivity-analysis-example",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#sensitivity-analysis-example",
    "title": "Equity Valuation and Simulation",
    "section": "Sensitivity Analysis, Example",
    "text": "Sensitivity Analysis, Example\n\nTo illustrate the use of Sensitivity Analysis, we can vary \\(r\\), the discount rate, and observe the change in the firm’s value for each distinct \\(r\\):\n\n\n\n\nDiscount Rate (\\(r\\))\nFirm Value (in millions)\n\n\n\n\n\\(6\\%\\)\n\\(\\$125\\)\n\n\n\\(8\\%\\)\n\\(\\$110\\)\n\n\n\\(10\\%\\)\n\\(\\$98\\)\n\n\n\nKey Points on Sensitivity Analysis\n\nOn the one hand, it helps to answer how sensitive is the valuation to some of the assumptions that were used in the model\nOn the other hand, there is a clear limitation: the fact that we are varying one variable at a time doesn’t capture interactions between variables!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#scenario-analysis",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#scenario-analysis",
    "title": "Equity Valuation and Simulation",
    "section": "Scenario Analysis",
    "text": "Scenario Analysis\n\nAs described before, a clear limitation of Sensitivity Analysis is the fact that only one input is evaluated at a time\nWhat if we wanted to see the combined effect of multiple variables changing at the same time?\n\n\n\n\n\n\n\nDefinition\n\n\n\nA Scenario Analysis evaluates the effect of multiple variables changing at the same time\n\n\nIt is used to create alternative scenarios (e.g, Best, Base, and Worst Case Scenarios) based on business logic\nIt is more realistic than sensitivity analysis, but still limited to a few discrete outcomes\n\n\nIt is very reasonable to assume that more than one driver is going to change at a time. To do that, we can create a grid of values and simulate changes in inputs at the same time"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#scenario-analysis-continued",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#scenario-analysis-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Scenario Analysis, continued",
    "text": "Scenario Analysis, continued\n\n\n\n\n\n\n\n\n\n\nScenario\nRevenue Growth\nDiscount Rate\nTerminal Value\nFirm Value\n\n\n\n\nBest Case\n\\(8\\%\\)\n\\(6\\%\\)\nHigh\n\\(\\$140M\\)\n\n\nBase Case\n\\(5\\%\\)\n\\(8\\%\\)\nMedium\n\\(\\$110M\\)\n\n\nWorst Case\n\\(2\\%\\)\n\\(10\\%\\)\nLow\n\\(\\$85M\\)\n\n\n\nKey Points on Scenario Analysis\n\nIt highlights a range of possible outcomes under plausible assumptions\nHowever, it still lacks a probability component: we still don’t know how likely each of those scenarios are:\n\n\nBest, Base, and Worst cases are limited in the way that they can produce scenarios because we assume that all variables will go into the same direction\nFurthermore, our realized outcome fallS under a combination of those scenarios"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#from-scenarios-to-simulations",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#from-scenarios-to-simulations",
    "title": "Equity Valuation and Simulation",
    "section": "From Scenarios to Simulations",
    "text": "From Scenarios to Simulations\n\nWhat if we could extend the logit of scenario simulation to \\(N&gt;&gt;3\\) scenarios? To do that, we can use simulation techniques, such as the Monte Carlo simulation:\n\n\n\n\n\n\n\nDefinition:\n\n\nA Monte Carlo Simulation is a computational technique that uses random sampling to model the distribution of outcomes for a given random variable:\n\nWe define the parameters of our simulation as probability distributions (e.g., normal, lognormal).\nGenerate a large number of random scenarios for these variables\nCompute the outcome (e.g., firm value) for each scenario\nAnalyze the distribution of the desired results (e.g., mean, percentiles, risk of loss)\n\n\n\n\n\nWith Monte Carlo simulations, instead of getting a range of potential outcomes for the firm’s value, we actually get an empirical distribution of values!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#monte-carlo-implementation",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#monte-carlo-implementation",
    "title": "Equity Valuation and Simulation",
    "section": "Monte Carlo Implementation",
    "text": "Monte Carlo Implementation\n\nTo see how we can apply Monte Carlo Simulation to assess the distribution of Firm Value, let’s assume that both the free cash flow and the discount rate are random variables:\n\n\\(FCF_t \\sim \\mathcal{N}(\\mu_t, \\sigma_t^2)\\)\n\\(r \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)\\)\n\nIf we know the distribution of those random variables, we can draw \\(N\\) observations for each random variable and use it to calculate the desired outcome (i.e, the firm’s value). For draw \\(i\\), the estimated value of the firm is simply:\n\n\\[\n\\text{Firm Value}^{(i)} = \\sum_{t=1}^{T} \\frac{FCF_t^{(i)}}{(1 + r^{(i)})^t}\n\\]\n\nRepeat for \\(i = 1,2,..., N\\) simulations\nGet a distribution of firm values, not just a point estimate"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#monte-carlo-implementation-1",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#monte-carlo-implementation-1",
    "title": "Equity Valuation and Simulation",
    "section": "Monte Carlo implementation",
    "text": "Monte Carlo implementation\n\nSuppose we are using the Gordon’s Growth Model, where the value of a firm is given by the infinite stream of dividends:\n\n\\[\nV_t= \\dfrac{D_1}{r-g}\n\\]\nwhere \\(D1\\) s the dividend next year, \\(r\\) is the discount rate, and \\(g\\) is the perpetual growth rate. If we assume the following distributions:\n\nSince \\(D1\\) is the next period’s dividend, and it is fixed to \\(\\$2\\)\n\\(r \\sim \\mathcal{N}(0.08,0.01^2)\\)\n\\(g \\sim \\mathcal{N}(0.03,0.005^2)\\)\n\n\nIn what follows, we will simulate \\(10,000\\) distinct scenarios for draws of \\(r\\) and \\(g\\) and see how the \\(V_t\\) distributes over all potential combinations"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#monte-carlo-implementation-continued",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#monte-carlo-implementation-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Monte Carlo Implementation, continued",
    "text": "Monte Carlo Implementation, continued\n\nCodeOutput\n\n\n\n#Set seed for reproducibility\nset.seed(123)\n\n# Parameters\nn_sim &lt;- 10000\nD1 &lt;- 2\n\n# Simulate r and g\nr &lt;- rnorm(n_sim, mean = 0.08, sd = 0.01)\ng &lt;- rnorm(n_sim, mean = 0.03, sd = 0.005)\n\n# Compute values\nvalue &lt;- D1 / (r - g)\n\n# Filter for valid values where r &gt; g\nvalue &lt;- value[is.finite(value) & (r &gt; g)]\n\n# Plot\nlibrary(ggplot2)\nlibrary(scales)\n\nggplot(data.frame(Value = value), aes(x = Value)) +\n  geom_histogram(bins = 60, fill = \"#2c7fb8\", color = \"white\", alpha = 0.8) +\n  scale_x_continuous(labels=scales::dollar)+\n  labs(\n    title = \"Monte Carlo Valuation Distribution (Gordon Growth Model)\",\n    x = \"Firm Value\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size=20)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#hands-on-exercise",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#hands-on-exercise",
    "title": "Equity Valuation and Simulation",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\nRefer back to NetScape valuation model that you have worked on prior classes. Using Monte Carlo simulations, you were asked to stress-test such model by analyzing what would have happened to the base scenario as-if some inputs were changed\n\n\n\n\n\n\n\nInstructions\n\n\n\nWe will be using the NetScape case we have worked on previous classes\nYour first task is to translate the valuation model in such a way that you can seamlessly replicate the baseline results\nAfter that, you will be prompted with a series of questions that will require you to simulate \\(N\\) scenarios and analyze the effects on NetScape’s value\n\n\n\n\nGiven that the baseline price, at the time of the valuation, was $28.46, what would be your recommendation for this stock?"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#step-1-hardcoding-the-assumptions",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#step-1-hardcoding-the-assumptions",
    "title": "Equity Valuation and Simulation",
    "section": "Step 1: Hardcoding the assumptions",
    "text": "Step 1: Hardcoding the assumptions\n\nRecall that our Free Cash Flow estimation in peiod \\(t\\), \\(FCF_t\\), is given by:\n\n\\[\nFCF_t = EBIT\\times (1-\\tau) \\pm \\text{Depreciation} \\pm \\text{CAPEX} \\pm \\Delta NWC\n\\] Where \\(\\tau\\) is the marginal tax-rate, CAPEX is Capital Expenditures, and NWC is Net Working Capital\n\nIn what follows, we will simulate the value of Netscape using a 10-year discounted cash flow (DCF) model with structured assumptions on top of the historical revenue levels from 1995"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#step-1-hardcoding-the-assumptions-continued",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#step-1-hardcoding-the-assumptions-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Step 1: Hardcoding the assumptions, continued",
    "text": "Step 1: Hardcoding the assumptions, continued\n\n#Number of years\nn_years=10\n\n# Assumptions (copied and simplified)\nnetscape_assumptions &lt;- list(\n  revenue_growth = rep(0.65,n_years),\n  cost_of_sales_pct = rep(0.1044,n_years),\n  rd_pct = rep(0.3676,n_years),\n  tax_rate = rep(0.34,n_years),\n  other_op_exp_pct = c(0.80,0.65,0.55,0.45,0.35,0.25,rep(0.2,4)),\n  capex_pct = c(0.45,0.4,0.3,0.2,rep(0.1,6)),\n  nwc_pct = rep(0,n_years),\n  depreciation_pct = rep(0.055,n_years),\n  beta=rep(1.5,n_years),\n  rf=rep(0.0671,n_years),\n  mrp=rep(0.075,n_years),\n  shares_outstanding = 38000,\n  terminal_growth = 0.04,\n  terminal_r=0.1796\n)\n\n# Base year (1995)\nnetscape_base_rev &lt;- 33250"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#step-2-getting-the-forecasted-free-cash-flow",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#step-2-getting-the-forecasted-free-cash-flow",
    "title": "Equity Valuation and Simulation",
    "section": "Step 2: Getting the forecasted Free Cash Flow",
    "text": "Step 2: Getting the forecasted Free Cash Flow\n\nAfter we got our assumptions in place, it is time to put them together and generate the Free Cash Flow values for \\(t=1,2,...,10\\)\nFor that, we will create a function, get_projections, that has two arguments:\n\nassumptions: a named list containing vectors of financial assumptions (growth rates, margins, etc) for each year\nbase: the base year revenue (e.g., from 1995).\n\nWe initialize an empty data frame projection with columns for each financial metric over the 10-year period, and fill out the results using the Free Cash Flow definition\nWith this function, you should be able to replicate the Free Cash Flows from the base scenario by calling get_projection(netscape_assumptions,netscape_base_rev)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#step-2-getting-the-forecasted-free-cash-flow-continued",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#step-2-getting-the-forecasted-free-cash-flow-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Step 2: Getting the forecasted Free Cash Flow, continued",
    "text": "Step 2: Getting the forecasted Free Cash Flow, continued\n\n# Projection over 10 years\nget_projection &lt;- function(assumptions,base){\n  \n  projection=data.frame(\n      Year = 1:n_years,\n      Revenue = NA,\n      EBIT = NA,\n      Taxes = NA,\n      NOPAT = NA,\n      Depreciation = NA,\n      CAPEX = NA,\n      NWC = NA,\n      Delta_NWC = NA,\n      Discount_Rate=NA)\n\n  # Fill in projections for revenue\n  for (t in 1:n_years) {\n    if (t == 1) {\n      projection$Revenue[t] &lt;- base * (1 + assumptions$revenue_growth[t])\n    } else {\n      projection$Revenue[t] &lt;- projection$Revenue[t - 1] * (1 + assumptions$revenue_growth[t])\n    }\n  }\n  \n  #Fill in FCF terms\n  projection$EBIT &lt;- projection$Revenue * (1 - assumptions$cost_of_sales_pct - assumptions$rd_pct - assumptions$other_op_exp_pct - assumptions$depreciation_pct)\n  projection$Taxes &lt;- projection$EBIT * assumptions$tax_rate\n  projection$NOPAT &lt;- projection$EBIT - projection$Taxes\n  projection$Depreciation &lt;- projection$Revenue * assumptions$depreciation_pct\n  projection$CAPEX &lt;- projection$Revenue * assumptions$capex_pct\n  projection$NWC &lt;- projection$Revenue*assumptions$nwc_pct\n  projection$Delta_NWC &lt;- 0\n  projection$Discount_Rate &lt;- 1/(1+(assumptions$rf+assumptions$mrp*assumptions$beta))^projection$Year\n  \n  return(projection)\n}"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#valuing-the-company-using-fcf",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#valuing-the-company-using-fcf",
    "title": "Equity Valuation and Simulation",
    "section": "Valuing the company using FCF",
    "text": "Valuing the company using FCF\n\nWe will now create two additional functions:\n\nThe get_dcf() function will calculate the Discounted Cash Flow for the whole period, based on the projections we have just created. It returns a scalar value representing the present value of projected Free Cash Flows\nThe get_terminal_value() will calculate an estimate of the present value of the perpetuity (in Year 0) based on the projections and the assumptions regarding the long-term r and g. ItcComputes the terminal value of the firm beyond the projection horizon using a growing perpetuity\n\nAdding up these two values yields the total value of the firm, \\(V\\), which is embedded in the get_EV() function:\n\n\nget_EV(netscape_assumptions,\n       netscape_base_rev,\n       netscape_assumptions$terminal_r,\n       netscape_assumptions$terminal_growth)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#valuing-the-company-using-fcf-continued",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#valuing-the-company-using-fcf-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Valuing the company using FCF, continued",
    "text": "Valuing the company using FCF, continued\n\nget_dcf &lt;- function(projection){\n  \n  dcf=projection%&gt;%\n    mutate(FCF = NOPAT + Depreciation - CAPEX - Delta_NWC)%&gt;%\n    reframe(DCF=FCF*Discount_Rate)%&gt;%\n    pull(DCF)%&gt;%\n    sum()\n  \n  return(dcf)\n  \n}\n\nget_terminal_value &lt;- function(projection,r,g){\n\n  last_fcf=projection%&gt;%\n    mutate(FCF = NOPAT + Depreciation - CAPEX - Delta_NWC)%&gt;%\n    pull(FCF)%&gt;%\n    tail(1)\n  \n  return(last_fcf*(1+g)/(r-g)/(1+r)^(n_years+1))\n}\n\nget_EV &lt;-function(assumptions,base,r,g){\n\n  PV_FCF = get_projection(assumptions,base)%&gt;%get_dcf()\n  PV_Terminal = get_projection(assumptions,base)%&gt;%get_terminal_value(r,g)\n  \n  return(PV_FCF+PV_Terminal)\n\n  }"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#exercise-1-changing-revenue-growth",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#exercise-1-changing-revenue-growth",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 1: changing revenue growth",
    "text": "Exercise 1: changing revenue growth\n\nCodeOutput\n\n\n\n  #Set seed for reproducibility\n  set.seed(123)\n\n  # Initialize an empty dataframe\n  results &lt;- numeric(0)\n  sim_assumptions &lt;- netscape_assumptions\n  n_sim=10000\n  \n  #Run the simulation\n  for (i in 1:n_sim){\n\n    # Simulate revenue growth for 10 years from a normal distribution\n    rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n    \n    # Modify assumptions with simulated growth\n    sim_assumptions$revenue_growth &lt;- rev_growth\n    \n    # Compute EV for this simulation\n    ev &lt;- get_EV(sim_assumptions,\n                 netscape_base_rev,\n                 sim_assumptions$terminal_r,\n                 sim_assumptions$terminal_growth)\n    \n    results[i] &lt;- ev\n  }\n  \n\n  #Create a ggplot histogram\n  results%&gt;%\n    as.tibble()%&gt;%\n    ggplot(aes(x = value)) +\n    geom_histogram(fill = \"skyblue\", color = \"white\", bins = 50)+\n    scale_x_continuous(labels = scales::dollar) +\n    labs(\n      title = \"Monte Carlo Simulation of Enterprise Value\",\n      subtitle = paste0('Drawing ',comma(n_sim), ' simulations of Revenue Growth.'), \n      x = \"Enterprise Value (USD, Thousands)\",\n      y = \"Frequency\"\n    ) +\n    theme_minimal(base_size = 14)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#exercise-2-changing-revenue-growth-and-cogs",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#exercise-2-changing-revenue-growth-and-cogs",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 2: Changing Revenue Growth and COGS",
    "text": "Exercise 2: Changing Revenue Growth and COGS\n\nCodeOutput\n\n\n\n  #Set seed for reproducibility\n  set.seed(123)\n\n  # Initialize an empty dataframe\n  results &lt;- numeric(0)\n  sim_assumptions &lt;- netscape_assumptions\n  n_sim=10000\n\n#Run the simulation\nfor (i in 1:n_sim){\n  \n  # Simulate revenue growth for 10 years from a normal distribution\n  rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n  cogs_perc &lt;- rnorm(n_years, mean = 0.10, sd = 0.025)\n  \n  # Modify assumptions with simulated growth\n  sim_assumptions$revenue_growth &lt;- rev_growth\n  sim_assumptions$cost_of_sales_pct &lt;- cogs_perc\n  \n  # Compute EV for this simulation\n  ev &lt;- get_EV(sim_assumptions,\n               netscape_base_rev,\n               sim_assumptions$terminal_r,\n               sim_assumptions$terminal_growth)\n  \n  results[i] &lt;- ev\n}\n\n#Create a ggplot histogram\nresults%&gt;%\n  as.tibble()%&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(fill = \"skyblue\", color = \"white\", bins = 50)+\n  scale_x_continuous(labels = scales::dollar) +\n  labs(\n    title = \"Monte Carlo Simulation of Enterprise Value\",\n    subtitle = paste0('Drawing ',comma(n_sim), ' simulations of Revenue Growth and % COGS.'), \n    x = \"Enterprise Value (USD, Thousands)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 20)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#exercise-3-changing-revenue-growth-cogs-and-capex",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#exercise-3-changing-revenue-growth-cogs-and-capex",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 3: Changing Revenue Growth, COGS, and CAPEX",
    "text": "Exercise 3: Changing Revenue Growth, COGS, and CAPEX\n\nCodeOutput\n\n\n\n#Set seed for reproducibility\nset.seed(123)\n\n# Initialize an empty dataframe\nresults &lt;- numeric(0)\nsim_assumptions &lt;- netscape_assumptions\nn_sim=10000\n\n#Run the simulation\nfor (i in 1:n_sim){\n  \n  # Simulate revenue growth for 10 years from a normal distribution\n  rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n  cogs_perc &lt;- rnorm(n_years, mean = 0.10, sd = 0.025)\n  capex_perc &lt;- seq(rnorm(1, mean = 0.45, sd = 0.1),to=0.1,length.out = 10)\n  \n  # Modify assumptions with simulated growth\n  sim_assumptions$revenue_growth &lt;- rev_growth\n  sim_assumptions$cost_of_sales_pct &lt;- cogs_perc\n  sim_assumptions$capex_pct &lt;- capex_perc\n  \n  # Compute EV for this simulation\n  ev &lt;- get_EV(sim_assumptions,\n               netscape_base_rev,\n               sim_assumptions$terminal_r,\n               sim_assumptions$terminal_growth)\n  \n  results[i] &lt;- ev\n}\n\n#Create a ggplot histogram\nresults%&gt;%\n  as.tibble()%&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(fill = \"skyblue\", color = \"white\", bins = 50)+\n  scale_x_continuous(labels = scales::dollar) +\n  labs(\n    title = \"Monte Carlo Simulation of Enterprise Value\",\n    subtitle = paste0('Drawing ',comma(n_sim), ' simulations of Revenue Growth, % COGS, and % CAPEX.'), \n    x = \"Enterprise Value (USD, Thousands)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 20)\n\nfinal_results=results"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#exercise-4-different-levels-for-terminal-r-and-g",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#exercise-4-different-levels-for-terminal-r-and-g",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 4: Different levels for terminal \\(r\\) and \\(g\\)",
    "text": "Exercise 4: Different levels for terminal \\(r\\) and \\(g\\)\n\nCodeOutput\n\n\n\n#Set seed for reproducibility\nset.seed(123)\n\n# Initialize an empty dataframe\nresults &lt;- numeric(0)\nnew_data &lt;- data.frame()\nsim_assumptions &lt;- netscape_assumptions\nn_sim=1000\ng_sequence &lt;- seq(0.05,0.03,length.out=10)\nr_sequence &lt;- seq(0.15,0.225,length.out=10)\n\nfor(s in 1:10){\n  \n  #Run the simulation\n  for (i in 1:n_sim){\n    \n    # Simulate revenue growth for 10 years from a normal distribution\n    rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n    cogs_perc &lt;- rnorm(n_years, mean = 0.10, sd = 0.025)\n    capex_perc &lt;- seq(rnorm(1, mean = 0.45, sd = 0.1),to=0.1,length.out = 10)\n    \n    # Modify assumptions with simulated growth\n    sim_assumptions$revenue_growth &lt;- rev_growth\n    sim_assumptions$cost_of_sales_pct &lt;- cogs_perc\n    sim_assumptions$capex_pct &lt;- capex_perc\n    sim_assumptions$terminal_growth = g_sequence[s]\n    sim_assumptions$terminal_r = r_sequence[s]\n    \n    # Compute EV for this simulation\n    ev &lt;- get_EV(sim_assumptions,\n                 netscape_base_rev,\n                 sim_assumptions$terminal_r,\n                 sim_assumptions$terminal_growth)\n    \n    results[i] &lt;- ev\n  }\n  \n  #Store the Pairs\n  new_data=new_data%&gt;%\n    rbind(data.frame(Scenario = paste0('Scenario ', s),\n                     EV=results))\n          \n  message(paste0('Finished simulation for pair number ',s,'.'))\n}\n\n\n#install.packages('ggridges')\nlibrary(ggridges)\n\n#Create a ggplot histogram\nnew_data%&gt;%\n  mutate(Scenario = factor(Scenario,levels=paste0('Scenario ',10:1)))%&gt;%\n  ggplot(aes(x = EV,\n             y = Scenario,\n             fill=..x..)) +\n  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01)+\n  scale_x_continuous(labels = scales::dollar) +\n  scale_fill_viridis_c(name = \"Enterprise Value\", option = \"plasma\",labels=scales::dollar)+\n  labs(\n    title = \"Monte Carlo Simulation of Enterprise Value\",\n    subtitle = paste0('Drawing ',comma(n_sim), ' simulations, best-to-worse scenarios.'), \n    x = \"\",\n    y = \"Frequency\",\n    fill = \"Enterprise Value (USD, Thousands)\")+\n  theme_minimal(base_size = 14)+\n  theme(legend.position = 'bottom',\n        legend.title = element_text(size = 14),\n        legend.text = element_text(size = 12),\n        legend.key.height = unit(0.6, \"cm\"),\n        legend.key.width = unit(2.5, \"cm\"))"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#final-question-should-we-invest",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#final-question-should-we-invest",
    "title": "Equity Valuation and Simulation",
    "section": "Final Question: should we invest?",
    "text": "Final Question: should we invest?\n\nCodeOutput\n\n\n\n#Create a ggplot histogram\nprices=final_results%&gt;%\n  as.tibble()%&gt;%\n  mutate(Price=value/netscape_assumptions$shares_outstanding)%&gt;%\n  mutate(Situation=ifelse(Price&gt;=28.37,'Undervalued','Overvalued'))\n\nprices%&gt;%\n  ggplot(aes(x = Price)) +\n  geom_histogram(aes(fill = Situation), bins = 50)+\n  scale_x_continuous(labels = scales::dollar)+\n  scale_fill_manual(values=c('darkgreen','darkred'))+\n  geom_vline(xintercept=28.37,linetype='dashed',size=1)+\n  annotate(geom='text',x=40,y=500,label= paste0('True Price is higher than \\nbaseline price only in ',\n                                                percent(mean(prices$Price&gt;=28.37)),\n                                                ' of the cases!'))+\n  labs(\n    title = \"Monte Carlo Simulation of Share Price\",\n    subtitle = paste0('Drawing 10,000 simulations'), \n    x = \"Enterprise Value (USD, Thousands)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 20)+\n  theme(legend.position='bottom')"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#overall-thoughts-on-monte-carlo-simulation",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#overall-thoughts-on-monte-carlo-simulation",
    "title": "Equity Valuation and Simulation",
    "section": "Overall thoughts on Monte Carlo Simulation",
    "text": "Overall thoughts on Monte Carlo Simulation\n\nMonte Carlo Simulation is a powerful technique to handle model uncertainty:\n\nIt helps managers think beyond the point estimate of the valuation model and handle what-if questions\nIt also helps stress-testing the valuation model\n\nIt should be used along with business logic to help tailor the simulation exercise:\n\nWe don’t want to create overly complex simulations\nWe can incorporate more structure to create meaningful scenarios - for example, imposing a correlation structure between the variables\n\nBy setting up proper simulation parameters (number of simulations, the distributions and its parameters, etc), we can leverage the role of uncertainty in valuation models in a much more pronounced way and guide decision-making!"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#extensions",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#extensions",
    "title": "Equity Valuation and Simulation",
    "section": "Extensions",
    "text": "Extensions\nQuestion: how can we improve our simulation exercise?\n\nIn what follows, we’ll take a look at several ways by which you could extend your simulation coding exercise to cope with real-world features. To illustrate, we will adopt a simple valuation approach of the following form:\n\n\\[\n\\small V_i = \\sum_{t=1}^{t=5}\\dfrac{FCF_t}{(1+r)^t}+ PV_0\\bigg(\\dfrac{FCF_5\\times(1+g)}{r-g}\\bigg)\n\\]\n\nIn words, we are breaking down the firms value into:\n\nA 1-5 year horizon\nA growing perpetuity, discounted back to Year 0\n\nFrom that, we can extend the simulation routines to allow for important features that are present in real-world data"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#base-case-simulation",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#base-case-simulation",
    "title": "Equity Valuation and Simulation",
    "section": "Base Case Simulation",
    "text": "Base Case Simulation\n\nOutlineCodeOutput\n\n\n\nThe base case denotes the Free Cash Flow in period \\(t\\) as:\n\n\\[\nFCF_t = Sales_t\\times(1-COGS) \\pm CAPEX \\pm \\Delta NWC\n\\]\n\nInitial Sales levels are \\(\\$100\\)\nThe percent of Cost of Goods Sold (COGS) is \\(60\\%\\)\nCapital Expenditures (CAPEX) is \\(15\\%\\) of Sales\nNet Working Capital investment is \\(5%\\) of sales\n\n\n\n\nlibrary(glue)\nlibrary(MASS)\n\nset.seed(123)\nn_sim &lt;- 10000\n\n# Parameters\ninitial_sales &lt;- 1000\ncogs_pct &lt;- 0.6\ncapex_pct &lt;- 0.15\nnwc_pct &lt;- 0.05\n\nsimulate_values &lt;- function(r, g, label='All') {\n  firm_value &lt;- numeric(n_sim)\n  \n  for (i in 1:n_sim) {\n    sales &lt;- initial_sales\n    nwc_last &lt;- sales * nwc_pct\n    fa_last &lt;- sales * capex_pct\n    fcf &lt;- numeric(5)\n    \n    for (t in 1:5) {\n      sales &lt;- sales * (1 + g[i])\n      nwc_current &lt;- sales * nwc_pct\n      fa_current &lt;- sales * capex_pct\n      \n      delta_nwc &lt;- nwc_current - nwc_last\n      delta_capex &lt;- fa_current - fa_last\n      \n      nwc_last &lt;- nwc_current\n      fa_last &lt;- fa_current\n      \n      fcf[t] &lt;- sales * (1 - cogs_pct) - delta_capex - delta_nwc\n    }\n    \n    fcf_terminal &lt;- fcf[5] * (1 + g[i])\n    perp_value &lt;- fcf_terminal / (r[i] - g[i])\n    \n    discounted_fcf &lt;- sum(fcf / (1 + r[i])^(1:5))\n    discounted_perp &lt;- perp_value / (1 + r[i])^5\n    \n    firm_value[i] &lt;- discounted_fcf + discounted_perp\n  }\n  \n  firm_value &lt;- firm_value[is.finite(firm_value) & (r &gt; g)]\n  \n  data.frame(Value = firm_value, Scenario = label)\n}\n\n# Normal distributions\nr_norm &lt;- rnorm(n_sim, mean = 0.08, sd = 0.01)\ng_norm &lt;- rnorm(n_sim, mean = 0.03, sd = 0.005)\ndf_norm &lt;- simulate_values(r_norm, g_norm, \"Normal\")\n\nbase_norm &lt;- simulate_values(r_norm, g_norm, \"Normal\")\n\nggplot(base_norm, aes(x = Value)) +\n  geom_histogram(bins = 60, fill = \"#2c7fb8\", color = \"white\", alpha = 0.8)+\n  scale_x_continuous(labels = dollar) +\n  labs(\n    title = \"Monte Carlo Valuation Distribution Normal vs. Alternative Distributions\",\n    subtitle = glue('Based on {comma(n_sim)} simulations.'),\n    x = \"Firm Value\",\n    y = \"Frequency\",\n    fill = \"Scenario\",\n    color = \"Scenario\"\n  ) +\n  theme_minimal(base_size = 20)+\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#extension-1-alternative-distributions",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#extension-1-alternative-distributions",
    "title": "Equity Valuation and Simulation",
    "section": "Extension 1: Alternative Distributions",
    "text": "Extension 1: Alternative Distributions\n\nOutlineCodeOutput\n\n\n\nKey Point: Monte Carlo doesn’t assume normality — any distribution that fits the problem can be used!\n\nWhile normal distributions are common, other distributions (e.g., uniform, log-normal, etc.) can also be used:\nFor example, if \\(r \\sim \\text{Uniform}(0.05, 0.10)\\), the growth rate could be chosen randomly from a uniform distribution between \\(5\\%\\) and \\(10\\%\\)\n\nDistributions like the log-normal may be more appropriate for modeling returns, as they respect the non-negativity constraint - $r $. Therefore, for a given variable \\(X\\), it could be sampled from any distribution\nIn what follows, we’ll simulate results drawing from an Uniform distribution for \\(r\\) and a Beta for \\(g\\)\n\n\n\n\n# Baseline case\nr_norm &lt;- rnorm(n_sim, mean = 0.08, sd = 0.01)\ng_norm &lt;- rnorm(n_sim, mean = 0.03, sd = 0.005)\ndf_norm &lt;- simulate_values(r_norm, g_norm, \"Normal\")\n\n\n# Alternative distributions\nr_unif &lt;- runif(n_sim, min = 0.06, max = 0.10)\ng_beta &lt;- 0.06 * rbeta(n_sim, 2, 5)\ndf_alt &lt;- simulate_values(r_unif, g_beta, \"Alternative\")\n\n# Combine results\ndf_all &lt;- rbind(df_norm, df_alt)\n\n# Plot combined chart\n\nggplot(df_all, aes(x = Value, fill = Scenario, color = Scenario)) +\n  geom_density(alpha = 0.5, size = 1) +\n  scale_x_continuous(labels = dollar) +\n  scale_fill_manual(values = c(\"Normal\" = \"#2c7fb8\", \"Alternative\" = \"#f03b20\")) +\n  scale_color_manual(values = c(\"Normal\" = \"#2c7fb8\", \"Alternative\" = \"#f03b20\")) +\n  labs(\n    title = \"Monte Carlo Valuation Distribution Normal vs. Alternative Distributions\",\n    subtitle = glue('Based on {comma(n_sim)} simulations.'),\n    x = \"Firm Value\",\n    y = \"Density\",\n    fill = \"Scenario\",\n    color = \"Scenario\"\n  ) +\n  theme_minimal(base_size = 20)+\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#extension-2-correlated-variables",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#extension-2-correlated-variables",
    "title": "Equity Valuation and Simulation",
    "section": "Extension 2: Correlated Variables",
    "text": "Extension 2: Correlated Variables\n\nOutlineCodeOutput\n\n\n\nIn the baseline case, we have each random variable sampled independently from a normal distribution (i.e, i.i.d sampling). This can lead to higher variability and wider spread in the results that are unlikely to happen\nBy introducing a correlation structure between variables, we can constrain the possible outcomes, narrowing the distribution, leading to more realistic and stable simulations where changes in one variable influence the others\nIn what follows, we will keep \\(r\\) and \\(g\\) mean and standard deviation, but will now assume that the correlation between these two variables is \\(\\rho=0.9\\):\n\nWhenever \\(g\\) is high, it might be because the firm is in its earlier stages, so \\(r\\) should be higher\nWhenever \\(g\\) is low, firm may have reached its market peak, so \\(r\\) should be lower\n\n\n\n\n\nr_norm &lt;- rnorm(n_sim, mean = 0.08, sd = 0.01)\ng_norm &lt;- rnorm(n_sim, mean = 0.03, sd = 0.005)\ndf_norm &lt;- simulate_values(r_norm, g_norm, \"Uncorrelated\")\n\n# Simulate r and g with negative correlation\nmu_r &lt;- 0.08\nmu_g &lt;- 0.03\nsigma_r &lt;- 0.01\nsigma_g &lt;- 0.005\ncorr_rg &lt;- 0.9\n\n# Covariance matrix\ncov_matrix &lt;- matrix(c(sigma_r^2, corr_rg * sigma_r * sigma_g, corr_rg * sigma_r * sigma_g, sigma_g^2), \n                     nrow = 2, ncol = 2)\n\n# Simulate r and g from the bivariate normal distribution\nr_g_correlated &lt;- mvrnorm(n_sim, mu = c(mu_r, mu_g), Sigma = cov_matrix)\nr_corr &lt;- r_g_correlated[, 1]\ng_corr &lt;- r_g_correlated[, 2]\ndf_corr &lt;- simulate_values(r_corr, g_corr, \"Correlated\")\n\n# Combine results\ndf_all &lt;- rbind(df_norm, df_corr)\n\nggplot(df_all, aes(x = Value, fill = Scenario, color = Scenario)) +\n  geom_density(alpha = 0.6, size = 1.5) +\n  scale_x_continuous(labels = dollar, limits = c(0, 20000)) +  # Set x-axis limits\n  scale_fill_manual(values = c(\"Uncorrelated\" = \"#2c7fb8\", \"Correlated\" = \"#f03b20\")) +\n  scale_color_manual(values = c(\"Uncorrelated\" = \"#2c7fb8\", \"Correlated\" = \"#f03b20\")) +\n  labs(\n    title = \"Monte Carlo Valuation Distribution Uncorrelated vs. Strong Correlation\",\n    x = \"Firm Value\",\n    y = \"Density\",\n    fill = \"Scenario\",\n    color = \"Scenario\"\n  ) +\n  theme_minimal(base_size = 20)+\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#extension-3-varying-the-sample-size",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#extension-3-varying-the-sample-size",
    "title": "Equity Valuation and Simulation",
    "section": "Extension 3: Varying the Sample Size",
    "text": "Extension 3: Varying the Sample Size\n\nOutlineCodeOutput\n\n\n\nIn short, your Monte Carlo simulation outcome will depend on the number of simulations: the more simulations we run, the more stable and reliable the estimates become.\n\nWith smaller samples, (e.g., \\(n&lt;100\\) or \\(n&lt;1,000\\), results show high variability and may not capture the true distribution.\nWith larger samples, as \\(n\\) (e.g., \\(100,000\\) or \\(1,000,000\\)), the estimates converge toward the expected value\n\nAll in all, increasing sample size reduces random noise, improving the precision and the stability of simulation results\n\n\n\n\n# Simulation sizes\nn_sim_values &lt;- c(100, 1000, 5000, 10000, 100000, 1000000)\n\n# Store all results\nall_results &lt;- data.frame()\n\n# Run simulations\n\nfor (n in n_sim_values) {\n  r_norm &lt;- rnorm(n, mean = 0.08, sd = 0.01)\n  g_norm &lt;- rnorm(n, mean = 0.03, sd = 0.005)\n  \n  df &lt;- simulate_values(r_norm, g_norm, comma(n))\n  all_results &lt;- bind_rows(all_results, df)\n}\n\nall_results$Scenario &lt;-factor(all_results$Scenario,levels=comma(n_sim_values))\n\n# Plot histograms overlaid in one chart\nggplot(all_results, aes(x = Value, fill = Scenario)) +\n  geom_histogram(aes(y=..density..),bins = 60, color = \"white\") +\n  geom_density(size=0.5,fill=NA)+\n  scale_x_continuous(labels = scales::dollar,limits=c(0,25000)) +\n  facet_wrap(Scenario~.,scales='free')+\n  labs(\n    title = \"Monte Carlo Valuation Distribution by Number of Simulations\",\n    subtitle = \"Varying the sample size for draws\",\n    x = \"Firm Value\",\n    y = \"Density\",\n    fill = \"Simulation Size\"\n  ) +\n  theme_minimal(base_size = 15) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#extension-4-bootstrapping",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#extension-4-bootstrapping",
    "title": "Equity Valuation and Simulation",
    "section": "Extension 4: Bootstrapping",
    "text": "Extension 4: Bootstrapping\n\nOutlineCodeOutput\n\n\n\nBootstrapping is a resampling technique that draws samples with replacement from observed data\n\nIt is useful to incorporate empirical variability without assuming a specific parametric distribution\nHelps capture the uncertainty present in historical or observed data\n\nGiven a sample \\(x_1, x_2, \\dots, x_N\\), draw \\(B\\) bootstrap samples, you draw a given number of samples with replacement and compute the statistic of interest (e.g., mean, median) on each bootstrap sample\nBootstrapping can be used, for example, if we have a very limited series of historical values for our random values (e.g, \\(n=20\\)) but want to incorporate variability into the analysis\n\n\n\n\n#Number of simulations\nn_sim=10000\n\n# Historical series\nhistorical_cogs &lt;- c(0.58, 0.6, 0.62, 0.59, 0.61,0.75,0.54,0.66,0.29,0.78)\nhistorical_capex &lt;- c(0.48, 0.5, 0.52, 0.49, 0.51,0.40,0.52,0.59,0.58,0.81)\n\n# For each simulation, bootstrap 10 values and take the mean → length n_sim vector\nboot_cogs_pct &lt;- replicate(n_sim, mean(sample(historical_cogs, 10, replace = TRUE)))\nboot_capex_pct &lt;- replicate(n_sim, mean(sample(historical_capex, 10, replace = TRUE)))\n\nsimulate_values &lt;- function(r, g, cogs_pct, capex_pct, label = 'Simulation') {\n  firm_value &lt;- numeric(n_sim)\n  \n  for (i in 1:n_sim) {\n    sales &lt;- initial_sales\n    nwc_last &lt;- sales * nwc_pct\n    fa_last &lt;- sales * capex_pct[i]\n    fcf &lt;- numeric(5)\n    \n    for (t in 1:5) {\n      sales &lt;- sales * (1 + g[i])\n      nwc_current &lt;- sales * nwc_pct\n      fa_current &lt;- sales * capex_pct[i]\n      \n      delta_nwc &lt;- nwc_current - nwc_last\n      delta_capex &lt;- fa_current - fa_last\n      \n      nwc_last &lt;- nwc_current\n      fa_last &lt;- fa_current\n      \n      fcf[t] &lt;- sales * (1 - cogs_pct[i]) - delta_capex - delta_nwc\n    }\n    \n    fcf_terminal &lt;- fcf[5] * (1 + g[i])\n    perp_value &lt;- fcf_terminal / (r[i] - g[i])\n    \n    discounted_fcf &lt;- sum(fcf / (1 + r[i])^(1:5))\n    discounted_perp &lt;- perp_value / (1 + r[i])^5\n    \n    firm_value[i] &lt;- discounted_fcf + discounted_perp\n  }\n  \n  firm_value &lt;- firm_value[is.finite(firm_value) & (r &gt; g)]\n  \n  data.frame(Value = firm_value, Scenario = label)\n}\n\n\n# Normal scenario (fixed percentages for COGS and CAPEX)\ndf_norm &lt;- simulate_values(r_norm, g_norm, cogs_pct = rep(0.6,n_sim), capex_pct = rep(0.5,n_sim), label = \"Normal\")\n\n# Bootstrap scenario (bootstrapped mean percentages)\ndf_bootstrap &lt;- simulate_values(r_norm, g_norm, cogs_pct = boot_cogs_pct, capex_pct = boot_capex_pct, label = \"Bootstrap\")\n\n#Plot\nggplot() +\n  geom_density(data = df_norm, aes(x = Value, fill = \"Normal\"), alpha = 0.5) +\n  geom_density(data = df_bootstrap, aes(x = Value, fill = \"Bootstrap\"), alpha = 0.5) +\n  scale_x_continuous(labels = dollar)+\n  labs(\n    title = \"Monte Carlo Valuation Distribution: Normal vs. Bootstrap\",\n    x = \"Firm Value\",\n    y = \"Frequency\",\n    fill = 'Approach',\n    color = \"Scenario\"\n  ) +\n  theme_minimal(base_size = 20) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#references",
    "href": "quant-fin/coursework/Lecture 8 - Equity Valuation and Simulation/index.html#references",
    "title": "Equity Valuation and Simulation",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with R. Chapman & Hall/CRC. https://www.tidy-finance.org/r/.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#example-1-a-geographical-deep-dive-through-the-u.s",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#example-1-a-geographical-deep-dive-through-the-u.s",
    "title": "Introduction to Shiny",
    "section": "Example #1: a geographical deep-dive through the U.S",
    "text": "Example #1: a geographical deep-dive through the U.S"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#example-2-a-historical-movie-explorer",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#example-2-a-historical-movie-explorer",
    "title": "Introduction to Shiny",
    "section": "Example #2: a historical movie explorer",
    "text": "Example #2: a historical movie explorer"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#example-3-a-momentum-based-backtester",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#example-3-a-momentum-based-backtester",
    "title": "Introduction to Shiny",
    "section": "Example #3: a momentum-based backtester",
    "text": "Example #3: a momentum-based backtester"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#my-very-first-app",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#my-very-first-app",
    "title": "Introduction to Shiny",
    "section": "My very first app",
    "text": "My very first app"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#adding-a-stock-selector",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#adding-a-stock-selector",
    "title": "Introduction to Shiny",
    "section": "#1: Adding a stock selector",
    "text": "#1: Adding a stock selector\n\nOutlinePromptCode\n\n\n\nYou are off to a good start - however, there are some important points to consider:\n\n\nUsers may want to select stocks other than the Magnificent Seven\nNotwithstanding, asking users to write down the list of assets is too cumbersome, let alone the fact that this is highly prone to human error\nBecause of that, you could ask them to select from a predefined drop-down list that contains a list of valid tickers\n\n\nIn what follows, we will add a drop-down menu that contains all S&P ticker constituents that can be retrieved from the tq_index function, allowing users to select the set of stocks that will be part of the analysis\n\n\n\n\nThis is great! Now, I need to add a stock selector. Use the tq_index() function to retrieve all tickers from S&P500. Add the following structure to the app:\n\n1. Remove the Magnificent Seven. The default selector should have zero stocks.\n2. Add a drop-down menu with all the stocks from the `tq_index('SP500')` function. Allow for a \"Select All\" button.\n\n\n\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(bslib)\nlibrary(scales)\nlibrary(shinyWidgets) # Added for pickerInput\n\n# Get S&P 500 tickers and industry information\nsp500_stocks &lt;- tq_index(\"SP500\") %&gt;%\n  select(symbol, company, sector) %&gt;%\n  arrange(sector, company)\n\nui &lt;- page_sidebar(\n  title = \"S&P 500 Return Comparison\",\n  \n  sidebar = sidebar(\n    title = \"Settings\",\n    \n    dateRangeInput(\n      \"date_range\",\n      \"Select date range:\",\n      start = Sys.Date() - 365,\n      end = Sys.Date(),\n      min = \"2015-01-01\",\n      max = Sys.Date()\n    ),\n    \n    hr(),\n    \n    # Stock selector with search functionality\n    pickerInput(\n      inputId = \"stock_selector\",\n      label = \"Select Stocks:\",\n      choices = setNames(sp500_stocks$symbol, paste0(sp500_stocks$symbol, \" - \", sp500_stocks$company)),\n      selected = NULL, # Default to no selection\n      multiple = TRUE,\n      options = list(\n        `actions-box` = TRUE,\n        `live-search` = TRUE,\n        `selected-text-format` = \"count &gt; 3\",\n        `count-selected-text` = \"{0} stocks selected\"\n      )\n    )\n  ),\n  \n  card(\n    full_screen = TRUE,\n    card_header(\"Cumulative Returns Comparison\"),\n    plotOutput(\"returns_plot\", height = \"500px\")\n  ),\n  \n  card(\n    card_header(\"Performance Summary\"),\n    dataTableOutput(\"data_summary\")\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  # Reactive to store current stock selection\n  selected_stocks &lt;- reactiveVal(NULL) # Start with no stocks selected\n  \n  # Update selected_stocks when stock_selector changes\n  observeEvent(input$stock_selector, {\n    selected_stocks(input$stock_selector)\n  })\n  \n  # Get full list of stocks to retrieve\n  stocks_to_get &lt;- reactive({\n    stocks &lt;- selected_stocks()\n    \n    # Ensure we have at least one stock to display\n    if (length(stocks) == 0) {\n      return(NULL)\n    }\n    \n    return(stocks)\n  })\n  \n  # Reactive expression to get and process stock data\n  stock_data &lt;- reactive({\n    # Validate that we have dates and stocks\n    req(input$date_range)\n    req(length(stocks_to_get()) &gt; 0)\n    \n    # Download data\n    tq_data &lt;- tq_get(\n      stocks_to_get(),\n      from = input$date_range[1],\n      to = input$date_range[2],\n      get = \"stock.prices\"\n    )\n    \n    # Check if we got data\n    req(nrow(tq_data) &gt; 0)\n    \n    # Calculate daily returns\n    returns_data &lt;- tq_data %&gt;%\n      group_by(symbol) %&gt;%\n      tq_transmute(\n        select = adjusted,\n        mutate_fun = periodReturn,\n        period = \"daily\",\n        col_rename = \"daily_return\"\n      )\n    \n    # Calculate cumulative returns\n    cum_returns &lt;- returns_data %&gt;%\n      group_by(symbol) %&gt;%\n      mutate(cumulative_return = cumprod(1 + daily_return) - 1) %&gt;%\n      ungroup()\n    \n    list(\n      raw_data = tq_data,\n      cum_returns = cum_returns\n    )\n  })\n  \n  # Get company names for better labels\n  stock_names &lt;- reactive({\n    \n    # Create lookup from sp500 stocks\n    name_lookup &lt;- setNames(sp500_stocks$company, sp500_stocks$symbol)\n    \n  })\n  \n  # Generate returns plot\n  output$returns_plot &lt;- renderPlot({\n    req(stock_data())\n    \n    cum_returns &lt;- stock_data()$cum_returns\n    names_lookup &lt;- stock_names()\n    \n    # Limit the number of stocks shown for better visualization\n    if(nrow(cum_returns) &gt; 0) {\n      ggplot(cum_returns, aes(x = date, y = cumulative_return, color = symbol)) +\n        geom_line(linewidth = 1) +\n        scale_y_continuous(labels = percent) +\n        scale_color_discrete(labels = function(x) ifelse(x %in% names(names_lookup), names_lookup[x], x)) +\n        labs(\n          title = \"Cumulative Returns Comparison\",\n          x = \"Date\",\n          y = \"Cumulative Return\",\n          color = \"Company\"\n        ) +\n        theme_minimal() +\n        theme(\n          legend.position = \"bottom\",\n          legend.box = \"horizontal\",\n          plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n          axis.title = element_text(face = \"bold\"),\n          legend.title = element_text(face = \"bold\")\n        )\n    }\n  })\n  \n  # Generate data summary\n  output$data_summary &lt;- renderDataTable({\n    req(stock_data())\n    \n    cum_returns &lt;- stock_data()$cum_returns\n    names_lookup &lt;- stock_names()\n    \n    # Summary statistics for each asset\n    summary_data &lt;- cum_returns %&gt;%\n      group_by(symbol) %&gt;%\n      summarize(\n        Start_Date = min(date),\n        End_Date = max(date),\n        Total_Return = last(cumulative_return) %&gt;% round(4),\n        Annualized_Return = ((1 + last(cumulative_return))^(252 / n())) - 1 %&gt;% round(4),\n        Volatility = sd(daily_return, na.rm = TRUE) * sqrt(252) %&gt;% round(4)\n      ) %&gt;%\n      mutate(\n        Company = ifelse(symbol %in% names(names_lookup), names_lookup[symbol], symbol),\n        Total_Return = percent(Total_Return, accuracy = 0.01),\n        Annualized_Return = percent(Annualized_Return, accuracy = 0.01),\n        Volatility = percent(Volatility, accuracy = 0.01)\n      ) %&gt;%\n      select(symbol, Company, Start_Date, End_Date, Total_Return, Annualized_Return, Volatility)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#interactivity",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#interactivity",
    "title": "Introduction to Shiny",
    "section": "#2: Interactivity",
    "text": "#2: Interactivity\n\nOutlinePromptCode\n\n\n\nHooray! You have a minimal working solution for all S&P 500 stocks covered in Yahoo! Finance. It is now time to look at the aesthetics:\n\nAs you increase the number of stocks, understanding the line chart becomes increasingly difficult\nOne way to enhance the user experience is to add interactivity to your chart, allowing users to hover on specific data points, filtering the date ranges directly in the chart, and zoom in/out on a specific region\n\nIn what follows, we will change the charting engine from ggplot2 to highcharter, a solution that allows for interactive time-series visualization\n\n\n\n\nThis is great! Now, I need to change the way that the chart aesthetics are looking. Please considers the instructions below:\n\n  1. Change the plotting engine from ggplot2 to highcharter, using the `hchart` function to create a line chart\n  2. Allow for time range selectors and, when users hover on a specific data point, use the tooltip to show the cumulative return up to that point, and the stock name. The tooltip and the y-axis must show the cumulative returns with the following format: '0.X%\"\n\n\n\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(bslib)\nlibrary(scales)\nlibrary(shinyWidgets) # For pickerInput\nlibrary(highcharter) # For interactive charts\n\n# Get S&P 500 tickers and industry information\nsp500_stocks &lt;- tq_index(\"SP500\") %&gt;%\n  select(symbol, company, sector) %&gt;%\n  arrange(sector, company)\n\nui &lt;- page_sidebar(\n  title = \"S&P 500 Return Comparison\",\n  \n  sidebar = sidebar(\n    title = \"Settings\",\n    \n    dateRangeInput(\n      \"date_range\",\n      \"Select date range:\",\n      start = Sys.Date() - 365,\n      end = Sys.Date(),\n      min = \"2015-01-01\",\n      max = Sys.Date()\n    ),\n    \n    hr(),\n    \n    # Stock selector with search functionality\n    pickerInput(\n      inputId = \"stock_selector\",\n      label = \"Select Stocks:\",\n      choices = setNames(sp500_stocks$symbol, paste0(sp500_stocks$symbol, \" - \", sp500_stocks$company)),\n      selected = NULL, # Default to no selection\n      multiple = TRUE,\n      options = list(\n        `actions-box` = TRUE,\n        `live-search` = TRUE,\n        `selected-text-format` = \"count &gt; 3\",\n        `count-selected-text` = \"{0} stocks selected\"\n      )\n    )\n  ),\n  \n  card(\n    full_screen = TRUE,\n    card_header(\"Cumulative Returns Comparison\"),\n    highchartOutput(\"returns_plot\", height = \"500px\")\n  ),\n  \n  card(\n    card_header(\"Performance Summary\"),\n    dataTableOutput(\"data_summary\")\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  # Reactive to store current stock selection\n  selected_stocks &lt;- reactiveVal(NULL) # Start with no stocks selected\n  \n  # Update selected_stocks when stock_selector changes\n  observeEvent(input$stock_selector, {\n    selected_stocks(input$stock_selector)\n  })\n  \n  # Get full list of stocks to retrieve\n  stocks_to_get &lt;- reactive({\n    stocks &lt;- selected_stocks()\n    \n    # Ensure we have at least one stock to display\n    if (length(stocks) == 0) {\n      return(NULL)\n    }\n    \n    return(stocks)\n  })\n  \n  # Reactive expression to get and process stock data\n  stock_data &lt;- reactive({\n    # Validate that we have dates and stocks\n    req(input$date_range)\n    req(length(stocks_to_get()) &gt; 0)\n    \n    # Download data\n    tq_data &lt;- tq_get(\n      stocks_to_get(),\n      from = input$date_range[1],\n      to = input$date_range[2],\n      get = \"stock.prices\"\n    )\n    \n    # Check if we got data\n    req(nrow(tq_data) &gt; 0)\n    \n    # Calculate returns\n    returns_data &lt;- tq_data %&gt;%\n      group_by(symbol) %&gt;%\n      arrange(date) %&gt;%\n      # Calculate daily returns\n      tq_transmute(\n        select = adjusted,\n        mutate_fun = periodReturn,\n        period = \"daily\",\n        col_rename = \"daily_return\"\n      ) \n    \n    # Join the returns data with original price data\n    returns_with_prices &lt;- returns_data %&gt;%\n      left_join(\n        tq_data %&gt;% select(symbol, date, adjusted),\n        by = c(\"symbol\", \"date\")\n      )\n    \n    # Calculate cumulative returns from the first date of the period\n    cum_returns &lt;- returns_with_prices %&gt;%\n      group_by(symbol) %&gt;%\n      arrange(date) %&gt;%\n      mutate(\n        # For each stock, calculate the cumulative return starting from the first date\n        cumulative_return = cumprod(1 + daily_return) - 1\n      ) %&gt;%\n      ungroup()\n    \n    list(\n      raw_data = tq_data,\n      cum_returns = cum_returns\n    )\n  })\n  \n  # Get company names for better labels\n  stock_names &lt;- reactive({\n    # Create lookup from sp500 stocks\n    name_lookup &lt;- setNames(sp500_stocks$company, sp500_stocks$symbol)\n    \n    # Return\n    name_lookup\n  })\n  \n  # Generate returns plot with highcharter\n  output$returns_plot &lt;- renderHighchart({\n    req(stock_data())\n    \n    cum_returns &lt;- stock_data()$cum_returns\n    names_lookup &lt;- stock_names()\n    \n    # Convert to format suitable for highcharter\n    if(nrow(cum_returns) &gt; 0) {\n      # Create highchart\n      hc &lt;- highchart(type = \"stock\") %&gt;%\n        hc_title(text = \"Cumulative Returns Comparison\") %&gt;%\n        hc_xAxis(type = \"datetime\", \n                 title = list(text = \"Date\"),\n                 dateTimeLabelFormats = list(\n                   day = '%e of %b',\n                   month = '%b %Y',\n                   year = '%Y'\n                 )) %&gt;%\n        hc_yAxis(\n          title = list(text = \"Cumulative Return\"),\n          labels = list(formatter = JS(\"function() { return (this.value * 100).toFixed(2) + '%'; }\"))\n        ) %&gt;%\n        hc_tooltip(\n          headerFormat = '&lt;span style=\"font-size: 10px\"&gt;{point.key}&lt;/span&gt;&lt;br/&gt;',\n          pointFormatter = JS(\"function() { return '&lt;span style=\\\"color:' + this.color + '\\\"&gt;●&lt;/span&gt; &lt;b&gt;' + this.series.name + '&lt;/b&gt;: &lt;b&gt;' + (this.y * 100).toFixed(2) + '%&lt;/b&gt;&lt;br/&gt;'; }\"),\n          shared = TRUE,\n          crosshairs = TRUE\n        ) %&gt;%\n        hc_plotOptions(series = list(\n          marker = list(enabled = FALSE),\n          states = list(hover = list(lineWidthPlus = 0))\n        )) %&gt;%\n        hc_legend(enabled = TRUE)\n      \n      # Add data for each stock\n      for(stock in unique(cum_returns$symbol)) {\n        stock_data &lt;- cum_returns %&gt;%\n          filter(symbol == stock) %&gt;%\n          select(date, cumulative_return)\n        \n        # Get proper company name\n        company_name &lt;- ifelse(stock %in% names(names_lookup), \n                               names_lookup[stock], \n                               stock)\n        \n        # Format data for highcharter - simplified approach\n        hc_data &lt;- stock_data %&gt;%\n          mutate(\n            x = datetime_to_timestamp(date),\n            y = cumulative_return  # Keep as decimal for proper calculations\n          ) %&gt;%\n          select(x, y) %&gt;%\n          list_parse2()\n        \n        # Add series to chart\n        hc &lt;- hc %&gt;%\n          hc_add_series(\n            data = hc_data,\n            name = paste0(stock, \" - \", company_name),\n            type = \"line\"\n          )\n      }\n      \n      # Return the chart\n      hc\n    }\n  })\n  \n  # Generate data summary\n  output$data_summary &lt;- renderDataTable({\n    req(stock_data())\n    \n    cum_returns &lt;- stock_data()$cum_returns\n    names_lookup &lt;- stock_names()\n    \n    # Summary statistics for each asset\n    summary_data &lt;- cum_returns %&gt;%\n      group_by(symbol) %&gt;%\n      summarize(\n        Start_Date = min(date),\n        End_Date = max(date),\n        Total_Return = last(cumulative_return) %&gt;% round(4),\n        Annualized_Return = ((1 + last(cumulative_return))^(252 / n())) - 1 %&gt;% round(4),\n        Volatility = sd(daily_return, na.rm = TRUE) * sqrt(252) %&gt;% round(4)\n      ) %&gt;%\n      mutate(\n        Company = ifelse(symbol %in% names(names_lookup), names_lookup[symbol], symbol),\n        Total_Return = percent(Total_Return, accuracy = 0.01),\n        Annualized_Return = percent(Annualized_Return, accuracy = 0.01),\n        Volatility = percent(Volatility, accuracy = 0.01)\n      ) %&gt;%\n      select(symbol, Company, Start_Date, End_Date, Total_Return, Annualized_Return, Volatility)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#aesthetic-adjustments",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#aesthetic-adjustments",
    "title": "Introduction to Shiny",
    "section": "#3: Aesthetic Adjustments",
    "text": "#3: Aesthetic Adjustments\n\nOutlinePromptCode\n\n\n\nYou’re on a streak! You app is looking neat: you should consider showcasing that!\nBefore you do that, let’s make sure users can get the most of your app while interacting:\n\nThe width of the sidebar needs to be increased a little bit to allow for text to be displayed\nThe dropdown menu section needs to be more organized\nFinally, you can also make the charts and the table to look nicer by changing the size of the line charts and renaming the column names to be displayed in Title Format\n\nIn what follows, we will ask for these changes with simple instructions to tweak the current code\n\n\n\n\nOk, this is looking neat! Now, I need you to change a few points:\n\n  1. Increase the width of the sidebar by 25%\n\n  2. Use the pickerInput() from the shinyWidgets package to select the stocks. It should be a combination of the form \"ticker (Company)\"\n\n  3. Increase the size of the lines in the hchart() call\n  \n  4. Rename the column names in the table and make it in proper format (e.g, remove underlines and ensure Title Format)\n\n\n\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(bslib)\nlibrary(scales)\nlibrary(shinyWidgets) # For pickerInput and dropdownButton\nlibrary(highcharter) # For interactive charts\n\n# Get S&P 500 tickers and industry information\nsp500_stocks &lt;- tq_index(\"SP500\") %&gt;%\n  select(symbol, company, sector) %&gt;%\n  arrange(sector, company)\n\n# Create choices with company name as supplementary info in front of ticker\n# Format: \"Company Name (TICKER)\"\nticker_labels &lt;- paste0(sp500_stocks$company, \" (\", sp500_stocks$symbol, \")\")\nticker_choices &lt;- setNames(sp500_stocks$symbol, ticker_labels)\n\nui &lt;- page_sidebar(\n  title = \"S&P 500 Return Comparison\",\n  \n  sidebar = sidebar(\n    title = \"Settings\",\n    width = 440, # Increased width \n    \n    dateRangeInput(\n      \"date_range\",\n      \"Select date range:\",\n      start = Sys.Date() - 365,\n      end = Sys.Date(),\n      min = \"2015-01-01\",\n      max = Sys.Date()\n    ),\n    \n    hr(),\n    \n    # Use dropdownButton for stock selection\n    dropdownButton(\n      inputId = \"dropdown_stocks\",\n      label = \"Select Stocks\", \n      icon = icon(\"filter\"),\n      status = \"primary\",\n      width = \"100%\",\n      circle = FALSE,\n      \n      # Stock selector with search functionality\n      pickerInput(\n        inputId = \"stock_selector\",\n        label = \"Select Stocks:\",\n        choices = ticker_choices,\n        options = list(\n          `actions-box` = TRUE,\n          `live-search` = TRUE,\n          `selected-text-format` = \"count &gt; 3\",\n          `count-selected-text` = \"{0} stocks selected\"\n        ),\n        selected = NULL, # Default to no selection\n        multiple = TRUE,\n        width = \"100%\"\n      )\n    )\n  ),\n  \n  card(\n    full_screen = TRUE,\n    card_header(\"Cumulative Returns Comparison\"),\n    highchartOutput(\"returns_plot\", height = \"500px\")\n  ),\n  \n  card(\n    card_header(\"Performance Summary\"),\n    dataTableOutput(\"data_summary\")\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  # Reactive to store current stock selection\n  selected_stocks &lt;- reactiveVal(NULL) # Start with no stocks selected\n  \n  # Update selected_stocks when stock_selector changes\n  observeEvent(input$stock_selector, {\n    selected_stocks(input$stock_selector)\n  })\n  \n  # Get full list of stocks to retrieve\n  stocks_to_get &lt;- reactive({\n    stocks &lt;- selected_stocks()\n    \n    # Ensure we have at least one stock to display\n    if (length(stocks) == 0) {\n      return(NULL)\n    }\n    \n    return(stocks)\n  })\n  \n  # Reactive expression to get and process stock data\n  stock_data &lt;- reactive({\n    # Validate that we have dates and stocks\n    req(input$date_range)\n    req(length(stocks_to_get()) &gt; 0)\n    \n    # Download data\n    tq_data &lt;- tq_get(\n      stocks_to_get(),\n      from = input$date_range[1],\n      to = input$date_range[2],\n      get = \"stock.prices\"\n    )\n    \n    # Check if we got data\n    req(nrow(tq_data) &gt; 0)\n    \n    # Calculate returns\n    returns_data &lt;- tq_data %&gt;%\n      group_by(symbol) %&gt;%\n      arrange(date) %&gt;%\n      # Calculate daily returns\n      tq_transmute(\n        select = adjusted,\n        mutate_fun = periodReturn,\n        period = \"daily\",\n        col_rename = \"daily_return\"\n      ) \n    \n    # Join the returns data with original price data\n    returns_with_prices &lt;- returns_data %&gt;%\n      left_join(\n        tq_data %&gt;% select(symbol, date, adjusted),\n        by = c(\"symbol\", \"date\")\n      )\n    \n    # Calculate cumulative returns from the first date of the period\n    cum_returns &lt;- returns_with_prices %&gt;%\n      group_by(symbol) %&gt;%\n      arrange(date) %&gt;%\n      mutate(\n        # For each stock, calculate the cumulative return starting from the first date\n        cumulative_return = cumprod(1 + daily_return) - 1\n      ) %&gt;%\n      ungroup()\n    \n    list(\n      raw_data = tq_data,\n      cum_returns = cum_returns\n    )\n  })\n  \n  # Get company names for better labels\n  stock_names &lt;- reactive({\n    # Create lookup from sp500 stocks\n    name_lookup &lt;- setNames(sp500_stocks$company, sp500_stocks$symbol)\n    \n    # Return\n    name_lookup\n  })\n  \n  # Generate returns plot with highcharter\n  output$returns_plot &lt;- renderHighchart({\n    req(stock_data())\n    \n    cum_returns &lt;- stock_data()$cum_returns\n    names_lookup &lt;- stock_names()\n    \n    # Convert to format suitable for highcharter\n    if(nrow(cum_returns) &gt; 0) {\n      # Create highchart\n      hc &lt;- highchart(type = \"stock\") %&gt;%\n        hc_title(text = \"Cumulative Returns Comparison\") %&gt;%\n        hc_xAxis(type = \"datetime\", \n                 title = list(text = \"Date\"),\n                 dateTimeLabelFormats = list(\n                   day = '%e of %b',\n                   month = '%b %Y',\n                   year = '%Y'\n                 )) %&gt;%\n        hc_yAxis(\n          title = list(text = \"Cumulative Return\"),\n          labels = list(formatter = JS(\"function() { return (this.value * 100).toFixed(2) + '%'; }\"))\n        ) %&gt;%\n        hc_tooltip(\n          headerFormat = '&lt;span style=\"font-size: 10px\"&gt;{point.key}&lt;/span&gt;&lt;br/&gt;',\n          pointFormatter = JS(\"function() { return '&lt;span style=\\\"color:' + this.color + '\\\"&gt;●&lt;/span&gt; &lt;b&gt;' + this.series.name + '&lt;/b&gt;: &lt;b&gt;' + (this.y * 100).toFixed(2) + '%&lt;/b&gt;&lt;br/&gt;'; }\"),\n          shared = TRUE,\n          crosshairs = TRUE\n        ) %&gt;%\n        hc_plotOptions(series = list(\n          marker = list(enabled = FALSE),\n          states = list(hover = list(lineWidthPlus = 0)),\n          lineWidth = 3  # Increased line thickness\n        )) %&gt;%\n        hc_legend(enabled = TRUE)\n      \n      # Add data for each stock\n      for(stock in unique(cum_returns$symbol)) {\n        stock_data &lt;- cum_returns %&gt;%\n          filter(symbol == stock) %&gt;%\n          select(date, cumulative_return)\n        \n        # Format data for highcharter - simplified approach\n        hc_data &lt;- stock_data %&gt;%\n          mutate(\n            x = datetime_to_timestamp(date),\n            y = cumulative_return  # Keep as decimal for proper calculations\n          ) %&gt;%\n          select(x, y) %&gt;%\n          list_parse2()\n        \n        # Add series to chart - only use the ticker symbol in the name\n        hc &lt;- hc %&gt;%\n          hc_add_series(\n            data = hc_data,\n            name = stock,\n            type = \"line\"\n          )\n      }\n      \n      # Return the chart\n      hc\n    }\n  })\n  \n  # Generate data summary\n  output$data_summary &lt;- renderDataTable({\n    req(stock_data())\n    \n    cum_returns &lt;- stock_data()$cum_returns\n    names_lookup &lt;- stock_names()\n    \n    # Summary statistics for each asset\n    summary_data &lt;- cum_returns %&gt;%\n      group_by(symbol) %&gt;%\n      summarize(\n        Start_Date = min(date),\n        End_Date = max(date),\n        Total_Return = last(cumulative_return) %&gt;% round(4),\n        Annualized_Return = ((1 + last(cumulative_return))^(252 / n())) - 1 %&gt;% round(4),\n        Volatility = sd(daily_return, na.rm = TRUE) * sqrt(252) %&gt;% round(4)\n      ) %&gt;%\n      mutate(\n        Company = ifelse(symbol %in% names(names_lookup), names_lookup[symbol], symbol),\n        Total_Return = percent(Total_Return, accuracy = 0.01),\n        Annualized_Return = percent(Annualized_Return, accuracy = 0.01),\n        Volatility = percent(Volatility, accuracy = 0.01)\n      ) %&gt;%\n      select(\n        Symbol = symbol, \n        Company, \n        `Start Date` = Start_Date, \n        `End Date` = End_Date, \n        `Total Return` = Total_Return, \n        `Annualized Return` = Annualized_Return, \n        Volatility\n      )\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#final-adjustments",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#final-adjustments",
    "title": "Introduction to Shiny",
    "section": "#4: Final Adjustments",
    "text": "#4: Final Adjustments\n\nOutlinePromptCode\n\n\n\nLike a pro! Your app is now looking like it’s something you would pay for!\nBefore you showcase that to our boss, remember: you should make sure the app is not crashing due to user interaction:\n\nFor example, if a user selects “All Stocks”, tq_get() will automatically start downloading a lot of data - which might break the server\nFurthermore, users should have the ability to confirm their selections and, eventually, download a summary of their findings. The submit button should send an alert saying that the list has been submitted and results will appear soon.\n\nIn what follows, we will create some selection handling procedures and insert a Download Button that creates a downloadable .csv file\n\n\n\n\nWay to go! Now, here's a list of final adjustments to be made:\n\n1. Create a \"Submit List\" button that, when triggered, will update the charts and tables. The app shouldn't update without that.\n\n2. Allow for dark mode using a switch button in the header\n\n3. Create a download button that, when triggered, saves a .csv file with the contents from the top panel (time series of cumulative returns for each stock). The submit button should send an alert saying that the list has been submitted and results will appear soon using the sendSweetAlert() function.\n\n4. Add icons in the title, Settings, and Submit configuration buttons  \n\n\n\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(bslib)\nlibrary(scales)\nlibrary(shinyWidgets) # For pickerInput, dropdownButton, and sweetAlert\nlibrary(highcharter) # For interactive charts\n\n# Get S&P 500 tickers and industry information\nsp500_stocks &lt;- tq_index(\"SP500\") %&gt;%\n  select(symbol, company, sector) %&gt;%\n  arrange(sector, company)\n\n# Create choices with company name as supplementary info in front of ticker\n# Format: \"Company Name (TICKER)\"\nticker_labels &lt;- paste0(sp500_stocks$company, \" (\", sp500_stocks$symbol, \")\")\nticker_choices &lt;- setNames(sp500_stocks$symbol, ticker_labels)\n\nui &lt;- page_sidebar(\n  title = span(icon(\"chart-line\"), \"S&P 500 Return Comparison\"),\n  theme = bs_theme(version = 5, preset = \"default\"),\n  \n  header = tags$div(\n    style = \"display: flex; justify-content: flex-end; padding: 5px;\",\n    materialSwitch(\n      inputId = \"dark_mode\",\n      label = span(icon(\"moon\"), \"Dark Mode\"),\n      status = \"primary\",\n      right = TRUE\n    )\n  ),\n  \n  sidebar = sidebar(\n    title = span(icon(\"gear\"), \"Settings\"),\n    width = 440, # Increased width \n    \n    dateRangeInput(\n      \"date_range\",\n      \"Select date range:\",\n      start = Sys.Date() - 365,\n      end = Sys.Date(),\n      min = \"2015-01-01\",\n      max = Sys.Date()\n    ),\n    \n    hr(),\n    \n    # Use dropdownButton for stock selection\n    dropdownButton(\n      inputId = \"dropdown_stocks\",\n      label = span(icon(\"filter\"), \"Select Stocks\"), \n      status = \"primary\",\n      width = \"100%\",\n      circle = FALSE,\n      \n      # Stock selector with search functionality\n      pickerInput(\n        inputId = \"stock_selector\",\n        label = \"Select Stocks:\",\n        choices = ticker_choices,\n        options = list(\n          `actions-box` = TRUE,\n          `live-search` = TRUE,\n          `selected-text-format` = \"count &gt; 3\",\n          `count-selected-text` = \"{0} stocks selected\"\n        ),\n        selected = NULL, # Default to no selection\n        multiple = TRUE,\n        width = \"100%\"\n      )\n    ),\n    \n    hr(),\n    \n    # Submit button that triggers the data update\n    actionButton(\n      \"submit_btn\", \n      span(icon(\"paper-plane\"), \"Submit List\"),\n      width = \"100%\",\n      class = \"btn-primary\"\n    )\n  ),\n  \n  card(\n    full_screen = TRUE,\n    card_header(\n      div(\n        style = \"display: flex; justify-content: space-between; align-items: center;\",\n        span(\"Cumulative Returns Comparison\"),\n        downloadButton(\"download_data\", \"Download Data\", class = \"btn-sm btn-outline-primary\")\n      )\n    ),\n    highchartOutput(\"returns_plot\", height = \"500px\")\n  ),\n  \n  card(\n    card_header(\"Performance Summary\"),\n    dataTableOutput(\"data_summary\")\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  # Toggle dark mode when the switch is clicked\n  observeEvent(input$dark_mode, {\n    if (input$dark_mode) {\n      session$setCurrentTheme(bs_theme(version = 5, preset = \"darkly\"))\n    } else {\n      session$setCurrentTheme(bs_theme(version = 5, preset = \"default\"))\n    }\n  })\n  \n  # Reactive values to store selected stocks\n  selected_stocks &lt;- reactiveVal(NULL)\n  \n  # Only update the stocks to fetch when the submit button is clicked\n  observeEvent(input$submit_btn, {\n    # Show a SweetAlert notification\n    sendSweetAlert(\n      session = session,\n      title = \"List Submitted\", \n      text = \"Your stock list has been submitted. Results will appear soon.\",\n      type = \"success\",\n      timer = 3000,\n      showConfirmButton = FALSE\n    )\n    \n    # Update the selected stocks\n    selected_stocks(input$stock_selector)\n  })\n  \n  # Get full list of stocks to retrieve\n  stocks_to_get &lt;- reactive({\n    stocks &lt;- selected_stocks()\n    \n    # Ensure we have at least one stock to display\n    if (length(stocks) == 0) {\n      return(NULL)\n    }\n    \n    return(stocks)\n  })\n  \n  # Reactive expression to get and process stock data\n  stock_data &lt;- reactive({\n    # Validate that we have dates and stocks\n    req(input$date_range)\n    req(length(stocks_to_get()) &gt; 0)\n    \n    # Download data\n    tq_data &lt;- tq_get(\n      stocks_to_get(),\n      from = input$date_range[1],\n      to = input$date_range[2],\n      get = \"stock.prices\"\n    )\n    \n    # Check if we got data\n    req(nrow(tq_data) &gt; 0)\n    \n    # Calculate returns\n    returns_data &lt;- tq_data %&gt;%\n      group_by(symbol) %&gt;%\n      arrange(date) %&gt;%\n      # Calculate daily returns\n      tq_transmute(\n        select = adjusted,\n        mutate_fun = periodReturn,\n        period = \"daily\",\n        col_rename = \"daily_return\"\n      ) \n    \n    # Join the returns data with original price data\n    returns_with_prices &lt;- returns_data %&gt;%\n      left_join(\n        tq_data %&gt;% select(symbol, date, adjusted),\n        by = c(\"symbol\", \"date\")\n      )\n    \n    # Calculate cumulative returns from the first date of the period\n    cum_returns &lt;- returns_with_prices %&gt;%\n      group_by(symbol) %&gt;%\n      arrange(date) %&gt;%\n      mutate(\n        # For each stock, calculate the cumulative return starting from the first date\n        cumulative_return = cumprod(1 + daily_return) - 1\n      ) %&gt;%\n      ungroup()\n    \n    list(\n      raw_data = tq_data,\n      cum_returns = cum_returns\n    )\n  })\n  \n  # Create downloadable data\n  downloadable_data &lt;- reactive({\n    req(stock_data())\n    \n    # Format the cumulative returns data for download\n    download_data &lt;- stock_data()$cum_returns %&gt;%\n      select(symbol, date, cumulative_return) %&gt;%\n      pivot_wider(\n        names_from = symbol,\n        values_from = cumulative_return,\n        names_prefix = \"return_\"\n      ) %&gt;%\n      arrange(date)\n    \n    return(download_data)\n  })\n  \n  # Get company names for better labels\n  stock_names &lt;- reactive({\n    # Create lookup from sp500 stocks\n    name_lookup &lt;- setNames(sp500_stocks$company, sp500_stocks$symbol)\n    \n    # Return\n    name_lookup\n  })\n  \n  # Generate returns plot with highcharter\n  output$returns_plot &lt;- renderHighchart({\n    req(stock_data())\n    \n    cum_returns &lt;- stock_data()$cum_returns\n    names_lookup &lt;- stock_names()\n    \n    # Convert to format suitable for highcharter\n    if(nrow(cum_returns) &gt; 0) {\n      # Create highchart\n      hc &lt;- highchart(type = \"stock\") %&gt;%\n        hc_title(text = \"Cumulative Returns Comparison\") %&gt;%\n        hc_xAxis(type = \"datetime\", \n                 title = list(text = \"Date\"),\n                 dateTimeLabelFormats = list(\n                   day = '%e of %b',\n                   month = '%b %Y',\n                   year = '%Y'\n                 )) %&gt;%\n        hc_yAxis(\n          title = list(text = \"Cumulative Return\"),\n          labels = list(formatter = JS(\"function() { return (this.value * 100).toFixed(2) + '%'; }\"))\n        ) %&gt;%\n        hc_tooltip(\n          headerFormat = '&lt;span style=\"font-size: 10px\"&gt;{point.key}&lt;/span&gt;&lt;br/&gt;',\n          pointFormatter = JS(\"function() { return '&lt;span style=\\\"color:' + this.color + '\\\"&gt;●&lt;/span&gt; &lt;b&gt;' + this.series.name + '&lt;/b&gt;: &lt;b&gt;' + (this.y * 100).toFixed(2) + '%&lt;/b&gt;&lt;br/&gt;'; }\"),\n          shared = TRUE,\n          crosshairs = TRUE\n        ) %&gt;%\n        hc_plotOptions(series = list(\n          marker = list(enabled = FALSE),\n          states = list(hover = list(lineWidthPlus = 0)),\n          lineWidth = 3  # Increased line thickness\n        )) %&gt;%\n        hc_legend(enabled = TRUE)\n      \n      # Add data for each stock\n      for(stock in unique(cum_returns$symbol)) {\n        stock_data &lt;- cum_returns %&gt;%\n          filter(symbol == stock) %&gt;%\n          select(date, cumulative_return)\n        \n        # Format data for highcharter - simplified approach\n        hc_data &lt;- stock_data %&gt;%\n          mutate(\n            x = datetime_to_timestamp(date),\n            y = cumulative_return  # Keep as decimal for proper calculations\n          ) %&gt;%\n          select(x, y) %&gt;%\n          list_parse2()\n        \n        # Add series to chart - only use the ticker symbol in the name\n        hc &lt;- hc %&gt;%\n          hc_add_series(\n            data = hc_data,\n            name = stock,\n            type = \"line\"\n          )\n      }\n      \n      # Return the chart\n      hc\n    }\n  })\n  \n  # Generate data summary\n  output$data_summary &lt;- renderDataTable({\n    req(stock_data())\n    \n    cum_returns &lt;- stock_data()$cum_returns\n    names_lookup &lt;- stock_names()\n    \n    # Summary statistics for each asset\n    summary_data &lt;- cum_returns %&gt;%\n      group_by(symbol) %&gt;%\n      summarize(\n        Start_Date = min(date),\n        End_Date = max(date),\n        Total_Return = last(cumulative_return) %&gt;% round(4),\n        Annualized_Return = ((1 + last(cumulative_return))^(252 / n())) - 1 %&gt;% round(4),\n        Volatility = sd(daily_return, na.rm = TRUE) * sqrt(252) %&gt;% round(4)\n      ) %&gt;%\n      mutate(\n        Company = ifelse(symbol %in% names(names_lookup), names_lookup[symbol], symbol),\n        Total_Return = percent(Total_Return, accuracy = 0.01),\n        Annualized_Return = percent(Annualized_Return, accuracy = 0.01),\n        Volatility = percent(Volatility, accuracy = 0.01)\n      ) %&gt;%\n      select(\n        Symbol = symbol, \n        Company, \n        `Start Date` = Start_Date, \n        `End Date` = End_Date, \n        `Total Return` = Total_Return, \n        `Annualized Return` = Annualized_Return, \n        Volatility\n      )\n  })\n  \n  # Download handler for CSV export\n  output$download_data &lt;- downloadHandler(\n    filename = function() {\n      paste(\"sp500_returns_\", format(Sys.Date(), \"%Y%m%d\"), \".csv\", sep = \"\")\n    },\n    content = function(file) {\n      write.csv(downloadable_data(), file, row.names = FALSE)\n    }\n  )\n\n}\n  \nshinyApp(ui, server)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#adding-interactivity",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#adding-interactivity",
    "title": "Introduction to Shiny",
    "section": "#2: Adding Interactivity",
    "text": "#2: Adding Interactivity\n\nOutlinePromptCode\n\n\n\nHooray! You have a minimal working solution for all S&P 500 stocks covered in Yahoo! Finance. It is now time to look at the aesthetics:\n\nAs you increase the number of stocks, understanding the line chart becomes increasingly difficult\nOne way to enhance the user experience is to add interactivity to your chart, allowing users to hover on specific data points, filtering the date ranges directly in the chart, and zoom in/out on a specific region\n\nIn what follows, we will change the charting engine from ggplot2 to highcharter, a solution that allows for interactive time-series visualization\n\n\n\n\nThis is great! Now, I need to change the way that the chart aesthetics are looking. Please considers the instructions below:\n\n  1. Change the plotting engine from ggplot2 to highcharter, using the `hchart` function to create a line chart\n  2. Allow for time range selectors and, when users hover on a specific data point, use the tooltip to show the cumulative return up to that point, and the stock name. The tooltip and the y-axis must show the cumulative returns with the following format: '0.X%\"\n\n\n\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(bslib)\nlibrary(scales)\nlibrary(shinyWidgets) # For pickerInput\nlibrary(highcharter) # For interactive charts\n\n# Get S&P 500 tickers and industry information\nsp500_stocks &lt;- tq_index(\"SP500\") %&gt;%\n  select(symbol, company, sector) %&gt;%\n  arrange(sector, company)\n\nui &lt;- page_sidebar(\n  title = \"S&P 500 Return Comparison\",\n  \n  sidebar = sidebar(\n    title = \"Settings\",\n    \n    dateRangeInput(\n      \"date_range\",\n      \"Select date range:\",\n      start = Sys.Date() - 365,\n      end = Sys.Date(),\n      min = \"2015-01-01\",\n      max = Sys.Date()\n    ),\n    \n    hr(),\n    \n    # Stock selector with search functionality\n    pickerInput(\n      inputId = \"stock_selector\",\n      label = \"Select Stocks:\",\n      choices = setNames(sp500_stocks$symbol, paste0(sp500_stocks$symbol, \" - \", sp500_stocks$company)),\n      selected = NULL, # Default to no selection\n      multiple = TRUE,\n      options = list(\n        `actions-box` = TRUE,\n        `live-search` = TRUE,\n        `selected-text-format` = \"count &gt; 3\",\n        `count-selected-text` = \"{0} stocks selected\"\n      )\n    )\n  ),\n  \n  card(\n    full_screen = TRUE,\n    card_header(\"Cumulative Returns Comparison\"),\n    highchartOutput(\"returns_plot\", height = \"500px\")\n  ),\n  \n  card(\n    card_header(\"Performance Summary\"),\n    dataTableOutput(\"data_summary\")\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  # Reactive to store current stock selection\n  selected_stocks &lt;- reactiveVal(NULL) # Start with no stocks selected\n  \n  # Update selected_stocks when stock_selector changes\n  observeEvent(input$stock_selector, {\n    selected_stocks(input$stock_selector)\n  })\n  \n  # Get full list of stocks to retrieve\n  stocks_to_get &lt;- reactive({\n    stocks &lt;- selected_stocks()\n    \n    # Ensure we have at least one stock to display\n    if (length(stocks) == 0) {\n      return(NULL)\n    }\n    \n    return(stocks)\n  })\n  \n  # Reactive expression to get and process stock data\n  stock_data &lt;- reactive({\n    # Validate that we have dates and stocks\n    req(input$date_range)\n    req(length(stocks_to_get()) &gt; 0)\n    \n    # Download data\n    tq_data &lt;- tq_get(\n      stocks_to_get(),\n      from = input$date_range[1],\n      to = input$date_range[2],\n      get = \"stock.prices\"\n    )\n    \n    # Check if we got data\n    req(nrow(tq_data) &gt; 0)\n    \n    # Calculate returns\n    returns_data &lt;- tq_data %&gt;%\n      group_by(symbol) %&gt;%\n      arrange(date) %&gt;%\n      # Calculate daily returns\n      tq_transmute(\n        select = adjusted,\n        mutate_fun = periodReturn,\n        period = \"daily\",\n        col_rename = \"daily_return\"\n      ) \n    \n    # Join the returns data with original price data\n    returns_with_prices &lt;- returns_data %&gt;%\n      left_join(\n        tq_data %&gt;% select(symbol, date, adjusted),\n        by = c(\"symbol\", \"date\")\n      )\n    \n    # Calculate cumulative returns from the first date of the period\n    cum_returns &lt;- returns_with_prices %&gt;%\n      group_by(symbol) %&gt;%\n      arrange(date) %&gt;%\n      mutate(\n        # For each stock, calculate the cumulative return starting from the first date\n        cumulative_return = cumprod(1 + daily_return) - 1\n      ) %&gt;%\n      ungroup()\n    \n    list(\n      raw_data = tq_data,\n      cum_returns = cum_returns\n    )\n  })\n  \n  # Get company names for better labels\n  stock_names &lt;- reactive({\n    # Create lookup from sp500 stocks\n    name_lookup &lt;- setNames(sp500_stocks$company, sp500_stocks$symbol)\n    \n    # Return\n    name_lookup\n  })\n  \n  # Generate returns plot with highcharter\n  output$returns_plot &lt;- renderHighchart({\n    req(stock_data())\n    \n    cum_returns &lt;- stock_data()$cum_returns\n    names_lookup &lt;- stock_names()\n    \n    # Convert to format suitable for highcharter\n    if(nrow(cum_returns) &gt; 0) {\n      # Create highchart\n      hc &lt;- highchart(type = \"stock\") %&gt;%\n        hc_title(text = \"Cumulative Returns Comparison\") %&gt;%\n        hc_xAxis(type = \"datetime\", \n                 title = list(text = \"Date\"),\n                 dateTimeLabelFormats = list(\n                   day = '%e of %b',\n                   month = '%b %Y',\n                   year = '%Y'\n                 )) %&gt;%\n        hc_yAxis(\n          title = list(text = \"Cumulative Return\"),\n          labels = list(formatter = JS(\"function() { return (this.value * 100).toFixed(2) + '%'; }\"))\n        ) %&gt;%\n        hc_tooltip(\n          headerFormat = '&lt;span style=\"font-size: 10px\"&gt;{point.key}&lt;/span&gt;&lt;br/&gt;',\n          pointFormatter = JS(\"function() { return '&lt;span style=\\\"color:' + this.color + '\\\"&gt;●&lt;/span&gt; &lt;b&gt;' + this.series.name + '&lt;/b&gt;: &lt;b&gt;' + (this.y * 100).toFixed(2) + '%&lt;/b&gt;&lt;br/&gt;'; }\"),\n          shared = TRUE,\n          crosshairs = TRUE\n        ) %&gt;%\n        hc_plotOptions(series = list(\n          marker = list(enabled = FALSE),\n          states = list(hover = list(lineWidthPlus = 0))\n        )) %&gt;%\n        hc_legend(enabled = TRUE)\n      \n      # Add data for each stock\n      for(stock in unique(cum_returns$symbol)) {\n        stock_data &lt;- cum_returns %&gt;%\n          filter(symbol == stock) %&gt;%\n          select(date, cumulative_return)\n        \n        # Get proper company name\n        company_name &lt;- ifelse(stock %in% names(names_lookup), \n                               names_lookup[stock], \n                               stock)\n        \n        # Format data for highcharter - simplified approach\n        hc_data &lt;- stock_data %&gt;%\n          mutate(\n            x = datetime_to_timestamp(date),\n            y = cumulative_return  # Keep as decimal for proper calculations\n          ) %&gt;%\n          select(x, y) %&gt;%\n          list_parse2()\n        \n        # Add series to chart\n        hc &lt;- hc %&gt;%\n          hc_add_series(\n            data = hc_data,\n            name = paste0(stock, \" - \", company_name),\n            type = \"line\"\n          )\n      }\n      \n      # Return the chart\n      hc\n    }\n  })\n  \n  # Generate data summary\n  output$data_summary &lt;- renderDataTable({\n    req(stock_data())\n    \n    cum_returns &lt;- stock_data()$cum_returns\n    names_lookup &lt;- stock_names()\n    \n    # Summary statistics for each asset\n    summary_data &lt;- cum_returns %&gt;%\n      group_by(symbol) %&gt;%\n      summarize(\n        Start_Date = min(date),\n        End_Date = max(date),\n        Total_Return = last(cumulative_return) %&gt;% round(4),\n        Annualized_Return = ((1 + last(cumulative_return))^(252 / n())) - 1 %&gt;% round(4),\n        Volatility = sd(daily_return, na.rm = TRUE) * sqrt(252) %&gt;% round(4)\n      ) %&gt;%\n      mutate(\n        Company = ifelse(symbol %in% names(names_lookup), names_lookup[symbol], symbol),\n        Total_Return = percent(Total_Return, accuracy = 0.01),\n        Annualized_Return = percent(Annualized_Return, accuracy = 0.01),\n        Volatility = percent(Volatility, accuracy = 0.01)\n      ) %&gt;%\n      select(symbol, Company, Start_Date, End_Date, Total_Return, Annualized_Return, Volatility)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#hosting-our-application",
    "href": "quant-fin/coursework/Lecture 10 - Introduction to Shiny/index.html#hosting-our-application",
    "title": "Introduction to Shiny",
    "section": "Hosting our application",
    "text": "Hosting our application\n\nUsing the first option for deployment, inside your RStudio session, hit Publish and then select shinyapps.io. This option prompts a screen where you can log into your account. You can create a free account and host a limited number of applications in a free-tier option\nAfter following the instructions, you should see your app going live on the internet straight from RStudio’s command line:\n\n\n\n\n\n\n\n\nCurious to see the outcome? Check the live app here"
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3-solutions.html",
    "href": "quant-fin/datacases/data-case-3/data-case-3-solutions.html",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "This Data Case is part of the Practical Applications in Quantitative Finance course, held at FGV-EAESP’s undergraduate course in business. Carefully follow the instructions contained in the data case as well as eClass® before you make your submission.\n\n\nIn financial valuation, estimating the fair price of an asset often requires assumptions about future growth rates and discount rates. A common tool is the Gordon Growth Model, which expresses the value of a stock as:\n\\[\nP_0 = \\dfrac{D_1}{r-g}\n\\]\nWhere \\(P_0\\) is the current, share price, \\(D_1\\) is the next’s period dividend, \\(r\\) is the discount rate, and \\(g\\) is the growth rate in perpetuity. In words, the share price in period \\(0\\) is the sum of the (expected) future dividends (beginning in period \\(1\\)) up to infinity discounted by the cost of opportunity. Dividends in periods \\(2,3,...,t\\rightarrow\\infty\\) are expected grow linearly by a factor \\(g\\).\nIn practice, you generally know the next period’s dividend, but \\(r\\) and \\(g\\) are both uncertain. In this data case, you will explore how Monte Carlo simulations can be used to evaluate the distribution of potential stock prices under uncertainty in inputs.\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®.\n\n\n\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\", \"mvtnorm\", \"rsample\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('mvtnorm')\n  install.packages('rsample')\n\n#Load\n  library(tidyverse)\n  library(mvtnorm)\n  library(rsample)\n\n\n\n\nSuppose that \\(D_1=\\$2.00\\). Assume the growth rate is fixed at \\(g=3\\%\\). Simulate 10,000 values of \\(r\\), assuming it follows a normal distribution with mean \\(\\mu=8\\%\\) and standard deviation \\(\\sigma=1\\%\\). Calculate the corresponding distribution of stock prices.\n\n\n\n\n\n\nHint\n\n\n\nMake sure to set a seed (set.seed(123)) for reproducibility and ensure that \\(r&gt;g\\) for each simulation.\n\n\n\nset.seed(123)\nD1 &lt;- 2\ng &lt;- 0.03\nr_sim &lt;- rnorm(10000, mean = 0.08, sd = 0.01)\nr_sim &lt;- r_sim[r_sim &gt; g]  # Filter out invalid draws\nP0_sim &lt;- D1 / (r_sim - g)\n\ntibble(P0 = P0_sim) %&gt;%\n  ggplot(aes(x = P0)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", color = \"white\") +\n  scale_y_continuous(labels=scales::comma)+\n  scale_x_continuous(labels=scales::dollar)+\n  labs(title = \"Distribution of P0 from Uncertainty in r\",\n       subtitle = 'Drawing 10,000 simulations.',\n       x = \"P0\", y = \"Frequency\")+\n  theme_minimal(base_size = 20)+\n  theme(plot.title  = element_text(face='bold'))\n\n\n\n\n\n\n\n\n\n\n\nNow, suppose both \\(r\\) and \\(g\\) are random variables:\n\\[\nr \\sim \\mathcal N(0.08,0.01^2)\n\\] \\[\ng \\sim \\mathcal N(0.03,0.005^2)\n\\]\nSimulate \\(10,000\\) independent draws for each and calculate the distribution of prices.\n\nset.seed(123)\nr_sim &lt;- rnorm(10000, 0.08, 0.01)\ng_sim &lt;- rnorm(10000, 0.03, 0.005)\n\nvalid &lt;- r_sim &gt; g_sim\nr_sim &lt;- r_sim[valid]\ng_sim &lt;- g_sim[valid]\nP0_sim &lt;- D1 / (r_sim - g_sim)\n\ntibble(P0 = P0_sim) %&gt;%\n  ggplot(aes(x = P0)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", color = \"white\") +\n  scale_y_continuous(labels=scales::comma)+\n  scale_x_continuous(labels=scales::dollar)+\n    labs(title = \"Distribution of P0 from Uncertainty in r and g\",\n         subtitle = 'Drawing 10,000 simulations.',\n         x = \"P0\", y = \"Frequency\")+\n  theme_minimal(base_size = 20)\n\n\n\n\n\n\n\n\n\n\n\nAssume that the correlation between \\(r\\) and \\(g\\) is \\(\\rho=0.9\\). Simulate \\(10,000\\) observations from a bivariate normal distribution where the means and the standard deviations of both \\(r\\) and \\(g\\) remain the same, but now you are imposing a correlation structure of \\(\\rho=0.9\\) when drawing their distributions. How does this result compare to the previous result you found (i.e, when \\(r\\) and \\(g\\) were draw independently from each other)?\n\n\n\n\n\n\nHint\n\n\n\n\nYou can use the mvtnorm to draw multivariate normal distributions.\nTo that matter, you can use the following notation:\n\n\nset.seed(123)\n\nmu &lt;- c(mean1,mean2)\nsigma &lt;- matrix(c(sd1^2, cor*sd1*sd2,\n                  cor*sd1*sd2, sd2^2), ncol = 2)\n\nsim &lt;- rmvnorm(10000, mean = mu, sigma = sigma)\n\nIn words, note that sigma is nothing more than the matrix-covariance matrix of \\(r\\) and \\(g\\).\n\n\n\nmu &lt;- c(0.08, 0.03)\nsigma &lt;- matrix(c(0.01^2, 0.9*0.01*0.005,\n                  0.9*0.01*0.005, 0.005^2), ncol = 2)\n\nset.seed(123)\nsim &lt;- rmvnorm(10000, mean = mu, sigma = sigma)\nr_sim &lt;- sim[, 1]\ng_sim &lt;- sim[, 2]\n\nvalid &lt;- r_sim &gt; g_sim\nP0_sim &lt;- D1 / (r_sim[valid] - g_sim[valid])\n\ntibble(P0 = P0_sim) %&gt;%\n  ggplot(aes(x = P0)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", color = \"white\") +\n  scale_y_continuous(labels=scales::comma)+\n  scale_x_continuous(labels=scales::dollar)+\n    labs(title = \"Distribution of P0 from Uncertainty in r and g\",\n         subtitle = 'Drawing 10,000 simulations from a bivariate normal distribution.',\n         x = \"P0\", y = \"Frequency\")+\n  theme_minimal(base_size = 20)\n\n\n\n\n\n\n\n\n\n\n\nYou are given a sample of \\(10\\) estimates of \\(r\\) and \\(g\\) from comparable firms:\n\n# Historical series\nhistorical_r &lt;- c(0.08, 0.07, 0.065, 0.095, 0.067,0.088,0.08,0.085,0.079,0.077)\nhistorical_g &lt;- c(0.03, 0.035, 0.02, 0.05, 0.045,0.045,0.06,0.05,0.04,0.01)\n\nUse bootstrap sampling to simulate \\(10,000\\) draws of \\((r,g)\\) pairs and compute the stock price distribution. Use a sample size of \\(10\\) with replacement for each random variable.\n\n\n\n\n\n\nHint\n\n\n\n\nUse the replicate() function together with the mean and the sample function to replicate the mean from sampling \\(10\\) observations for \\(r\\) and \\(g\\), with replacement.\nWith that, create a for loop that goes from 1:10000 and calculates p_0 given each pair (r_i,g_i). Store the results in a vector and iterate.\n\n\n\n\n# Simulated example dataset\nset.seed(123)\n\n#Number of simulations\nn_sim=10000\n\n# Historical series\nhistorical_r &lt;- c(0.08, 0.07, 0.065, 0.095, 0.067,0.088,0.08,0.085,0.079,0.077)\nhistorical_g &lt;- c(0.03, 0.035, 0.02, 0.05, 0.045,0.045,0.06,0.05,0.04,0.01)\n\n# For each simulation, bootstrap 10 values and take the mean → length n_sim vector\nboot_r &lt;- replicate(n_sim, mean(sample(historical_r, 10, replace = TRUE)))\nboot_g &lt;- replicate(n_sim, mean(sample(historical_g, 10, replace = TRUE)))\n\nP0_sim &lt;- vector()\n\nfor(i in 1:n_sim){  \n  r_i &lt;- boot_r[i]\n  g_i &lt;- boot_g[i]\n  \n  p_i = D1/(r_i-g_i)\n  \n  P0_sim=c(P0_sim,p_i)\n  }\n\nP0_sim &lt;- na.omit(P0_sim)\n\ntibble(P0 = P0_sim) %&gt;%\n  ggplot(aes(x = P0)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", color = \"white\") +\n  scale_y_continuous(labels=scales::comma)+\n  scale_x_continuous(labels=scales::dollar)+\n    labs(title = \"Bootstrapped P0 simulation\",\n         subtitle = 'Drawing 10,000 simulations from a bivariate normal distribution.',\n         x = \"P0\", y = \"Frequency\")+\n  theme_minimal(base_size = 20)\n\n\n\n\n\n\n\n\n\n\n\nSuppose that the baseline price for the firm is \\(\\$40.35\\). Based on our findings from Exercise 4, what is the % of scenarios where you would see an undervalued investment opportunity and, therefore, would signal a buying opportunity?\n\ntibble(P0 = P0_sim) %&gt;%\n  mutate(Signal=ifelse(P0&gt;=40.35,\"Long\",\"Short\"))%&gt;%\n  ggplot(aes(x = P0,fill=Signal)) +\n  geom_histogram(bins = 50, color = \"white\") +\n  scale_fill_manual(values=c('darkgreen','darkred'))+\n  annotate(\"text\",\n           x=70,\n           y=400,\n           label=paste0(\n             'Simulated price is higher\\n than baseline in ',\n             scales::percent(mean(P0_sim&gt;40.35)),' of the cases'))+\n  scale_y_continuous(labels=scales::comma)+\n  scale_x_continuous(labels=scales::dollar)+\n    labs(title = \"Bootstrapped P0 simulation\",\n         subtitle = 'Drawing 10,000 simulations from a bivariate normal distribution.',\n         x = \"P0\", y = \"Frequency\")+\n  theme_minimal(base_size = 20)+\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3-solutions.html#case-outline",
    "href": "quant-fin/datacases/data-case-3/data-case-3-solutions.html#case-outline",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "In financial valuation, estimating the fair price of an asset often requires assumptions about future growth rates and discount rates. A common tool is the Gordon Growth Model, which expresses the value of a stock as:\n\\[\nP_0 = \\dfrac{D_1}{r-g}\n\\]\nWhere \\(P_0\\) is the current, share price, \\(D_1\\) is the next’s period dividend, \\(r\\) is the discount rate, and \\(g\\) is the growth rate in perpetuity. In words, the share price in period \\(0\\) is the sum of the (expected) future dividends (beginning in period \\(1\\)) up to infinity discounted by the cost of opportunity. Dividends in periods \\(2,3,...,t\\rightarrow\\infty\\) are expected grow linearly by a factor \\(g\\).\nIn practice, you generally know the next period’s dividend, but \\(r\\) and \\(g\\) are both uncertain. In this data case, you will explore how Monte Carlo simulations can be used to evaluate the distribution of potential stock prices under uncertainty in inputs.\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®."
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3-solutions.html#tech-setup",
    "href": "quant-fin/datacases/data-case-3/data-case-3-solutions.html#tech-setup",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\", \"mvtnorm\", \"rsample\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('mvtnorm')\n  install.packages('rsample')\n\n#Load\n  library(tidyverse)\n  library(mvtnorm)\n  library(rsample)"
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3-solutions.html#exercise-1",
    "href": "quant-fin/datacases/data-case-3/data-case-3-solutions.html#exercise-1",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "Suppose that \\(D_1=\\$2.00\\). Assume the growth rate is fixed at \\(g=3\\%\\). Simulate 10,000 values of \\(r\\), assuming it follows a normal distribution with mean \\(\\mu=8\\%\\) and standard deviation \\(\\sigma=1\\%\\). Calculate the corresponding distribution of stock prices.\n\n\n\n\n\n\nHint\n\n\n\nMake sure to set a seed (set.seed(123)) for reproducibility and ensure that \\(r&gt;g\\) for each simulation.\n\n\n\nset.seed(123)\nD1 &lt;- 2\ng &lt;- 0.03\nr_sim &lt;- rnorm(10000, mean = 0.08, sd = 0.01)\nr_sim &lt;- r_sim[r_sim &gt; g]  # Filter out invalid draws\nP0_sim &lt;- D1 / (r_sim - g)\n\ntibble(P0 = P0_sim) %&gt;%\n  ggplot(aes(x = P0)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", color = \"white\") +\n  scale_y_continuous(labels=scales::comma)+\n  scale_x_continuous(labels=scales::dollar)+\n  labs(title = \"Distribution of P0 from Uncertainty in r\",\n       subtitle = 'Drawing 10,000 simulations.',\n       x = \"P0\", y = \"Frequency\")+\n  theme_minimal(base_size = 20)+\n  theme(plot.title  = element_text(face='bold'))"
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3-solutions.html#exercise-2",
    "href": "quant-fin/datacases/data-case-3/data-case-3-solutions.html#exercise-2",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "Now, suppose both \\(r\\) and \\(g\\) are random variables:\n\\[\nr \\sim \\mathcal N(0.08,0.01^2)\n\\] \\[\ng \\sim \\mathcal N(0.03,0.005^2)\n\\]\nSimulate \\(10,000\\) independent draws for each and calculate the distribution of prices.\n\nset.seed(123)\nr_sim &lt;- rnorm(10000, 0.08, 0.01)\ng_sim &lt;- rnorm(10000, 0.03, 0.005)\n\nvalid &lt;- r_sim &gt; g_sim\nr_sim &lt;- r_sim[valid]\ng_sim &lt;- g_sim[valid]\nP0_sim &lt;- D1 / (r_sim - g_sim)\n\ntibble(P0 = P0_sim) %&gt;%\n  ggplot(aes(x = P0)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", color = \"white\") +\n  scale_y_continuous(labels=scales::comma)+\n  scale_x_continuous(labels=scales::dollar)+\n    labs(title = \"Distribution of P0 from Uncertainty in r and g\",\n         subtitle = 'Drawing 10,000 simulations.',\n         x = \"P0\", y = \"Frequency\")+\n  theme_minimal(base_size = 20)"
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3-solutions.html#exercise-3",
    "href": "quant-fin/datacases/data-case-3/data-case-3-solutions.html#exercise-3",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "Assume that the correlation between \\(r\\) and \\(g\\) is \\(\\rho=0.9\\). Simulate \\(10,000\\) observations from a bivariate normal distribution where the means and the standard deviations of both \\(r\\) and \\(g\\) remain the same, but now you are imposing a correlation structure of \\(\\rho=0.9\\) when drawing their distributions. How does this result compare to the previous result you found (i.e, when \\(r\\) and \\(g\\) were draw independently from each other)?\n\n\n\n\n\n\nHint\n\n\n\n\nYou can use the mvtnorm to draw multivariate normal distributions.\nTo that matter, you can use the following notation:\n\n\nset.seed(123)\n\nmu &lt;- c(mean1,mean2)\nsigma &lt;- matrix(c(sd1^2, cor*sd1*sd2,\n                  cor*sd1*sd2, sd2^2), ncol = 2)\n\nsim &lt;- rmvnorm(10000, mean = mu, sigma = sigma)\n\nIn words, note that sigma is nothing more than the matrix-covariance matrix of \\(r\\) and \\(g\\).\n\n\n\nmu &lt;- c(0.08, 0.03)\nsigma &lt;- matrix(c(0.01^2, 0.9*0.01*0.005,\n                  0.9*0.01*0.005, 0.005^2), ncol = 2)\n\nset.seed(123)\nsim &lt;- rmvnorm(10000, mean = mu, sigma = sigma)\nr_sim &lt;- sim[, 1]\ng_sim &lt;- sim[, 2]\n\nvalid &lt;- r_sim &gt; g_sim\nP0_sim &lt;- D1 / (r_sim[valid] - g_sim[valid])\n\ntibble(P0 = P0_sim) %&gt;%\n  ggplot(aes(x = P0)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", color = \"white\") +\n  scale_y_continuous(labels=scales::comma)+\n  scale_x_continuous(labels=scales::dollar)+\n    labs(title = \"Distribution of P0 from Uncertainty in r and g\",\n         subtitle = 'Drawing 10,000 simulations from a bivariate normal distribution.',\n         x = \"P0\", y = \"Frequency\")+\n  theme_minimal(base_size = 20)"
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3-solutions.html#exercise-4",
    "href": "quant-fin/datacases/data-case-3/data-case-3-solutions.html#exercise-4",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "You are given a sample of \\(10\\) estimates of \\(r\\) and \\(g\\) from comparable firms:\n\n# Historical series\nhistorical_r &lt;- c(0.08, 0.07, 0.065, 0.095, 0.067,0.088,0.08,0.085,0.079,0.077)\nhistorical_g &lt;- c(0.03, 0.035, 0.02, 0.05, 0.045,0.045,0.06,0.05,0.04,0.01)\n\nUse bootstrap sampling to simulate \\(10,000\\) draws of \\((r,g)\\) pairs and compute the stock price distribution. Use a sample size of \\(10\\) with replacement for each random variable.\n\n\n\n\n\n\nHint\n\n\n\n\nUse the replicate() function together with the mean and the sample function to replicate the mean from sampling \\(10\\) observations for \\(r\\) and \\(g\\), with replacement.\nWith that, create a for loop that goes from 1:10000 and calculates p_0 given each pair (r_i,g_i). Store the results in a vector and iterate.\n\n\n\n\n# Simulated example dataset\nset.seed(123)\n\n#Number of simulations\nn_sim=10000\n\n# Historical series\nhistorical_r &lt;- c(0.08, 0.07, 0.065, 0.095, 0.067,0.088,0.08,0.085,0.079,0.077)\nhistorical_g &lt;- c(0.03, 0.035, 0.02, 0.05, 0.045,0.045,0.06,0.05,0.04,0.01)\n\n# For each simulation, bootstrap 10 values and take the mean → length n_sim vector\nboot_r &lt;- replicate(n_sim, mean(sample(historical_r, 10, replace = TRUE)))\nboot_g &lt;- replicate(n_sim, mean(sample(historical_g, 10, replace = TRUE)))\n\nP0_sim &lt;- vector()\n\nfor(i in 1:n_sim){  \n  r_i &lt;- boot_r[i]\n  g_i &lt;- boot_g[i]\n  \n  p_i = D1/(r_i-g_i)\n  \n  P0_sim=c(P0_sim,p_i)\n  }\n\nP0_sim &lt;- na.omit(P0_sim)\n\ntibble(P0 = P0_sim) %&gt;%\n  ggplot(aes(x = P0)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", color = \"white\") +\n  scale_y_continuous(labels=scales::comma)+\n  scale_x_continuous(labels=scales::dollar)+\n    labs(title = \"Bootstrapped P0 simulation\",\n         subtitle = 'Drawing 10,000 simulations from a bivariate normal distribution.',\n         x = \"P0\", y = \"Frequency\")+\n  theme_minimal(base_size = 20)"
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3-solutions.html#exercise-5",
    "href": "quant-fin/datacases/data-case-3/data-case-3-solutions.html#exercise-5",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "Suppose that the baseline price for the firm is \\(\\$40.35\\). Based on our findings from Exercise 4, what is the % of scenarios where you would see an undervalued investment opportunity and, therefore, would signal a buying opportunity?\n\ntibble(P0 = P0_sim) %&gt;%\n  mutate(Signal=ifelse(P0&gt;=40.35,\"Long\",\"Short\"))%&gt;%\n  ggplot(aes(x = P0,fill=Signal)) +\n  geom_histogram(bins = 50, color = \"white\") +\n  scale_fill_manual(values=c('darkgreen','darkred'))+\n  annotate(\"text\",\n           x=70,\n           y=400,\n           label=paste0(\n             'Simulated price is higher\\n than baseline in ',\n             scales::percent(mean(P0_sim&gt;40.35)),' of the cases'))+\n  scale_y_continuous(labels=scales::comma)+\n  scale_x_continuous(labels=scales::dollar)+\n    labs(title = \"Bootstrapped P0 simulation\",\n         subtitle = 'Drawing 10,000 simulations from a bivariate normal distribution.',\n         x = \"P0\", y = \"Frequency\")+\n  theme_minimal(base_size = 20)+\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3.html",
    "href": "quant-fin/datacases/data-case-3/data-case-3.html",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "This Data Case is part of the Practical Applications in Quantitative Finance course, held at FGV-EAESP’s undergraduate course in business. Carefully follow the instructions contained in the data case as well as eClass® before you make your submission.\n\n\nIn financial valuation, estimating the fair price of an asset often requires assumptions about future growth rates and discount rates. A common tool is the Gordon Growth Model, which expresses the value of a stock as:\n\\[\nP_0 = \\dfrac{D_1}{r-g}\n\\]\nWhere \\(P_0\\) is the current, share price, \\(D_1\\) is the next’s period dividend, \\(r\\) is the discount rate, and \\(g\\) is the growth rate in perpetuity. In words, the share price in period \\(0\\) is the sum of the (expected) future dividends (beginning in period \\(1\\)) up to infinity discounted by the cost of opportunity. Dividends in periods \\(2,3,...,t\\rightarrow\\infty\\) are expected grow linearly by a factor \\(g\\).\nIn practice, you generally know the next period’s dividend, but \\(r\\) and \\(g\\) are both uncertain. In this data case, you will explore how Monte Carlo simulations can be used to evaluate the distribution of potential stock prices under uncertainty in inputs.\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®.\n\n\n\n\n\nBefore you start, make sure that you have your R session correctly configured with all the following packages running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\", \"mvtnorm\", \"rsample\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('mvtnorm')\n  install.packages('rsample')\n\n#Load\n  library(tidyverse)\n  library(mvtnorm)\n  library(rsample)\n\n\n\n\nSuppose that \\(D_1=\\$2.00\\). Assume the growth rate is fixed at \\(g=3\\%\\). Simulate 10,000 values of \\(r\\), assuming it follows a normal distribution with mean \\(\\mu=8\\%\\) and standard deviation \\(\\sigma=1\\%\\). Calculate the corresponding distribution of stock prices.\n\n\n\n\n\n\nHint\n\n\n\nMake sure to set a seed (set.seed(123)) for reproducibility and ensure that \\(r&gt;g\\) for each simulation.\n\n\n\n\n\nNow, suppose both \\(r\\) and \\(g\\) are random variables:\n\\[\nr \\sim \\mathcal N(0.08,0.01^2)\n\\]\n\\[\ng \\sim \\mathcal N(0.03,0.005^2)\n\\]\nSimulate \\(10,000\\) independent draws for each and calculate the distribution of prices.\n\n\n\nAssume that the correlation between \\(r\\) and \\(g\\) is \\(\\rho=0.9\\). Simulate \\(10,000\\) observations from a bivariate normal distribution where the means and the standard deviations of both \\(r\\) and \\(g\\) remain the same, but now you are imposing a correlation structure of \\(\\rho=0.9\\) when drawing their distributions. How does this result compare to the previous result you found (i.e, when \\(r\\) and \\(g\\) were draw independently from each other)?\n\n\n\n\n\n\nHint\n\n\n\n\nYou can use the mvtnorm to draw multivariate normal distributions.\nTo that matter, you can use the following notation:\n\n\nset.seed(123)\n\nmu &lt;- c(mean1,mean2)\nsigma &lt;- matrix(c(sd1^2, cor*sd1*sd2,\n                  cor*sd1*sd2, sd2^2), ncol = 2)\n\nsim &lt;- rmvnorm(10000, mean = mu, sigma = sigma)\n\nIn words, note that sigma is nothing more than the matrix-covariance matrix of \\(r\\) and \\(g\\).\n\n\n\n\n\nYou are given a sample of \\(10\\) estimates of \\(r\\) and \\(g\\) from comparable firms:\n\n# Historical series\nhistorical_r &lt;- c(0.08, 0.07, 0.065, 0.095, 0.067,0.088,0.08,0.085,0.079,0.077)\nhistorical_g &lt;- c(0.03, 0.035, 0.02, 0.05, 0.045,0.045,0.06,0.05,0.04,0.01)\n\nUse bootstrap sampling to simulate \\(10,000\\) draws of \\((r,g)\\) pairs and compute the stock price distribution. Use a sample size of \\(10\\) with replacement for each random variable.\n\n\n\n\n\n\nHint\n\n\n\n\nUse the replicate() function together with the mean and the sample function to replicate the mean from sampling \\(10\\) observations for \\(r\\) and \\(g\\), with replacement.\nWith that, create a for loop that goes from 1:10000 and calculates p_0 given each pair (r_i,g_i). Store the results in a vector and iterate.\n\n\n\n\n\n\nSuppose that the baseline price for the firm is \\(\\$40.35\\). Based on our findings from Exercise 4, what is the % of scenarios where you would see an undervalued investment opportunity and, therefore, would signal a buying opportunity?"
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3.html#case-outline",
    "href": "quant-fin/datacases/data-case-3/data-case-3.html#case-outline",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "In financial valuation, estimating the fair price of an asset often requires assumptions about future growth rates and discount rates. A common tool is the Gordon Growth Model, which expresses the value of a stock as:\n\\[\nP_0 = \\dfrac{D_1}{r-g}\n\\]\nWhere \\(P_0\\) is the current, share price, \\(D_1\\) is the next’s period dividend, \\(r\\) is the discount rate, and \\(g\\) is the growth rate in perpetuity. In words, the share price in period \\(0\\) is the sum of the (expected) future dividends (beginning in period \\(1\\)) up to infinity discounted by the cost of opportunity. Dividends in periods \\(2,3,...,t\\rightarrow\\infty\\) are expected grow linearly by a factor \\(g\\).\nIn practice, you generally know the next period’s dividend, but \\(r\\) and \\(g\\) are both uncertain. In this data case, you will explore how Monte Carlo simulations can be used to evaluate the distribution of potential stock prices under uncertainty in inputs.\n\n\n\n\n\n\nDeliverable\n\n\n\nEach group is expected to deliver a single assignment. A submission must be either an .R script or a .qmd (Quarto) file, ensuring that both code and interpretations of the results are clearly presented. Whenever applicable, include concise explanations alongside your code to demonstrate your understanding of the analysis. The due date for this submission is specified on eClass®, so please check the platform for details. You are required to clearly document your workflow and assumptions, and provide meaningful explanations alongside your outputs.\nTo help you structure your submission, I have provided a Quarto mock template, which is already available for you to use. This template is designed to help you seamlessly integrate your code and analysis, ensuring a clear and organized presentation of your work. Feel free to use it as a starting point to format your responses effectively. The mock template can be found in the Data Cases folder on eClass®."
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3.html#tech-setup",
    "href": "quant-fin/datacases/data-case-3/data-case-3.html#tech-setup",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "Before you start, make sure that you have your R session correctly configured with all the following packages running the code below:\n\n# Package names\npackages &lt;- c(\"tidyverse\", \"mvtnorm\", \"rsample\")\n\n# Install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# Load all packages\ninvisible(lapply(packages, library, character.only = TRUE))\n\nAlternatively, you can simply call:\n\n#Install if not already available\n  install.packages('tidyverse')\n  install.packages('mvtnorm')\n  install.packages('rsample')\n\n#Load\n  library(tidyverse)\n  library(mvtnorm)\n  library(rsample)"
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3.html#exercise-1",
    "href": "quant-fin/datacases/data-case-3/data-case-3.html#exercise-1",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "Suppose that \\(D_1=\\$2.00\\). Assume the growth rate is fixed at \\(g=3\\%\\). Simulate 10,000 values of \\(r\\), assuming it follows a normal distribution with mean \\(\\mu=8\\%\\) and standard deviation \\(\\sigma=1\\%\\). Calculate the corresponding distribution of stock prices.\n\n\n\n\n\n\nHint\n\n\n\nMake sure to set a seed (set.seed(123)) for reproducibility and ensure that \\(r&gt;g\\) for each simulation."
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3.html#exercise-2",
    "href": "quant-fin/datacases/data-case-3/data-case-3.html#exercise-2",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "Now, suppose both \\(r\\) and \\(g\\) are random variables:\n\\[\nr \\sim \\mathcal N(0.08,0.01^2)\n\\]\n\\[\ng \\sim \\mathcal N(0.03,0.005^2)\n\\]\nSimulate \\(10,000\\) independent draws for each and calculate the distribution of prices."
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3.html#exercise-3",
    "href": "quant-fin/datacases/data-case-3/data-case-3.html#exercise-3",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "Assume that the correlation between \\(r\\) and \\(g\\) is \\(\\rho=0.9\\). Simulate \\(10,000\\) observations from a bivariate normal distribution where the means and the standard deviations of both \\(r\\) and \\(g\\) remain the same, but now you are imposing a correlation structure of \\(\\rho=0.9\\) when drawing their distributions. How does this result compare to the previous result you found (i.e, when \\(r\\) and \\(g\\) were draw independently from each other)?\n\n\n\n\n\n\nHint\n\n\n\n\nYou can use the mvtnorm to draw multivariate normal distributions.\nTo that matter, you can use the following notation:\n\n\nset.seed(123)\n\nmu &lt;- c(mean1,mean2)\nsigma &lt;- matrix(c(sd1^2, cor*sd1*sd2,\n                  cor*sd1*sd2, sd2^2), ncol = 2)\n\nsim &lt;- rmvnorm(10000, mean = mu, sigma = sigma)\n\nIn words, note that sigma is nothing more than the matrix-covariance matrix of \\(r\\) and \\(g\\)."
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3.html#exercise-4",
    "href": "quant-fin/datacases/data-case-3/data-case-3.html#exercise-4",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "You are given a sample of \\(10\\) estimates of \\(r\\) and \\(g\\) from comparable firms:\n\n# Historical series\nhistorical_r &lt;- c(0.08, 0.07, 0.065, 0.095, 0.067,0.088,0.08,0.085,0.079,0.077)\nhistorical_g &lt;- c(0.03, 0.035, 0.02, 0.05, 0.045,0.045,0.06,0.05,0.04,0.01)\n\nUse bootstrap sampling to simulate \\(10,000\\) draws of \\((r,g)\\) pairs and compute the stock price distribution. Use a sample size of \\(10\\) with replacement for each random variable.\n\n\n\n\n\n\nHint\n\n\n\n\nUse the replicate() function together with the mean and the sample function to replicate the mean from sampling \\(10\\) observations for \\(r\\) and \\(g\\), with replacement.\nWith that, create a for loop that goes from 1:10000 and calculates p_0 given each pair (r_i,g_i). Store the results in a vector and iterate."
  },
  {
    "objectID": "quant-fin/datacases/data-case-3/data-case-3.html#exercise-5",
    "href": "quant-fin/datacases/data-case-3/data-case-3.html#exercise-5",
    "title": "Data Case III - Monte Carlo Simulation",
    "section": "",
    "text": "Suppose that the baseline price for the firm is \\(\\$40.35\\). Based on our findings from Exercise 4, what is the % of scenarios where you would see an undervalued investment opportunity and, therefore, would signal a buying opportunity?"
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#course-organization-selected-topics",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#course-organization-selected-topics",
    "title": "Course Introduction",
    "section": "Course Organization, selected topics",
    "text": "Course Organization, selected topics\n\nAlthough some topics may vary as we move along the course, we aim to cover the following areas:\n\nValuation - rationale and basic concepts\nRelative Valuation\nThe Free Cash Flow in practice\nValuation with different leverage policies\nIncorporating Uncertainty in Valuation Models\nEquity Valuation\nMergers and Acquisitions\n\n\n\n\n\n\n\n\nComplementary Content\n\n\nStudents are encouraged to bring in new topics and/or in-depth discussions of the covered topics that are of the interest of a broader audience. Whenever applicable, the professor will provide supplementary content in the form of whitepapers, policy papers, and academic papers to foster the discussion."
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#what-is-the-value-of-a-firm",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#what-is-the-value-of-a-firm",
    "title": "Course Introduction",
    "section": "What is the Value of a Firm?",
    "text": "What is the Value of a Firm?\n\n\nMarket Value: reflects what investors are currently willing to pay\n\n\nChanges in real time—driven by expectations, sentiment, and news\nExample: Tesla’s market cap surged past $1 trillion despite profits trailing legacy automakers\n\n\nAccounting (Book) Value: based on historical costs and accounting rules\n\n\nOften underestimates firms with intangible assets\n\nExample: Amazon’s book value was far below its market cap during its growth years\n\n\nIntrinsic Value: theoretical value based on expected future cash flows, risk, and growth\n\n\nMay differ from market value—basis for investment decisions.\n\nExample: Warren Buffett invests when market price is below its (estimated) intrinsic value"
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#what-is-a-valuation-about",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#what-is-a-valuation-about",
    "title": "Course Introduction",
    "section": "What is a Valuation About?",
    "text": "What is a Valuation About?\n\n\n\n\n\n\nDefinition\n\n\nValuation is the process of estimating what something is worth today, based on its expected future benefits. In corporate finance, this typically involves discounting future cash flows using an appropriate discount rate that reflects the risk of those cash flows (Berk and DeMarzo 2023)\n\n\n\n\nValuation ≠ Price Forecasting: it estimates fundamental value, not short-term market movements.\nPurposes of Valuation: Investment decisions, fundraising, M&A, taxation, strategic planning.\nA Blend of Science and Judgment: Relies on models and assumptions—cash flows, risk, growth, reinvestment.\n\n\n\\(\\rightarrow\\) Example: WeWork’s $47B pre-IPO valuation collapsed after governance and business model scrutiny, revealing an inflated narrative disconnected from fundamentals"
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#valuation-methodologies",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#valuation-methodologies",
    "title": "Course Introduction",
    "section": "Valuation Methodologies",
    "text": "Valuation Methodologies\n\nThe fastest way to assess the value of a firm is to compare and contrast it to similar firms in which we assume to know the value upfront\nThis is what we call Relative Valuation:\n\n\nCompares a firm’s value to that of similar companies using market-based ratios\nCommon multiples include P/E, EV/EBITDA, P/B, and EV/Sales\nAssumes that comparable firms are correctly priced by the market\nSimpler and faster than DCF, but less grounded in fundamentals and sensitive to choice of peers and potential market mispricing\n\n\n\\(\\rightarrow\\) Example: SaaS startups valued at 10x–20x revenue during bull markets"
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#impacts-on-valuation",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#impacts-on-valuation",
    "title": "Course Introduction",
    "section": "Impacts on Valuation",
    "text": "Impacts on Valuation\n\nValuation is not static—new information constantly reshapes assumptions.\nExamples:\n\nRising interest rates → lower present value of future earnings.\nNew patents or tech → boosts future cash flow expectations.\nRegulatory risks → raise discount rates or reduce expected margins.\n\nCase: Meta (Facebook) lost $200B in market cap in 2022 after weak earnings and guidance—market revised growth expectations sharply."
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#the-dark-side-of-valuation",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#the-dark-side-of-valuation",
    "title": "Course Introduction",
    "section": "The Dark Side of Valuation",
    "text": "The Dark Side of Valuation\n“Every valuation tells a story. The danger lies when the story is fiction.” – Aswath Damodaran\n\nWhat happens when standard valuation models don’t work to frame a specific valuation target?\nThere are common real-world examples that pose difficulties for the standard valuation methods:\n\nStartups: no earnings, high uncertainty. In this case, the use of scenario-based or option pricing approaches are more suitable for the task\n\n\\(\\rightarrow\\) Example: Uber pre-IPO required projecting market share in unproven markets\n\nSPACs and Shells: High valuation based on expectations alone\n\n\\(\\rightarrow\\) Example: Nikola surged on hydrogen truck hype—collapsed after short-seller revelations"
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#valuation-methodologies-continued",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#valuation-methodologies-continued",
    "title": "Course Introduction",
    "section": "Valuation Methodologies, continued",
    "text": "Valuation Methodologies, continued\n\nThe Discounted Cash Flow (DCF) is based on expected free cash flows discounted at the firm’s cost of capital:\n\n\\[\nV=\\sum_{t=0}^{\\infty}\\dfrac{FCF}{(1+r)^t}\n\\]\n\nRequires detailed assumptions about growth, margins, reinvestment, and risk\nGrounded in fundamentals, making it robust for long-term valuation\nSensitive to small changes in assumptions, especially terminal value and discount rate\n\n\\(\\rightarrow\\) Example: valuing the introduction of a new project within a given company requires estimates around (expected) future cash flows and its associated risk, as well as the investment needed"
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#valuation-methodologies-continued-1",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#valuation-methodologies-continued-1",
    "title": "Course Introduction",
    "section": "Valuation Methodologies, continued",
    "text": "Valuation Methodologies, continued\n\nThe Liquidation (or Salvage) Value: an Asset-Based view of the firm by the sum of tangible assets minus liabilities\n\n\nEstimates the value of a firm’s assets if sold separately in a distressed or orderly sale (“fire sale”)\nBecause it ignores the firm’s ability to generate future cash flows, it is typically lower than going-concern value due to forced-sale discounts\nUseful for valuing distressed firms or setting a valuation floor\nOften used in bankruptcy, restructuring, or lender recovery scenarios\n\n\\(\\rightarrow\\) Example: the liquidation value of Lehman Brothers post-crisis"
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#valuation-methodologies-continued-2",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#valuation-methodologies-continued-2",
    "title": "Course Introduction",
    "section": "Valuation Methodologies, continued",
    "text": "Valuation Methodologies, continued\nLiquidation (or Salvage) Value: an Asset-Based view of the firm by the sum of tangible assets minus liabilities.\n\nExample: Liquidation value of Lehman Brothers post-crisis."
  },
  {
    "objectID": "valuation/coursework/Lecture 1 - Introduction/index.html#impact-of-new-information-on-valuation",
    "href": "valuation/coursework/Lecture 1 - Introduction/index.html#impact-of-new-information-on-valuation",
    "title": "Course Introduction",
    "section": "Impact of New Information on Valuation",
    "text": "Impact of New Information on Valuation\n\nValuation is not static - new information constantly reshapes assumptions about expected returns and risk:\n\n\n\n\nRising interest rates lower the present value of future earnings\nNew patents or tech boosts future cash flow expectations\nRegulatory risks raise discount rates or reduce expected margins, plummeting stock prices\n\n\n\n\n\nAs such, as new information arrives publicly, investors reassess expectations regarding expected cash-flows, risk, and future growth, and the expected value of a firm changes\n\n\n\n\\(\\rightarrow\\) Example: Meta lost $200B in market cap in 2022 after weak earnings and guidance—market revised growth expectations sharply"
  },
  {
    "objectID": "valuation.html#about-the-course",
    "href": "valuation.html#about-the-course",
    "title": "Valuation",
    "section": "About the course",
    "text": "About the course\nThis is a hands-on, applied course on Valuation designed for professional master’s students in management. The course bridges theory and practice, equipping students with the essential tools to assess company value in a variety of settings. Students will explore the core concepts and methodologies used in valuation, developing the ability to critically analyze corporate performance, investment opportunities, and strategic decisions.\nThe course will be structured around topics that are highly relevant to management professionals, including: introduction to valuation principles, relative valuation, discounted cash flow (DCF) analysis, the role of leverage in valuation, managing project uncertainty, equity valuation techniques, and valuation in mergers and acquisitions. As a final deliverable, students will complete a capstone project applying one or more of these valuation methods to a real-world company or transaction.\n\n\n\n\n\n\nFor students\n\n\n\nBelow you find the persistent links to all lectures of the course. As they are continuously updated with fixes and new implementations, you might expect some changes from time to time in the contents of each file.\nThese slides leverage Quarto, an open-source scientific and technical publishing system. They contain both static and dynamic content that will be displayed in your internet browser. Some useful tips for using these slides:\n\nHit F for full-screen mode\nIf you are interest in getting a .pdf version of the slides, hit E to switch to print mode and then Ctrl + P"
  },
  {
    "objectID": "fin-strat/Lecture 1 - Equity Financing/index.html#ipo-and-seos-in-practice",
    "href": "fin-strat/Lecture 1 - Equity Financing/index.html#ipo-and-seos-in-practice",
    "title": "Equity Financing",
    "section": "IPO and SEOs in Practice",
    "text": "IPO and SEOs in Practice\n\nTake a look at an example of an IPO Prospectus for Raízen S.A, made in 2021 by a syndicated group of underwriters, where BTG Pactual was the lead underwriter - access on eClass®"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html",
    "title": "Lucas S. Macoris",
    "section": "",
    "text": "#title: “Risk and Return” author: “Lucas S. Macoris” format: revealjs: title: ‘Relative Vluation’ theme: [default, ../~ Metadata/custom.scss] auto-stretch: false author: ‘Lucas S. Macoris (FGV-EAESP)’ logo: ‘Images/logo.jpg’ footer: “@ Website | @ Slides | @ Office-hour appointments” toc: false cls: ../~ Metadata/abntex2.cls incremental: false bibliography: ‘../~ Metadata/Bibliography.bib’ slide-number: true show-slide-number: all transition: slide background-transition: fade chalkboard: true width: 1600 height: 900 smaller: false\neditor: visual from: markdown+emoji"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#disclaimer",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#disclaimer",
    "title": "Relative Valuation",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\n\n\n\n\nDisclaimer\n\n\nThe information presented in this lecture is for educational and informational purposes only and should not be construed as investment advice. Nothing discussed constitutes a recommendation to buy, sell, or hold any financial instrument or security. Investment decisions should be made based on individual research and consultation with a qualified financial professional. The presenter assumes no responsibility for any financial decisions made based on this content.\nAll publicly available content used in this lecture is available and also shared on my GitHub page. Participants are encouraged to review, modify, and use it for their own learning and research purposes. However, no guarantees are made regarding the accuracy, completeness, or suitability of the code for any specific application.\nFor any questions or concerns, please feel free to reach out via email at lucas.macoris@fgv.br"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#what-is-relative-valuation",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#what-is-relative-valuation",
    "title": "Relative Valuation",
    "section": "What is Relative Valuation:",
    "text": "What is Relative Valuation:\n\n\n\n\n\n\nDefinition\n\n\nMultiples analysis (or relative valuation) is the process of comparing a company’s value metrics to those of similar companies:\n\nIt relies on market-derived multiples rather than intrinsic forecasts\nIt is often faster and easier than DCF, but more dependent on market sentiment\n\nNotwithstanding, a relative valuation comparison is always made under the assumption that similar assets should trade at similar prices - the extent to which such assumption holds true in practice affects the quality of the analysis being carried out\n\n\n\n\nRelative valuation can be adopted as the “gut-check” on the valuation process:\n\nQuickly estimate valuation in M&A and IPO pricing\nCross-check results from intrinsic valuation methods\nBenchmark a company against industry peers\nValue firms in n industries with short product cycles or limited cash flow predictability\nEstimate the value of private companies using public companies"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#market-valuation-ratios---the-mb-ratio",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#market-valuation-ratios---the-mb-ratio",
    "title": "Relative Valuation",
    "section": "Market-valuation Ratios - the M/B Ratio",
    "text": "Market-valuation Ratios - the M/B Ratio\nThe first valuation multiple that we can think of contrasts market and book measurements:\n\n\n\n\n\n\n\nDefinition\n\n\nThe Market-to-Book Ratio (or simply M/B Ratio) highlights the differences in fundamental firm characteristics as well as the value added by management:\n\\[\n\\text{M/B Ratio}=\\dfrac{\\text{Market Value of Equity}}{\\text{Book Value of Equity}}\n\\]\nWhere market capitalization is defined as before, and the Book Value of Equity is the accounting value of the firm’s equity (i.e, Assets - Liabilities).\n\n\n\n\nSome common patterns on M/B Ratios:\n\nA successful firm’s Market-to-Book ratio typically exceeds 1.\nLow Market-to-Book ratios \\(\\rightarrow\\) also known as value stocks\nHigh Market-to-Book ratios \\(\\rightarrow\\) also known as growth stocks"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#market-valuation-ratios---the-pe-ratio",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#market-valuation-ratios---the-pe-ratio",
    "title": "Relative Valuation",
    "section": "Market-valuation Ratios - the P/E Ratio",
    "text": "Market-valuation Ratios - the P/E Ratio\n\n\n\n\n\n\nDefinition\n\n\nThe Price-Earnings Ratio (or simply P/E Ratio) is a simple measure that is used to assess whether a stock is over- or under-valued based on the idea that the value of a stock should be proportional to the level of earnings it can generate for its shareholders\n\\[\n\\text{P/E Ratio} = \\dfrac{\\text{Market Capitalization}}{\\text{Net Income}}\\equiv \\dfrac{\\text{Share Price}}{\\text{Earnings per Share}}\n\\]\n\n\n\n\nThis indicator tends to be highest for industries with high expected growth rates\nBecause the P/E Ratio considers the value of the firm’s equity, it is sensitive to the firm’s choice of leverage\nWe can avoid this limitation by instead assessing the market value of the underlying business using valuation ratios based on the firm’s enterprise value1\n\nSee (Berk and DeMarzo 2023) Comment on mismatched ratios, p. 77"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#multiples-mostly-used-in-practice",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#multiples-mostly-used-in-practice",
    "title": "Relative Valuation",
    "section": "Multiples Mostly Used in Practice",
    "text": "Multiples Mostly Used in Practice\n\nPrice-based multiples:\n\nPrice-to-Earnings (P/E)\nPrice-to-Book (P/B) 3 Price-to-Sales (P/S)\n\nEnterprise value–based multiples (firm side):\n\nEV/EBITDA\nEV/EBIT\nEV/Sales\n\nSector-specific multiples:\n\nEV/Subscriber (telecom, SaaS)\nEV/Barrel of Oil Equivalent (energy)\nPrice per Square Meter (real estate)"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#common-pitfalls",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#common-pitfalls",
    "title": "Relative Valuation",
    "section": "Common Pitfalls",
    "text": "Common Pitfalls\n\nAlthough widespread in practice, employing a valuation analysis using multiples needs to take into account a series of potential limitations:\n\nPoor choice of comparables\nIgnoring accounting differences\nMarket mispricing\nOne-size-fits-all multiple use\nFailure to adjust for control premiums or illiquidity\nOverlooking cyclical factors and macroeconomic context\n\n\n\n\\(\\rightarrow\\) In what follows, we’ll discuss each pitfall in detail"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#pitfall-1-poor-choice-of-comparables",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#pitfall-1-poor-choice-of-comparables",
    "title": "Relative Valuation",
    "section": "Pitfall 1: Poor Choice of Comparables",
    "text": "Pitfall 1: Poor Choice of Comparables\n\nWhen creating a relative valuation analysis, we start from the assumption that all firms under consideration are thought of good comparables (e.g, have similar growth rates, margins, and risk profiles)\nNotwithstanding, there are a couple of dimensions by which firms can differ that may have a significant impact on its intrinsic value:\n\nSize differences\nGeographic exposure and competitive positioning matter\nDifferent life-cycle stages"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#pitfall-2-ignoring-accounting-differences",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#pitfall-2-ignoring-accounting-differences",
    "title": "Relative Valuation",
    "section": "Pitfall 2: Ignoring Accounting Differences",
    "text": "Pitfall 2: Ignoring Accounting Differences\n\nAccounting differences across firms can lead to calculations that are not an apples-to-apples comparison\nFor example, firms may use different accounting policies for:\n\nDepreciation methods\nLease capitalization\nRevenue recognition\n\nIFRS vs. USGAAP differences can change ratios\nAs a rule-of-thumb, always normalize financial statements before comparing!"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#pitfall-3-market-mispricing",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#pitfall-3-market-mispricing",
    "title": "Relative Valuation",
    "section": "Pitfall 3: Market Mispricing",
    "text": "Pitfall 3: Market Mispricing\n\nIf an industry is overvalued or undervalued, relative valuation will reflect that bias:\n\nHerd mentality can inflate or depress multiples\nConsider cross-checking with intrinsic valuation to detect anomalies\nMacro shocks (e.g., interest rate hikes) can quickly change sector pricing"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#pitfall-4-one-size-fits-all-multiple-use",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#pitfall-4-one-size-fits-all-multiple-use",
    "title": "Relative Valuation",
    "section": "Pitfall 4: One-Size-Fits-All Multiple Use",
    "text": "Pitfall 4: One-Size-Fits-All Multiple Use\n\nDifferent industries have different value drivers\nUsing P/E for early-stage, loss-making firms is meaningless\nCapital-intensive businesses often better valued with EV/EBITDA or EV/EBIT\nMatch the multiple to the sector’s key performance metric"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#pitfall-5-failure-to-adjust-for-control-premiums-or-illiquidity",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#pitfall-5-failure-to-adjust-for-control-premiums-or-illiquidity",
    "title": "Relative Valuation",
    "section": "Pitfall 5: Failure to Adjust for Control Premiums or Illiquidity",
    "text": "Pitfall 5: Failure to Adjust for Control Premiums or Illiquidity\n\nTransaction values often include a control premium (15–30%)\nMinority stakes trade at discounts\nIlliquid assets or markets may require valuation haircuts\nIgnoring these adjustments can overstate or understate true value"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#pitfall-6-overlooking-cyclical-factors-and-macroeconomic-context",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#pitfall-6-overlooking-cyclical-factors-and-macroeconomic-context",
    "title": "Relative Valuation",
    "section": "Pitfall 6: Overlooking Cyclical Factors and Macroeconomic Context",
    "text": "Pitfall 6: Overlooking Cyclical Factors and Macroeconomic Context\n\nMultiples expand in booms and contract in recessions\nSector cyclicality can distort comparisons if companies are at different cycle stages\nAdjust for normalized earnings in highly cyclical sectors\nMonitor interest rates, inflation, and currency trends impacting valuations"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#exercise-valuing-using-multiples",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#exercise-valuing-using-multiples",
    "title": "Relative Vluation",
    "section": "Exercise: Valuing Using Multiples",
    "text": "Exercise: Valuing Using Multiples\nSuppose that you are valuing TechCo, a mid-sized SaaS company. You have identified three public comparables with similar growth, margins, and business models:\nCompany | EV (USDM) | Revenue (USDM) | EBITDA (USD M) | Net Income (USD M) |\n||–:|-:|:|-:| | Alpha | 1,200 | 240 | 60 | 30 | | Beta | 2,000 | 400 | 100 | 45 | | Gamma | 1,500 | 300 | 80 | 40 |\nTechCo’s Financials: - Revenue: USD 280M - EBITDA: USD 70M - Net Income: USD 32M"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#exercise-instructions",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#exercise-instructions",
    "title": "Relative Vluation",
    "section": "Exercise Instructions",
    "text": "Exercise Instructions\n\nStep 1 — Compute multiples for comparables\n\nEV/Revenue\n\nEV/EBITDA\n\nP/E (assume equity value ≈ EV – Debt + Cash; no debt or cash for simplicity)\n\nStep 2 — Take the average multiple for each metric.\nStep 3 — Apply each average multiple to TechCo’s metrics to estimate:\n\nImplied Enterprise Value (EV) using EV/Revenue and EV/EBITDA\nImplied Equity Value using P/E\n\nStep 4 — Compare results\n\nWhich metric gives the highest implied value?\nWhich is most relevant for SaaS companies and why?"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#discussion-prompts",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#discussion-prompts",
    "title": "Relative Vluation",
    "section": "Discussion Prompts",
    "text": "Discussion Prompts\n\nHow sensitive is TechCo’s valuation to the choice of multiple?\nHow would results change if the comparables were in a different stage of the business cycle?\nWhat adjustments would you make if TechCo had significant debt or excess cash?"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#step-1-compute-company-multiples-equity-firm-side",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#step-1-compute-company-multiples-equity-firm-side",
    "title": "Relative Vluation",
    "section": "Step 1 — Compute company multiples (equity / firm side)",
    "text": "Step 1 — Compute company multiples (equity / firm side)\nEV / Revenue\n\nAlpha: 1200 ÷ 240 = 5.00\n\nBeta: 2000 ÷ 400 = 5.00\n\nGamma: 1500 ÷ 300 = 5.00\n\nEV / EBITDA\n\nAlpha: 1200 ÷ 60 = 20.00\n\nBeta: 2000 ÷ 100 = 20.00\n\nGamma: 1500 ÷ 80 = 18.75\n\nP / E (here equity ≈ EV given no debt / cash)\n\nAlpha: 1200 ÷ 30 = 40.00\n\nBeta: 2000 ÷ 45 = 44.444444… → 44.44\n\nGamma: 1500 ÷ 40 = 37.50"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#step-2-average-multiples-simple-mean",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#step-2-average-multiples-simple-mean",
    "title": "Relative Vluation",
    "section": "Step 2 — Average multiples (simple mean)",
    "text": "Step 2 — Average multiples (simple mean)\n\nAverage EV / Revenue = (5.00 + 5.00 + 5.00) ÷ 3 = 5.00\nAverage EV / EBITDA = (20.00 + 20.00 + 18.75) ÷ 3\n= 58.75 ÷ 3 = 19.583333… → 19.58 (rounded)\nAverage P / E = (40.00 + 44.444444… + 37.50) ÷ 3\n= 121.944444… ÷ 3 = 40.648148… → 40.65 (rounded)"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#step-3-apply-averages-to-techco-implied-values",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#step-3-apply-averages-to-techco-implied-values",
    "title": "Relative Vluation",
    "section": "Step 3 — Apply averages to TechCo (implied values)",
    "text": "Step 3 — Apply averages to TechCo (implied values)\nImplied Enterprise Value (EV) — EV / Revenue approach\n\nEV = Avg(EV/Revenue) × TechCo Revenue\n\nEV = 5.00 × 280 = 1,400.00 (USD M)\n\nImplied Enterprise Value (EV) — EV / EBITDA approach\n\nEV = Avg(EV/EBITDA) × TechCo EBITDA\n\nEV = 19.583333… × 70 = 1,370.833333…\n\nRounded: 1,370.83 (USD M)\n\nImplied Equity Value — P / E approach\n(Recall: we assumed equity ≈ EV for comparables; here we follow the same simplifying assumption.)\n\nEquity ≈ Avg(P/E) × TechCo Net Income\n\nEquity = 40.648148… × 32 = 1,300.740740…\n\nRounded: 1,300.74 (USD M)"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#summary-table-implied-values",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#summary-table-implied-values",
    "title": "Relative Vluation",
    "section": "Summary Table — Implied Values",
    "text": "Summary Table — Implied Values\nMethod | Average Multiple | TechCo Metric | Implied Value (USD M) |\n|–|:|–:|-:| | EV / Revenue | 5.00 | Revenue = 280 | 1,400.00 | | EV / EBITDA | 19.58 | EBITDA = 70 | 1,370.83 | | P / E (equity) | 40.65 | Net Income = 32 | 1,300.74 |\n(All numbers rounded to 2 decimals where shown.)"
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#interpretation-which-metric-is-most-relevant",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#interpretation-which-metric-is-most-relevant",
    "title": "Relative Vluation",
    "section": "Interpretation — Which metric is most relevant?",
    "text": "Interpretation — Which metric is most relevant?\n\nHighest implied value: EV / Revenue → 1,400 M\nEV/Revenue gives the highest implied value, but the differences versus EV/EBITDA are small (~29 M, ~2% of EV).\nP/E implied value (1,300.74 M) is lower — note it is expressed as an equity value; because we assumed no debt/cash, it’s comparable to EV in this simplified exercise. In real life you must convert EV ⇄ Equity by adding net debt.\nFor SaaS companies:\n\nEV/Revenue is commonly used for high-growth or early-stage SaaS (where earnings/EBITDA may be low or negative).\n\nEV/EBITDA (or EV/EBIT) is preferred for profitable, mature SaaS firms.\n\nP/E only meaningful if comparable firms have stable, positive net income and similar capital structure and tax rates."
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#sensitivity-adjustments-short",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#sensitivity-adjustments-short",
    "title": "Relative Vluation",
    "section": "Sensitivity & Adjustments (short)",
    "text": "Sensitivity & Adjustments (short)\n\nDebt / Cash: If TechCo had debt or excess cash, convert implied EV → equity:\nEquity = EV − Net Debt (or Equity = EV + Cash − Debt).\n\nUse of medians vs means: If multiples are skewed, the median is often more robust than the mean.\n\nRange of outcomes: Show min / max implied values from the peer set to highlight valuation dispersion.\n\nEV/EBITDA range here: min = 18.75, max = 20 → implies EV using 18.75×70 = 1,312.5 M and 20×70 = 1,400 M.\n\n\nAdjust for non-recurring items, accounting differences, or growth differences before applying multiples."
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#takeaway-slide-to-show-after-students-attempt",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#takeaway-slide-to-show-after-students-attempt",
    "title": "Relative Vluation",
    "section": "Takeaway Slide (to show after students attempt)",
    "text": "Takeaway Slide (to show after students attempt)\n\nAlways check comparability (growth, margins, capital structure).\n\nMatch the multiple to the industry and the company’s profitability.\n\nPresent a range (low/median/high) and explain why different multiples give different values.\n\nCross-check relative results with an intrinsic method (DCF) to detect sector-wide mispricing."
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#references",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#references",
    "title": "Relative Valuation",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ."
  },
  {
    "objectID": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#footnotes",
    "href": "valuation/coursework/Lecture 2 - Relative Valuation/index.html#footnotes",
    "title": "Lucas S. Macoris",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee [@BDM] Comment on mismatched ratios, p. 77↩︎"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#disclaimer",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#disclaimer",
    "title": "The Discounted Cash Flow Method",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\n\n\n\n\nDisclaimer\n\n\nThe information presented in this lecture is for educational and informational purposes only and should not be construed as investment advice. Nothing discussed constitutes a recommendation to buy, sell, or hold any financial instrument or security. Investment decisions should be made based on individual research and consultation with a qualified financial professional. The presenter assumes no responsibility for any financial decisions made based on this content.\nAll publicly available content used in this lecture is available and also shared on my GitHub page. Participants are encouraged to review, modify, and use it for their own learning and research purposes. However, no guarantees are made regarding the accuracy, completeness, or suitability of the code for any specific application.\nFor any questions or concerns, please feel free to reach out via email at lucas.macoris@fgv.br"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#what-is-capital-budgeting",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#what-is-capital-budgeting",
    "title": "Capital Budgeting and Free Cash Flow",
    "section": "What is Capital Budgeting?",
    "text": "What is Capital Budgeting?\n\nAn important responsibility of corporate financial managers is determining which projects or investments a firm should undertake\n\n\n\n\n\n\n\n\nDefinition\n\n\nCapital Budgeting is the process of analyzing investment opportunities and deciding which ones to accept. It consists of a list of all projects and investments that a company plans to undertake in the near future.\n\n\n\n\nIn our previous lecture, you saw how you can use firm’s financials to draw insightful metrics about its performance and draw forecasts regarding the firms’ value\nYou can draw the same rationale to analyze not only a company that is already up and running, but also an investment project that has not yet started\nAs we’ll see, the Net Present Value (NPV) is the most accurate metric to evaluate investment projects, and there are a couple of other metrics that can help you get a better understanding of the investment opportunity"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#analyzing-expected-future-performance",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#analyzing-expected-future-performance",
    "title": "The Discounted Cash Flow Method",
    "section": "Analyzing (expected) future performance",
    "text": "Analyzing (expected) future performance\n\nSo far, we’ve been concerned about the firm’s past performance:\n\nHow profitability was trending?\nHow does working capital was financed?\n\nNow, we’ll turn our attention to focus on (expected) future performance:\n\nHow much value can a new project bring to the firm?\nWhat are the expected future cash inflows/outflows?\n\nIn what follows, you will learn how to estimate the future cash-flows that will serve as a input to our analysis!"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#introducing-the-free-cash-flow-fcf",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#introducing-the-free-cash-flow-fcf",
    "title": "The Discounted Cash Flow Method",
    "section": "Introducing the Free Cash Flow (FCF)",
    "text": "Introducing the Free Cash Flow (FCF)\n\nHow to measure the incremental gains/losses due to the acceptance of a project? For this, we’ll use the Free Cash Flow measure\n\n\n\n\n\n\n\n\n\nDefinition\n\n\nThe Free Cash Flow is a measure of the project’s available cash to be repaid to its investors after all costs and investments needed to sustain the business plan were taken into account. It is calculated by:\n\nProjecting direct expected Revenue and Cost Estimates\nConsidering indirect revenues/expenses\nCalculating EBIT, Taxes, and NOPAT\nAdjusting for non-cash effects\nConsidering future investments\nDetermining eventual adjustments, if needed\n\n\n\n\n\n\nWe’ll be doing this in using a case study that will guide us through all the steps"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#case-walkthrough",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#case-walkthrough",
    "title": "The Discounted Cash Flow Method",
    "section": "Case walkthrough",
    "text": "Case walkthrough\n\nCia. Amazônia is a manufacturer of sports shoes that is analyzing the possibility of investing in a new line of sneakers, having even incurred research and market testing costs worth \\(\\small \\$125,000\\). The shoes would be manufactured in a warehouse next to the company’s factory, fully depreciated, which is vacant and could be rented for \\(\\small\\$38,000\\) per year.\nThe cost of the machine is \\(\\small \\$200,000\\), depreciated over five years using the straight-line method. Its market value, estimated at the end of five years, is \\(\\small \\$35,000\\)\nThe company needs to maintain a certain investment in working capital. As it is an industrial company, it will purchase raw materials before producing and selling the final product, which will result in an investment in inventories. The firm will maintain a cash balance as protection against unforeseen expenses. Credit sales will generate accounts receivable. In sum, working capital will represent \\(\\small10\\%\\) of sales revenue."
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#case-walkthrough-continued",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#case-walkthrough-continued",
    "title": "The Discounted Cash Flow Method",
    "section": "Case walkthrough, continued",
    "text": "Case walkthrough, continued\n\nThe company projects the following sales over a \\(\\small5\\)-year horizon\n\nYear 1: \\(\\small 7,000\\)\nYear 2: \\(\\small 9,000\\)\nYear 3: $10,000\nYear 4: \\(\\small11,000\\)\nYear 5: \\(\\small9,000\\)\n\nThe unit price is \\(\\small\\$28\\), and the unit cost is \\(\\small\\$14\\). It is estimated that its operating costs will rise at an average rate of \\(\\small6\\%\\) each year\nOn the other hand, the company knows that due to market competition, it will not be able to fully pass this on to prices and projects an average increase in sales prices of \\(\\small4\\%\\) each year"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-1-revenue-and-cost-estimates",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-1-revenue-and-cost-estimates",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 1: Revenue and Cost Estimates",
    "text": "Step 1: Revenue and Cost Estimates\n\nEarnings are not actual cash flows. However, as a practical matter, to derive the forecasted cash flows of a project, financial managers often begin by forecasting earnings\nThus, we begin by determining the incremental earnings of a project—that is, the amount by which the firm’s earnings are expected to change as a result of the investment decision.\nIn our case, we begin by determining the direct earnings and cost estimates from the operation:\n\n\n\\[\n\\small \\text{Gross Profit}_{t}=\\text{Sales}_t\\times(\\text{Price per Unit}_t-\\text{Cost per Unit}_t)\n\\]\n\nThe Gross Profit is our starting point for estimating incremental earnings"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-1-revenue-and-cost-estimates-1",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-1-revenue-and-cost-estimates-1",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 1: Revenue and Cost Estimates",
    "text": "Step 1: Revenue and Cost Estimates\n\nYour Gross Profit estimation should look like the following:\n\n\n\n\n\n\n\n\nBefore we calculate tax expenses, we need to deduct all other costs that may affect taxes:\n\nFor example, depreciation and amortization are non-cash items, but they are generally tax-deductible\nFurthermore, all other incremental costs, even if they are indirect, need to be taken into account"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-2-consider-indirect-effects",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-2-consider-indirect-effects",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 2: Consider indirect effects",
    "text": "Step 2: Consider indirect effects\n\nWhen computing the incremental earnings of an investment decision, we should include all changes between:\n\nThe firm’s earnings with the project;\nThe firm’s earnings without the project\n\nThere are two important sources of indirect costs that need to be considered:\n\nOpportunity Costs: many projects use a resource that the company already owns. However, in many cases the resource could provide value for the firm in another opportunity or project.\nProject externalities: indirect effects of the project that may increase or decrease the profits of other business activities of the firm"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-2-consider-indirect-effects-1",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-2-consider-indirect-effects-1",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 2: Consider indirect effects",
    "text": "Step 2: Consider indirect effects\n\nIn our case, we saw that the firm will use existing assets that otherwise would yield $38,000 yearly. Because of that, we need to take into consideration as an opportunity cost\nWhat about the \\(\\small\\$125,000\\) R&D expenses incurred? This is an example of a sunk cost:\n\nSunk costs have been or will be paid regardless of the decision about whether or not to proceed with the project\nTherefore, they are not incremental with respect to the current decision and should not be included in its analysis\nIf our decision does not affect the cash flow, then the cash flow should not affect our decision!\n\nExamples of sunk costs may include, but are not limited to: past R&D expenses, fixed overhead costs, and unavoidable competition effects"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-3-ebit-and-taxes",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-3-ebit-and-taxes",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 3: EBIT and Taxes",
    "text": "Step 3: EBIT and Taxes\n\nBefore we calculate tax expenses, if we assume that depreciation is tax-deductible, we can then use the \\(\\small\\$40,000\\) value from the straight-line depreciation to deduct our taxable earnings:\n\n\n\\[\n\\small EBIT_{t}= [\\text{Sales}_t\\times(\\text{Price per Unit}_t-\\text{Cost per Unit}_t)-\\text{Depreciation}_t-\\text{Other Costs}_t]\n\\]\n\nAnd the Income Tax that we’ll deduct is:\n\n\n\n\\[\n\\small \\text{Income Tax}_{t}= EBIT_{t}\\times\\tau_t\n\\]\n\n\n\nWhich tax-rate to use? The correct tax rate to use is the firm’s marginal corporate tax rate, which is the tax rate it will pay on an incremental dollar of pre-tax income"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-3-ebit-and-taxes-1",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-3-ebit-and-taxes-1",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 3: EBIT and Taxes",
    "text": "Step 3: EBIT and Taxes\n\nWhat if part of the upfront investment was financed using debt? Do we need to include interest expenses in the calculation?\nBecause the Free Cash Flow is the measure of available resources to all investors of the firm (creditors + equityholders), whenever we evaluating a capital budgeting decision, we do not include interest expenses in the calculation\n\nWe wish to evaluate the project on its own, separate from the financing decision\nFurthermore, the cost of debt (along with its tax shield) can be considered in an appropriate estimate of the cost of capital for the project\nFor these reasons, we also call our incremental earnings as unlevered net income\n\n\n\n\\(\\rightarrow\\) Therefore, in our Free Cash Flow estimations, we’ll be focusing on the operating portion as if it were financed without any debt!"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-3-ebit-and-taxes-2",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-3-ebit-and-taxes-2",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 3: EBIT and Taxes",
    "text": "Step 3: EBIT and Taxes\n\nConsidering both Depreciation and the Opportunity Costs, our Unlevered Net Income is:"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-4-adjust-for-non-cash-effects",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-4-adjust-for-non-cash-effects",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 4: Adjust for non-cash effects",
    "text": "Step 4: Adjust for non-cash effects\n\nAs discussed in previous lectures, earnings are merely an accounting measure of the firm’s performance:\n\nThey do not represent real profits\nAs a consequence, the firm cannot use its earnings to buy goods, pay employees, fund new investments, or pay dividends to shareholders\n\nOn the other hand, cash does!\n\nBecause of this, to evaluate a capital budgeting decision, we must determine its consequences for the firm’s available cash\nThe incremental effect of a project on the firm’s available cash, separate from any financing decisions, is the project’s Free Cash Flow"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-4-adjust-for-non-cash-effects-1",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-4-adjust-for-non-cash-effects-1",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 4: Adjust for non-cash effects",
    "text": "Step 4: Adjust for non-cash effects\n\nThere are important differences between earnings and cash flow:\n\nEarnings include non-cash charges, such as depreciation…\nBut do not include the cost of capital investment!\n\nTo determine the Free Cash Flow, we must adjust for these differences by:\n\nAdding back Depreciation: because depreciation is not a cash flow, we do not include it in the cash flow forecast\nCapital Expenditures (CAPEX): to account for the cash that will be used to fund the equipments, we include the actual cash cost of the asset when it is purchased."
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-4-adjust-for-non-cash-effects-2",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-4-adjust-for-non-cash-effects-2",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 4: Adjust for non-cash effects",
    "text": "Step 4: Adjust for non-cash effects\n\nIn our case, we have the following adjustments:\n\n\nFor Depreciation, we need to add back \\(\\small\\$50,000\\) across Year 1-5 to account for non-cash items\nOn the other hand, to consider the actual cost of the machinery by the time that it was bought, we need to include \\(\\small\\$200,000\\) in Year 0 of the analysis\n\n\nWith these adjustments in place, we should have the following values:"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-5-consider-future-investments-in-working-capital",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-5-consider-future-investments-in-working-capital",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 5: Consider future investments in working capital",
    "text": "Step 5: Consider future investments in working capital\n\nNow that we have considered all cash effects from the investment that is needed, is there anything else that needs to be taken into consideration?\nMost projects will require the firm to continuosly invest in net working capital as time goes by:\n\nFirms may need to maintain a minimum cash balance to meet unexpected expenditures\nInventories of raw materials and finished products are needed to accommodate uncertainties and demand fluctuations\nFinally, customers may not pay for the goods they purchase immediately, and the firm may have credit with its suppliers\n\nAlthough it is difficult to consider all potential fluctuations on working capital, it is expected that a portion of it should be positively correlated with sales:\n\nAs sales go up, firms may want to keep its past terms with suppliers and customers\nAll else equal, an increase in sales should increase the amount of working capital needed"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-5-consider-future-investments-in-working-capital-1",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-5-consider-future-investments-in-working-capital-1",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 5: Consider future investments in working capital",
    "text": "Step 5: Consider future investments in working capital\n\nIn our case, we summarized this idea by taking into consideration that working capital is 10% of the Sales revenue\nTherefore, our year-over-year change in net working capital reflects the additions/deductions on the amount of net working capital for each year:\n\n\n\\[\n\\small \\Delta NWC_{t}=NWC_{t}-NWC_{t-1}\n\\]\n\nIn our case, our net working capital should look as follows:"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-5-consider-future-investments-in-working-capital-2",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-5-consider-future-investments-in-working-capital-2",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 5: Consider future investments in working capital",
    "text": "Step 5: Consider future investments in working capital\n\n\n\n\n\n\nIn the beginning of Year 0, we forecast Year 1’s sales and invest in working capital\nFor each Year 1-4, we look forward to period \\(\\small t+1\\) to determine the adequate level of working capital in \\(t\\)\nAt the end of Year 5, we know that the \\(\\small NWC=0\\), assuming that the project ends\nTherefore, \\(\\small \\Delta NWC_{t=5}\\) shows that the firm can recover its investment in working capital"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-6-calculating-the-free-cash-flow",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-6-calculating-the-free-cash-flow",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 6: Calculating the Free Cash Flow",
    "text": "Step 6: Calculating the Free Cash Flow\n\nWe can summarize what we have so far by:\n\n\n(+) Revenues\n(-) Costs\n(-) Depreciation\n(=) EBIT\n(-) Tax Expenses\n(=) Unlevered Net Income\n(+) Depreciation\n(-) CAPEX\n(-) \\(\\Delta\\) NWC\n(=) Free Cash Flow\n\nThis is the standard estimate of a Free Cash Flow, which is the amount of incremental cash that a project can actually bring to the firm!"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-6-calculating-the-free-cash-flow-1",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-6-calculating-the-free-cash-flow-1",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 6: Calculating the Free Cash Flow",
    "text": "Step 6: Calculating the Free Cash Flow\n\nWe can summarize the Free Cash Flow calculation as follows:\n\n\n\\[\n\\small FCF_{t}= \\underbrace{(\\text{Revenues}-\\text{Costs}-\\text{Depreciation})\\times(1-\\tau)}_{\\text{Unlevered Net Income}}+\\text{Depreciation}-\\text{CAPEX}-\\Delta NWC\n\\]\n\nNote that we first deduct depreciation when computing the project’s incremental earnings, and then add it back (because it is a non-cash expense) when computing Free Cash Flow\nThus, the only effect of depreciation is to reduce the firm’s taxable income!\nBecause of this, we can rewtrite the same equation as:\n\n\n\n\\[\n\\small FCF_{t}= (\\text{Revenues}-\\text{Costs})\\times(1-\\tau)-\\text{CAPEX}-\\Delta NWC+\\tau\\times\\text{Depreciation}\n\\]\n\nWhere the last term is the depreciation tax shield"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-7-adjustments-to-the-calculating-the-free-cash-flow",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-7-adjustments-to-the-calculating-the-free-cash-flow",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 7: Adjustments to the Calculating the Free Cash Flow",
    "text": "Step 7: Adjustments to the Calculating the Free Cash Flow\n\nOur standard Free Cash Flow estimates should look like the following:"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-7-adjustments-to-the-calculating-the-free-cash-flow-1",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-7-adjustments-to-the-calculating-the-free-cash-flow-1",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 7: Adjustments to the Calculating the Free Cash Flow",
    "text": "Step 7: Adjustments to the Calculating the Free Cash Flow\n\nOur final step is to account for any eventual adjustments needed. Some examples include (but are not limited) to:\n\nOther non-cash items: amortization of intangibles, for example, can be taken into consideration\nTiming of cash flows: can be estimated on a monthly or quarterly basis\nDifferent depreciation patterns: straight line depreciation may not apply to all cases\nLiquidation or Salvage value: assets that are no longer needed often have a resale value, or some salvage value if the parts are sold for scrap\nTermination Value: value for the subsequent periods whenever we have infinite-horizon projects"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-7-adjustments-to-the-calculating-the-free-cash-flow-2",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#step-7-adjustments-to-the-calculating-the-free-cash-flow-2",
    "title": "The Discounted Cash Flow Method",
    "section": "Step 7: Adjustments to the Calculating the Free Cash Flow",
    "text": "Step 7: Adjustments to the Calculating the Free Cash Flow\n\nIn our case, we know that the market-value of the machinery is \\(\\small 35,000\\). Since it has been fully depreciated at Year 5, we know that the capital gain is simply \\(\\small 35,000 - 0 = 35,000\\)\nTherefore, we also need to consider that, in Year 5, as the project has ended, we can sell the machine, pay taxes on it, and recover part the liquidation value of our investment:\n\n\n\\[\n\\small \\text{Liquidation Value}= 35,000 \\times (1-\\tau)\\rightarrow 35,000\\times(1-34\\%)=23,100\n\\]\n\nAdding this value as a cash-inflow in the last year of the project:\n\n\n\n\\[\n\\small FCF_{t=5}=107,584+23,100=130,684\n\\]"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#final-result",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#final-result",
    "title": "The Discounted Cash Flow Method",
    "section": "Final Result",
    "text": "Final Result"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#bridging-npv-and-free-cash-flow",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#bridging-npv-and-free-cash-flow",
    "title": "The Discounted Cash Flow Method",
    "section": "Bridging NPV and Free Cash Flow",
    "text": "Bridging NPV and Free Cash Flow\n\nWe can finally use our FCF estimates in the NPV formula to gauge the value of the project:\n\n\n\\[\n\\small\nNPV= \\dfrac{-219,000}{(1+15\\%)^0}+\\dfrac{46,592}{(1+15\\%)^1}+\\dfrac{69,266}{(1+15\\%)^2}+\\dfrac{80,218}{(1+15\\%)^3}+\\dfrac{101,293}{(1+15\\%)^4}+\\dfrac{130,684}{(1+15\\%)^5}=48,922.22\n\\]\n\nWhat else needs to be done? See the Appendix for a detailed discussion on some of the most common adjustments and extensions1:\n\nAdjust FCF estimates to incorporate different depreciation patterns\nIncorporate long-term value for projects that have an infinite-horizon (for example, a firm!)\nTake uncertainty of the inputs that we’re using into account through Sensitivity, Break-Even, and Scenario Analysis\n\n\n\nAll extensions discussed in the Appendix have an accompanying Excel solution that has been posted on eClass®."
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#supplementary-reading",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#supplementary-reading",
    "title": "The Discounted Cash Flow Method",
    "section": "Supplementary Reading",
    "text": "Supplementary Reading\n\nSee Note on Capital Budgeting for a detailed discussion on more aspects of the capital budgeting process in practice\n\n\n\\(\\rightarrow\\) All contents are available on eClass®."
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#different-depreciation-patterns",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#different-depreciation-patterns",
    "title": "The Discounted Cash Flow Method",
    "section": "Different Depreciation Patterns",
    "text": "Different Depreciation Patterns\nSuppose that instead of using foreign suppliers when buying your machinery, you have received an offer to use new, national supplier, which can provide machinery that is supposedly as efficient as the original one. Notwithstanding, because you’re financing a national capital good, you have access to an accelerated depreciation benefit:\n\nThe market-value of the machine is still worth \\(\\small \\$200,000\\), to be paid in Year 0\nYou’ll fully depreciate the machine in the first three years: \\(\\small 30\\%\\) in the first year, and \\(\\small 35\\%\\) in the second and third year\n\n\nWould you accept the offer?\n\n\n\\(\\rightarrow\\) Answer provided in Excel"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#tax-carryforwards",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#tax-carryforwards",
    "title": "The Discounted Cash Flow Method",
    "section": "Tax Carryforwards",
    "text": "Tax Carryforwards\nSuppose that instead of having the previous accelerated depreciation alternative, you have received a more aggressive one: depreciate 100% of the machine in the first year. You also notice that you can deduct your taxable income with tax carryforward up to a limit of 30% of the taxable income. Calculate the new NPV of the project and discuss how it has changed.\n\n\\(\\rightarrow\\) Answer provided in Excel"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#continuation-value",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#continuation-value",
    "title": "The Discounted Cash Flow Method",
    "section": "Continuation Value",
    "text": "Continuation Value\nIn our standard setting, our assumption was that the firm’s operation would cease after Year 5. Because of that, we initially included in our Free Cash Flow estimates a Termination Value that represented the sale of unused assets, after taking into account its tax effects.\n\nSuppose that the firm is able to continue its operations indefinitely after Year 5, and that the FCF is expected to stay the same as of Year 5. Calculate the new NPV of the project assuming the same cost of capital. Justify why the values have changed significantly.\nHow your answer would change if the FCF grew at a 2% rate after Year 5, indefinitely?\n\n\n\\(\\rightarrow\\) Answer provided in Excel"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty",
    "title": "The Discounted Cash Flow Method",
    "section": "Dealing with Uncertainty",
    "text": "Dealing with Uncertainty\n\nThe most difficult part of capital budgeting is deciding how to estimate the cash flows and cost of capital. Unfortunately, these estimates are often subject to significant uncertainty:\nHow we can assess the importance of this uncertainty and identify the drivers of value in the project?\nIn what follows, we’ll look at some examples outlining ways to incorporate uncertainty in our valuation model"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty-1-break-even-analysis",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty-1-break-even-analysis",
    "title": "The Discounted Cash Flow Method",
    "section": "Dealing with Uncertainty #1: Break-Even Analysis",
    "text": "Dealing with Uncertainty #1: Break-Even Analysis\nShareholders at Cia Amazônia are concerned that rising costs of activity may hinder any profitable investment opportunity projected in the original project. More specifically, their main concern is that the average growth rate in Unit Costs, which was estimated to be 6%, is estimated using very unreasonable scenario, and that higher increases in unit costs might induce the project’s NPV to be negative. Estimate what is the maximum average increase in unit costs over the years that would change the decision to invest in the project.\n\\(\\rightarrow\\) Answer provided in Excel"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty-2-sensitivity-analysis",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty-2-sensitivity-analysis",
    "title": "The Discounted Cash Flow Method",
    "section": "Dealing with Uncertainty #2: Sensitivity Analysis",
    "text": "Dealing with Uncertainty #2: Sensitivity Analysis\nShareholders might be reluctant to trust the NPV estimates if they are unable to understand which drivers potentially affect more the value of the project in best and worst-case situations. Consider that you now have three scenarios: base, worst, and best case scenarios. In each one, you have the following configuration:\n\nBase: the baseline exercise from Cia Amazônia. Growth rate of Unit Costs: \\(\\small4\\%\\); Growth rate of Unit Prices: \\(\\small6%\\); Cost of Capital: \\(\\small15\\%\\)\nWorst: Growth rate of Unit Costs: \\(\\small0%\\); Growth rate of Unit Prices: \\(\\small10\\%\\); Cost of Capital: \\(\\small19\\%\\)\nBest: Growth rate of Unit Costs: \\(\\small10\\%\\); Growth rate of Unit Prices: \\(\\small1\\%\\); Cost of Capital: \\(\\small10\\%\\)\n\n\nEstimate how your NPV estimates change along with your inputs, each one at a time, and identify which input is more important for the project’s NPV.\n\\(\\rightarrow\\) Answer provided in Excel"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty-3-scenario-analysis",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#dealing-with-uncertainty-3-scenario-analysis",
    "title": "The Discounted Cash Flow Method",
    "section": "Dealing with Uncertainty #3: Scenario Analysis",
    "text": "Dealing with Uncertainty #3: Scenario Analysis\nWhat if you want to vary over more than one input at a time? Say that, for example, you want to think about a combination of growth rates for unit costs AND unit revenues at the same time? It is very reasonable to assume that more than one driver is going to change at a time. To do that, create a 3x3 grid of combinations considering the growth rates of Unit Costs and Unit Price and show how your NPV estimates change for each pair of growth rate estimates.\n\n\\(\\rightarrow\\) Answer provided in Excel"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#references",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#references",
    "title": "The Discounted Cash Flow Method",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#moving-past-relative-valuation",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#moving-past-relative-valuation",
    "title": "The Discounted Cash Flow Method",
    "section": "Moving past relative valuation",
    "text": "Moving past relative valuation\n\nAs you saw in the previous lecture, although Relative Valuation can provide a ballpark estimate of what would have been the firm value as-if it was priced similarly to other comparable firms, there were a series of drawbacks and limitations:\n\nSensitive to market mood and choice of comparables\nCaptures market sentiment and industry positioning\nGenerally backward-looking\n\nTo that matter, we need a way to estimate the intrinsic value of a company taking into account specific information regarding its ability to generate future profits"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#the-discounted-cash-flow-dcf-method",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#the-discounted-cash-flow-dcf-method",
    "title": "The Discounted Cash Flow Method",
    "section": "The Discounted Cash Flow (DCF) Method",
    "text": "The Discounted Cash Flow (DCF) Method\n\n\n\n\n\n\n\nDefinition\n\n\nValuation method that estimates the present value of an asset based on its expected future cash flows.. It takes into account measures of expected profits (\\(\\small 1,2,...,\\infty\\) periods ahead) and its underlying risk to come up with an estimate of the opportunity value in \\(\\small t_0\\).\n\n\n\n\nCore Idea: if we are able to estimate the firms’ future cash flows, we can adjust them to present value using a discount rate\nKey Inputs: Cash flow projections, discount rate (often WACC), and terminal value assumptions\nStrengths: Grounded in fundamentals; flexible to different scenarios\nWeaknesses: time-consuming, fully dependend on the forecast assumptions"
  },
  {
    "objectID": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#the-discounted-cash-flow-dcf-method-1",
    "href": "valuation/coursework/Lecture 3 - Fundamentals of Capital Budgeting/index.html#the-discounted-cash-flow-dcf-method-1",
    "title": "The Discounted Cash Flow Method",
    "section": "The Discounted Cash Flow (DCF) Method",
    "text": "The Discounted Cash Flow (DCF) Method\n\nLet \\(CF_t\\) be the amount of cash flow available in period \\(t\\) to all owners of the company, for periods \\(\\small t=1,2,3,...,\\infty\\). The Discounted Cash Flow method estimates the intrinsic value of a company, \\(V_i\\), as:\n\n\\[\nV_i = \\sum_{t=1}^{\\infty} \\dfrac{CF_i}{(1+r_i)},\n\\]\n\nwhere \\(r_i\\) is the opportunity cost of capital associated with undertaking the project\nQuestion: how can we use such rationale to estimate the value of a new investment opportunity?"
  },
  {
    "objectID": "valuation/capstone/guidelines.html",
    "href": "valuation/capstone/guidelines.html",
    "title": "Capstone Project - Valuation",
    "section": "",
    "text": "As the final evaluation for the Valation course, students will develop a Capstone Project demonstrating their ability to apply the methods discussed in class to evaluate a publicly-traded Brazilian company. Students will work in groups to conduct a comprehensive valuation, integrating both relative valuation and discounted cash flow (DCF) approaches, to come up with a final investment recommendation.\nPlease take some time to review the specific guidelines that should be the basis of your project."
  },
  {
    "objectID": "valuation/capstone/guidelines.html#objective",
    "href": "valuation/capstone/guidelines.html#objective",
    "title": "Capstone Project - Valuation",
    "section": "",
    "text": "As the final evaluation for the Valation course, students will develop a Capstone Project demonstrating their ability to apply the methods discussed in class to evaluate a publicly-traded Brazilian company. Students will work in groups to conduct a comprehensive valuation, integrating both relative valuation and discounted cash flow (DCF) approaches, to come up with a final investment recommendation.\nPlease take some time to review the specific guidelines that should be the basis of your project."
  },
  {
    "objectID": "valuation/capstone/guidelines.html#group-formation",
    "href": "valuation/capstone/guidelines.html#group-formation",
    "title": "Capstone Project - Practical Applications in Quantitative Finance",
    "section": "Group Formation",
    "text": "Group Formation\nStudents must work in teams of up to five members, maximum. Each team is responsible for defining roles and organizing their workflow effectively to ensure timely completion."
  },
  {
    "objectID": "valuation/capstone/guidelines.html#project-scope",
    "href": "valuation/capstone/guidelines.html#project-scope",
    "title": "Capstone Project - Practical Applications in Quantitative Finance",
    "section": "Project Scope",
    "text": "Project Scope\nTeams are expected to generate an analysis using one or more finance concepts studied in this course or other finance-related concepts that align with the course’s focus. The analysis should be conducted using the R and incorporate the tools and techniques covered, such as the tidyverse, tidyquant, ggplot2, among others.\nMore specifically, the expected output must be structured containing the following deliverables:\n\nA PowerPoint presentation that summarizes the investment recommendation; and\nAn Excel spreadsheet containing the calculations conducted throughout the project\n\nThe professor will assign each group with a publicly-traded company. Students are expected to complete the following tasks:\n1. Historical Performance Analysis & Relative Valuation: analyze the company’s historical performance over the past 5 years, including revenue growth, profitability, capital efficiency, and leverage trends. - Calculate and compare valuation multiples (e.g., P/E, EV/EBITDA, P/B, EV/Sales) against the peer group. - Interpret differences between the subject company and its peers.\n\\(\\rightarrow\\) FactSet to identify and select an appropriate peer group (minimum of 5 comparable firms).\n2. Calculating Financial Indexes: using the balance-sheet, income statement, and cash-flow statements, calculate, for the latest 5 (five) fiscal years:\n\nEarnings Before Interest and Taxes (EBIT)\nNet Operating Profit After Taxes (NOPAT)\nFree Cash Flow (FCF)\n\nClearly document assumptions and any adjustments (e.g., normalization of earnings, one-off expenses).\n3. Forecasting Free Cash Flows: project the company’s Free Cash Flow for the next 5 years. State and justify key forecast assumptions (e.g., revenue growth rate, operating margins, reinvestment needs, tax rates, discount rate).\n4. Terminal Value Estimation: Estimate the (or continuation value) using both the perpetuity growth method or an exit multiple method. Discuss the implications and differences between the two.\n5. Calculate the firm value, equity value, and the estimated share price using the DCF model. - Perform sensitivity analysis for key inputs (e.g., WACC, terminal growth rate, exit multiple, revenue growth). - Present results in tables and charts."
  },
  {
    "objectID": "valuation/capstone/guidelines.html#evaluation-criteria",
    "href": "valuation/capstone/guidelines.html#evaluation-criteria",
    "title": "Capstone Project - Valuation",
    "section": "Evaluation Criteria",
    "text": "Evaluation Criteria\nThe evaluation criteria will be mainly take two aspects into consideration: a written report (\\(50\\%\\)) and a project showcase (\\(30\\%\\)). First, the written report, will be evaluated on the basis of:\n\nThe quality and adherence of the deliverable relative to the expected goals outlined in the guidelines\nThe group’s ability to interpret data using finance theory and draw insights from their findings\nOrganization, providing clean, concise, and reproducible results\n\nFurthermore, at the end of the course, each group is also expected to present their project to the course audience. All individuals from the group are expected to attend and present. During this showcase, which should last no longer than 10 (ten) minutes, students will be evaluated on the basis of:\n\nAbility to Answer Practical and Technical Questions\n\nDemonstrate a deep understanding of their analysis, methods, and results\nPresenters should be prepared to respond to questions regarding data sources, methodology, and financial interpretation\n\nOrganization\n\nThe project should be well-structured, with a logical flow from problem statement to conclusion\nCode and documentation should be clear, well-commented, and reproducible\n\nClear Communication\n\nPresentations should be concise, engaging, and accessible to the audience\nVisualizations and explanations should effectively convey key insights"
  },
  {
    "objectID": "valuation/capstone/guidelines.html#expectations-best-practices",
    "href": "valuation/capstone/guidelines.html#expectations-best-practices",
    "title": "Capstone Project - Valuation",
    "section": "Expectations & Best Practices",
    "text": "Expectations & Best Practices\nAll in all, the Capstone Project is an opportunity for students not only to showcase their work not only for the course audience, but also for external audiences, like HR managers, recruiters, among others. At the same time you can expect to receive feedback from your colleagues, be prepared to also provide useful feedback to other groups regarding their work.\nTo make sure that you are set up for success, try to follow these best practices to best of our ability:\n\nStart early: effective time management is crucial for success. Make sure to devote enough time for somewhat cumbersome tasks, such as data collection and manipulation\nCollaborate efficiently: distribute tasks within your team to leverage individual strengths. Test code snippets and make sure that the whole analysis is reproducible\nMaintain clarity: ensure that your analysis and conclusions are well-documented and easy to follow\nReview and iterate: test your calculations, validate results, and refine your presentation"
  },
  {
    "objectID": "valuation/capstone/guidelines.html#deadlines",
    "href": "valuation/capstone/guidelines.html#deadlines",
    "title": "Capstone Project - Valuation",
    "section": "Deadlines",
    "text": "Deadlines\nThe deadline for the submission of the Capstone Project is Sunday, 28th, at 11:59 PM, Brazilian time. Submissions made after this date will not be accepted. Make sure that one (and only one) member of your group posts the file on eClass®."
  },
  {
    "objectID": "valuation/capstone/guidelines.html#final-thoughts",
    "href": "valuation/capstone/guidelines.html#final-thoughts",
    "title": "Capstone Project - Valuation",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nStudents are highly encouraged to share their codes so that everyone can replicate and contribute to their analyses. This capstone project represents an opportunity to apply your knowledge in a meaningful way, demonstrating both technical and analytical skills. Make the most of it!\nIn case of any questions, feel free to reach out to lucas.macoris@fgv.br"
  },
  {
    "objectID": "valuation/capstone/guidelines.html#project-scope-group-formation",
    "href": "valuation/capstone/guidelines.html#project-scope-group-formation",
    "title": "Capstone Project - Valuation",
    "section": "Project Scope Group Formation",
    "text": "Project Scope Group Formation\nStudents must work in teams of up to five members, maximum. Each team is responsible for defining roles and organizing their workflow effectively to ensure timely completion. The expected output must be structured containing the following deliverables:\n\nA PowerPoint presentation that summarizes the investment recommendation; and\nAn Excel spreadsheet containing the calculations conducted throughout the project"
  },
  {
    "objectID": "valuation/capstone/guidelines.html#step-by-step",
    "href": "valuation/capstone/guidelines.html#step-by-step",
    "title": "Capstone Project - Valuation",
    "section": "Step-by-step",
    "text": "Step-by-step\nThe professor will assign each group with a publicly-traded company. Students are expected to complete the following tasks:\n1. Historical Performance Analysis & Relative Valuation: analyze the company’s historical performance over the past 5 years, including revenue growth, profitability, capital efficiency, and leverage trends:\n\nCalculate and compare valuation multiples (e.g., P/E, EV/EBITDA, P/B, EV/Sales) against the peer group\nInterpret differences between the subject company and its peers\n\n\\(\\rightarrow\\) Students can use FactSet to identify and select an appropriate peer group. For details on how to redeem access through FGV, follow the instructions outlined on (eClass).\n2. Calculating Financial Indexes: using the balance-sheet, income statement, and cash-flow statements, calculate, for the latest 5 (five) fiscal years:\n\nEarnings Before Interest and Taxes (EBIT)\nNet Operating Profit After Taxes (NOPAT)\nFree Cash Flow (FCF)\n\nClearly document assumptions and any adjustments (e.g., normalization of earnings, one-off expenses).\n3. Forecasting Free Cash Flows: project the company’s Free Cash Flow for the next 5 years. State and justify key forecast assumptions (e.g., revenue growth rate, operating margins, reinvestment needs, tax rates, discount rate).\n4. Terminal Value Estimation: Estimate the (or continuation value) using both the perpetuity growth method or an exit multiple method. Discuss the implications and differences between the two.\n5. Implied share price: Calculate the firm value, equity value, and the estimated share price using the DCF model\n\nPerform sensitivity analysis for key inputs (e.g., WACC, terminal growth rate, exit multiple, revenue growth).\nPresent results in tables and charts."
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#disclaimer",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#disclaimer",
    "title": "Valuation with Leverage",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\n\n\n\n\nDisclaimer\n\n\nThe information presented in this lecture is for educational and informational purposes only and should not be construed as investment advice. Nothing discussed constitutes a recommendation to buy, sell, or hold any financial instrument or security. Investment decisions should be made based on individual research and consultation with a qualified financial professional. The presenter assumes no responsibility for any financial decisions made based on this content.\nAll publicly available content used in this lecture is available and also shared on my GitHub page. Participants are encouraged to review, modify, and use it for their own learning and research purposes. However, no guarantees are made regarding the accuracy, completeness, or suitability of the code for any specific application.\nFor any questions or concerns, please feel free to reach out via email at lucas.macoris@fgv.br"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#revisiting-the-free-cash-flow",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#revisiting-the-free-cash-flow",
    "title": "Valuation with Leverage",
    "section": "Revisiting the Free Cash Flow",
    "text": "Revisiting the Free Cash Flow\n\n(+) Revenues\n(-) Costs\n(-) Depreciation\n(=) EBIT\n(-) Tax Expenses\n(=) Unlevered Net Income\n(+) Depreciation\n(-) CAPEX\n(-) \\(\\Delta\\) NWC\n(=) Free Cash Flow\n\nThis is the standard estimate of a Free Cash Flow, which is the amount of incremental cash that a project can actually bring to the firm!"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#introducing-different-financing-decisions",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#introducing-different-financing-decisions",
    "title": "Valuation with Leverage",
    "section": "Introducing different Financing Decisions",
    "text": "Introducing different Financing Decisions\n\nSo far, we’ve assumed that this project was financed only through equity\nHow does the financing decision of a firm can affect both the cost of capital and the set of cash flows that we ultimately discount?\nThere are three main methods that can consider leverage decisions and market imperfections:\n\nThe Weighted-Average Cost of Capital (WACC) method\nThe Adjusted Present Value (APV) method\nThe Flow-to-Equity (FTE) method\n\nWhile their details differ, when appropriately applied each method produces the same estimate of an investment’s (or firm’s) value"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#underlying-assumptions",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#underlying-assumptions",
    "title": "Valuation with Leverage",
    "section": "Underlying assumptions",
    "text": "Underlying assumptions\n\nThe project has average risk: in essence, the market risk of the project is equivalent to the average market risk of the firm’s investments. In that case, the project’s cost of capital can be assessed based on the risk of the firm\nThe firm’s debt-equity ratio is constant: we consider a firm that adjusts its leverage to maintain a constant debt-equity ratio in terms of market values\nCorporate taxes are the only imperfection: we assume that the main effect of leverage on valuation is due to the corporate tax shield. Other effects, such as issuance costs, personal costs, and bankruptcy costs, are abstracted away\n\n\n\n\n\n\n\n\nImportant\n\n\nWe will be applying each method to a single example in which we have made a number of simplifying assumptions. Although we will only cover the Weighted Average Cost of Capital (WACC), the Appendix contains a detailed discussion on the Adjusted Present Value (APV) and the Flow-to-Equity (FTE) methods. While the assumptions discussed herein are restrictive, they are also a reasonable approximation for many projects and firms."
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-weighted-average-cost-of-capital-wacc",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-weighted-average-cost-of-capital-wacc",
    "title": "Valuation with Leverage",
    "section": "The Weighted Average Cost of Capital (WACC)",
    "text": "The Weighted Average Cost of Capital (WACC)\n\nRecall that our definition of free cash flow measures the after-tax cash flow of a project before regardless how it is financed\nIn a perfect capital markets, choosing debt of equity shouldn’t change the value of the firm. However, because interest expenses are tax deductible, leverage reduces the firm’s total tax liability, enhancing its value!\nWe can directly incorporate market imperfections using the WACC method:\n\n\n\\[\nr_{\\text{WACC}}=\\underbrace{\\dfrac{E}{D+E}}_{\\text{% of Equity}}\\times r_e+ \\underbrace{\\dfrac{D}{D+E}}_{\\text{% of Debt}}\\times r_{D}\\times (1-\\tau)\n\\]\nwhere \\(E\\) is the market-value of Equity, \\(D\\) is the market-value of debt, \\(r_e\\) is the cost of equity, \\(r_d\\) is the cost of debt, and \\(\\tau\\) is the marginal tax rate"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-weighted-average-cost-of-capital-wacc-continued",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-weighted-average-cost-of-capital-wacc-continued",
    "title": "Valuation with Leverage",
    "section": "The Weighted Average Cost of Capital (WACC), continued",
    "text": "The Weighted Average Cost of Capital (WACC), continued\n\nBecause the WACC incorporates the tax savings from debt, we can compute the levered value of an investment by looking at its stream of cash flows discounted by \\(r_{\\text{WACC}}\\):\n\n\n\\[\nV^{L}= \\dfrac{FCF_1}{(1+r_{\\text{WACC}})}+ \\dfrac{FCF_2}{(1+r_{\\text{WACC}})^2}+...+\\dfrac{FCF_n}{(1+r_{\\text{WACC}})^n}\n\\]\n\nIn what follows, we’ll be using an example taken from (Berk and DeMarzo 2023), Chapter 18, to see how the WACC and the other methods can be applied in practice for the RFX project that is being studied by AVCO’s company"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#practical-application-wacc",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#practical-application-wacc",
    "title": "Valuation with Leverage",
    "section": "Practical Application: WACC",
    "text": "Practical Application: WACC\n\n\n\n\n\n\\(\\rightarrow\\) See accompaining Excel document for the calculations"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#practical-application-wacc-continued",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#practical-application-wacc-continued",
    "title": "Valuation with Leverage",
    "section": "Practical Application: WACC (continued)",
    "text": "Practical Application: WACC (continued)\n\nAs said before, we’ll be assuming that the market risk of the RFX project is expected to be similar to that for the company’s other lines of business\nBecause of that, we can use Avco’s equity and debt to determine the weighted average cost of capital for the new project:\n\n\n\n\n\n\n\n\\(\\rightarrow\\) Important: because market values reflect the true economic claim of each type of financing, while calculating the WACC, market value weights for each financing element (equity, debt, etc.) must be used, and not historical, book values"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#practical-application-wacc-continued-1",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#practical-application-wacc-continued-1",
    "title": "Valuation with Leverage",
    "section": "Practical Application: WACC (continued)",
    "text": "Practical Application: WACC (continued)\n\nUsing our example, we can calculate the WACC as1:\n\n\n\\[\n\\small\nr_{\\text{WACC}}=\\underbrace{\\dfrac{300}{300+300}}_{\\text{% of Equity}}\\times 10\\%+ \\underbrace{\\dfrac{300}{300+300}}_{\\text{% of Debt}}\\times 6\\%\\times (1-25\\%)=7.25\\%\n\\]\n\nNow, using \\(\\small r_{\\text{WACC}}=7.25\\%\\), we can calculate the value of the project, including the tax shield from debt, by calculating the present value of its future free cash flows:\n\n\n\n\\[\n\\small\nV^{L}=\\sum_{T=1}^{T=4}\\dfrac{21}{(1+7.25\\%)^t}=70.73\n\\]\n\nBecause the investment in \\(\\small t=0\\) is \\(\\small 29\\), NPV is \\(\\small 70.73-29=\\$41.73\\) million.\n\n\nNote that we’re using Net Debt \\(\\small (320-20)\\) to weigh in the debt potion in the capital structure"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#general-thoughts-on-the-wacc-method",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#general-thoughts-on-the-wacc-method",
    "title": "Valuation with Leverage",
    "section": "General thoughts on the WACC method",
    "text": "General thoughts on the WACC method\n\nThis is the method that is most commonly used in practice for capital budgeting purposes. An important advantage of the WACC method is that you do not need to know how this leverage policy is implemented in order to make the capital budgeting decision\nAfter calculating \\(r_{\\text{WACC}}\\), the rate can then be used throughout the firm assuming that:\n\nThis rate represents the company-wide cost of capital for new investments that are of comparable risk to the rest of the firm\nPursuing the project will not alter the firm’s Debt-to-Equity ratio\n\nIf the firm’s Debt-to-Equity ratio is not constant anymore, we cannot reliably use the WACC method anymore\n\n\n\\(\\rightarrow\\) Refer to the Appendix for a detailed discussion on the Adjusted Present Value (APV) method"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#projects-based-cost-of-capital",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#projects-based-cost-of-capital",
    "title": "Valuation with Leverage",
    "section": "Project’s Based Cost of Capital",
    "text": "Project’s Based Cost of Capital\n\nWe began the last section with three simplifying assumptions:\n\nThe project has average risk: in essence, the market risk of the project is equivalent to the average market risk of the firm’s investments. In that case, the project’s cost of capital can be assessed based on the risk of the firm\nThe firm’s debt-equity ratio is constant: we consider a firm that adjusts its leverage to maintain a constant debt-equity ratio in terms of market values\nCorporate taxes are the only imperfection: we assume that the main effect of leverage on valuation is due to the corporate tax shield. Other effects, such as issuance costs, personal costs, and bankruptcy costs, are abstracted away\n\nQuestion: what if we now relax Assumptions 1 and 2?"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#projects-based-cost-of-capital-1",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#projects-based-cost-of-capital-1",
    "title": "Valuation with Leverage",
    "section": "Project’s Based Cost of Capital",
    "text": "Project’s Based Cost of Capital\n\nRelaxing hypothesis about the project’s risk and leverage does have a lot of practical relevance:\n\nSpecific projects often differ from the average investment made by the firm. Therefore, a given project may well be much riskier than the average firm\nFurthermore, acquisitions of real estate or capital equipment are often highly levered, whereas investments in intellectual property are not. Thus, depending on the specific investment being made, the leverage policy used may differ substantially from the firm’s average leverage policy\n\nTo take into account the differences in the project relative to the average firm’s risk and leverage, we will proceed by:\n\nEstimating \\(r_U\\), the unlevered cost of capital, based on a sample of comparable projects;\n(Re)lever the result based on the specific leverage policy adopted"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-unlevered-cost-of-capital",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-unlevered-cost-of-capital",
    "title": "Valuation with Leverage",
    "section": "The (un)levered cost of capital",
    "text": "The (un)levered cost of capital\n\nHow can we estimate the cost of capital for a project based on a sample of comparable firms? If the comparable firms are 100% equity, their cost of equity, (\\(\\small r_E\\)) can be used to assess our project’s \\(\\small r_E\\) - all in all, the firm’s underlying business risk is fully reflect in the cost of equity\nThe situation is a bit more complicated if the comparable firms have debt. In this case, the cash flows generated by the firm’s assets are used to pay both debt and equity holders\nConsequently, the returns of the firm’s equity (which are being measured using the \\(\\beta\\) from its equity returns) alone are not representative of the underlying assets risk!\n\nIn fact, because of the firm’s leverage, the equity will often be much riskier\nThus, the cost of equity of a sample of levered firm will not be a good estimate of the cost of equity our project!\n\n\n\n\\(\\rightarrow\\) Whenever we are evaluating a project’s cost of capital based on a sample of firm comparables that have debt, we cannot directly use the cost of equity!"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-unlevered-cost-of-capital-1",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-unlevered-cost-of-capital-1",
    "title": "Valuation with Leverage",
    "section": "The (un)levered cost of capital",
    "text": "The (un)levered cost of capital\n\n\n\n\n\n\nRecall that a firm’s asset cost of capital or unlevered cost of capital (\\(\\small r_U\\)) is the expected return required by the firm’s investors to hold the firm’s underlying assets, and is a weighted average of the firm’s equity and debt costs of capital:\n\n\n\\[\\small r_U = \\frac{E}{E+D}\\times r_E + \\frac{D}{E+D} \\times r_D\\]"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-unlevered-cost-of-capital-continued",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-unlevered-cost-of-capital-continued",
    "title": "Valuation with Leverage",
    "section": "The (un)levered cost of capital, continued",
    "text": "The (un)levered cost of capital, continued\n\nWhy we should unlever the return on equity1? Note that whenever we measure the cost of equity, we are measuring the expected returns based on the equity risk, which has some implications if a given firm has debt:\n\nBecause debt payments are given, equityholders are referred to as residual claimants - they’ll receive their compensation only after the debtholders receive their payments\nAs such, if a given firm has debt in its financing structure, this makes the equity to be riskier - all in all, an equityholder may not receive anything after paying out debtholders!\n\nBut if you are evaluating a project based on a comparable firm that has debt, you want to consider only the risk of the underlying business, but not the risk due to financial leverage!\nAs a consequence, unlevering the required return makes the comparison to be relative to the investments of a company, regardless of the financing structure!\n\nA similar rationale can be applied to the equity \\(\\small \\beta\\) - See (Berk and DeMarzo 2023) and the Appendix for a detailed discussion."
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#step-1-projects-based-cost-of-capital",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#step-1-projects-based-cost-of-capital",
    "title": "Valuation with Leverage",
    "section": "Step 1: Project’s Based Cost of Capital",
    "text": "Step 1: Project’s Based Cost of Capital\n\nThe first step involves estimating \\(r_U\\) not based on the firm’s unlevered cost of capital, but rather a set of comparable projects that share similar risks. Suppose that our project relates to a new plastics manufacturing division that faces different market risks than the firm’s main packaging business:\n\n\n\n\n\n\n\n\n\n\n\nFirm\nEquity Cost of Capital\nDebt Cost of Capital\nD/(D+E)\n\n\n\n\n1\n12%\n6%\n40%\n\n\n2\n10.7%\n5.5%\n25%\n\n\n\n\nWe can estimate \\(r_U\\) for the plastics division by looking at other single-division plastics firms that have similar business risks. For example, suppose two firms are comparable to the plastics division and have the following characteristics:"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#step-1-projects-based-cost-of-capital-1",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#step-1-projects-based-cost-of-capital-1",
    "title": "Valuation with Leverage",
    "section": "Step 1: Project’s Based Cost of Capital",
    "text": "Step 1: Project’s Based Cost of Capital\n\n\n\n\n\n\n\n\n\n\nFirm\nEquity Cost of Capital\nDebt Cost of Capital\nD/(D+E)\n\n\n\n\n1\n12%\n6%\n40%\n\n\n2\n10.7%\n5.5%\n25%\n\n\n\n\nBased on this, we calculate each firm’s \\(r_U\\) and get the average:\n\\(\\small r_U^1= 0.6 \\times 12\\% + 0.4 \\times 6\\% = 9.6\\%\\)\n\\(\\small r_U^2= 0.75 \\times 10.7\\% + 0.25 \\times 5.5\\% = 9.4\\%\\)\nIn this way, a reasonable estimate for \\(r_U\\) of our project is around \\(\\small 9.5\\%\\). If we wanted to use the APV approach to calculate the value of the project, we could use this estimate\nIf we wanted to use the WACC or FTE methods, however, we still need to estimate \\(r_E\\), which will depend on the incremental debt the firm will take on as a result of the project"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#step-2-projects-based-cost-of-capital",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#step-2-projects-based-cost-of-capital",
    "title": "Valuation with Leverage",
    "section": "Step 2: Project’s Based Cost of Capital",
    "text": "Step 2: Project’s Based Cost of Capital\n\nRecall that our expression for the unlevered cost of capital, \\(r_U\\), was:\n\n\n\\[\n\\small r_U= \\dfrac{E}{E+D}\\times r_E + \\dfrac{D}{E+D}\\times r_D\n\\]\n\nRearranging terms, we have:\n\n\n\n\\[\n\\small  \\dfrac{E}{E+D}\\times r_E = r_U - \\dfrac{D}{E+D}\\times r_D \\\\\n\\small r_E=\\dfrac{E+D}{E}\\times r_U - \\dfrac{D}{E}\\times r_D \\\\\n\\small r_E = r_U+ \\dfrac{D}{E}\\times r_U - \\dfrac{D}{E}\\times r_D \\\\\n\\small r_E = r_U+ \\dfrac{D}{E}( r_U -  r_D)\n\\]"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#step-2-projects-based-cost-of-capital-1",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#step-2-projects-based-cost-of-capital-1",
    "title": "Valuation with Leverage",
    "section": "Step 2: Project’s Based Cost of Capital",
    "text": "Step 2: Project’s Based Cost of Capital\n\nOur last equation shows us that:\n\n\n\\[\n\\small r_E = r_U+ \\dfrac{D}{E}( r_U -  r_D)\n\\]\n\nIn words, the project’s cost of capital depends on:\n\nThe unlevered cost of capital, \\(r_U\\)\nThe specific debt-to-equity ratio that the project will use\n\nSuppose that the firm will use a debt-to-equity ratio of 1, and the cost of debt remains at \\(\\small6\\%\\). Then, we can calculate \\(\\small r_E\\) as:\n\n\n\n\\[\n\\small r_E = 9.5\\% + \\dfrac{0.5}{0.5}(9.5\\% - 6\\%) = 13\\%\n\\]"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#step-2-projects-based-cost-of-capital-2",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#step-2-projects-based-cost-of-capital-2",
    "title": "Valuation with Leverage",
    "section": "Step 2: Project’s Based Cost of Capital",
    "text": "Step 2: Project’s Based Cost of Capital\n\nWe can finally plug the estimate of \\(\\small r_E\\) to estimate the project’s WACC, assuming that the tax-rate if 25%:\n\n\n\\[\n\\small r_{\\text{WACC}}=50\\% \\times 13\\% + 50\\% \\times 6\\% \\times(1-25\\%)= 8.75\\%\n\\]\n\nBased on these estimates, Avco should use a WACC of \\(\\small8.75\\%\\) for the plastics division, compared to the WACC of \\(\\small7.25\\%\\) that has been previously estimated based on the firm’s overall\nIntuition: because the project had a higher unlevered risk (\\(\\small9.5\\%\\) versus \\(\\small8\\%\\)), after applying the adopted leverage policy, will also have a higher cost of capital"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#common-misconception-i-determining-the-incremental-leverage-of-a-project",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#common-misconception-i-determining-the-incremental-leverage-of-a-project",
    "title": "Valuation with Leverage",
    "section": "Common Misconception I: determining the incremental leverage of a project",
    "text": "Common Misconception I: determining the incremental leverage of a project\n\nTo determine the equity or weighted average cost of capital for a project, we need to know the amount of debt to associate with the project\nHow to determine the correct \\(\\small D/(D+E)\\) ratio to use in our estimations?\n\n\nSuppose a project involves buying a new warehouse, and the purchase of the warehouse is financed with a mortgage for \\(\\small90\\%\\) of its value\nHowever, if the firm has an overall policy to maintain a \\(\\small40\\%\\) debt-to-value ratio, it will reduce debt elsewhere in the firm once the warehouse is purchased in an effort to maintain that ratio\n\n\n\\(\\rightarrow\\) In that case, the appropriate debt-to-value ratio to use when evaluating the warehouse project is \\(\\small40\\%\\), not \\(\\small90\\%\\)! For capital budgeting purposes, the project’s financing is the change in the firm’s total debt (net of cash) with the project versus without the project!"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#common-misconception-ii-relevering-the-wacc",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#common-misconception-ii-relevering-the-wacc",
    "title": "Valuation with Leverage",
    "section": "Common Misconception II: (re)levering the WACC",
    "text": "Common Misconception II: (re)levering the WACC\n\nSuppose that a firm has a debt-to-value ratio of \\(\\small25\\%\\), a debt cost of capital of \\(\\small5.33\\%\\), an equity cost of capital of \\(\\small12\\%\\), and a tax rate of \\(\\small25\\%\\). The current WACC is:\n\n\n\\[\n\\small r_{\\text{WACC}}=0.75 \\times 12\\% + 0.25\\times 5.33\\% \\times (1- 25\\%) = 10\\%\n\\]\n\nWhat happens to WACC if the firm increases its debt-to-value ratio to \\(\\small50\\%\\)? It is tempting to do:\n\n\n\n\\[\n\\small  r_{\\text{WACC}}=0.5 \\times 12\\% + 0.5\\times 5.33\\% \\times (1- 25\\%) = 8\\%\n\\]\n\nNote, however, that this is wrong, because we’re keeping \\(\\small r_E\\) and \\(\\small r_D\\) fixed! Since these are the cost of equity and debt, we should expect these to increase with leverage, as the risk of both shareholders and debt holders increase!"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#common-misconception-relevering-the-wacc",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#common-misconception-relevering-the-wacc",
    "title": "Valuation with Leverage",
    "section": "Common misconception: (re)levering the WACC",
    "text": "Common misconception: (re)levering the WACC\n\nWhen the firm increases leverage, the risk of its equity and debt will increase, increasing \\(\\small r_E\\) and \\(\\small r_D\\)! To compute the new WACC correctly, we must first determine the firm’s unlevered cost of capital:\n\n\n\\[\n\\small\nr_U = 0.75 \\times 12\\% + 0.25\\times 5.33\\% = 10.33\\%\n\\]\n\nIf \\(\\small r_D\\) has risen to \\(\\small 6.67\\%\\) with the change in leverage, then:\n\n\n\n\\[\n\\small\nr_E = 10.33\\% + \\dfrac{0.5}{0.5}\\times(10.33\\%-6.67\\%)=14\\%\n\\]\n\nFinally, the correct new WACC is:\n\n\n\n\\[\n\\small\nr_{\\text{WACC}}=0.5 \\times 14\\% + 0.5\\times 6.67\\% \\times (1- 25\\%) = 9.5\\%\n\\]"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#industry-betas-for-estimating-a-projects-cost-of-capital",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#industry-betas-for-estimating-a-projects-cost-of-capital",
    "title": "Valuation with Leverage",
    "section": "Industry betas for estimating a project’s cost of capital",
    "text": "Industry betas for estimating a project’s cost of capital\n\nUsing a single comparable firm is often not a good idea, as there might be a lot of noise in the estimation of the results\nHowever, it is possible to combine estimates of asset betas for multiple firms in the same industry to reduce our estimation error and improve the estimation accuracy:\n\n\nFor example, instead of using only one comparable firm to find the unlevered cost of capital, we may use the average (or median) of several firms that are thought of as comparable peers\nAs you imagine, unlevered betas within an industry are much more stable than the pure equity betas: large differences in the firms’ equity betas are mainly due to differences in leverage, whereas the firms’ asset betas are much more similar, suggesting that the underlying businesses in this industry have similar market risk\n\n\n\\(\\rightarrow\\) See Damodaran (here) industry betas for the U.S."
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#comparing-the-main-methods-for-valuing-levered-firms-and-projects",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#comparing-the-main-methods-for-valuing-levered-firms-and-projects",
    "title": "Valuation with Leverage",
    "section": "Comparing the main methods for valuing levered firms and projects",
    "text": "Comparing the main methods for valuing levered firms and projects\n\nThere are three that we could use value a project’s cash flows when there is debt financing:\n\nThe Weigthed Average Cost of Capital (WACC) method\nThe Adjusted Present Value (APV) method\nThe Flow-to-Equity (FTE) method\n\nStarting from the same assumptions, all methods yield the same results. However:\n\n\nWACC is the method that is the easiest to use when the firm will maintain a fixed debt-to-value ratio over the life of the investment\nFor alternative leverage policies, the APV method is usually the most straightforward approach\nThe FTE method is typically used only in complicated settings for which the values of other securities in the firm’s capital structure or the interest tax shield are difficult to determine"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#other-effects-of-financing",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#other-effects-of-financing",
    "title": "Valuation with Leverage",
    "section": "Other Effects of Financing",
    "text": "Other Effects of Financing\n\nPreviously, we assumed that: Corporate taxes were the only imperfection. In words, we assumed that the main effect of leverage on valuation is due to the corporate tax shield. Other effects, such as issuance costs, personal costs, and bankruptcy costs, were abstracted away\nThe three methods that we saw determine the value of an investment incorporating the tax shields associated with leverage. What if we have more than one market imperfection?:\n\n\nIssuing Costs\nSecurity Mispricing\nFinancial Distress and Bankruptcy Costs\n\n\nIn order to for these three potential imperfections, we generally use the APV method, since it is the most flexible way of adjusting the estimates that stem from the use of leverage, although we can also adjust the other methods with some underlying assumptions"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#supplementary-reading",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#supplementary-reading",
    "title": "Valuation with Leverage",
    "section": "Supplementary Reading",
    "text": "Supplementary Reading\n\nSee Note on Cash Flow Valuation Methods: Comparison of WACC, FTE, CCF and APV Approaches for a detailed discussion on the valuation methods\nSee (Berk and DeMarzo 2023), Chapters 14 to 17, to understand how to account our valuation for the most common market imperfections, such as taxes, financial distress costs, and asymmetric information\n\n\n\\(\\rightarrow\\) All contents are available on eClass®."
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#levered-firms-when-debt-to-equity-is-not-constant",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#levered-firms-when-debt-to-equity-is-not-constant",
    "title": "Valuation with Leverage",
    "section": "Levered Firms when Debt-to-Equity is not constant",
    "text": "Levered Firms when Debt-to-Equity is not constant\n\nQuestion: how to ensure that the Debt-to-Equity will remain constant when implementing new projects?\n\nThus far, we have simply assumed the firm adopted a policy of keeping its debt-equity ratio constant\nNevertheless, keeping the Debt-to-Equity ratio constant has implications for how the firm’s total debt will change with new investment - we’ll refer to this as Debt-Capacity"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#debt-capacity",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#debt-capacity",
    "title": "Valuation with Leverage",
    "section": "Debt Capacity",
    "text": "Debt Capacity\n\nDebt Capacity refers to the the amount of debt that a firm needs to raise in order to keep its debt-to-equity ratio constant. Why is that important?\n\n\nWACC is a weighted average based on the proportions of Equity and Debt\nBecause of that, any changes in these proportions affect the WACC\nTherefore, after calculating \\(r_\\text{WACC}\\), to ensure that you can use it over the years, you need to ensure that the firm maintains the same debt-to-equity ratio\n\n\nYou can find the the debt capacity for a given period \\(t\\) by:\n\n\n\\[\nD_t=d\\times V^L_t\n\\]\nWhere \\(d\\) is the debt-to-value ratio, which is the proportion of (market-value) debt over the market value of the firm or project (debt + equity)"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#debt-capacity-1",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#debt-capacity-1",
    "title": "Valuation with Leverage",
    "section": "Debt Capacity",
    "text": "Debt Capacity\n\nYou can estimate the value of the levered firm, \\(V^L_t\\), over each period \\(t\\) by summing up the discounted stream of cash-flows remaining:\n\n\n\\[\nV^L_t=\\dfrac{FCF_{t+1}+V^L_{t+1}}{(1+r_{\\text{WACC}})}\n\\] where \\(V^L_{t+1}\\) refers to the continuation value – see the accompanying Excel spreadsheet for a comprehensive example\n\nWhile the WACC does not require you to know exactly the debt capacity of the project, this component is essential when calculating the value of the project using other methods, such as the APV, as we’ll see in the next set of slides"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-apv-adjusted-present-value-apv-method",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-apv-adjusted-present-value-apv-method",
    "title": "Valuation with Leverage",
    "section": "The APV Adjusted Present Value (APV) method",
    "text": "The APV Adjusted Present Value (APV) method\n\nOur previous method estimated the value of a levered firm, \\(\\small V^L\\), by considering the interest-tax shields into the cost of capital calculation, \\(r_{\\text{WACC}}\\)\nWhat if we wanted to gauge the impact of the interest tax-shields separately from the actual value of the unlevered project?\nThe Adjusted Present Value (APV) method does it so by calculating two components: \\(V^U\\), which is the present value of the unlevered project (i.e, no debt) and the present value of the interest tax-shields stemming from the financing decision:\n\n\n\\[\n\\small\nV^{L}_{APV}=V^U+ PV(\\text{Interest Tax Shield})\n\\]\n\nThe APV method incorporates the value of the interest tax shield directly, rather than by adjusting the discount rate as in the WACC method"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-apv-method-in-practice",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-apv-method-in-practice",
    "title": "Valuation with Leverage",
    "section": "The APV method in practice",
    "text": "The APV method in practice\n\nThe first step in the APV method is to calculate the value of these free cash flows using the project’s cost of capital if it were financed without leverage \\(\\rightarrow V^U\\)\nWhat is the project’s unlevered cost of capital?\n\nBecause the RFX project has similar risk to Avco’s other investments, its unlevered cost of capital is the same as for the firm as a whole\nWe can calculate the unlevered cost of capital using Avco’s pre-tax WACC, the average return the firm’s investors expect to earn\n\n\n\n\\[\nr_U = \\dfrac{E}{E+D}\\times r_e + \\dfrac{D}{E+D}\\times r_d=\\text{Pre-Tax WACC}\n\\]\n\nNote that this formula is the same as of the \\(r_{WACC}\\), but we’re not including the tax-shield effect, \\((1-\\tau)\\), into account!"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued",
    "title": "Valuation with Leverage",
    "section": "The APV method in practice (continued)",
    "text": "The APV method in practice (continued)\n\nTo understand why the firm’s unlevered cost of capital equals its pre-tax WACC, note that the pre-tax WACC represents investors’ required return for holding the entire business (equity and debt)\nSo long as the firm’s leverage choice does not change the overall risk of the firm, the pre-tax WACC must be the same whether the firm is levered or unlevered!\nApplying it to our case, we have:\n\n\n\\[\nr_U = 0.5\\times 10\\% + 0.5\\times 6\\% = 8\\%\n\\]\n\nWith that, our estimate for \\(V^U\\) is:\n\n\n\n\\[\n\\small\nV^{U}=\\sum_{T=1}^{T=4}\\dfrac{21}{(1+8\\%)^t}=69.55\n\\]"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued-1",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued-1",
    "title": "Valuation with Leverage",
    "section": "The APV method in practice (continued)",
    "text": "The APV method in practice (continued)\n\nThe value of the unlevered project, \\(V^U\\), does not include the value of the tax shield provided by the interest payments on debt\nKnowing the project’s debt capacity for the future, we can explicitly calculate the the present value of the interest tax-shields. First, determine the amount of interest expenses at each period \\(t\\):\n\n\n\\[\n\\small \\text{Interest Expenses}_t= r_D\\times D_{t-1}\n\\]\n\nAfter that, assuming a corporate tax rate of \\(\\tau\\), the interest tax-shield is just:\n\n\n\n\\[\n\\small \\text{Interest Tax-Shield}= \\text{Interest Expenses}_t\\times \\tau\n\\]"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued-2",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued-2",
    "title": "Valuation with Leverage",
    "section": "The APV method in practice (continued)",
    "text": "The APV method in practice (continued)\n\n\n\n\n\n\nTo compute the present value of the interest tax shield, we need to determine the appropriate cost of capital. Which rate shall we use? Note that:\n\nIf the project does well, its value will be higher \\(\\rightarrow\\) more debt \\(\\rightarrow\\) more interest tax-shield\nIf the project performs poorly, its value will be lower \\(\\rightarrow\\) less debt \\(\\rightarrow\\) less interest tax-shield\n\nBecause the interest tax-shield fluctuates with the risk of the project, in this specific case should discount it using the same rate1, \\(r_U\\)!\n\nThe decision on which rate to use is heavily dependent upon the project’s characteristics - see (Berk and DeMarzo 2023) for a detailed discussion on the most common cases."
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued-3",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-apv-method-in-practice-continued-3",
    "title": "Valuation with Leverage",
    "section": "The APV method in practice (continued)",
    "text": "The APV method in practice (continued)\n\nUsing \\(\\small r_U=8\\%\\) and evaluating the present value of the interest tax-shield, we have:\n\n\n\\[\n\\small PV(\\text{Interest Tax-Shield})=\\dfrac{0.53}{(1+8\\%)}+\\dfrac{0.41}{(1+8\\%)^2}+\\dfrac{0.28}{(1+8\\%)^3}+\\dfrac{0.15}{(1+8\\%)^4}=1.18\n\\]\n\nNow, to determine the value of the levered firm, \\(V^L\\), we add the value of the interest tax shield to the unlevered value of the project:\n\n\n\n\\[\n\\small V^L=V^U+PV(\\text{Interest Tax-Shield})= 69.55+1.18=70.73\n\\]\n\nWhich is exactly the same value that we’ve found using the WACC method!"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#general-thoughts-on-the-apv-method",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#general-thoughts-on-the-apv-method",
    "title": "Valuation with Leverage",
    "section": "General thoughts on the APV method",
    "text": "General thoughts on the APV method\n\nIn the APV method, we separately calculated the value of the unlevered firm and the value stemming from the tax-shields\nIn this case, the APV method is more complicated than the WACC method because we must compute two separate valuations\nNotwithstanding, the APV method has some advantages:\n\nIt can be easier to apply than the WACC method when the firm does not maintain a constant debt-equity ratio\nIt also provides managers with an explicit valuation of the tax shield itself\n\nThere could be cases where the value of the project heavily depends on the tax-shield, and not on the operating gains themselves \\(\\rightarrow\\) if taxes change, the value of the project may be severely affected!"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#exercise-apv",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#exercise-apv",
    "title": "Valuation with Leverage",
    "section": "Exercise: APV",
    "text": "Exercise: APV\nConsider again Avco’s acquisition from previous examples. The acquisition will contribute \\(\\small \\$4.25\\) million in free cash flows the first year, which will grow by \\(\\small 3\\%\\) per year thereafter. The acquisition cost of \\(\\small \\$80\\) million will be financed with \\(\\small \\$50\\) million in new debt initially. Compute the value of the acquisition using the APV method, assuming Avco will maintain a constant debt-equity ratio for the acquisition.\n\\(\\rightarrow\\) Taken from (Berk and DeMarzo 2023), p. 689\n\nSolution Rationale: proceed in the following steps to compute the value using the APV method:\n\nCalculate \\(V^U\\) - the value of the unlevered project\nCalculate the present value of the tax-shields\nSum them up\n\n\nNote that, because the project will grow at a \\(\\small3\\%\\) rate, debt capacity will also grow at the same rate. Therefore, the growth-rate of the interest tax-shield is also \\(\\small3\\%\\)"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#exercise-apv-1",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#exercise-apv-1",
    "title": "Valuation with Leverage",
    "section": "Exercise: APV",
    "text": "Exercise: APV\n\nCalculating \\(V^U\\): this is just the value of a growing perpetuity for the unlevered cash-flows:\n\n\n\\[\n\\small V^U= \\dfrac{FCFC}{r-g}=\\dfrac{4.25}{8\\%-3\\%}= 85\n\\]\n\nNow, if the firm will start with \\(\\small\\$50\\) million in debt, interest expenses are \\(\\small 50\\times6\\%=3\\) million. The present value of the interest tax-shield is:\n\n\n\n\\[\n\\small \\dfrac{25\\%\\times 3}{8\\%-3\\%}=\\dfrac{0.75}{5\\%}=15\n\\]\n\nTherefore, \\(\\small V^L=V^U+PV(\\text{Tax-Shield})=85+15=100\\)"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-flow-to-equity-fte-method",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-flow-to-equity-fte-method",
    "title": "Valuation with Leverage",
    "section": "The Flow-to-Equity (FTE) Method",
    "text": "The Flow-to-Equity (FTE) Method\n\nIn the WACC and APV methods, we value a project based on its free cash flow, which is computed ignoring interest and debt payments\nWhat if we take these into consideration and value the cash flows that pertain only to shareholders? The Flow to Equity method does this by:\n\nExplicitly calculating the free cash flow available to equity holders after taking into account all payments to and from debt holders\nThe cashflow to equity holders are then discounted using the equity cost of capital\n\nDespite this difference in implementation, the FTE method produces the same assessment of the project’s value as the WACC or APV methods"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-flow-to-equity-fte-method-continued",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#the-flow-to-equity-fte-method-continued",
    "title": "Valuation with Leverage",
    "section": "The Flow-to-Equity (FTE) Method, continued",
    "text": "The Flow-to-Equity (FTE) Method, continued\n\nIn order to implement the FTE method, we need to compute the Free Cash Flow to Equity (FCFE), which shows the available proceeds for the shareholders of the firm after paying out all costs, considering all working capital and CAPEX investments, deducting interest expenses and considering the firm’s net borrowing activity:\n\n\n\\[\n\\small FCFE = FCF - (1-\\tau)\\times (\\text{Interest Expenses})\\pm \\text{Net Borrowing}\n\\]\n\nCompared to our previous case, there will be two differences:\n\nFirst, we deduct interest expenses before calculating taxes\nWe add the proceeds from the firm’s net borrowing activity.\n\nThese will be positive when the firm increases its net debt\nOn the other hand, these will be negative when the firm reduces its net debt"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#previous-estimation-of-free-cash-flow-for-wacc-and-apv",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#previous-estimation-of-free-cash-flow-for-wacc-and-apv",
    "title": "Valuation with Leverage",
    "section": "Previous Estimation of Free Cash Flow (for WACC and APV)",
    "text": "Previous Estimation of Free Cash Flow (for WACC and APV)"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#estimation-of-free-cash-flow-to-equity",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#estimation-of-free-cash-flow-to-equity",
    "title": "Valuation with Leverage",
    "section": "Estimation of Free Cash Flow to Equity",
    "text": "Estimation of Free Cash Flow to Equity\n\n\n\n\n\n\\(\\rightarrow\\) See accompaining Excel document for the calculations"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#estimation-of-free-cash-flow-to-equity-explanation",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#estimation-of-free-cash-flow-to-equity-explanation",
    "title": "Valuation with Leverage",
    "section": "Estimation of Free Cash Flow to Equity, explanation",
    "text": "Estimation of Free Cash Flow to Equity, explanation\n\nFrom the previous table, you can see that we have made two major changes relative to our regular Free Cash Flow estimation:\n\nWe explicitly included interest expenses - as calculated in our previous class - before calculating taxes. As a consequence, our taxable income was lower, and so does the tax expense for each year\nBecause we’re measuring the cash flows to equity holders and not all the claimants of the firm, we need to include all dynamics in debt levels (inclusions or deductions). We can do it by considering changes in debt levels from one period to the other:\n\n\n\n\\[\n\\small \\text{Net Borrowing}_t= Debt_t-Debt_{t-1}\n\\]\n\nAs we did in our previous class when calculating the amount of necessary Debt that the firm needed in order to keep the debt-to-equity ratio constant, we can use the calculated debt capacity to calculate the increases/decreases in net debt for each period"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#valuing-equity-cash-flows",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#valuing-equity-cash-flows",
    "title": "Valuation with Leverage",
    "section": "Valuing Equity Cash Flows",
    "text": "Valuing Equity Cash Flows\n\nYou now have the cash flows that pertain exclusively to the shareholders of the firm. Now what?\nThe project’s free cash flow to equity shows the expected amount of additional cash the firm will have available to pay dividends (or conduct share repurchases) each year\nBecause these cash flows represent payments to equity holders, they should be discounted at the project’s equity cost of capital.\nGiven that the risk and leverage of the RFX project are the same as for Avco overall, we can use Avco’s equity cost of capital of (\\(r_e=10\\%\\)):\n\n\n\\[\n\\small NPV(FCFE)=6.37 + \\dfrac{11.47}{1.10}+ \\dfrac{11.25}{1.10^2} +\\dfrac{11.02}{1.10^3}+ \\dfrac{10.77}{1.10^4}=41.73\n\\]\n… which yields exactly the same NPV as of the previous methods!"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#overall-thoughts-on-the-fte-method",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#overall-thoughts-on-the-fte-method",
    "title": "Valuation with Leverage",
    "section": "Overall thoughts on the FTE method",
    "text": "Overall thoughts on the FTE method\n\nSteps to compute the value using the FTE method:\n\nCompute the Free Cash Flow to Equity by directly including interest expenses and net debt\nCalculate the project’s cost of equity, \\(r_e\\)\nDiscount the cash flows using \\(r_e\\)\n\nApplying the FTE method was simplified in our example because the project’s risk and leverage matched the firm’s, and the firm’s equity cost of capital was expected to remain constant.\nJust as with the WACC, however, this assumption is reasonable only if the firm maintains a constant debt-equity ratio. If the debt-equity ratio changes over time, the risk of equity—and, therefore, its cost of capital—will change as well"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#overall-thoughts-on-the-fte-method-continued",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#overall-thoughts-on-the-fte-method-continued",
    "title": "Valuation with Leverage",
    "section": "Overall thoughts on the FTE method, continued",
    "text": "Overall thoughts on the FTE method, continued\n\nLimitations: the FTE method carries the same limitations as of the APV method: we need to compute the project’s debt capacity to determine interest and net borrowing before we can make the capital budgeting decision. Because of that, the WACC method is easier to apply\n\n\nBenefits: whenever we have a complex capital structure, using the FTE has some advantages over the other two methods:\n\nThe APV and WACC methods estimate the the firm’s enterprise value, and need a separate valuation of the other components to separate the value of equity\nIn constrast, the FTE method can be used to estimate the equity value directly\nFinally, by emphasizing a project’s implications for the firm’s payouts to equity, the FTE method may be viewed as a more transparent method for discussing a project’s benefit to shareholders—a managerial concern."
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#other-effects-of-financing-issuing-costs",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#other-effects-of-financing-issuing-costs",
    "title": "Valuation with Leverage",
    "section": "Other Effects of Financing: Issuing Costs",
    "text": "Other Effects of Financing: Issuing Costs\n\nWhen a firm takes out a loan or raises capital by issuing securities, the banks that provide the loan or underwrite the sale of the securities charge fees\nThe fees associated with the financing of the project are a cost that should be included as part of the project’s required investment, reducing the NPV of the project\n\n\n\\[\n\\small NPV = V^L - \\text{Investment} - \\text{Issuance Costs}\n\\]\n\nThis calculation presumes the cash flows generated by the project will be paid out. If instead they will be reinvested in a new project, and thereby save future issuance costs, the present value of these savings should also be incorporated and will offset the current issuance costs."
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#other-effects-of-financing-security-mispricing",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#other-effects-of-financing-security-mispricing",
    "title": "Valuation with Leverage",
    "section": "Other Effects of Financing: Security Mispricing",
    "text": "Other Effects of Financing: Security Mispricing\n\nWith perfect capital markets, all securities are fairly priced and issuing securities is a zero-NPV transaction. However, there are situations where the pricing is more (or less) relative to the true value!\nEquity mispricing: if management believes that the equity will sell at a price that is less than its true value, this mispricing is a cost of the project for the existing shareholders. It can be deducted from the project NPV in addition to other issuance costs\nLoan mispricing: a firm may pay an interest rate that is too high if news that would improve its credit rating has not yet become public\n\nWith the WACC, we could adjust it using the higher interest rate\nWith the APV, we must add to the value of the project the NPV of the loan cash flows when evaluated at the “correct” rate that corresponds to their actual risk"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#other-effects-of-financing-financial-distress",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#other-effects-of-financing-financial-distress",
    "title": "Valuation with Leverage",
    "section": "Other Effects of Financing: Financial Distress",
    "text": "Other Effects of Financing: Financial Distress\n\nOne consequence of debt financing is the possibility of financial distress and agency costs:\n\nWhen the debt level - and, therefore, the probability of financial distress - is high, the expected free cash flow will be reduced by the expected costs associated with financial distress and agency problems\nFinancial distress costs therefore tend to increase the sensitivity of the firm’s value to market risk, further raising the cost of capital for highly levered firms\n\nHow to adjust for potential financial distress and agency costs?\n\n\nOne approach is to adjust our free cash flow estimates to account for the costs, and increased risk, resulting from financial distress\nAn alternative method is to first value the project ignoring these costs, and then add the present value of the incremental cash flows associated with financial distress and agency problems separately"
  },
  {
    "objectID": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#references",
    "href": "valuation/coursework/Lecture 4 - Valuation with Leverage/index.html#references",
    "title": "Valuation with Leverage",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments\n\n\n\n\nBerk, J., and P. DeMarzo. 2023. Corporate Finance, Global Edition. Pearson. https://books.google.com.br/books?id=m78oEAAAQBAJ."
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#disclaimer",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#disclaimer",
    "title": "Equity Valuation and Simulation",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\n\n\n\n\nDisclaimer\n\n\nThe information presented in this lecture is for educational and informational purposes only and should not be construed as investment advice. Nothing discussed constitutes a recommendation to buy, sell, or hold any financial instrument or security. Investment decisions should be made based on individual research and consultation with a qualified financial professional. The presenter assumes no responsibility for any financial decisions made based on this content.\nAll code used in this lecture is publicly available and is also shared on my GitHub page. Participants are encouraged to review, modify, and use the code for their own learning and research purposes. However, no guarantees are made regarding the accuracy, completeness, or suitability of the code for any specific application.\nFor any questions or concerns, please feel free to reach out via email at lucas.macoris@fgv.br"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#dealing-with-uncertainty-in-valuation-models",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#dealing-with-uncertainty-in-valuation-models",
    "title": "Equity Valuation and Simulation",
    "section": "Dealing with Uncertainty in Valuation Models",
    "text": "Dealing with Uncertainty in Valuation Models\n\nTraditional valuation methods (e.g., Discounted Cash Flow) rely on single-point estimates:\n\nWe assume a given level of revenue growth…\nWe also fix the appropriate discount rates, \\(r\\), over time…\nFinally, we come up with assumptions regarding the firm’s Terminal Value (i.e, the perpetuity value)!\n\nNote, however, that these estimates are subject to significant uncertainty:\n\nMarket conditions\nCompetition\nRegulation\nMacroeconomic and exogenous shocks, such as COVID-19\n\n\n\nQuestion: how can get take into account the role of uncertainty in valuation models?"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#dealing-with-uncertainty-in-valuation-models-continued",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#dealing-with-uncertainty-in-valuation-models-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Dealing with Uncertainty in Valuation Models, continued",
    "text": "Dealing with Uncertainty in Valuation Models, continued\n\nSuppose we have the following Discounted Cash Flow estimate:\n\n\\[\n\\text{Firm Value} = \\sum_{t=1}^{T} \\frac{FCF_t}{(1 + r)^t}\n\\]\n\n\\(FCF_t\\) depends on a variety of firm-level factors, such as sales growth, gross margins, taxes, among others\nSimilarly, \\(r\\), the discount rate, if modeled using the CAPM, depends upon factors such as the sensitivity to market risk (\\(\\beta\\)), the risk-free rate, \\(r_f\\), and the market risk premium\n\n\nChanges in those variables can severely affect the outcomes of our valuation: for example, if the realized \\(FCF\\)’s are lower than what we assumed, we might be overestimating the firm’s value!"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#sensitivity-analysis",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#sensitivity-analysis",
    "title": "Equity Valuation and Simulation",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\n\nA first way in which we can take uncertainty into account is through Sensitivity Analysis:\n\n\n\n\n\n\n\nDefinition\n\n\n\nSensitivity analysis tests how changes in a single input affect the valuation output, while all other variables are held constant. It is useful to identify key value drivers and assess their impact.\n\n\n\n\n\nFix the main drivers of your outcome variable (e.g, growth rate of COGS, growth rate of Sales, Cost of Capital, etc)\nFor each driver, create scenarios in where you vary the input of interest within a given range\nFor that specific scenario, collect the new outcome variable calculated when everything is fixed, but the specific driver has changed\nRepeat this across all scenarios and drivers"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#sensitivity-analysis-example",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#sensitivity-analysis-example",
    "title": "Equity Valuation and Simulation",
    "section": "Sensitivity Analysis, Example",
    "text": "Sensitivity Analysis, Example\n\nTo illustrate the use of Sensitivity Analysis, we can vary \\(r\\), the discount rate, and observe the change in the firm’s value for each distinct \\(r\\):\n\n\n\n\nDiscount Rate (\\(r\\))\nFirm Value (in millions)\n\n\n\n\n\\(6\\%\\)\n\\(\\$125\\)\n\n\n\\(8\\%\\)\n\\(\\$110\\)\n\n\n\\(10\\%\\)\n\\(\\$98\\)\n\n\n\nKey Points on Sensitivity Analysis\n\nOn the one hand, it helps to answer how sensitive is the valuation to some of the assumptions that were used in the model\nOn the other hand, there is a clear limitation: the fact that we are varying one variable at a time doesn’t capture interactions between variables!"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#scenario-analysis",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#scenario-analysis",
    "title": "Equity Valuation and Simulation",
    "section": "Scenario Analysis",
    "text": "Scenario Analysis\n\nAs described before, a clear limitation of Sensitivity Analysis is the fact that only one input is evaluated at a time\nWhat if we wanted to see the combined effect of multiple variables changing at the same time?\n\n\n\n\n\n\n\nDefinition\n\n\n\nA Scenario Analysis evaluates the effect of multiple variables changing at the same time\n\n\nIt is used to create alternative scenarios (e.g, Best, Base, and Worst Case Scenarios) based on business logic\nIt is more realistic than sensitivity analysis, but still limited to a few discrete outcomes\n\n\nIt is very reasonable to assume that more than one driver is going to change at a time. To do that, we can create a grid of values and simulate changes in inputs at the same time"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#scenario-analysis-continued",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#scenario-analysis-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Scenario Analysis, continued",
    "text": "Scenario Analysis, continued\n\n\n\n\n\n\n\n\n\n\nScenario\nRevenue Growth\nDiscount Rate\nTerminal Value\nFirm Value\n\n\n\n\nBest Case\n\\(8\\%\\)\n\\(6\\%\\)\nHigh\n\\(\\$140M\\)\n\n\nBase Case\n\\(5\\%\\)\n\\(8\\%\\)\nMedium\n\\(\\$110M\\)\n\n\nWorst Case\n\\(2\\%\\)\n\\(10\\%\\)\nLow\n\\(\\$85M\\)\n\n\n\nKey Points on Scenario Analysis\n\nIt highlights a range of possible outcomes under plausible assumptions\nHowever, it still lacks a probability component: we still don’t know how likely each of those scenarios are:\n\n\nBest, Base, and Worst cases are limited in the way that they can produce scenarios because we assume that all variables will go into the same direction\nFurthermore, our realized outcome fallS under a combination of those scenarios"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#from-scenarios-to-simulations",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#from-scenarios-to-simulations",
    "title": "Equity Valuation and Simulation",
    "section": "From Scenarios to Simulations",
    "text": "From Scenarios to Simulations\n\nWhat if we could extend the logit of scenario simulation to \\(N&gt;&gt;3\\) scenarios? To do that, we can use simulation techniques, such as the Monte Carlo simulation:\n\n\n\n\n\n\n\nDefinition:\n\n\nA Monte Carlo Simulation is a computational technique that uses random sampling to model the distribution of outcomes for a given random variable:\n\nWe define the parameters of our simulation as probability distributions (e.g., normal, lognormal).\nGenerate a large number of random scenarios for these variables\nCompute the outcome (e.g., firm value) for each scenario\nAnalyze the distribution of the desired results (e.g., mean, percentiles, risk of loss)\n\n\n\n\n\nWith Monte Carlo simulations, instead of getting a range of potential outcomes for the firm’s value, we actually get an empirical distribution of values!"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#monte-carlo-implementation",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#monte-carlo-implementation",
    "title": "Equity Valuation and Simulation",
    "section": "Monte Carlo Implementation",
    "text": "Monte Carlo Implementation\n\nTo see how we can apply Monte Carlo Simulation to assess the distribution of Firm Value, let’s assume that both the free cash flow and the discount rate are random variables:\n\n\\(FCF_t \\sim \\mathcal{N}(\\mu_t, \\sigma_t^2)\\)\n\\(r \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)\\)\n\nIf we know the distribution of those random variables, we can draw \\(N\\) observations for each random variable and use it to calculate the desired outcome (i.e, the firm’s value). For draw \\(i\\), the estimated value of the firm is simply:\n\n\\[\n\\text{Firm Value}^{(i)} = \\sum_{t=1}^{T} \\frac{FCF_t^{(i)}}{(1 + r^{(i)})^t}\n\\]\n\nRepeat for \\(i = 1,2,..., N\\) simulations\nGet a distribution of firm values, not just a point estimate"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#monte-carlo-implementation-1",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#monte-carlo-implementation-1",
    "title": "Equity Valuation and Simulation",
    "section": "Monte Carlo implementation",
    "text": "Monte Carlo implementation\n\nSuppose we are using the Gordon’s Growth Model, where the value of a firm is given by the infinite stream of dividends:\n\n\\[\nV_t= \\dfrac{D_1}{r-g}\n\\]\nwhere \\(D1\\) s the dividend next year, \\(r\\) is the discount rate, and \\(g\\) is the perpetual growth rate. If we assume the following distributions:\n\nSince \\(D1\\) is the next period’s dividend, and it is fixed to \\(\\$2\\)\n\\(r \\sim \\mathcal{N}(0.08,0.01^2)\\)\n\\(g \\sim \\mathcal{N}(0.03,0.005^2)\\)\n\n\nIn what follows, we will simulate \\(10,000\\) distinct scenarios for draws of \\(r\\) and \\(g\\) and see how the \\(V_t\\) distributes over all potential combinations"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#monte-carlo-implementation-continued",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#monte-carlo-implementation-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Monte Carlo Implementation, continued",
    "text": "Monte Carlo Implementation, continued\n\nCodeOutput\n\n\n\n#Set seed for reproducibility\nset.seed(123)\n\n# Parameters\nn_sim &lt;- 10000\nD1 &lt;- 2\n\n# Simulate r and g\nr &lt;- rnorm(n_sim, mean = 0.08, sd = 0.01)\ng &lt;- rnorm(n_sim, mean = 0.03, sd = 0.005)\n\n# Compute values\nvalue &lt;- D1 / (r - g)\n\n# Filter for valid values where r &gt; g\nvalue &lt;- value[is.finite(value) & (r &gt; g)]\n\n# Plot\nlibrary(ggplot2)\nlibrary(scales)\n\nggplot(data.frame(Value = value), aes(x = Value)) +\n  geom_histogram(bins = 60, fill = \"#2c7fb8\", color = \"white\", alpha = 0.8) +\n  scale_x_continuous(labels=scales::dollar)+\n  labs(\n    title = \"Monte Carlo Valuation Distribution (Gordon Growth Model)\",\n    x = \"Firm Value\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size=20)"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#hands-on-exercise",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#hands-on-exercise",
    "title": "Equity Valuation and Simulation",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\nRefer back to NetScape valuation model that you have worked on prior classes. Using Monte Carlo simulations, you were asked to stress-test such model by analyzing what would have happened to the base scenario as-if some inputs were changed\n\n\n\n\n\n\n\nInstructions\n\n\n\nWe will be using the NetScape case we have worked on previous classes\nYour first task is to translate the valuation model in such a way that you can seamlessly replicate the baseline results\nAfter that, you will be prompted with a series of questions that will require you to simulate \\(N\\) scenarios and analyze the effects on NetScape’s value\n\n\n\n\nGiven that the baseline price, at the time of the valuation, was $28.46, what would be your recommendation for this stock?"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#step-1-hardcoding-the-assumptions",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#step-1-hardcoding-the-assumptions",
    "title": "Equity Valuation and Simulation",
    "section": "Step 1: Hardcoding the assumptions",
    "text": "Step 1: Hardcoding the assumptions\n\nRecall that our Free Cash Flow estimation in peiod \\(t\\), \\(FCF_t\\), is given by:\n\n\\[\nFCF_t = EBIT\\times (1-\\tau) \\pm \\text{Depreciation} \\pm \\text{CAPEX} \\pm \\Delta NWC\n\\] Where \\(\\tau\\) is the marginal tax-rate, CAPEX is Capital Expenditures, and NWC is Net Working Capital\n\nIn what follows, we will simulate the value of Netscape using a 10-year discounted cash flow (DCF) model with structured assumptions on top of the historical revenue levels from 1995"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#step-1-hardcoding-the-assumptions-continued",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#step-1-hardcoding-the-assumptions-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Step 1: Hardcoding the assumptions, continued",
    "text": "Step 1: Hardcoding the assumptions, continued\n\n#Number of years\nn_years=10\n\n# Assumptions (copied and simplified)\nnetscape_assumptions &lt;- list(\n  revenue_growth = rep(0.65,n_years),\n  cost_of_sales_pct = rep(0.1044,n_years),\n  rd_pct = rep(0.3676,n_years),\n  tax_rate = rep(0.34,n_years),\n  other_op_exp_pct = c(0.80,0.65,0.55,0.45,0.35,0.25,rep(0.2,4)),\n  capex_pct = c(0.45,0.4,0.3,0.2,rep(0.1,6)),\n  nwc_pct = rep(0,n_years),\n  depreciation_pct = rep(0.055,n_years),\n  beta=rep(1.5,n_years),\n  rf=rep(0.0671,n_years),\n  mrp=rep(0.075,n_years),\n  shares_outstanding = 38000,\n  terminal_growth = 0.04,\n  terminal_r=0.1796\n)\n\n# Base year (1995)\nnetscape_base_rev &lt;- 33250"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#step-2-getting-the-forecasted-free-cash-flow",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#step-2-getting-the-forecasted-free-cash-flow",
    "title": "Equity Valuation and Simulation",
    "section": "Step 2: Getting the forecasted Free Cash Flow",
    "text": "Step 2: Getting the forecasted Free Cash Flow\n\nAfter we got our assumptions in place, it is time to put them together and generate the Free Cash Flow values for \\(t=1,2,...,10\\)\nFor that, we will create a function, get_projections, that has two arguments:\n\nassumptions: a named list containing vectors of financial assumptions (growth rates, margins, etc) for each year\nbase: the base year revenue (e.g., from 1995).\n\nWe initialize an empty data frame projection with columns for each financial metric over the 10-year period, and fill out the results using the Free Cash Flow definition\nWith this function, you should be able to replicate the Free Cash Flows from the base scenario by calling get_projection(netscape_assumptions,netscape_base_rev)"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#step-2-getting-the-forecasted-free-cash-flow-continued",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#step-2-getting-the-forecasted-free-cash-flow-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Step 2: Getting the forecasted Free Cash Flow, continued",
    "text": "Step 2: Getting the forecasted Free Cash Flow, continued\n\n# Projection over 10 years\nget_projection &lt;- function(assumptions,base){\n  \n  projection=data.frame(\n      Year = 1:n_years,\n      Revenue = NA,\n      EBIT = NA,\n      Taxes = NA,\n      NOPAT = NA,\n      Depreciation = NA,\n      CAPEX = NA,\n      NWC = NA,\n      Delta_NWC = NA,\n      Discount_Rate=NA)\n\n  # Fill in projections for revenue\n  for (t in 1:n_years) {\n    if (t == 1) {\n      projection$Revenue[t] &lt;- base * (1 + assumptions$revenue_growth[t])\n    } else {\n      projection$Revenue[t] &lt;- projection$Revenue[t - 1] * (1 + assumptions$revenue_growth[t])\n    }\n  }\n  \n  #Fill in FCF terms\n  projection$EBIT &lt;- projection$Revenue * (1 - assumptions$cost_of_sales_pct - assumptions$rd_pct - assumptions$other_op_exp_pct - assumptions$depreciation_pct)\n  projection$Taxes &lt;- projection$EBIT * assumptions$tax_rate\n  projection$NOPAT &lt;- projection$EBIT - projection$Taxes\n  projection$Depreciation &lt;- projection$Revenue * assumptions$depreciation_pct\n  projection$CAPEX &lt;- projection$Revenue * assumptions$capex_pct\n  projection$NWC &lt;- projection$Revenue*assumptions$nwc_pct\n  projection$Delta_NWC &lt;- 0\n  projection$Discount_Rate &lt;- 1/(1+(assumptions$rf+assumptions$mrp*assumptions$beta))^projection$Year\n  \n  return(projection)\n}"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#valuing-the-company-using-fcf",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#valuing-the-company-using-fcf",
    "title": "Equity Valuation and Simulation",
    "section": "Valuing the company using FCF",
    "text": "Valuing the company using FCF\n\nWe will now create two additional functions:\n\nThe get_dcf() function will calculate the Discounted Cash Flow for the whole period, based on the projections we have just created. It returns a scalar value representing the present value of projected Free Cash Flows\nThe get_terminal_value() will calculate an estimate of the present value of the perpetuity (in Year 0) based on the projections and the assumptions regarding the long-term r and g. ItcComputes the terminal value of the firm beyond the projection horizon using a growing perpetuity\n\nAdding up these two values yields the total value of the firm, \\(V\\), which is embedded in the get_EV() function:\n\n\nget_EV(netscape_assumptions,\n       netscape_base_rev,\n       netscape_assumptions$terminal_r,\n       netscape_assumptions$terminal_growth)"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#valuing-the-company-using-fcf-continued",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#valuing-the-company-using-fcf-continued",
    "title": "Equity Valuation and Simulation",
    "section": "Valuing the company using FCF, continued",
    "text": "Valuing the company using FCF, continued\n\nget_dcf &lt;- function(projection){\n  \n  dcf=projection%&gt;%\n    mutate(FCF = NOPAT + Depreciation - CAPEX - Delta_NWC)%&gt;%\n    reframe(DCF=FCF*Discount_Rate)%&gt;%\n    pull(DCF)%&gt;%\n    sum()\n  \n  return(dcf)\n  \n}\n\nget_terminal_value &lt;- function(projection,r,g){\n\n  last_fcf=projection%&gt;%\n    mutate(FCF = NOPAT + Depreciation - CAPEX - Delta_NWC)%&gt;%\n    pull(FCF)%&gt;%\n    tail(1)\n  \n  return(last_fcf*(1+g)/(r-g)/(1+r)^(n_years+1))\n}\n\nget_EV &lt;-function(assumptions,base,r,g){\n\n  PV_FCF = get_projection(assumptions,base)%&gt;%get_dcf()\n  PV_Terminal = get_projection(assumptions,base)%&gt;%get_terminal_value(r,g)\n  \n  return(PV_FCF+PV_Terminal)\n\n  }"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#exercise-1-changing-revenue-growth",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#exercise-1-changing-revenue-growth",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 1: changing revenue growth",
    "text": "Exercise 1: changing revenue growth\n\nCodeOutput\n\n\n\n  #Set seed for reproducibility\n  set.seed(123)\n\n  # Initialize an empty dataframe\n  results &lt;- numeric(0)\n  sim_assumptions &lt;- netscape_assumptions\n  n_sim=10000\n  \n  #Run the simulation\n  for (i in 1:n_sim){\n\n    # Simulate revenue growth for 10 years from a normal distribution\n    rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n    \n    # Modify assumptions with simulated growth\n    sim_assumptions$revenue_growth &lt;- rev_growth\n    \n    # Compute EV for this simulation\n    ev &lt;- get_EV(sim_assumptions,\n                 netscape_base_rev,\n                 sim_assumptions$terminal_r,\n                 sim_assumptions$terminal_growth)\n    \n    results[i] &lt;- ev\n  }\n  \n\n  #Create a ggplot histogram\n  results%&gt;%\n    as.tibble()%&gt;%\n    ggplot(aes(x = value)) +\n    geom_histogram(fill = \"skyblue\", color = \"white\", bins = 50)+\n    scale_x_continuous(labels = scales::dollar) +\n    labs(\n      title = \"Monte Carlo Simulation of Enterprise Value\",\n      subtitle = paste0('Drawing ',comma(n_sim), ' simulations of Revenue Growth.'), \n      x = \"Enterprise Value (USD, Thousands)\",\n      y = \"Frequency\"\n    ) +\n    theme_minimal(base_size = 14)"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#exercise-2-changing-revenue-growth-and-cogs",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#exercise-2-changing-revenue-growth-and-cogs",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 2: Changing Revenue Growth and COGS",
    "text": "Exercise 2: Changing Revenue Growth and COGS\n\nCodeOutput\n\n\n\n  #Set seed for reproducibility\n  set.seed(123)\n\n  # Initialize an empty dataframe\n  results &lt;- numeric(0)\n  sim_assumptions &lt;- netscape_assumptions\n  n_sim=10000\n\n#Run the simulation\nfor (i in 1:n_sim){\n  \n  # Simulate revenue growth for 10 years from a normal distribution\n  rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n  cogs_perc &lt;- rnorm(n_years, mean = 0.10, sd = 0.025)\n  \n  # Modify assumptions with simulated growth\n  sim_assumptions$revenue_growth &lt;- rev_growth\n  sim_assumptions$cost_of_sales_pct &lt;- cogs_perc\n  \n  # Compute EV for this simulation\n  ev &lt;- get_EV(sim_assumptions,\n               netscape_base_rev,\n               sim_assumptions$terminal_r,\n               sim_assumptions$terminal_growth)\n  \n  results[i] &lt;- ev\n}\n\n#Create a ggplot histogram\nresults%&gt;%\n  as.tibble()%&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(fill = \"skyblue\", color = \"white\", bins = 50)+\n  scale_x_continuous(labels = scales::dollar) +\n  labs(\n    title = \"Monte Carlo Simulation of Enterprise Value\",\n    subtitle = paste0('Drawing ',comma(n_sim), ' simulations of Revenue Growth and % COGS.'), \n    x = \"Enterprise Value (USD, Thousands)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 20)"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#exercise-3-changing-revenue-growth-cogs-and-capex",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#exercise-3-changing-revenue-growth-cogs-and-capex",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 3: Changing Revenue Growth, COGS, and CAPEX",
    "text": "Exercise 3: Changing Revenue Growth, COGS, and CAPEX\n\nCodeOutput\n\n\n\n#Set seed for reproducibility\nset.seed(123)\n\n# Initialize an empty dataframe\nresults &lt;- numeric(0)\nsim_assumptions &lt;- netscape_assumptions\nn_sim=10000\n\n#Run the simulation\nfor (i in 1:n_sim){\n  \n  # Simulate revenue growth for 10 years from a normal distribution\n  rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n  cogs_perc &lt;- rnorm(n_years, mean = 0.10, sd = 0.025)\n  capex_perc &lt;- seq(rnorm(1, mean = 0.45, sd = 0.1),to=0.1,length.out = 10)\n  \n  # Modify assumptions with simulated growth\n  sim_assumptions$revenue_growth &lt;- rev_growth\n  sim_assumptions$cost_of_sales_pct &lt;- cogs_perc\n  sim_assumptions$capex_pct &lt;- capex_perc\n  \n  # Compute EV for this simulation\n  ev &lt;- get_EV(sim_assumptions,\n               netscape_base_rev,\n               sim_assumptions$terminal_r,\n               sim_assumptions$terminal_growth)\n  \n  results[i] &lt;- ev\n}\n\n#Create a ggplot histogram\nresults%&gt;%\n  as.tibble()%&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(fill = \"skyblue\", color = \"white\", bins = 50)+\n  scale_x_continuous(labels = scales::dollar) +\n  labs(\n    title = \"Monte Carlo Simulation of Enterprise Value\",\n    subtitle = paste0('Drawing ',comma(n_sim), ' simulations of Revenue Growth, % COGS, and % CAPEX.'), \n    x = \"Enterprise Value (USD, Thousands)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 20)\n\nfinal_results=results"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#exercise-4-different-levels-for-terminal-r-and-g",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#exercise-4-different-levels-for-terminal-r-and-g",
    "title": "Equity Valuation and Simulation",
    "section": "Exercise 4: Different levels for terminal \\(r\\) and \\(g\\)",
    "text": "Exercise 4: Different levels for terminal \\(r\\) and \\(g\\)\n\nCodeOutput\n\n\n\n#Set seed for reproducibility\nset.seed(123)\n\n# Initialize an empty dataframe\nresults &lt;- numeric(0)\nnew_data &lt;- data.frame()\nsim_assumptions &lt;- netscape_assumptions\nn_sim=1000\ng_sequence &lt;- seq(0.05,0.03,length.out=10)\nr_sequence &lt;- seq(0.15,0.225,length.out=10)\n\nfor(s in 1:10){\n  \n  #Run the simulation\n  for (i in 1:n_sim){\n    \n    # Simulate revenue growth for 10 years from a normal distribution\n    rev_growth &lt;- rnorm(n_years, mean = 0.65, sd = 0.1)\n    cogs_perc &lt;- rnorm(n_years, mean = 0.10, sd = 0.025)\n    capex_perc &lt;- seq(rnorm(1, mean = 0.45, sd = 0.1),to=0.1,length.out = 10)\n    \n    # Modify assumptions with simulated growth\n    sim_assumptions$revenue_growth &lt;- rev_growth\n    sim_assumptions$cost_of_sales_pct &lt;- cogs_perc\n    sim_assumptions$capex_pct &lt;- capex_perc\n    sim_assumptions$terminal_growth = g_sequence[s]\n    sim_assumptions$terminal_r = r_sequence[s]\n    \n    # Compute EV for this simulation\n    ev &lt;- get_EV(sim_assumptions,\n                 netscape_base_rev,\n                 sim_assumptions$terminal_r,\n                 sim_assumptions$terminal_growth)\n    \n    results[i] &lt;- ev\n  }\n  \n  #Store the Pairs\n  new_data=new_data%&gt;%\n    rbind(data.frame(Scenario = paste0('Scenario ', s),\n                     EV=results))\n          \n  message(paste0('Finished simulation for pair number ',s,'.'))\n}\n\n\n#install.packages('ggridges')\nlibrary(ggridges)\n\n#Create a ggplot histogram\nnew_data%&gt;%\n  mutate(Scenario = factor(Scenario,levels=paste0('Scenario ',10:1)))%&gt;%\n  ggplot(aes(x = EV,\n             y = Scenario,\n             fill=..x..)) +\n  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01)+\n  scale_x_continuous(labels = scales::dollar) +\n  scale_fill_viridis_c(name = \"Enterprise Value\", option = \"plasma\",labels=scales::dollar)+\n  labs(\n    title = \"Monte Carlo Simulation of Enterprise Value\",\n    subtitle = paste0('Drawing ',comma(n_sim), ' simulations, best-to-worse scenarios.'), \n    x = \"\",\n    y = \"Frequency\",\n    fill = \"Enterprise Value (USD, Thousands)\")+\n  theme_minimal(base_size = 14)+\n  theme(legend.position = 'bottom',\n        legend.title = element_text(size = 14),\n        legend.text = element_text(size = 12),\n        legend.key.height = unit(0.6, \"cm\"),\n        legend.key.width = unit(2.5, \"cm\"))"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#final-question-should-we-invest",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#final-question-should-we-invest",
    "title": "Equity Valuation and Simulation",
    "section": "Final Question: should we invest?",
    "text": "Final Question: should we invest?\n\nCodeOutput\n\n\n\n#Create a ggplot histogram\nprices=final_results%&gt;%\n  as.tibble()%&gt;%\n  mutate(Price=value/netscape_assumptions$shares_outstanding)%&gt;%\n  mutate(Situation=ifelse(Price&gt;=28.37,'Undervalued','Overvalued'))\n\nprices%&gt;%\n  ggplot(aes(x = Price)) +\n  geom_histogram(aes(fill = Situation), bins = 50)+\n  scale_x_continuous(labels = scales::dollar)+\n  scale_fill_manual(values=c('darkgreen','darkred'))+\n  geom_vline(xintercept=28.37,linetype='dashed',size=1)+\n  annotate(geom='text',x=40,y=500,label= paste0('True Price is higher than \\nbaseline price only in ',\n                                                percent(mean(prices$Price&gt;=28.37)),\n                                                ' of the cases!'))+\n  labs(\n    title = \"Monte Carlo Simulation of Share Price\",\n    subtitle = paste0('Drawing 10,000 simulations'), \n    x = \"Enterprise Value (USD, Thousands)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 20)+\n  theme(legend.position='bottom')"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#overall-thoughts-on-monte-carlo-simulation",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#overall-thoughts-on-monte-carlo-simulation",
    "title": "Equity Valuation and Simulation",
    "section": "Overall thoughts on Monte Carlo Simulation",
    "text": "Overall thoughts on Monte Carlo Simulation\n\nMonte Carlo Simulation is a powerful technique to handle model uncertainty:\n\nIt helps managers think beyond the point estimate of the valuation model and handle what-if questions\nIt also helps stress-testing the valuation model\n\nIt should be used along with business logic to help tailor the simulation exercise:\n\nWe don’t want to create overly complex simulations\nWe can incorporate more structure to create meaningful scenarios - for example, imposing a correlation structure between the variables\n\nBy setting up proper simulation parameters (number of simulations, the distributions and its parameters, etc), we can leverage the role of uncertainty in valuation models in a much more pronounced way and guide decision-making!"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#extensions",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#extensions",
    "title": "Equity Valuation and Simulation",
    "section": "Extensions",
    "text": "Extensions\nQuestion: how can we improve our simulation exercise?\n\nIn what follows, we’ll take a look at several ways by which you could extend your simulation coding exercise to cope with real-world features. To illustrate, we will adopt a simple valuation approach of the following form:\n\n\\[\n\\small V_i = \\sum_{t=1}^{t=5}\\dfrac{FCF_t}{(1+r)^t}+ PV_0\\bigg(\\dfrac{FCF_5\\times(1+g)}{r-g}\\bigg)\n\\]\n\nIn words, we are breaking down the firms value into:\n\nA 1-5 year horizon\nA growing perpetuity, discounted back to Year 0\n\nFrom that, we can extend the simulation routines to allow for important features that are present in real-world data"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#base-case-simulation",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#base-case-simulation",
    "title": "Equity Valuation and Simulation",
    "section": "Base Case Simulation",
    "text": "Base Case Simulation\n\nOutlineCodeOutput\n\n\n\nThe base case denotes the Free Cash Flow in period \\(t\\) as:\n\n\\[\nFCF_t = Sales_t\\times(1-COGS) \\pm CAPEX \\pm \\Delta NWC\n\\]\n\nInitial Sales levels are \\(\\$100\\)\nThe percent of Cost of Goods Sold (COGS) is \\(60\\%\\)\nCapital Expenditures (CAPEX) is \\(15\\%\\) of Sales\nNet Working Capital investment is \\(5%\\) of sales\n\n\n\n\nlibrary(glue)\nlibrary(MASS)\n\nset.seed(123)\nn_sim &lt;- 10000\n\n# Parameters\ninitial_sales &lt;- 1000\ncogs_pct &lt;- 0.6\ncapex_pct &lt;- 0.15\nnwc_pct &lt;- 0.05\n\nsimulate_values &lt;- function(r, g, label='All') {\n  firm_value &lt;- numeric(n_sim)\n  \n  for (i in 1:n_sim) {\n    sales &lt;- initial_sales\n    nwc_last &lt;- sales * nwc_pct\n    fa_last &lt;- sales * capex_pct\n    fcf &lt;- numeric(5)\n    \n    for (t in 1:5) {\n      sales &lt;- sales * (1 + g[i])\n      nwc_current &lt;- sales * nwc_pct\n      fa_current &lt;- sales * capex_pct\n      \n      delta_nwc &lt;- nwc_current - nwc_last\n      delta_capex &lt;- fa_current - fa_last\n      \n      nwc_last &lt;- nwc_current\n      fa_last &lt;- fa_current\n      \n      fcf[t] &lt;- sales * (1 - cogs_pct) - delta_capex - delta_nwc\n    }\n    \n    fcf_terminal &lt;- fcf[5] * (1 + g[i])\n    perp_value &lt;- fcf_terminal / (r[i] - g[i])\n    \n    discounted_fcf &lt;- sum(fcf / (1 + r[i])^(1:5))\n    discounted_perp &lt;- perp_value / (1 + r[i])^5\n    \n    firm_value[i] &lt;- discounted_fcf + discounted_perp\n  }\n  \n  firm_value &lt;- firm_value[is.finite(firm_value) & (r &gt; g)]\n  \n  data.frame(Value = firm_value, Scenario = label)\n}\n\n# Normal distributions\nr_norm &lt;- rnorm(n_sim, mean = 0.08, sd = 0.01)\ng_norm &lt;- rnorm(n_sim, mean = 0.03, sd = 0.005)\ndf_norm &lt;- simulate_values(r_norm, g_norm, \"Normal\")\n\nbase_norm &lt;- simulate_values(r_norm, g_norm, \"Normal\")\n\nggplot(base_norm, aes(x = Value)) +\n  geom_histogram(bins = 60, fill = \"#2c7fb8\", color = \"white\", alpha = 0.8)+\n  scale_x_continuous(labels = dollar) +\n  labs(\n    title = \"Monte Carlo Valuation Distribution Normal vs. Alternative Distributions\",\n    subtitle = glue('Based on {comma(n_sim)} simulations.'),\n    x = \"Firm Value\",\n    y = \"Frequency\",\n    fill = \"Scenario\",\n    color = \"Scenario\"\n  ) +\n  theme_minimal(base_size = 20)+\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#extension-1-alternative-distributions",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#extension-1-alternative-distributions",
    "title": "Equity Valuation and Simulation",
    "section": "Extension 1: Alternative Distributions",
    "text": "Extension 1: Alternative Distributions\n\nOutlineCodeOutput\n\n\n\nKey Point: Monte Carlo doesn’t assume normality — any distribution that fits the problem can be used!\n\nWhile normal distributions are common, other distributions (e.g., uniform, log-normal, etc.) can also be used:\nFor example, if \\(r \\sim \\text{Uniform}(0.05, 0.10)\\), the growth rate could be chosen randomly from a uniform distribution between \\(5\\%\\) and \\(10\\%\\)\n\nDistributions like the log-normal may be more appropriate for modeling returns, as they respect the non-negativity constraint - $r $. Therefore, for a given variable \\(X\\), it could be sampled from any distribution\nIn what follows, we’ll simulate results drawing from an Uniform distribution for \\(r\\) and a Beta for \\(g\\)\n\n\n\n\n# Baseline case\nr_norm &lt;- rnorm(n_sim, mean = 0.08, sd = 0.01)\ng_norm &lt;- rnorm(n_sim, mean = 0.03, sd = 0.005)\ndf_norm &lt;- simulate_values(r_norm, g_norm, \"Normal\")\n\n\n# Alternative distributions\nr_unif &lt;- runif(n_sim, min = 0.06, max = 0.10)\ng_beta &lt;- 0.06 * rbeta(n_sim, 2, 5)\ndf_alt &lt;- simulate_values(r_unif, g_beta, \"Alternative\")\n\n# Combine results\ndf_all &lt;- rbind(df_norm, df_alt)\n\n# Plot combined chart\n\nggplot(df_all, aes(x = Value, fill = Scenario, color = Scenario)) +\n  geom_density(alpha = 0.5, size = 1) +\n  scale_x_continuous(labels = dollar) +\n  scale_fill_manual(values = c(\"Normal\" = \"#2c7fb8\", \"Alternative\" = \"#f03b20\")) +\n  scale_color_manual(values = c(\"Normal\" = \"#2c7fb8\", \"Alternative\" = \"#f03b20\")) +\n  labs(\n    title = \"Monte Carlo Valuation Distribution Normal vs. Alternative Distributions\",\n    subtitle = glue('Based on {comma(n_sim)} simulations.'),\n    x = \"Firm Value\",\n    y = \"Density\",\n    fill = \"Scenario\",\n    color = \"Scenario\"\n  ) +\n  theme_minimal(base_size = 20)+\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#extension-2-correlated-variables",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#extension-2-correlated-variables",
    "title": "Equity Valuation and Simulation",
    "section": "Extension 2: Correlated Variables",
    "text": "Extension 2: Correlated Variables\n\nOutlineCodeOutput\n\n\n\nIn the baseline case, we have each random variable sampled independently from a normal distribution (i.e, i.i.d sampling). This can lead to higher variability and wider spread in the results that are unlikely to happen\nBy introducing a correlation structure between variables, we can constrain the possible outcomes, narrowing the distribution, leading to more realistic and stable simulations where changes in one variable influence the others\nIn what follows, we will keep \\(r\\) and \\(g\\) mean and standard deviation, but will now assume that the correlation between these two variables is \\(\\rho=0.9\\):\n\nWhenever \\(g\\) is high, it might be because the firm is in its earlier stages, so \\(r\\) should be higher\nWhenever \\(g\\) is low, firm may have reached its market peak, so \\(r\\) should be lower\n\n\n\n\n\nr_norm &lt;- rnorm(n_sim, mean = 0.08, sd = 0.01)\ng_norm &lt;- rnorm(n_sim, mean = 0.03, sd = 0.005)\ndf_norm &lt;- simulate_values(r_norm, g_norm, \"Uncorrelated\")\n\n# Simulate r and g with negative correlation\nmu_r &lt;- 0.08\nmu_g &lt;- 0.03\nsigma_r &lt;- 0.01\nsigma_g &lt;- 0.005\ncorr_rg &lt;- 0.9\n\n# Covariance matrix\ncov_matrix &lt;- matrix(c(sigma_r^2, corr_rg * sigma_r * sigma_g, corr_rg * sigma_r * sigma_g, sigma_g^2), \n                     nrow = 2, ncol = 2)\n\n# Simulate r and g from the bivariate normal distribution\nr_g_correlated &lt;- mvrnorm(n_sim, mu = c(mu_r, mu_g), Sigma = cov_matrix)\nr_corr &lt;- r_g_correlated[, 1]\ng_corr &lt;- r_g_correlated[, 2]\ndf_corr &lt;- simulate_values(r_corr, g_corr, \"Correlated\")\n\n# Combine results\ndf_all &lt;- rbind(df_norm, df_corr)\n\nggplot(df_all, aes(x = Value, fill = Scenario, color = Scenario)) +\n  geom_density(alpha = 0.6, size = 1.5) +\n  scale_x_continuous(labels = dollar, limits = c(0, 20000)) +  # Set x-axis limits\n  scale_fill_manual(values = c(\"Uncorrelated\" = \"#2c7fb8\", \"Correlated\" = \"#f03b20\")) +\n  scale_color_manual(values = c(\"Uncorrelated\" = \"#2c7fb8\", \"Correlated\" = \"#f03b20\")) +\n  labs(\n    title = \"Monte Carlo Valuation Distribution Uncorrelated vs. Strong Correlation\",\n    x = \"Firm Value\",\n    y = \"Density\",\n    fill = \"Scenario\",\n    color = \"Scenario\"\n  ) +\n  theme_minimal(base_size = 20)+\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#extension-3-varying-the-sample-size",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#extension-3-varying-the-sample-size",
    "title": "Equity Valuation and Simulation",
    "section": "Extension 3: Varying the Sample Size",
    "text": "Extension 3: Varying the Sample Size\n\nOutlineCodeOutput\n\n\n\nIn short, your Monte Carlo simulation outcome will depend on the number of simulations: the more simulations we run, the more stable and reliable the estimates become.\n\nWith smaller samples, (e.g., \\(n&lt;100\\) or \\(n&lt;1,000\\), results show high variability and may not capture the true distribution.\nWith larger samples, as \\(n\\) (e.g., \\(100,000\\) or \\(1,000,000\\)), the estimates converge toward the expected value\n\nAll in all, increasing sample size reduces random noise, improving the precision and the stability of simulation results\n\n\n\n\n# Simulation sizes\nn_sim_values &lt;- c(100, 1000, 5000, 10000, 100000, 1000000)\n\n# Store all results\nall_results &lt;- data.frame()\n\n# Run simulations\n\nfor (n in n_sim_values) {\n  r_norm &lt;- rnorm(n, mean = 0.08, sd = 0.01)\n  g_norm &lt;- rnorm(n, mean = 0.03, sd = 0.005)\n  \n  df &lt;- simulate_values(r_norm, g_norm, comma(n))\n  all_results &lt;- bind_rows(all_results, df)\n}\n\nall_results$Scenario &lt;-factor(all_results$Scenario,levels=comma(n_sim_values))\n\n# Plot histograms overlaid in one chart\nggplot(all_results, aes(x = Value, fill = Scenario)) +\n  geom_histogram(aes(y=..density..),bins = 60, color = \"white\") +\n  geom_density(size=0.5,fill=NA)+\n  scale_x_continuous(labels = scales::dollar,limits=c(0,25000)) +\n  facet_wrap(Scenario~.,scales='free')+\n  labs(\n    title = \"Monte Carlo Valuation Distribution by Number of Simulations\",\n    subtitle = \"Varying the sample size for draws\",\n    x = \"Firm Value\",\n    y = \"Density\",\n    fill = \"Simulation Size\"\n  ) +\n  theme_minimal(base_size = 15) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#extension-4-bootstrapping",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#extension-4-bootstrapping",
    "title": "Equity Valuation and Simulation",
    "section": "Extension 4: Bootstrapping",
    "text": "Extension 4: Bootstrapping\n\nOutlineCodeOutput\n\n\n\nBootstrapping is a resampling technique that draws samples with replacement from observed data\n\nIt is useful to incorporate empirical variability without assuming a specific parametric distribution\nHelps capture the uncertainty present in historical or observed data\n\nGiven a sample \\(x_1, x_2, \\dots, x_N\\), draw \\(B\\) bootstrap samples, you draw a given number of samples with replacement and compute the statistic of interest (e.g., mean, median) on each bootstrap sample\nBootstrapping can be used, for example, if we have a very limited series of historical values for our random values (e.g, \\(n=20\\)) but want to incorporate variability into the analysis\n\n\n\n\n#Number of simulations\nn_sim=10000\n\n# Historical series\nhistorical_cogs &lt;- c(0.58, 0.6, 0.62, 0.59, 0.61,0.75,0.54,0.66,0.29,0.78)\nhistorical_capex &lt;- c(0.48, 0.5, 0.52, 0.49, 0.51,0.40,0.52,0.59,0.58,0.81)\n\n# For each simulation, bootstrap 10 values and take the mean → length n_sim vector\nboot_cogs_pct &lt;- replicate(n_sim, mean(sample(historical_cogs, 10, replace = TRUE)))\nboot_capex_pct &lt;- replicate(n_sim, mean(sample(historical_capex, 10, replace = TRUE)))\n\nsimulate_values &lt;- function(r, g, cogs_pct, capex_pct, label = 'Simulation') {\n  firm_value &lt;- numeric(n_sim)\n  \n  for (i in 1:n_sim) {\n    sales &lt;- initial_sales\n    nwc_last &lt;- sales * nwc_pct\n    fa_last &lt;- sales * capex_pct[i]\n    fcf &lt;- numeric(5)\n    \n    for (t in 1:5) {\n      sales &lt;- sales * (1 + g[i])\n      nwc_current &lt;- sales * nwc_pct\n      fa_current &lt;- sales * capex_pct[i]\n      \n      delta_nwc &lt;- nwc_current - nwc_last\n      delta_capex &lt;- fa_current - fa_last\n      \n      nwc_last &lt;- nwc_current\n      fa_last &lt;- fa_current\n      \n      fcf[t] &lt;- sales * (1 - cogs_pct[i]) - delta_capex - delta_nwc\n    }\n    \n    fcf_terminal &lt;- fcf[5] * (1 + g[i])\n    perp_value &lt;- fcf_terminal / (r[i] - g[i])\n    \n    discounted_fcf &lt;- sum(fcf / (1 + r[i])^(1:5))\n    discounted_perp &lt;- perp_value / (1 + r[i])^5\n    \n    firm_value[i] &lt;- discounted_fcf + discounted_perp\n  }\n  \n  firm_value &lt;- firm_value[is.finite(firm_value) & (r &gt; g)]\n  \n  data.frame(Value = firm_value, Scenario = label)\n}\n\n\n# Normal scenario (fixed percentages for COGS and CAPEX)\ndf_norm &lt;- simulate_values(r_norm, g_norm, cogs_pct = rep(0.6,n_sim), capex_pct = rep(0.5,n_sim), label = \"Normal\")\n\n# Bootstrap scenario (bootstrapped mean percentages)\ndf_bootstrap &lt;- simulate_values(r_norm, g_norm, cogs_pct = boot_cogs_pct, capex_pct = boot_capex_pct, label = \"Bootstrap\")\n\n#Plot\nggplot() +\n  geom_density(data = df_norm, aes(x = Value, fill = \"Normal\"), alpha = 0.5) +\n  geom_density(data = df_bootstrap, aes(x = Value, fill = \"Bootstrap\"), alpha = 0.5) +\n  scale_x_continuous(labels = dollar)+\n  labs(\n    title = \"Monte Carlo Valuation Distribution: Normal vs. Bootstrap\",\n    x = \"Firm Value\",\n    y = \"Frequency\",\n    fill = 'Approach',\n    color = \"Scenario\"\n  ) +\n  theme_minimal(base_size = 20) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#references",
    "href": "valuation/coursework/Lecture 6 - Equity Valuation and Simulation/index.html#references",
    "title": "Equity Valuation and Simulation",
    "section": "References",
    "text": "References\n\n\n\n\n@ Website | @ Slides | @ Office-hour appointments"
  }
]